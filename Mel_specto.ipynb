{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHaH5bVWG7FeNfvoFky/f8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Mel_specto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GkG3Kyqoxv34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bb6233-e087-4e50-a597-e1b2ceba1833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction and labeling"
      ],
      "metadata": {
        "id": "YEpEpaw46Uz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "\n",
        "# Step 1: Extract the zip file\n",
        "zip_path = '/content/drive/MyDrive/AD/Mel_Spectrograms.zip'\n",
        "extract_path = '/content'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction completed!\")\n",
        "print(\"Contents of /content:\")\n",
        "print(os.listdir('/content'))\n",
        "\n",
        "# Check if Mel_Spectrograms folder exists\n",
        "if 'Mel_Spectrograms' in os.listdir('/content'):\n",
        "    print(\"\\nMel_Spectrograms folder found!\")\n",
        "    mel_path = '/content/Mel_Spectrograms'\n",
        "    print(\"Contents of Mel_Spectrograms:\")\n",
        "    print(os.listdir(mel_path))\n",
        "\n",
        "    # Check subdirectories\n",
        "    subdirs = os.listdir(mel_path)\n",
        "    print(f\"\\nSubdirectories: {subdirs}\")\n",
        "\n",
        "    for subdir in subdirs:\n",
        "        subdir_path = os.path.join(mel_path, subdir)\n",
        "        if os.path.isdir(subdir_path):\n",
        "            files = os.listdir(subdir_path)\n",
        "            print(f\"Files in {subdir}: {len(files)} files\")\n",
        "            if len(files) > 0:\n",
        "                print(f\"Sample files: {files[:5]}\")\n",
        "else:\n",
        "    print(\"Mel_Spectrograms folder NOT found!\")\n",
        "    # List all contents to see what was actually extracted\n",
        "    for root, dirs, files in os.walk('/content'):\n",
        "        level = root.replace('/content', '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files:\n",
        "            print(f'{subindent}{file}')\n",
        "\n",
        "# Now try to find the correct paths\n",
        "base_path = '/content/Mel_Spectrograms'\n",
        "if not os.path.exists(base_path):\n",
        "    # Try to find where the folder actually is\n",
        "    for root, dirs, files in os.walk('/content'):\n",
        "        if 'Mel_Spectrograms' in dirs:\n",
        "            base_path = os.path.join(root, 'Mel_Spectrograms')\n",
        "            break\n",
        "\n",
        "print(f\"\\nUsing base path: {base_path}\")\n",
        "\n",
        "# Check if AD and CN directories exist\n",
        "ad_path = os.path.join(base_path, 'AD')\n",
        "cn_path = os.path.join(base_path, 'CN')\n",
        "\n",
        "print(f\"AD path exists: {os.path.exists(ad_path)}\")\n",
        "print(f\"CN path exists: {os.path.exists(cn_path)}\")\n",
        "\n",
        "if os.path.exists(ad_path):\n",
        "    print(f\"AD directory contents: {os.listdir(ad_path)[:10]}\")  # Show first 10 files\n",
        "if os.path.exists(cn_path):\n",
        "    print(f\"CN directory contents: {os.listdir(cn_path)[:10]}\")  # Show first 10 files\n",
        "\n",
        "# Try different file extensions\n",
        "extensions = ['*.png', '*.jpg', '*.jpeg', '*.npy', '*.npz']\n",
        "ad_files = []\n",
        "cn_files = []\n",
        "\n",
        "for ext in extensions:\n",
        "    if os.path.exists(ad_path):\n",
        "        ad_files = glob(os.path.join(ad_path, ext))\n",
        "        if ad_files:\n",
        "            print(f\"Found {len(ad_files)} AD files with extension {ext}\")\n",
        "            break\n",
        "\n",
        "for ext in extensions:\n",
        "    if os.path.exists(cn_path):\n",
        "        cn_files = glob(os.path.join(cn_path, ext))\n",
        "        if cn_files:\n",
        "            print(f\"Found {len(cn_files)} CN files with extension {ext}\")\n",
        "            break\n",
        "\n",
        "# Create labels\n",
        "labels = [1] * len(ad_files) + [0] * len(cn_files)\n",
        "file_paths = ad_files + cn_files\n",
        "\n",
        "print(f\"\\nTotal AD files: {len(ad_files)}\")\n",
        "print(f\"Total CN files: {len(cn_files)}\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'file_path': file_paths,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(\"\\nFirst few entries:\")\n",
        "print(df.head())\n",
        "\n",
        "# Save to CSV\n",
        "if not df.empty:\n",
        "    df.to_csv('/content/alzheimers_dataset.csv', index=False)\n",
        "    print(\"\\nDataset saved to /content/alzheimers_dataset.csv\")\n",
        "else:\n",
        "    print(\"\\nNo files found. Please check the zip file contents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEMvM-yD50aj",
        "outputId": "24fc8156-75cd-428b-a74a-c766621c71de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed!\n",
            "Contents of /content:\n",
            "['.config', 'alzheimers_dataset.csv', 'drive', 'Mel_Spectrograms', 'sample_data']\n",
            "\n",
            "Mel_Spectrograms folder found!\n",
            "Contents of Mel_Spectrograms:\n",
            "['AD', 'CN']\n",
            "\n",
            "Subdirectories: ['AD', 'CN']\n",
            "Files in AD: 87 files\n",
            "Sample files: ['adrso063.npy', 'adrso192.npy', 'adrso144.npy', 'adrso110.npy', 'adrso122.npy']\n",
            "Files in CN: 79 files\n",
            "Sample files: ['adrso316.npy', 'adrso012.npy', 'adrso310.npy', 'adrso160.npy', 'adrso309.npy']\n",
            "\n",
            "Using base path: /content/Mel_Spectrograms\n",
            "AD path exists: True\n",
            "CN path exists: True\n",
            "AD directory contents: ['adrso063.npy', 'adrso192.npy', 'adrso144.npy', 'adrso110.npy', 'adrso122.npy', 'adrso060.npy', 'adrso039.npy', 'adrso106.npy', 'adrso112.npy', 'adrso250.npy']\n",
            "CN directory contents: ['adrso316.npy', 'adrso012.npy', 'adrso310.npy', 'adrso160.npy', 'adrso309.npy', 'adrso292.npy', 'adrso014.npy', 'adrso315.npy', 'adrso157.npy', 'adrso022.npy']\n",
            "Found 87 AD files with extension *.npy\n",
            "Found 79 CN files with extension *.npy\n",
            "\n",
            "Total AD files: 87\n",
            "Total CN files: 79\n",
            "\n",
            "Dataset shape: (166, 2)\n",
            "\n",
            "First few entries:\n",
            "                                   file_path  label\n",
            "0  /content/Mel_Spectrograms/AD/adrso063.npy      1\n",
            "1  /content/Mel_Spectrograms/AD/adrso192.npy      1\n",
            "2  /content/Mel_Spectrograms/AD/adrso144.npy      1\n",
            "3  /content/Mel_Spectrograms/AD/adrso110.npy      1\n",
            "4  /content/Mel_Spectrograms/AD/adrso122.npy      1\n",
            "\n",
            "Dataset saved to /content/alzheimers_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.ndimage import convolve1d\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/alzheimers_dataset.csv')\n",
        "print(f\"Dataset loaded with {len(df)} samples\")\n",
        "\n",
        "# Function to compute delta features\n",
        "def compute_delta(features, N=2):\n",
        "    \"\"\"\n",
        "    Compute delta features using gradient approximation\n",
        "    features: 2D array (time, features)\n",
        "    N: window size for delta computation\n",
        "    \"\"\"\n",
        "    # Create delta coefficients\n",
        "    weights = np.arange(-N, N+1)\n",
        "    weights = weights/np.sum(weights**2)  # Normalize\n",
        "\n",
        "    # Apply convolution along time axis\n",
        "    delta = convolve1d(features, weights, axis=0, mode='nearest')\n",
        "    return delta\n",
        "\n",
        "# Function to compute log-mel spectrogram\n",
        "def compute_log_mel(mel_spectrogram):\n",
        "    \"\"\"\n",
        "    Compute log mel spectrogram\n",
        "    \"\"\"\n",
        "    # Add small constant to avoid log(0)\n",
        "    log_mel = np.log(mel_spectrogram + 1e-8)\n",
        "    return log_mel\n",
        "\n",
        "# Function to process all features for one spectrogram\n",
        "def process_spectrogram(file_path):\n",
        "    \"\"\"\n",
        "    Load spectrogram and compute all features\n",
        "    Returns: log_mel, delta, delta_delta\n",
        "    \"\"\"\n",
        "    # Load the mel spectrogram\n",
        "    mel_spectrogram = np.load(file_path)\n",
        "\n",
        "    # Ensure it's 2D (time, features)\n",
        "    if mel_spectrogram.ndim == 3:\n",
        "        mel_spectrogram = mel_spectrogram.squeeze()\n",
        "\n",
        "    # Compute log mel\n",
        "    log_mel = compute_log_mel(mel_spectrogram)\n",
        "\n",
        "    # Compute delta\n",
        "    delta = compute_delta(mel_spectrogram)\n",
        "\n",
        "    # Compute delta-delta\n",
        "    delta_delta = compute_delta(delta)\n",
        "\n",
        "    return log_mel, delta, delta_delta\n",
        "\n",
        "# Create directories for saving processed features\n",
        "processed_dir = '/content/Processed_Features'\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(processed_dir, 'log_mel'), exist_ok=True)\n",
        "os.makedirs(os.path.join(processed_dir, 'delta'), exist_ok=True)\n",
        "os.makedirs(os.path.join(processed_dir, 'delta_delta'), exist_ok=True)\n",
        "\n",
        "# Process all files\n",
        "print(\"Processing spectrograms...\")\n",
        "\n",
        "# Add new columns for processed file paths\n",
        "df['log_mel_path'] = ''\n",
        "df['delta_path'] = ''\n",
        "df['delta_delta_path'] = ''\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    file_path = row['file_path']\n",
        "\n",
        "    try:\n",
        "        # Process the spectrogram\n",
        "        log_mel, delta, delta_delta = process_spectrogram(file_path)\n",
        "\n",
        "        # Create new file paths\n",
        "        base_name = os.path.basename(file_path).replace('.npy', '')\n",
        "        log_mel_path = os.path.join(processed_dir, 'log_mel', f'{base_name}_log_mel.npy')\n",
        "        delta_path = os.path.join(processed_dir, 'delta', f'{base_name}_delta.npy')\n",
        "        delta_delta_path = os.path.join(processed_dir, 'delta_delta', f'{base_name}_delta_delta.npy')\n",
        "\n",
        "        # Save processed features\n",
        "        np.save(log_mel_path, log_mel)\n",
        "        np.save(delta_path, delta)\n",
        "        np.save(delta_delta_path, delta_delta)\n",
        "\n",
        "        # Update DataFrame\n",
        "        df.at[idx, 'log_mel_path'] = log_mel_path\n",
        "        df.at[idx, 'delta_path'] = delta_path\n",
        "        df.at[idx, 'delta_delta_path'] = delta_delta_path\n",
        "\n",
        "        if idx % 20 == 0:\n",
        "            print(f\"Processed {idx}/{len(df)} files\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"Feature extraction completed!\")\n",
        "\n",
        "# Save updated dataset\n",
        "df.to_csv('/content/alzheimers_dataset_processed.csv', index=False)\n",
        "print(\"Updated dataset saved to /content/alzheimers_dataset_processed.csv\")\n",
        "\n",
        "# Display dataset info\n",
        "print(f\"\\nDataset summary:\")\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"AD samples: {df['label'].sum()}\")\n",
        "print(f\"CN samples: {len(df) - df['label'].sum()}\")\n",
        "\n",
        "print(\"\\nFirst few entries:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "HkXEDmd17V-B",
        "outputId": "a0055848-cef7-4c43-d701-6a76da136d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 166 samples\n",
            "Processing spectrograms...\n",
            "Processed 0/166 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-149078079.py:31: RuntimeWarning: invalid value encountered in log\n",
            "  log_mel = np.log(mel_spectrogram + 1e-8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 20/166 files\n",
            "Processed 40/166 files\n",
            "Processed 60/166 files\n",
            "Processed 80/166 files\n",
            "Processed 100/166 files\n",
            "Processed 120/166 files\n",
            "Processed 140/166 files\n",
            "Processed 160/166 files\n",
            "Feature extraction completed!\n",
            "Updated dataset saved to /content/alzheimers_dataset_processed.csv\n",
            "\n",
            "Dataset summary:\n",
            "Total samples: 166\n",
            "AD samples: 87\n",
            "CN samples: 79\n",
            "\n",
            "First few entries:\n",
            "                                   file_path  label  \\\n",
            "0  /content/Mel_Spectrograms/AD/adrso063.npy      1   \n",
            "1  /content/Mel_Spectrograms/AD/adrso192.npy      1   \n",
            "2  /content/Mel_Spectrograms/AD/adrso144.npy      1   \n",
            "3  /content/Mel_Spectrograms/AD/adrso110.npy      1   \n",
            "4  /content/Mel_Spectrograms/AD/adrso122.npy      1   \n",
            "\n",
            "                                        log_mel_path  \\\n",
            "0  /content/Processed_Features/log_mel/adrso063_l...   \n",
            "1  /content/Processed_Features/log_mel/adrso192_l...   \n",
            "2  /content/Processed_Features/log_mel/adrso144_l...   \n",
            "3  /content/Processed_Features/log_mel/adrso110_l...   \n",
            "4  /content/Processed_Features/log_mel/adrso122_l...   \n",
            "\n",
            "                                          delta_path  \\\n",
            "0  /content/Processed_Features/delta/adrso063_del...   \n",
            "1  /content/Processed_Features/delta/adrso192_del...   \n",
            "2  /content/Processed_Features/delta/adrso144_del...   \n",
            "3  /content/Processed_Features/delta/adrso110_del...   \n",
            "4  /content/Processed_Features/delta/adrso122_del...   \n",
            "\n",
            "                                    delta_delta_path  \n",
            "0  /content/Processed_Features/delta_delta/adrso0...  \n",
            "1  /content/Processed_Features/delta_delta/adrso1...  \n",
            "2  /content/Processed_Features/delta_delta/adrso1...  \n",
            "3  /content/Processed_Features/delta_delta/adrso1...  \n",
            "4  /content/Processed_Features/delta_delta/adrso1...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the processed dataset\n",
        "df = pd.read_csv('/content/alzheimers_dataset_processed.csv')\n",
        "print(f\"Dataset loaded with {len(df)} samples\")\n",
        "\n",
        "# Function to load and combine the three channels\n",
        "def load_three_channel_data(log_mel_path, delta_path, delta_delta_path):\n",
        "    \"\"\"\n",
        "    Load and combine log-mel, delta, and delta-delta features into 3-channel data\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if all files exist\n",
        "        if not all(os.path.exists(path) for path in [log_mel_path, delta_path, delta_delta_path]):\n",
        "            return None\n",
        "\n",
        "        # Load each feature\n",
        "        log_mel = np.load(log_mel_path)\n",
        "        delta = np.load(delta_path)\n",
        "        delta_delta = np.load(delta_delta_path)\n",
        "\n",
        "        # Handle different dimensions\n",
        "        if log_mel.ndim == 1:\n",
        "            log_mel = log_mel.reshape(-1, 1)\n",
        "        if delta.ndim == 1:\n",
        "            delta = delta.reshape(-1, 1)\n",
        "        if delta_delta.ndim == 1:\n",
        "            delta_delta = delta_delta.reshape(-1, 1)\n",
        "\n",
        "        # Ensure all have the same shape (handle NaN/Inf values)\n",
        "        # Get valid data (remove NaN/Inf)\n",
        "        valid_idx = ~(np.isnan(log_mel).any(axis=1) | np.isinf(log_mel).any(axis=1) |\n",
        "                      np.isnan(delta).any(axis=1) | np.isinf(delta).any(axis=1) |\n",
        "                      np.isnan(delta_delta).any(axis=1) | np.isinf(delta_delta).any(axis=1))\n",
        "\n",
        "        if np.sum(valid_idx) == 0:\n",
        "            return None\n",
        "\n",
        "        log_mel = log_mel[valid_idx]\n",
        "        delta = delta[valid_idx]\n",
        "        delta_delta = delta_delta[valid_idx]\n",
        "\n",
        "        # Ensure all have the same shape\n",
        "        min_time = min(log_mel.shape[0], delta.shape[0], delta_delta.shape[0])\n",
        "        if min_time == 0:\n",
        "            return None\n",
        "\n",
        "        # Trim to same time dimension\n",
        "        log_mel = log_mel[:min_time]\n",
        "        delta = delta[:min_time]\n",
        "        delta_delta = delta_delta[:min_time]\n",
        "\n",
        "        # Handle feature dimension\n",
        "        if log_mel.ndim == 1:\n",
        "            log_mel = log_mel.reshape(-1, 1)\n",
        "        if delta.ndim == 1:\n",
        "            delta = delta.reshape(-1, 1)\n",
        "        if delta_delta.ndim == 1:\n",
        "            delta_delta = delta_delta.reshape(-1, 1)\n",
        "\n",
        "        min_features = min(log_mel.shape[1], delta.shape[1], delta_delta.shape[1])\n",
        "        if min_features == 0:\n",
        "            return None\n",
        "\n",
        "        # Trim to same feature dimension\n",
        "        log_mel = log_mel[:, :min_features]\n",
        "        delta = delta[:, :min_features]\n",
        "        delta_delta = delta_delta[:, :min_features]\n",
        "\n",
        "        # Stack along the channel dimension\n",
        "        three_channel = np.stack([log_mel, delta, delta_delta], axis=-1)\n",
        "\n",
        "        return three_channel\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to resize data to fixed size\n",
        "def resize_data(data, target_shape=(128, 128)):\n",
        "    \"\"\"\n",
        "    Resize 3D data (time, features, channels) to fixed size\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle case where data might be too small\n",
        "        if data.shape[0] == 0 or data.shape[1] == 0:\n",
        "            return None\n",
        "\n",
        "        # Transpose to (channels, time, features) for easier processing\n",
        "        data = np.transpose(data, (2, 0, 1))\n",
        "\n",
        "        resized_channels = []\n",
        "        for channel in data:\n",
        "            # Handle very small dimensions\n",
        "            if channel.shape[0] == 0 or channel.shape[1] == 0:\n",
        "                return None\n",
        "\n",
        "            # Resize using OpenCV\n",
        "            resized = cv2.resize(channel.astype(np.float32),\n",
        "                               (target_shape[1], target_shape[0]),\n",
        "                               interpolation=cv2.INTER_LINEAR)\n",
        "            resized_channels.append(resized)\n",
        "\n",
        "        # Stack and transpose back to (time, features, channels)\n",
        "        resized_data = np.stack(resized_channels, axis=-1)\n",
        "        return resized_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error resizing data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to standardize the data (more robust)\n",
        "def standardize_data(data):\n",
        "    \"\"\"\n",
        "    Standardize data to have zero mean and unit variance\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Handle NaN/Inf values\n",
        "        data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "        original_shape = data.shape\n",
        "        data_flat = data.reshape(-1, data.shape[-1])  # (time*features, channels)\n",
        "\n",
        "        # Standardize each channel independently\n",
        "        standardized_data = np.zeros_like(data_flat)\n",
        "        for i in range(data_flat.shape[1]):\n",
        "            channel_data = data_flat[:, i]\n",
        "            mean = np.mean(channel_data)\n",
        "            std = np.std(channel_data)\n",
        "            if std > 1e-8:  # Avoid division by zero\n",
        "                standardized_data[:, i] = (channel_data - mean) / std\n",
        "            else:\n",
        "                standardized_data[:, i] = channel_data - mean\n",
        "\n",
        "        # Reshape back\n",
        "        standardized_data = standardized_data.reshape(original_shape)\n",
        "        return standardized_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error standardizing data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load and preprocess all data\n",
        "print(\"\\n=== DATA PREPROCESSING ===\")\n",
        "X = []\n",
        "y = []\n",
        "valid_indices = []\n",
        "\n",
        "target_shape = (128, 128)  # You can adjust this based on your data\n",
        "\n",
        "print(\"Loading and preprocessing data...\")\n",
        "successful_loads = 0\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        # Load three-channel data\n",
        "        data = load_three_channel_data(row['log_mel_path'], row['delta_path'], row['delta_delta_path'])\n",
        "\n",
        "        if data is not None and data.shape[0] > 0 and data.shape[1] > 0:\n",
        "            # Resize to fixed size\n",
        "            data_resized = resize_data(data, target_shape)\n",
        "\n",
        "            if data_resized is not None:\n",
        "                # Standardize the data\n",
        "                data_standardized = standardize_data(data_resized)\n",
        "\n",
        "                if data_standardized is not None:\n",
        "                    X.append(data_standardized)\n",
        "                    y.append(row['label'])\n",
        "                    valid_indices.append(idx)\n",
        "                    successful_loads += 1\n",
        "\n",
        "                    if successful_loads % 20 == 0:\n",
        "                        print(f\"Processed {successful_loads} samples successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing sample {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "if len(X) == 0:\n",
        "    raise ValueError(\"No data could be loaded successfully. Please check your data files.\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"\\nFinal dataset shapes:\")\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "\n",
        "# Check for any remaining shape inconsistencies\n",
        "if len(X) > 0:\n",
        "    unique_shapes = [x.shape for x in X]\n",
        "    print(f\"Unique shapes in dataset: {pd.Series(unique_shapes).value_counts()}\")\n",
        "\n",
        "# Split the data (80% train, 20% validation) - test set will be separate\n",
        "print(\"\\n=== DATASET SPLITTING ===\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
        "\n",
        "# Check class distribution in splits\n",
        "print(f\"\\nTraining class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Validation class distribution: {np.bincount(y_val)}\")\n",
        "\n",
        "# Calculate class weights for imbalanced dataset\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(f\"\\nClass weights: {class_weight_dict}\")\n",
        "\n",
        "# Visualize some samples (if we have data)\n",
        "if len(X_train) > 0:\n",
        "    print(\"\\n=== SAMPLE VISUALIZATION ===\")\n",
        "    try:\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "        # Plot samples from both classes\n",
        "        for i in range(min(2, len(np.where(y_train == 1)[0]))):\n",
        "            ad_idx = np.where(y_train == 1)[0][i]\n",
        "\n",
        "            axes[i, 0].imshow(X_train[ad_idx, :, :, 0], aspect='auto', cmap='viridis')\n",
        "            axes[i, 0].set_title(f'AD - Log-Mel (Sample {ad_idx})')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            axes[i, 1].imshow(X_train[ad_idx, :, :, 1], aspect='auto', cmap='viridis')\n",
        "            axes[i, 1].set_title(f'AD - Delta')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            axes[i, 2].imshow(X_train[ad_idx, :, :, 2], aspect='auto', cmap='viridis')\n",
        "            axes[i, 2].set_title(f'AD - Delta-Delta')\n",
        "            axes[i, 2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create visualization: {e}\")\n",
        "\n",
        "# Save preprocessed data\n",
        "print(\"\\n=== SAVING PROCESSED DATA ===\")\n",
        "try:\n",
        "    np.save('/content/X_train.npy', X_train)\n",
        "    np.save('/content/X_val.npy', X_val)\n",
        "    np.save('/content/y_train.npy', y_train)\n",
        "    np.save('/content/y_val.npy', y_val)\n",
        "\n",
        "    print(\"Preprocessed data saved!\")\n",
        "    print(\"Files created:\")\n",
        "    print(\"- /content/X_train.npy\")\n",
        "    print(\"- /content/X_val.npy\")\n",
        "    print(\"- /content/y_train.npy\")\n",
        "    print(\"- /content/y_val.npy\")\n",
        "\n",
        "    # Create a summary dictionary\n",
        "    data_summary = {\n",
        "        'X_train_shape': X_train.shape,\n",
        "        'X_val_shape': X_val.shape,\n",
        "        'y_train_shape': y_train.shape,\n",
        "        'y_val_shape': y_val.shape,\n",
        "        'class_weights': class_weight_dict,\n",
        "        'target_shape': target_shape,\n",
        "        'successful_samples': len(X)\n",
        "    }\n",
        "\n",
        "    import pickle\n",
        "    with open('/content/data_summary.pkl', 'wb') as f:\n",
        "        pickle.dump(data_summary, f)\n",
        "\n",
        "    print(\"\\nData summary saved to /content/data_summary.pkl\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving data: {e}\")\n",
        "\n",
        "print(\"\\n=== PREPROCESSING COMPLETE ===\")\n",
        "print(f\"Successfully processed {len(X)} samples out of {len(df)} total samples\")\n",
        "print(\"Training and validation data is ready for model training!\")\n",
        "\n",
        "# Function to prepare test data (when you provide the test directory)\n",
        "def prepare_test_data(test_directory):\n",
        "    \"\"\"\n",
        "    Prepare test data from a separate directory\n",
        "    Expected structure:\n",
        "    test_directory/\n",
        "        AD/\n",
        "        CN/\n",
        "    \"\"\"\n",
        "    print(f\"Preparing test data from: {test_directory}\")\n",
        "\n",
        "    # Implement test data loading logic here when you provide the test directory structure\n",
        "    # This would be similar to the training data loading but without labels or with different label handling\n",
        "\n",
        "    pass\n",
        "\n",
        "print(\"\\nTo prepare your separate test directory, call prepare_test_data('path/to/test/directory')\")"
      ],
      "metadata": {
        "id": "dKIiYcQV8ApV",
        "outputId": "474f4ade-0769-4415-a391-a52c0a32695e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 166 samples\n",
            "\n",
            "=== DATA PREPROCESSING ===\n",
            "Loading and preprocessing data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No data could be loaded successfully. Please check your data files.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-465006750.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data could be loaded successfully. Please check your data files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No data could be loaded successfully. Please check your data files."
          ]
        }
      ]
    }
  ]
}