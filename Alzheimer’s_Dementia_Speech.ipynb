{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRgHQe9LTv3FBFAtEmvF/k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Alzheimer%E2%80%99s_Dementia_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.io import wavfile\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import soundfile as sf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import uuid\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "J9_mpqWeL2Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 1: Load Dataset from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths to the .tgz files\n",
        "data_path = '/content/drive/MyDrive/ADRESS021/'\n",
        "train_prog_tgz = os.path.join(data_path, 'ADRESS021-progression-train.tgz')\n",
        "test_prog_tgz = os.path.join(data_path, 'ADRESS021-progression-test.tgz')\n",
        "train_diag_tgz = os.path.join(data_path, 'ADRESS021-diagnosis-train.tgz')\n",
        "\n",
        "# Extract .tgz files\n",
        "extracted_path = '/content/adress021_data/'\n",
        "os.makedirs(extracted_path, exist_ok=True)\n",
        "\n",
        "def extract_tgz(tgz_path, extract_to):\n",
        "    with tarfile.open(tgz_path, 'r:gz') as tar:\n",
        "        tar.extractall(path=extract_to)\n",
        "\n",
        "for tgz in [train_prog_tgz, test_prog_tgz, train_diag_tgz]:\n",
        "    extract_tgz(tgz, extracted_path)\n",
        "\n",
        "# Find all WAV files (assuming audio files are in WAV format)\n",
        "audio_files = glob.glob(os.path.join(extracted_path, '**/*.wav'), recursive=True)\n",
        "print(f\"Found {len(audio_files)} audio files.\")\n",
        "\n",
        "# Load metadata (assuming there's a CSV or text file with labels)\n",
        "# Modify this based on actual metadata structure\n",
        "metadata_path = glob.glob(os.path.join(extracted_path, '**/*.csv'), recursive=True)\n",
        "if metadata_path:\n",
        "    metadata = pd.read_csv(metadata_path[0])\n",
        "    print(\"Metadata loaded:\")\n",
        "    print(metadata.head())\n",
        "else:\n",
        "    metadata = pd.DataFrame({'file': audio_files, 'label': 'unknown'})\n",
        "    print(\"No metadata CSV found. Using file paths only.\")\n",
        "\n",
        "# Step 2: Visualize and Explore Dataset\n",
        "# Select a few sample audio files\n",
        "sample_files = audio_files[:3]\n",
        "\n",
        "# Visualize waveforms\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, audio_file in enumerate(sample_files):\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    plt.subplot(len(sample_files), 1, i+1)\n",
        "    librosa.display.waveshow(y, sr=sr)\n",
        "    plt.title(f\"Waveform: {os.path.basename(audio_file)}\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/waveforms.png')\n",
        "plt.show()\n",
        "\n",
        "# Show sample metadata\n",
        "print(\"\\nSample Metadata:\")\n",
        "print(metadata.head())\n",
        "\n",
        "# Plot spectrogram for one sample\n",
        "y, sr = librosa.load(sample_files[0], sr=None)\n",
        "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
        "plt.figure(figsize=(10, 6))\n",
        "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title(f\"Spectrogram: {os.path.basename(sample_files[0])}\")\n",
        "plt.savefig('/content/spectrogram.png')\n",
        "plt.show()\n",
        "\n",
        "# Show label distribution (assuming metadata has a 'label' column)\n",
        "if 'label' in metadata.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.countplot(data=metadata, x='label')\n",
        "    plt.title('Label Distribution')\n",
        "    plt.savefig('/content/label_distribution.png')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No label column found in metadata.\")\n",
        "\n",
        "# Step 3: Preprocess Dataset\n",
        "processed_features = []\n",
        "output_dir = '/content/processed_features/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for audio_file in audio_files:\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(audio_file, sr=16000, mono=True)  # Standardize to 16kHz, mono\n",
        "\n",
        "    # Noise Removal (simple spectral gating)\n",
        "    y_denoised = librosa.decompose.nn_filter(y, aggregate=np.median, metric='cosine')\n",
        "\n",
        "    # Silence Trimming\n",
        "    y_trimmed, _ = librosa.effects.trim(y_denoised, top_db=20)\n",
        "\n",
        "    # Segmentation (Optional)\n",
        "    audio = AudioSegment.from_wav(audio_file)\n",
        "    chunks = split_on_silence(audio, min_silence_len=500, silence_thresh=-40)\n",
        "    segments = []\n",
        "    for chunk in chunks:\n",
        "        chunk.export(f\"/content/temp_{uuid.uuid4()}.wav\", format=\"wav\")\n",
        "        seg_y, seg_sr = librosa.load(f\"/content/temp_{uuid.uuid4()}.wav\", sr=16000)\n",
        "        segments.append(seg_y)\n",
        "    if not segments:\n",
        "        segments = [y_trimmed]  # Use trimmed audio if no segments\n",
        "\n",
        "    # Feature Extraction\n",
        "    for seg_y in segments:\n",
        "        # Acoustic Features (MFCCs)\n",
        "        mfccs = librosa.feature.mfcc(y=seg_y, sr=16000, n_mfcc=13)\n",
        "        mfccs_mean = np.mean(mfccs, axis=1)\n",
        "\n",
        "        # Prosodic Features (Pitch, Energy)\n",
        "        pitches, magnitudes = librosa.piptrack(y=seg_y, sr=16000)\n",
        "        pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0\n",
        "        energy = np.mean(librosa.feature.rms(y=seg_y))\n",
        "\n",
        "        # Linguistic Features (Placeholder: requires transcription)\n",
        "        # Example: word count, pause frequency (needs external ASR)\n",
        "        linguistic_features = np.array([0, 0])  # Placeholder\n",
        "\n",
        "        # Combine features\n",
        "        features = np.concatenate([mfccs_mean, [pitch_mean, energy], linguistic_features])\n",
        "        processed_features.append(features)\n",
        "\n",
        "    # Save processed audio (optional)\n",
        "    sf.write(os.path.join(output_dir, os.path.basename(audio_file)), y_trimmed, 16000)\n",
        "\n",
        "# Feature Normalization\n",
        "scaler = StandardScaler()\n",
        "processed_features = np.array(processed_features)\n",
        "normalized_features = scaler.fit_transform(processed_features)\n",
        "\n",
        "# Data Augmentation (e.g., add noise)\n",
        "augmented_features = []\n",
        "for features in normalized_features:\n",
        "    noise = np.random.normal(0, 0.01, features.shape)\n",
        "    augmented_features.append(features + noise)\n",
        "augmented_features = np.array(augmented_features)\n",
        "\n",
        "# Export Processed Features\n",
        "np.save(os.path.join(output_dir, 'normalized_features.npy'), normalized_features)\n",
        "np.save(os.path.join(output_dir, 'augmented_features.npy'), augmented_features)\n",
        "print(f\"Processed features saved to {output_dir}\")\n",
        "\n",
        "# Clean up temporary files\n",
        "for temp_file in glob.glob('/content/temp_*.wav'):\n",
        "    os.remove(temp_file)"
      ],
      "metadata": {
        "id": "wCpaMqopL0Zq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}