{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMakJYAb5n0FETWZ336590",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/July16_Speech_CompleteV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initial Setup and Data Loading\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required libraries\n",
        "!pip install -q librosa soundfile speechbrain whisper-jax transformers datasets torch torchaudio\n",
        "!pip install -q scikit-learn networkx plotly umap-learn\n",
        "\n",
        "# Memory optimization imports\n",
        "import psutil\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Monitor memory usage\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    return memory_info.rss / (1024 * 1024)  # MB\n",
        "\n",
        "print(f\"Initial memory usage: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "# Define paths\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"\n",
        "PATHS = {\n",
        "    'diagnosis_train_audio_ad': f\"{BASE_PATH}/diagnosis/train/audio/ad\",\n",
        "    'diagnosis_train_audio_cn': f\"{BASE_PATH}/diagnosis/train/audio/cn\",\n",
        "    'diagnosis_train_seg_ad': f\"{BASE_PATH}/diagnosis/train/segmentation/ad\",\n",
        "    'diagnosis_train_seg_cn': f\"{BASE_PATH}/diagnosis/train/segmentation/cn\",\n",
        "    'progression_test_audio': f\"{BASE_PATH}/progression/test-dist/audio\",\n",
        "    'progression_test_seg': f\"{BASE_PATH}/progression/test-dist/segmentation\",\n",
        "    'features': f\"{BASE_PATH}/features\",\n",
        "    'transcripts': f\"{BASE_PATH}/transcripts\",\n",
        "    'models': f\"{BASE_PATH}/models\",\n",
        "    'visualizations': f\"{BASE_PATH}/visualizations\"\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "for path in PATHS.values():\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for transcripts\n",
        "transcript_dirs = [\n",
        "    f\"{PATHS['transcripts']}/train/ad\",\n",
        "    f\"{PATHS['transcripts']}/train/cn\",\n",
        "    f\"{PATHS['transcripts']}/test-dist\",\n",
        "    f\"{PATHS['features']}/audio\",\n",
        "    f\"{PATHS['features']}/text\",\n",
        "    f\"{PATHS['models']}/bert\",\n",
        "    f\"{PATHS['models']}/vit\",\n",
        "    f\"{PATHS['models']}/two_branch\"\n",
        "]\n",
        "\n",
        "for dir_path in transcript_dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "def scan_dataset():\n",
        "    \"\"\"Scan and organize dataset files\"\"\"\n",
        "    dataset_info = {\n",
        "        'train_ad_audio': [],\n",
        "        'train_cn_audio': [],\n",
        "        'train_ad_seg': [],\n",
        "        'train_cn_seg': [],\n",
        "        'test_audio': [],\n",
        "        'test_seg': []\n",
        "    }\n",
        "\n",
        "    # Scan training data\n",
        "    if os.path.exists(PATHS['diagnosis_train_audio_ad']):\n",
        "        dataset_info['train_ad_audio'] = [f for f in os.listdir(PATHS['diagnosis_train_audio_ad']) if f.endswith('.wav')]\n",
        "\n",
        "    if os.path.exists(PATHS['diagnosis_train_audio_cn']):\n",
        "        dataset_info['train_cn_audio'] = [f for f in os.listdir(PATHS['diagnosis_train_audio_cn']) if f.endswith('.wav')]\n",
        "\n",
        "    if os.path.exists(PATHS['diagnosis_train_seg_ad']):\n",
        "        dataset_info['train_ad_seg'] = [f for f in os.listdir(PATHS['diagnosis_train_seg_ad']) if f.endswith('.csv')]\n",
        "\n",
        "    if os.path.exists(PATHS['diagnosis_train_seg_cn']):\n",
        "        dataset_info['train_cn_seg'] = [f for f in os.listdir(PATHS['diagnosis_train_seg_cn']) if f.endswith('.csv')]\n",
        "\n",
        "    # Scan test data\n",
        "    if os.path.exists(PATHS['progression_test_audio']):\n",
        "        dataset_info['test_audio'] = [f for f in os.listdir(PATHS['progression_test_audio']) if f.endswith('.wav')]\n",
        "\n",
        "    if os.path.exists(PATHS['progression_test_seg']):\n",
        "        dataset_info['test_seg'] = [f for f in os.listdir(PATHS['progression_test_seg']) if f.endswith('.csv')]\n",
        "\n",
        "    return dataset_info\n",
        "\n",
        "# Scan dataset\n",
        "dataset_info = scan_dataset()\n",
        "\n",
        "# Print dataset summary\n",
        "print(\"Dataset Summary:\")\n",
        "print(f\"Train AD audio files: {len(dataset_info['train_ad_audio'])}\")\n",
        "print(f\"Train CN audio files: {len(dataset_info['train_cn_audio'])}\")\n",
        "print(f\"Train AD segmentation files: {len(dataset_info['train_ad_seg'])}\")\n",
        "print(f\"Train CN segmentation files: {len(dataset_info['train_cn_seg'])}\")\n",
        "print(f\"Test audio files: {len(dataset_info['test_audio'])}\")\n",
        "print(f\"Test segmentation files: {len(dataset_info['test_seg'])}\")\n",
        "\n",
        "# Create metadata DataFrame\n",
        "def create_metadata_df():\n",
        "    \"\"\"Create metadata DataFrame for efficient data management\"\"\"\n",
        "    metadata = []\n",
        "\n",
        "    # Add training data\n",
        "    for file in dataset_info['train_ad_audio']:\n",
        "        metadata.append({\n",
        "            'file_id': file.replace('.wav', ''),\n",
        "            'audio_path': f\"{PATHS['diagnosis_train_audio_ad']}/{file}\",\n",
        "            'seg_path': f\"{PATHS['diagnosis_train_seg_ad']}/{file.replace('.wav', '.csv')}\",\n",
        "            'label': 'ad',\n",
        "            'split': 'train'\n",
        "        })\n",
        "\n",
        "    for file in dataset_info['train_cn_audio']:\n",
        "        metadata.append({\n",
        "            'file_id': file.replace('.wav', ''),\n",
        "            'audio_path': f\"{PATHS['diagnosis_train_audio_cn']}/{file}\",\n",
        "            'seg_path': f\"{PATHS['diagnosis_train_seg_cn']}/{file.replace('.wav', '.csv')}\",\n",
        "            'label': 'cn',\n",
        "            'split': 'train'\n",
        "        })\n",
        "\n",
        "    # Add test data\n",
        "    for file in dataset_info['test_audio']:\n",
        "        metadata.append({\n",
        "            'file_id': file.replace('.wav', ''),\n",
        "            'audio_path': f\"{PATHS['progression_test_audio']}/{file}\",\n",
        "            'seg_path': f\"{PATHS['progression_test_seg']}/{file.replace('.wav', '.csv')}\",\n",
        "            'label': 'unknown',\n",
        "            'split': 'test'\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(metadata)\n",
        "\n",
        "metadata_df = create_metadata_df()\n",
        "print(f\"\\nMetadata DataFrame created with {len(metadata_df)} entries\")\n",
        "print(f\"Memory usage after setup: {get_memory_usage():.2f} MB\")\n",
        "\n",
        "# Save metadata\n",
        "metadata_df.to_csv(f\"{BASE_PATH}/metadata.csv\", index=False)\n",
        "print(\"Setup completed successfully!\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nMetadata sample:\")\n",
        "print(metadata_df.head())\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(metadata_df[metadata_df['split'] == 'train']['label'].value_counts())"
      ],
      "metadata": {
        "id": "a0qFWZPLedmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}