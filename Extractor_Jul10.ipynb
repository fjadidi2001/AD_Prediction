{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1KYG9akf/HGyLVsARPm7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Extractor_Jul10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Speech Dataset Analysis Pipeline for ADResSO21\n",
        "# This pipeline processes audio files, generates transcripts, extracts acoustic features,\n",
        "# and analyzes semantic connections between audio and text\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import speech_recognition as sr\n",
        "import opensmile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up paths\n",
        "BASE_PATH = '/content/drive/MyDrive/Speech'\n",
        "FEATURES_SAVE_PATH = '/content/drive/MyDrive/Speech/extracted_features'\n",
        "TRANSCRIPTS_SAVE_PATH = '/content/drive/MyDrive/Speech/transcripts'\n",
        "\n",
        "# Create directories for saving features and transcripts\n",
        "os.makedirs(FEATURES_SAVE_PATH, exist_ok=True)\n",
        "os.makedirs(TRANSCRIPTS_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "class SpeechAnalyzer:\n",
        "    def __init__(self, base_path):\n",
        "        self.base_path = base_path\n",
        "        self.recognizer = sr.Recognizer()\n",
        "        self.smile = opensmile.Smile(\n",
        "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "            feature_level=opensmile.FeatureLevel.Functionals,\n",
        "        )\n",
        "\n",
        "    def load_dataset_structure(self):\n",
        "        \"\"\"Load and organize the dataset structure\"\"\"\n",
        "        dataset_info = {\n",
        "            'diagnosis_train': {\n",
        "                'audio': {\n",
        "                    'cn': f'{self.base_path}/extracted-diagnosis-train/audio/cn',\n",
        "                    'ad': f'{self.base_path}/extracted-diagnosis-train/audio/ad'\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'cn': f'{self.base_path}/extracted-diagnosis-train/segmentation/cn',\n",
        "                    'ad': f'{self.base_path}/extracted-diagnosis-train/segmentation/ad'\n",
        "                }\n",
        "            },\n",
        "            'progression_train': {\n",
        "                'audio': {\n",
        "                    'decline': f'{self.base_path}/extracted-progression-train/audio/decline',\n",
        "                    'no-decline': f'{self.base_path}/extracted-progression-train/audio/no-decline'\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'decline': f'{self.base_path}/extracted-progression-train/segmentation/decline',\n",
        "                    'no-decline': f'{self.base_path}/extracted-progression-train/segmentation/no-decline'\n",
        "                }\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'audio': f'{self.base_path}/extracted-progression-test/audio',\n",
        "                'segmentation': f'{self.base_path}/extracted-progression-test/segmentation'\n",
        "            }\n",
        "        }\n",
        "        return dataset_info\n",
        "\n",
        "    def get_audio_files(self, directory):\n",
        "        \"\"\"Get all audio files from a directory\"\"\"\n",
        "        if not os.path.exists(directory):\n",
        "            print(f\"Directory not found: {directory}\")\n",
        "            return []\n",
        "\n",
        "        audio_extensions = ['.wav', '.mp3', '.m4a', '.flac', '.aac']\n",
        "        audio_files = []\n",
        "\n",
        "        for file in os.listdir(directory):\n",
        "            if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
        "                audio_files.append(os.path.join(directory, file))\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def transcribe_audio(self, audio_file_path):\n",
        "        \"\"\"Transcribe audio file to text using speech recognition\"\"\"\n",
        "        try:\n",
        "            with sr.AudioFile(audio_file_path) as source:\n",
        "                # Adjust for ambient noise\n",
        "                self.recognizer.adjust_for_ambient_noise(source)\n",
        "                audio = self.recognizer.record(source)\n",
        "\n",
        "            # Try different recognition engines\n",
        "            try:\n",
        "                # Use Google's free service\n",
        "                transcript = self.recognizer.recognize_google(audio)\n",
        "                return transcript\n",
        "            except sr.UnknownValueError:\n",
        "                # Try with alternative service\n",
        "                try:\n",
        "                    transcript = self.recognizer.recognize_sphinx(audio)\n",
        "                    return transcript\n",
        "                except:\n",
        "                    return \"Unable to transcribe\"\n",
        "            except sr.RequestError as e:\n",
        "                return f\"Error with recognition service: {e}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error processing audio: {e}\"\n",
        "\n",
        "    def extract_egemaps_features(self, audio_file_path):\n",
        "        \"\"\"Extract eGeMAPS features using OpenSMILE\"\"\"\n",
        "        try:\n",
        "            features = self.smile.process_file(audio_file_path)\n",
        "            return features.values.flatten()\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting eGeMAPS features from {audio_file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_log_mel_spectrogram(self, audio_file_path, n_mels=128, n_fft=2048, hop_length=512):\n",
        "        \"\"\"Extract log-mel spectrogram features\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "            # Extract mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length\n",
        "            )\n",
        "\n",
        "            # Convert to log scale\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            # Compute statistics across time dimension\n",
        "            features = {\n",
        "                'mean': np.mean(log_mel_spec, axis=1),\n",
        "                'std': np.std(log_mel_spec, axis=1),\n",
        "                'max': np.max(log_mel_spec, axis=1),\n",
        "                'min': np.min(log_mel_spec, axis=1)\n",
        "            }\n",
        "\n",
        "            # Flatten all features\n",
        "            feature_vector = np.concatenate([features[key] for key in features.keys()])\n",
        "            return feature_vector\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting log-mel features from {audio_file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def extract_spectral_features(self, audio_file_path):\n",
        "        \"\"\"Extract additional spectral features\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "            # Extract various spectral features\n",
        "            features = {}\n",
        "\n",
        "            # Spectral centroid\n",
        "            features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "\n",
        "            # Spectral rolloff\n",
        "            features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "\n",
        "            # Spectral bandwidth\n",
        "            features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "\n",
        "            # Zero crossing rate\n",
        "            features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "\n",
        "            # MFCC features\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            for i in range(13):\n",
        "                features[f'mfcc_{i}'] = np.mean(mfccs[i])\n",
        "\n",
        "            # Chroma features\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            features['chroma_mean'] = np.mean(chroma)\n",
        "            features['chroma_std'] = np.std(chroma)\n",
        "\n",
        "            # Tempo\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            features['tempo'] = tempo\n",
        "\n",
        "            return np.array(list(features.values()))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting spectral features from {audio_file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_dataset(self):\n",
        "        \"\"\"Process the entire dataset\"\"\"\n",
        "        dataset_info = self.load_dataset_structure()\n",
        "        results = {\n",
        "            'files': [],\n",
        "            'labels': [],\n",
        "            'categories': [],\n",
        "            'transcripts': [],\n",
        "            'egemaps_features': [],\n",
        "            'logmel_features': [],\n",
        "            'spectral_features': []\n",
        "        }\n",
        "\n",
        "        print(\"Processing dataset...\")\n",
        "\n",
        "        # Process diagnosis training data\n",
        "        for label in ['cn', 'ad']:\n",
        "            audio_dir = dataset_info['diagnosis_train']['audio'][label]\n",
        "            audio_files = self.get_audio_files(audio_dir)\n",
        "\n",
        "            print(f\"Processing {label} diagnosis files: {len(audio_files)} files\")\n",
        "\n",
        "            for audio_file in audio_files:\n",
        "                print(f\"Processing: {os.path.basename(audio_file)}\")\n",
        "\n",
        "                # Transcribe audio\n",
        "                transcript = self.transcribe_audio(audio_file)\n",
        "\n",
        "                # Extract features\n",
        "                egemaps_feat = self.extract_egemaps_features(audio_file)\n",
        "                logmel_feat = self.extract_log_mel_spectrogram(audio_file)\n",
        "                spectral_feat = self.extract_spectral_features(audio_file)\n",
        "\n",
        "                # Store results\n",
        "                results['files'].append(audio_file)\n",
        "                results['labels'].append(label)\n",
        "                results['categories'].append('diagnosis')\n",
        "                results['transcripts'].append(transcript)\n",
        "                results['egemaps_features'].append(egemaps_feat)\n",
        "                results['logmel_features'].append(logmel_feat)\n",
        "                results['spectral_features'].append(spectral_feat)\n",
        "\n",
        "        # Process progression training data\n",
        "        for label in ['decline', 'no-decline']:\n",
        "            audio_dir = dataset_info['progression_train']['audio'][label]\n",
        "            audio_files = self.get_audio_files(audio_dir)\n",
        "\n",
        "            print(f\"Processing {label} progression files: {len(audio_files)} files\")\n",
        "\n",
        "            for audio_file in audio_files:\n",
        "                print(f\"Processing: {os.path.basename(audio_file)}\")\n",
        "\n",
        "                # Transcribe audio\n",
        "                transcript = self.transcribe_audio(audio_file)\n",
        "\n",
        "                # Extract features\n",
        "                egemaps_feat = self.extract_egemaps_features(audio_file)\n",
        "                logmel_feat = self.extract_log_mel_spectrogram(audio_file)\n",
        "                spectral_feat = self.extract_spectral_features(audio_file)\n",
        "\n",
        "                # Store results\n",
        "                results['files'].append(audio_file)\n",
        "                results['labels'].append(label)\n",
        "                results['categories'].append('progression')\n",
        "                results['transcripts'].append(transcript)\n",
        "                results['egemaps_features'].append(egemaps_feat)\n",
        "                results['logmel_features'].append(logmel_feat)\n",
        "                results['spectral_features'].append(spectral_feat)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_results(self, results):\n",
        "        \"\"\"Save extracted features and transcripts\"\"\"\n",
        "        # Save transcripts\n",
        "        transcripts_df = pd.DataFrame({\n",
        "            'file': results['files'],\n",
        "            'label': results['labels'],\n",
        "            'category': results['categories'],\n",
        "            'transcript': results['transcripts']\n",
        "        })\n",
        "        transcripts_df.to_csv(f'{TRANSCRIPTS_SAVE_PATH}/transcripts.csv', index=False)\n",
        "\n",
        "        # Save features\n",
        "        # Filter out None values\n",
        "        valid_indices = [i for i, feat in enumerate(results['egemaps_features']) if feat is not None]\n",
        "\n",
        "        if valid_indices:\n",
        "            # eGeMAPS features\n",
        "            egemaps_array = np.array([results['egemaps_features'][i] for i in valid_indices])\n",
        "            np.save(f'{FEATURES_SAVE_PATH}/egemaps_features.npy', egemaps_array)\n",
        "\n",
        "            # Log-mel features\n",
        "            logmel_valid = [i for i in valid_indices if results['logmel_features'][i] is not None]\n",
        "            if logmel_valid:\n",
        "                logmel_array = np.array([results['logmel_features'][i] for i in logmel_valid])\n",
        "                np.save(f'{FEATURES_SAVE_PATH}/logmel_features.npy', logmel_array)\n",
        "\n",
        "            # Spectral features\n",
        "            spectral_valid = [i for i in valid_indices if results['spectral_features'][i] is not None]\n",
        "            if spectral_valid:\n",
        "                spectral_array = np.array([results['spectral_features'][i] for i in spectral_valid])\n",
        "                np.save(f'{FEATURES_SAVE_PATH}/spectral_features.npy', spectral_array)\n",
        "\n",
        "            # Save metadata\n",
        "            metadata_df = pd.DataFrame({\n",
        "                'file': [results['files'][i] for i in valid_indices],\n",
        "                'label': [results['labels'][i] for i in valid_indices],\n",
        "                'category': [results['categories'][i] for i in valid_indices]\n",
        "            })\n",
        "            metadata_df.to_csv(f'{FEATURES_SAVE_PATH}/metadata.csv', index=False)\n",
        "\n",
        "        print(f\"Results saved to {FEATURES_SAVE_PATH} and {TRANSCRIPTS_SAVE_PATH}\")\n",
        "\n",
        "    def visualize_semantic_connections(self, results):\n",
        "        \"\"\"Visualize semantic connections between audio and text\"\"\"\n",
        "        # Create visualizations\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # 1. Transcript length distribution by label\n",
        "        transcript_lengths = [len(t.split()) if isinstance(t, str) else 0 for t in results['transcripts']]\n",
        "        transcript_df = pd.DataFrame({\n",
        "            'length': transcript_lengths,\n",
        "            'label': results['labels'],\n",
        "            'category': results['categories']\n",
        "        })\n",
        "\n",
        "        sns.boxplot(data=transcript_df, x='label', y='length', ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Transcript Length Distribution by Label')\n",
        "        axes[0, 0].set_ylabel('Number of Words')\n",
        "\n",
        "        # 2. Feature correlation heatmap (using spectral features)\n",
        "        valid_spectral = [f for f in results['spectral_features'] if f is not None]\n",
        "        if valid_spectral:\n",
        "            spectral_array = np.array(valid_spectral)\n",
        "            correlation_matrix = np.corrcoef(spectral_array.T)\n",
        "\n",
        "            sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', ax=axes[0, 1])\n",
        "            axes[0, 1].set_title('Spectral Features Correlation')\n",
        "\n",
        "        # 3. PCA visualization of acoustic features\n",
        "        if valid_spectral:\n",
        "            scaler = StandardScaler()\n",
        "            spectral_scaled = scaler.fit_transform(spectral_array)\n",
        "\n",
        "            pca = PCA(n_components=2)\n",
        "            pca_result = pca.fit_transform(spectral_scaled)\n",
        "\n",
        "            # Get corresponding labels for valid spectral features\n",
        "            valid_labels = [results['labels'][i] for i, f in enumerate(results['spectral_features']) if f is not None]\n",
        "\n",
        "            scatter = axes[0, 2].scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                                       c=[hash(label) for label in valid_labels],\n",
        "                                       alpha=0.6)\n",
        "            axes[0, 2].set_title('PCA of Acoustic Features')\n",
        "            axes[0, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "            axes[0, 2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "\n",
        "        # 4. Text-based analysis\n",
        "        # Word frequency analysis\n",
        "        all_words = []\n",
        "        for transcript in results['transcripts']:\n",
        "            if isinstance(transcript, str):\n",
        "                words = transcript.lower().split()\n",
        "                all_words.extend(words)\n",
        "\n",
        "        from collections import Counter\n",
        "        word_freq = Counter(all_words)\n",
        "        common_words = word_freq.most_common(10)\n",
        "\n",
        "        if common_words:\n",
        "            words, counts = zip(*common_words)\n",
        "            axes[1, 0].bar(words, counts)\n",
        "            axes[1, 0].set_title('Most Common Words in Transcripts')\n",
        "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 5. Semantic similarity analysis\n",
        "        # Calculate text similarity between different groups\n",
        "        def calculate_text_similarity(group1_transcripts, group2_transcripts):\n",
        "            similarities = []\n",
        "            for t1 in group1_transcripts:\n",
        "                for t2 in group2_transcripts:\n",
        "                    if isinstance(t1, str) and isinstance(t2, str):\n",
        "                        # Simple word overlap similarity\n",
        "                        words1 = set(t1.lower().split())\n",
        "                        words2 = set(t2.lower().split())\n",
        "                        similarity = len(words1.intersection(words2)) / len(words1.union(words2)) if words1.union(words2) else 0\n",
        "                        similarities.append(similarity)\n",
        "            return similarities\n",
        "\n",
        "        # Group transcripts by label\n",
        "        label_groups = {}\n",
        "        for i, label in enumerate(results['labels']):\n",
        "            if label not in label_groups:\n",
        "                label_groups[label] = []\n",
        "            label_groups[label].append(results['transcripts'][i])\n",
        "\n",
        "        # Calculate similarities between groups\n",
        "        similarity_data = []\n",
        "        labels = list(label_groups.keys())\n",
        "        for i, label1 in enumerate(labels):\n",
        "            for j, label2 in enumerate(labels):\n",
        "                if i <= j:\n",
        "                    similarities = calculate_text_similarity(label_groups[label1], label_groups[label2])\n",
        "                    similarity_data.extend([(label1, label2, sim) for sim in similarities])\n",
        "\n",
        "        if similarity_data:\n",
        "            sim_df = pd.DataFrame(similarity_data, columns=['Group1', 'Group2', 'Similarity'])\n",
        "            sim_df['Pair'] = sim_df['Group1'] + ' vs ' + sim_df['Group2']\n",
        "\n",
        "            sns.boxplot(data=sim_df, x='Pair', y='Similarity', ax=axes[1, 1])\n",
        "            axes[1, 1].set_title('Text Similarity Between Groups')\n",
        "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # 6. Audio-Text relationship\n",
        "        # Correlate audio features with text features\n",
        "        audio_text_correlations = []\n",
        "\n",
        "        for i, (transcript, spectral_feat) in enumerate(zip(results['transcripts'], results['spectral_features'])):\n",
        "            if isinstance(transcript, str) and spectral_feat is not None:\n",
        "                # Text features\n",
        "                word_count = len(transcript.split())\n",
        "                char_count = len(transcript)\n",
        "\n",
        "                # Audio features (take first few spectral features)\n",
        "                audio_energy = spectral_feat[0] if len(spectral_feat) > 0 else 0\n",
        "\n",
        "                audio_text_correlations.append({\n",
        "                    'word_count': word_count,\n",
        "                    'char_count': char_count,\n",
        "                    'audio_energy': audio_energy,\n",
        "                    'label': results['labels'][i]\n",
        "                })\n",
        "\n",
        "        if audio_text_correlations:\n",
        "            corr_df = pd.DataFrame(audio_text_correlations)\n",
        "\n",
        "            # Plot correlation between word count and audio energy\n",
        "            sns.scatterplot(data=corr_df, x='word_count', y='audio_energy',\n",
        "                          hue='label', ax=axes[1, 2])\n",
        "            axes[1, 2].set_title('Audio Energy vs Word Count')\n",
        "            axes[1, 2].set_xlabel('Word Count')\n",
        "            axes[1, 2].set_ylabel('Audio Energy')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{FEATURES_SAVE_PATH}/semantic_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "# Initialize and run the analysis\n",
        "print(\"Starting Speech Dataset Analysis...\")\n",
        "analyzer = SpeechAnalyzer(BASE_PATH)\n",
        "\n",
        "# Process the dataset\n",
        "results = analyzer.process_dataset()\n",
        "\n",
        "# Save results\n",
        "analyzer.save_results(results)\n",
        "\n",
        "# Create semantic visualizations\n",
        "analyzer.visualize_semantic_connections(results)\n",
        "\n",
        "print(\"Analysis complete! Check the saved files in your Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ssQ6wVTaAxEP",
        "outputId": "a143a5fa-fe52-48a8-af2d-e497a398365b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-166421662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Set up paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrkxyBL-91Tk",
        "outputId": "72659e46-2d70-442d-fb22-e04bbd356a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.14.1)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.3\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting audobject>=0.6.1 (from opensmile)\n",
            "  Downloading audobject-0.7.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting audinterface>=0.7.0 (from opensmile)\n",
            "  Downloading audinterface-1.3.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting audeer>=2.1.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audeer-2.2.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audformat-1.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audiofile-1.5.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audmath-1.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting asttokens>=2.0.0 (from audobject>=0.6.1->opensmile)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (8.7.0)\n",
            "Collecting oyaml (from audobject>=0.6.1->opensmile)\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from audeer>=2.1.1->audinterface>=0.7.0->opensmile) (4.67.1)\n",
            "Collecting iso639-lang (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso639_lang-2.6.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting iso3166 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (0.13.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.17.0)\n",
            "Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audinterface-1.3.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audobject-0.7.12-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading audeer-2.2.2-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audformat-1.3.2-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audiofile-1.5.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audmath-1.4.2-py3-none-any.whl (23 kB)\n",
            "Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading iso639_lang-2.6.1-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.9/324.9 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: oyaml, iso639-lang, iso3166, audresample, audmath, audeer, asttokens, audobject, audiofile, audformat, audinterface, opensmile\n",
            "Successfully installed asttokens-3.0.0 audeer-2.2.2 audformat-1.3.2 audinterface-1.3.1 audiofile-1.5.1 audmath-1.4.2 audobject-0.7.12 audresample-1.3.4 iso3166-2.1.1 iso639-lang-2.6.1 opensmile-2.5.1 oyaml-1.0\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (0.25.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,415 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,758 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,569 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,087 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,795 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,103 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,266 kB]\n",
            "Fetched 26.6 MB in 3s (10.6 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 0s (1,131 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  flac\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 130 kB of archives.\n",
            "After this operation, 359 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 flac amd64 1.3.3-2ubuntu0.2 [130 kB]\n",
            "Fetched 130 kB in 0s (926 kB/s)\n",
            "Selecting previously unselected package flac.\n",
            "(Reading database ... 126325 files and directories currently installed.)\n",
            "Preparing to unpack .../flac_1.3.3-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking flac (1.3.3-2ubuntu0.2) ...\n",
            "Setting up flac (1.3.3-2ubuntu0.2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=pyaudio-0.2.14-cp311-cp311-linux_x86_64.whl size=67424 sha256=6e7d8685520d0a8a002050162e0680086c00b99da9447ee0ad2223df36e46580\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/b1/c1/67e4ef443de2665d86031d4760508094eab5de37d5d64d9c27\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "Successfully installed pyaudio-0.2.14\n",
            "Collecting pocketsphinx\n",
            "  Downloading pocketsphinx-5.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting sounddevice (from pocketsphinx)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice->pocketsphinx) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.22)\n",
            "Downloading pocketsphinx-5.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, pocketsphinx\n",
            "Successfully installed pocketsphinx-5.0.4 sounddevice-0.5.2\n",
            "All required packages installed successfully!\n",
            "You can now run the main analysis script.\n",
            "Enhanced Speech Analyzer ready!\n",
            "To use the enhanced analyzer:\n",
            "1. First run the main analysis script\n",
            "2. Then use the following code:\n",
            "\n",
            "# Initialize enhanced analyzer\n",
            "enhanced_analyzer = EnhancedSpeechAnalyzer(FEATURES_SAVE_PATH, TRANSCRIPTS_SAVE_PATH)\n",
            "\n",
            "# Load saved data\n",
            "data = enhanced_analyzer.load_saved_data()\n",
            "\n",
            "if data is not None:\n",
            "    # Create word clouds\n",
            "    enhanced_analyzer.create_word_clouds(data['transcripts'])\n",
            "    \n",
            "    # Create interactive dashboard\n",
            "    enhanced_analyzer.create_interactive_dashboard(data)\n",
            "    \n",
            "    # Perform clustering analysis\n",
            "    cluster_report = enhanced_analyzer.clustering_analysis(data)\n",
            "    \n",
            "    # Comprehensive feature analysis\n",
            "    enhanced_analyzer.comprehensive_feature_analysis(data)\n",
            "    \n",
            "    # Generate analysis report\n",
            "    enhanced_analyzer.generate_analysis_report(data)\n",
            "else:\n",
            "    print(\"Could not load saved data. Please run the main analysis first.\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for speech analysis\n",
        "# Run this cell first before running the main analysis\n",
        "\n",
        "# Install required packages\n",
        "!pip install librosa\n",
        "!pip install SpeechRecognition\n",
        "!pip install opensmile\n",
        "!pip install pydub\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install plotly\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "\n",
        "# Additional setup for audio processing\n",
        "!apt-get update\n",
        "!apt-get install -y portaudio19-dev\n",
        "!apt-get install -y flac\n",
        "\n",
        "# Install additional speech processing libraries\n",
        "!pip install pyaudio\n",
        "!pip install pocketsphinx\n",
        "\n",
        "print(\"All required packages installed successfully!\")\n",
        "print(\"You can now run the main analysis script.\")\n",
        "\n",
        "# Additional utility functions for enhanced analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "class EnhancedSpeechAnalyzer:\n",
        "    \"\"\"Enhanced analyzer with additional visualization and analysis capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, features_path, transcripts_path):\n",
        "        self.features_path = features_path\n",
        "        self.transcripts_path = transcripts_path\n",
        "\n",
        "    def load_saved_data(self):\n",
        "        \"\"\"Load previously saved features and transcripts\"\"\"\n",
        "        try:\n",
        "            # Load transcripts\n",
        "            transcripts_df = pd.read_csv(f'{self.transcripts_path}/transcripts.csv')\n",
        "\n",
        "            # Load features\n",
        "            egemaps_features = np.load(f'{self.features_path}/egemaps_features.npy')\n",
        "            metadata_df = pd.read_csv(f'{self.features_path}/metadata.csv')\n",
        "\n",
        "            try:\n",
        "                logmel_features = np.load(f'{self.features_path}/logmel_features.npy')\n",
        "            except:\n",
        "                logmel_features = None\n",
        "\n",
        "            try:\n",
        "                spectral_features = np.load(f'{self.features_path}/spectral_features.npy')\n",
        "            except:\n",
        "                spectral_features = None\n",
        "\n",
        "            return {\n",
        "                'transcripts': transcripts_df,\n",
        "                'egemaps_features': egemaps_features,\n",
        "                'logmel_features': logmel_features,\n",
        "                'spectral_features': spectral_features,\n",
        "                'metadata': metadata_df\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading saved data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_text_sentiment(self, transcripts_df):\n",
        "        \"\"\"Analyze sentiment of transcripts\"\"\"\n",
        "        sentiments = []\n",
        "\n",
        "        for transcript in transcripts_df['transcript']:\n",
        "            if isinstance(transcript, str) and transcript != \"Unable to transcribe\":\n",
        "                blob = TextBlob(transcript)\n",
        "                sentiments.append({\n",
        "                    'polarity': blob.sentiment.polarity,\n",
        "                    'subjectivity': blob.sentiment.subjectivity\n",
        "                })\n",
        "            else:\n",
        "                sentiments.append({\n",
        "                    'polarity': 0,\n",
        "                    'subjectivity': 0\n",
        "                })\n",
        "\n",
        "        sentiment_df = pd.DataFrame(sentiments)\n",
        "        sentiment_df['label'] = transcripts_df['label']\n",
        "        sentiment_df['category'] = transcripts_df['category']\n",
        "\n",
        "        return sentiment_df\n",
        "\n",
        "    def create_word_clouds(self, transcripts_df):\n",
        "        \"\"\"Create word clouds for different groups\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        labels = transcripts_df['label'].unique()\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            if i < 4:  # Maximum 4 subplots\n",
        "                row = i // 2\n",
        "                col = i % 2\n",
        "\n",
        "                # Get transcripts for this label\n",
        "                label_transcripts = transcripts_df[transcripts_df['label'] == label]['transcript']\n",
        "\n",
        "                # Combine all transcripts for this label\n",
        "                combined_text = ' '.join([t for t in label_transcripts if isinstance(t, str) and t != \"Unable to transcribe\"])\n",
        "\n",
        "                if combined_text:\n",
        "                    wordcloud = WordCloud(width=800, height=400,\n",
        "                                        background_color='white',\n",
        "                                        max_words=50,\n",
        "                                        colormap='viridis').generate(combined_text)\n",
        "\n",
        "                    axes[row, col].imshow(wordcloud, interpolation='bilinear')\n",
        "                    axes[row, col].set_title(f'Word Cloud - {label}')\n",
        "                    axes[row, col].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.features_path}/word_clouds.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def create_interactive_dashboard(self, data):\n",
        "        \"\"\"Create an interactive dashboard using Plotly\"\"\"\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Feature Distribution', 'Sentiment Analysis',\n",
        "                          'Audio Features PCA', 'Text Length vs Audio Energy'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        metadata = data['metadata']\n",
        "        transcripts = data['transcripts']\n",
        "\n",
        "        # 1. Feature distribution (using first few eGeMAPS features)\n",
        "        if data['egemaps_features'] is not None:\n",
        "            features_sample = data['egemaps_features'][:, :5]  # First 5 features\n",
        "\n",
        "            for i in range(min(5, features_sample.shape[1])):\n",
        "                fig.add_trace(\n",
        "                    go.Box(y=features_sample[:, i], name=f'Feature {i+1}'),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "        # 2. Sentiment analysis\n",
        "        sentiment_df = self.analyze_text_sentiment(transcripts)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=sentiment_df['polarity'],\n",
        "                y=sentiment_df['subjectivity'],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    color=[hash(label) for label in sentiment_df['label']],\n",
        "                    size=10,\n",
        "                    opacity=0.7\n",
        "                ),\n",
        "                text=sentiment_df['label'],\n",
        "                name='Sentiment'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. PCA of audio features\n",
        "        if data['spectral_features'] is not None:\n",
        "            from sklearn.decomposition import PCA\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            features_scaled = scaler.fit_transform(data['spectral_features'])\n",
        "\n",
        "            pca = PCA(n_components=2)\n",
        "            pca_result = pca.fit_transform(features_scaled)\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=pca_result[:, 0],\n",
        "                    y=pca_result[:, 1],\n",
        "                    mode='markers',\n",
        "                    marker=dict(\n",
        "                        color=[hash(label) for label in metadata['label']],\n",
        "                        size=8,\n",
        "                        opacity=0.7\n",
        "                    ),\n",
        "                    text=metadata['label'],\n",
        "                    name='PCA'\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "        # 4. Text length vs audio energy correlation\n",
        "        text_lengths = [len(t.split()) if isinstance(t, str) else 0 for t in transcripts['transcript']]\n",
        "\n",
        "        if data['spectral_features'] is not None:\n",
        "            audio_energy = data['spectral_features'][:, 0]  # First spectral feature as proxy for energy\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=text_lengths,\n",
        "                    y=audio_energy,\n",
        "                    mode='markers',\n",
        "                    marker=dict(\n",
        "                        color=[hash(label) for label in metadata['label']],\n",
        "                        size=8,\n",
        "                        opacity=0.7\n",
        "                    ),\n",
        "                    text=metadata['label'],\n",
        "                    name='Text vs Audio'\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title_text=\"Speech Analysis Dashboard\",\n",
        "            showlegend=True,\n",
        "            height=800\n",
        "        )\n",
        "\n",
        "        # Save as HTML\n",
        "        fig.write_html(f'{self.features_path}/interactive_dashboard.html')\n",
        "        fig.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def clustering_analysis(self, data):\n",
        "        \"\"\"Perform clustering analysis on the features\"\"\"\n",
        "\n",
        "        if data['egemaps_features'] is None:\n",
        "            print(\"No eGeMAPS features available for clustering\")\n",
        "            return\n",
        "\n",
        "        # Prepare features for clustering\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "        features_scaled = scaler.fit_transform(data['egemaps_features'])\n",
        "\n",
        "        # Determine optimal number of clusters\n",
        "        silhouette_scores = []\n",
        "        k_range = range(2, min(10, len(features_scaled)))\n",
        "\n",
        "        for k in k_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(features_scaled)\n",
        "            silhouette_avg = silhouette_score(features_scaled, cluster_labels)\n",
        "            silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "        # Find optimal k\n",
        "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "        # Perform clustering with optimal k\n",
        "        kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(features_scaled)\n",
        "\n",
        "        # Visualize clustering results\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot silhouette scores\n",
        "        axes[0].plot(k_range, silhouette_scores, 'bo-')\n",
        "        axes[0].set_xlabel('Number of Clusters')\n",
        "        axes[0].set_ylabel('Silhouette Score')\n",
        "        axes[0].set_title('Optimal Number of Clusters')\n",
        "        axes[0].axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k={optimal_k}')\n",
        "        axes[0].legend()\n",
        "\n",
        "        # Plot clustering results using PCA\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(features_scaled)\n",
        "\n",
        "        scatter = axes[1].scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                                 c=cluster_labels, cmap='viridis', alpha=0.6)\n",
        "        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "        axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "        axes[1].set_title('Clustering Results (PCA visualization)')\n",
        "        plt.colorbar(scatter, ax=axes[1])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.features_path}/clustering_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Create cluster analysis report\n",
        "        cluster_report = pd.DataFrame({\n",
        "            'file': data['metadata']['file'],\n",
        "            'label': data['metadata']['label'],\n",
        "            'category': data['metadata']['category'],\n",
        "            'cluster': cluster_labels\n",
        "        })\n",
        "\n",
        "        cluster_report.to_csv(f'{self.features_path}/cluster_analysis.csv', index=False)\n",
        "\n",
        "        # Print cluster statistics\n",
        "        print(\"\\nCluster Analysis Results:\")\n",
        "        print(f\"Optimal number of clusters: {optimal_k}\")\n",
        "        print(f\"Silhouette score: {silhouette_scores[optimal_k-2]:.3f}\")\n",
        "\n",
        "        print(\"\\nCluster distribution by label:\")\n",
        "        cluster_label_crosstab = pd.crosstab(cluster_report['cluster'], cluster_report['label'])\n",
        "        print(cluster_label_crosstab)\n",
        "\n",
        "        return cluster_report\n",
        "\n",
        "    def comprehensive_feature_analysis(self, data):\n",
        "        \"\"\"Comprehensive analysis of all extracted features\"\"\"\n",
        "\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "\n",
        "        # 1. Feature importance analysis (using correlation with labels)\n",
        "        if data['egemaps_features'] is not None:\n",
        "            # Convert labels to numeric for correlation\n",
        "            label_mapping = {label: i for i, label in enumerate(data['metadata']['label'].unique())}\n",
        "            numeric_labels = [label_mapping[label] for label in data['metadata']['label']]\n",
        "\n",
        "            # Calculate correlation between each feature and the labels\n",
        "            correlations = []\n",
        "            for i in range(data['egemaps_features'].shape[1]):\n",
        "                corr = np.corrcoef(data['egemaps_features'][:, i], numeric_labels)[0, 1]\n",
        "                correlations.append(abs(corr))\n",
        "\n",
        "            # Plot top 20 most correlated features\n",
        "            top_features = np.argsort(correlations)[-20:]\n",
        "            axes[0, 0].barh(range(len(top_features)), [correlations[i] for i in top_features])\n",
        "            axes[0, 0].set_yticks(range(len(top_features)))\n",
        "            axes[0, 0].set_yticklabels([f'Feature {i}' for i in top_features])\n",
        "            axes[0, 0].set_xlabel('Absolute Correlation with Labels')\n",
        "            axes[0, 0].set_title('Top 20 Most Discriminative Features')\n",
        "\n",
        "        # 2. Distribution of transcript lengths by category\n",
        "        transcripts = data['transcripts']\n",
        "        text_lengths = [len(t.split()) if isinstance(t, str) and t != \"Unable to transcribe\" else 0\n",
        "                       for t in transcripts['transcript']]\n",
        "\n",
        "        length_df = pd.DataFrame({\n",
        "            'length': text_lengths,\n",
        "            'label': transcripts['label'],\n",
        "            'category': transcripts['category']\n",
        "        })\n",
        "\n",
        "        sns.violinplot(data=length_df, x='category', y='length', hue='label', ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Transcript Length Distribution')\n",
        "        axes[0, 1].set_ylabel('Number of Words')\n",
        "\n",
        "        # 3. Feature stability analysis (coefficient of variation)\n",
        "        if data['egemaps_features'] is not None:\n",
        "            cv_values = []\n",
        "            for i in range(data['egemaps_features'].shape[1]):\n",
        "                feature_values = data['egemaps_features'][:, i]\n",
        "                cv = np.std(feature_values) / np.mean(feature_values) if np.mean(feature_values) != 0 else 0\n",
        "                cv_values.append(cv)\n",
        "\n",
        "            axes[1, 0].hist(cv_values, bins=30, alpha=0.7, color='skyblue')\n",
        "            axes[1, 0].set_xlabel('Coefficient of Variation')\n",
        "            axes[1, 0].set_ylabel('Number of Features')\n",
        "            axes[1, 0].set_title('Feature Stability Distribution')\n",
        "            axes[1, 0].axvline(x=np.mean(cv_values), color='red', linestyle='--', label=f'Mean CV: {np.mean(cv_values):.2f}')\n",
        "            axes[1, 0].legend()\n",
        "\n",
        "        # 4. Semantic similarity heatmap\n",
        "        # Create TF-IDF vectors for transcripts\n",
        "        valid_transcripts = [t for t in transcripts['transcript']\n",
        "                           if isinstance(t, str) and t != \"Unable to transcribe\" and len(t.strip()) > 0]\n",
        "\n",
        "        if len(valid_transcripts) > 1:\n",
        "            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "            tfidf_matrix = vectorizer.fit_transform(valid_transcripts)\n",
        "\n",
        "            # Calculate similarity matrix\n",
        "            similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
        "\n",
        "            # Plot heatmap (sample if too large)\n",
        "            if similarity_matrix.shape[0] > 50:\n",
        "                indices = np.random.choice(similarity_matrix.shape[0], 50, replace=False)\n",
        "                similarity_matrix = similarity_matrix[np.ix_(indices, indices)]\n",
        "\n",
        "            sns.heatmap(similarity_matrix, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
        "            axes[1, 1].set_title('Transcript Semantic Similarity Matrix')\n",
        "\n",
        "        # 5. Audio-text feature correlation\n",
        "        if data['spectral_features'] is not None:\n",
        "            # Calculate text-based features\n",
        "            text_features = []\n",
        "            for transcript in transcripts['transcript']:\n",
        "                if isinstance(transcript, str) and transcript != \"Unable to transcribe\":\n",
        "                    blob = TextBlob(transcript)\n",
        "                    text_features.append({\n",
        "                        'word_count': len(transcript.split()),\n",
        "                        'char_count': len(transcript),\n",
        "                        'sentence_count': len(blob.sentences),\n",
        "                        'polarity': blob.sentiment.polarity,\n",
        "                        'subjectivity': blob.sentiment.subjectivity\n",
        "                    })\n",
        "                else:\n",
        "                    text_features.append({\n",
        "                        'word_count': 0,\n",
        "                        'char_count': 0,\n",
        "                        'sentence_count': 0,\n",
        "                        'polarity': 0,\n",
        "                        'subjectivity': 0\n",
        "                    })\n",
        "\n",
        "            text_df = pd.DataFrame(text_features)\n",
        "\n",
        "            # Calculate correlations between audio and text features\n",
        "            audio_text_corr = []\n",
        "            for i in range(min(5, data['spectral_features'].shape[1])):  # First 5 audio features\n",
        "                for text_col in text_df.columns:\n",
        "                    corr = np.corrcoef(data['spectral_features'][:, i], text_df[text_col])[0, 1]\n",
        "                    audio_text_corr.append({\n",
        "                        'audio_feature': f'Audio_{i}',\n",
        "                        'text_feature': text_col,\n",
        "                        'correlation': corr\n",
        "                    })\n",
        "\n",
        "            corr_df = pd.DataFrame(audio_text_corr)\n",
        "            corr_pivot = corr_df.pivot(index='audio_feature', columns='text_feature', values='correlation')\n",
        "\n",
        "            sns.heatmap(corr_pivot, annot=True, cmap='coolwarm', center=0, ax=axes[2, 0])\n",
        "            axes[2, 0].set_title('Audio-Text Feature Correlations')\n",
        "\n",
        "        # 6. Classification performance simulation\n",
        "        if data['egemaps_features'] is not None:\n",
        "            from sklearn.model_selection import cross_val_score\n",
        "            from sklearn.ensemble import RandomForestClassifier\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "            # Prepare data for classification\n",
        "            X = data['egemaps_features']\n",
        "            y = data['metadata']['label']\n",
        "\n",
        "            # Encode labels\n",
        "            le = LabelEncoder()\n",
        "            y_encoded = le.fit_transform(y)\n",
        "\n",
        "            # Perform cross-validation\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            cv_scores = cross_val_score(rf, X, y_encoded, cv=5, scoring='accuracy')\n",
        "\n",
        "            # Plot cross-validation scores\n",
        "            axes[2, 1].bar(range(1, 6), cv_scores, alpha=0.7, color='lightcoral')\n",
        "            axes[2, 1].axhline(y=np.mean(cv_scores), color='navy', linestyle='--',\n",
        "                              label=f'Mean CV Score: {np.mean(cv_scores):.3f}')\n",
        "            axes[2, 1].set_xlabel('Fold')\n",
        "            axes[2, 1].set_ylabel('Accuracy')\n",
        "            axes[2, 1].set_title('Cross-Validation Performance')\n",
        "            axes[2, 1].legend()\n",
        "            axes[2, 1].set_ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.features_path}/comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_analysis_report(self, data):\n",
        "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
        "\n",
        "        report = []\n",
        "        report.append(\"# Speech Dataset Analysis Report\\n\")\n",
        "        report.append(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
        "        report.append(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "        # Dataset overview\n",
        "        report.append(\"## Dataset Overview\\n\")\n",
        "        report.append(f\"- Total files processed: {len(data['metadata'])}\\n\")\n",
        "        report.append(f\"- Categories: {data['metadata']['category'].unique()}\\n\")\n",
        "        report.append(f\"- Labels: {data['metadata']['label'].unique()}\\n\")\n",
        "\n",
        "        label_counts = data['metadata']['label'].value_counts()\n",
        "        report.append(\"\\n### Label Distribution:\\n\")\n",
        "        for label, count in label_counts.items():\n",
        "            report.append(f\"- {label}: {count} files\\n\")\n",
        "\n",
        "        # Transcript analysis\n",
        "        report.append(\"\\n## Transcript Analysis\\n\")\n",
        "        transcripts = data['transcripts']\n",
        "\n",
        "        # Count successful transcriptions\n",
        "        successful_transcripts = sum(1 for t in transcripts['transcript']\n",
        "                                   if isinstance(t, str) and t != \"Unable to transcribe\")\n",
        "        report.append(f\"- Successful transcriptions: {successful_transcripts}/{len(transcripts)}\\n\")\n",
        "\n",
        "        # Average transcript length\n",
        "        text_lengths = [len(t.split()) if isinstance(t, str) and t != \"Unable to transcribe\" else 0\n",
        "                       for t in transcripts['transcript']]\n",
        "        report.append(f\"- Average transcript length: {np.mean(text_lengths):.1f} words\\n\")\n",
        "\n",
        "        # Feature analysis\n",
        "        report.append(\"\\n## Feature Analysis\\n\")\n",
        "        if data['egemaps_features'] is not None:\n",
        "            report.append(f\"- eGeMAPS features: {data['egemaps_features'].shape[1]} dimensions\\n\")\n",
        "            report.append(f\"- Feature matrix shape: {data['egemaps_features'].shape}\\n\")\n",
        "\n",
        "        if data['spectral_features'] is not None:\n",
        "            report.append(f\"- Spectral features: {data['spectral_features'].shape[1]} dimensions\\n\")\n",
        "\n",
        "        if data['logmel_features'] is not None:\n",
        "            report.append(f\"- Log-mel features: {data['logmel_features'].shape[1]} dimensions\\n\")\n",
        "\n",
        "        # Sentiment analysis summary\n",
        "        sentiment_df = self.analyze_text_sentiment(transcripts)\n",
        "        report.append(\"\\n## Sentiment Analysis Summary\\n\")\n",
        "        report.append(f\"- Average polarity: {sentiment_df['polarity'].mean():.3f}\\n\")\n",
        "        report.append(f\"- Average subjectivity: {sentiment_df['subjectivity'].mean():.3f}\\n\")\n",
        "\n",
        "        # Group-wise sentiment analysis\n",
        "        for label in sentiment_df['label'].unique():\n",
        "            label_sentiment = sentiment_df[sentiment_df['label'] == label]\n",
        "            report.append(f\"- {label} - Polarity: {label_sentiment['polarity'].mean():.3f}, \"\n",
        "                         f\"Subjectivity: {label_sentiment['subjectivity'].mean():.3f}\\n\")\n",
        "\n",
        "        # Recommendations\n",
        "        report.append(\"\\n## Recommendations\\n\")\n",
        "        report.append(\"1. **Feature Selection**: Consider using feature selection techniques to identify the most discriminative features.\\n\")\n",
        "        report.append(\"2. **Data Augmentation**: Given the limited dataset size, consider audio augmentation techniques.\\n\")\n",
        "        report.append(\"3. **Deep Learning**: Explore deep learning approaches for better feature representation.\\n\")\n",
        "        report.append(\"4. **Multimodal Learning**: Combine audio and text features for improved classification.\\n\")\n",
        "        report.append(\"5. **Cross-validation**: Use proper cross-validation techniques for robust model evaluation.\\n\")\n",
        "\n",
        "        # Save report\n",
        "        with open(f'{self.features_path}/analysis_report.txt', 'w') as f:\n",
        "            f.writelines(report)\n",
        "\n",
        "        print(\"Analysis report generated and saved!\")\n",
        "        print(\"\".join(report))\n",
        "\n",
        "        return report\n",
        "\n",
        "# Usage example:\n",
        "print(\"Enhanced Speech Analyzer ready!\")\n",
        "print(\"To use the enhanced analyzer:\")\n",
        "print(\"1. First run the main analysis script\")\n",
        "print(\"2. Then use the following code:\")\n",
        "print(\"\"\"\n",
        "# Initialize enhanced analyzer\n",
        "enhanced_analyzer = EnhancedSpeechAnalyzer(FEATURES_SAVE_PATH, TRANSCRIPTS_SAVE_PATH)\n",
        "\n",
        "# Load saved data\n",
        "data = enhanced_analyzer.load_saved_data()\n",
        "\n",
        "if data is not None:\n",
        "    # Create word clouds\n",
        "    enhanced_analyzer.create_word_clouds(data['transcripts'])\n",
        "\n",
        "    # Create interactive dashboard\n",
        "    enhanced_analyzer.create_interactive_dashboard(data)\n",
        "\n",
        "    # Perform clustering analysis\n",
        "    cluster_report = enhanced_analyzer.clustering_analysis(data)\n",
        "\n",
        "    # Comprehensive feature analysis\n",
        "    enhanced_analyzer.comprehensive_feature_analysis(data)\n",
        "\n",
        "    # Generate analysis report\n",
        "    enhanced_analyzer.generate_analysis_report(data)\n",
        "else:\n",
        "    print(\"Could not load saved data. Please run the main analysis first.\")\n",
        "\"\"\")"
      ]
    }
  ]
}