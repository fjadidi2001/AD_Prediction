{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPRtlED0di4zvaYsvaKbHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Extractor_Jul10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrkxyBL-91Tk"
      },
      "outputs": [],
      "source": [
        "# Install required packages for speech analysis\n",
        "# Run this cell first before running the main analysis\n",
        "\n",
        "# Install required packages\n",
        "!pip install librosa\n",
        "!pip install SpeechRecognition\n",
        "!pip install opensmile\n",
        "!pip install pydub\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install plotly\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "\n",
        "# Additional setup for audio processing\n",
        "!apt-get update\n",
        "!apt-get install -y portaudio19-dev\n",
        "!apt-get install -y flac\n",
        "\n",
        "# Install additional speech processing libraries\n",
        "!pip install pyaudio\n",
        "!pip install pocketsphinx\n",
        "\n",
        "print(\"All required packages installed successfully!\")\n",
        "print(\"You can now run the main analysis script.\")\n",
        "\n",
        "# Additional utility functions for enhanced analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "class EnhancedSpeechAnalyzer:\n",
        "    \"\"\"Enhanced analyzer with additional visualization and analysis capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, features_path, transcripts_path):\n",
        "        self.features_path = features_path\n",
        "        self.transcripts_path = transcripts_path\n",
        "\n",
        "    def load_saved_data(self):\n",
        "        \"\"\"Load previously saved features and transcripts\"\"\"\n",
        "        try:\n",
        "            # Load transcripts\n",
        "            transcripts_df = pd.read_csv(f'{self.transcripts_path}/transcripts.csv')\n",
        "\n",
        "            # Load features\n",
        "            egemaps_features = np.load(f'{self.features_path}/egemaps_features.npy')\n",
        "            metadata_df = pd.read_csv(f'{self.features_path}/metadata.csv')\n",
        "\n",
        "            try:\n",
        "                logmel_features = np.load(f'{self.features_path}/logmel_features.npy')\n",
        "            except:\n",
        "                logmel_features = None\n",
        "\n",
        "            try:\n",
        "                spectral_features = np.load(f'{self.features_path}/spectral_features.npy')\n",
        "            except:\n",
        "                spectral_features = None\n",
        "\n",
        "            return {\n",
        "                'transcripts': transcripts_df,\n",
        "                'egemaps_features': egemaps_features,\n",
        "                'logmel_features': logmel_features,\n",
        "                'spectral_features': spectral_features,\n",
        "                'metadata': metadata_df\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading saved data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_text_sentiment(self, transcripts_df):\n",
        "        \"\"\"Analyze sentiment of transcripts\"\"\"\n",
        "        sentiments = []\n",
        "\n",
        "        for transcript in transcripts_df['transcript']:\n",
        "            if isinstance(transcript, str) and transcript != \"Unable to transcribe\":\n",
        "                blob = TextBlob(transcript)\n",
        "                sentiments.append({\n",
        "                    'polarity': blob.sentiment.polarity,\n",
        "                    'subjectivity': blob.sentiment.subjectivity\n",
        "                })\n",
        "            else:\n",
        "                sentiments.append({\n",
        "                    'polarity': 0,\n",
        "                    'subjectivity': 0\n",
        "                })\n",
        "\n",
        "        sentiment_df = pd.DataFrame(sentiments)\n",
        "        sentiment_df['label'] = transcripts_df['label']\n",
        "        sentiment_df['category'] = transcripts_df['category']\n",
        "\n",
        "        return sentiment_df\n",
        "\n",
        "    def create_word_clouds(self, transcripts_df):\n",
        "        \"\"\"Create word clouds for different groups\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        labels = transcripts_df['label'].unique()\n",
        "\n",
        "        for i, label in enumerate(labels):\n",
        "            if i < 4:  # Maximum 4 subplots\n",
        "                row = i // 2\n",
        "                col = i % 2\n",
        "\n",
        "                # Get transcripts for this label\n",
        "                label_transcripts = transcripts_df[transcripts_df['label'] == label]['transcript']\n",
        "\n",
        "                # Combine all transcripts for this label\n",
        "                combined_text = ' '.join([t for t in label_transcripts if isinstance(t, str) and t != \"Unable to transcribe\"])\n",
        "\n",
        "                if combined_text:\n",
        "                    wordcloud = WordCloud(width=800, height=400,\n",
        "                                        background_color='white',\n",
        "                                        max_words=50,\n",
        "                                        colormap='viridis').generate(combined_text)\n",
        "\n",
        "                    axes[row, col].imshow(wordcloud, interpolation='bilinear')\n",
        "                    axes[row, col].set_title(f'Word Cloud - {label}')\n",
        "                    axes[row, col].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.features_path}/word_clouds.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def create_interactive_dashboard(self, data):\n",
        "        \"\"\"Create an interactive dashboard using Plotly\"\"\"\n",
        "\n",
        "        # Create subplots\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Feature Distribution', 'Sentiment Analysis',\n",
        "                          'Audio Features PCA', 'Text Length vs Audio Energy'),\n",
        "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        "        )\n",
        "\n",
        "        metadata = data['metadata']\n",
        "        transcripts = data['transcripts']\n",
        "\n",
        "        # 1. Feature distribution (using first few eGeMAPS features)\n",
        "        if data['egemaps_features'] is not None:\n",
        "            features_sample = data['egemaps_features'][:, :5]  # First 5 features\n",
        "\n",
        "            for i in range(min(5, features_sample.shape[1])):\n",
        "                fig.add_trace(\n",
        "                    go.Box(y=features_sample[:, i], name=f'Feature {i+1}'),\n",
        "                    row=1, col=1\n",
        "                )\n",
        "\n",
        "        # 2. Sentiment analysis\n",
        "        sentiment_df = self.analyze_text_sentiment(transcripts)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=sentiment_df['polarity'],\n",
        "                y=sentiment_df['subjectivity'],\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    color=[hash(label) for label in sentiment_df['label']],\n",
        "                    size=10,\n",
        "                    opacity=0.7\n",
        "                ),\n",
        "                text=sentiment_df['label'],\n",
        "                name='Sentiment'\n",
        "            ),\n",
        "            row=1, col=2\n",
        "        )\n",
        "\n",
        "        # 3. PCA of audio features\n",
        "        if data['spectral_features'] is not None:\n",
        "            from sklearn.decomposition import PCA\n",
        "            from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            features_scaled = scaler.fit_transform(data['spectral_features'])\n",
        "\n",
        "            pca = PCA(n_components=2)\n",
        "            pca_result = pca.fit_transform(features_scaled)\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=pca_result[:, 0],\n",
        "                    y=pca_result[:, 1],\n",
        "                    mode='markers',\n",
        "                    marker=dict(\n",
        "                        color=[hash(label) for label in metadata['label']],\n",
        "                        size=8,\n",
        "                        opacity=0.7\n",
        "                    ),\n",
        "                    text=metadata['label'],\n",
        "                    name='PCA'\n",
        "                ),\n",
        "                row=2, col=1\n",
        "            )\n",
        "\n",
        "        # 4. Text length vs audio energy correlation\n",
        "        text_lengths = [len(t.split()) if isinstance(t, str) else 0 for t in transcripts['transcript']]\n",
        "\n",
        "        if data['spectral_features'] is not None:\n",
        "            audio_energy = data['spectral_features'][:, 0]  # First spectral feature as proxy for energy\n",
        "\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=text_lengths,\n",
        "                    y=audio_energy,\n",
        "                    mode='markers',\n",
        "                    marker=dict(\n",
        "                        color=[hash(label) for label in metadata['label']],\n",
        "                        size=8,\n",
        "                        opacity=0.7\n",
        "                    ),\n",
        "                    text=metadata['label'],\n",
        "                    name='Text vs Audio'\n",
        "                ),\n",
        "                row=2, col=2\n",
        "            )\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title_text=\"Speech Analysis Dashboard\",\n",
        "            showlegend=True,\n",
        "            height=800\n",
        "        )\n",
        "\n",
        "        # Save as HTML\n",
        "        fig.write_html(f'{self.features_path}/interactive_dashboard.html')\n",
        "        fig.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def clustering_analysis(self, data):\n",
        "        \"\"\"Perform clustering analysis on the features\"\"\"\n",
        "\n",
        "        if data['egemaps_features'] is None:\n",
        "            print(\"No eGeMAPS features available for clustering\")\n",
        "            return\n",
        "\n",
        "        # Prepare features for clustering\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "        scaler = StandardScaler()\n",
        "        features_scaled = scaler.fit_transform(data['egemaps_features'])\n",
        "\n",
        "        # Determine optimal number of clusters\n",
        "        silhouette_scores = []\n",
        "        k_range = range(2, min(10, len(features_scaled)))\n",
        "\n",
        "        for k in k_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(features_scaled)\n",
        "            silhouette_avg = silhouette_score(features_scaled, cluster_labels)\n",
        "            silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "        # Find optimal k\n",
        "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "        # Perform clustering with optimal k\n",
        "        kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "        cluster_labels = kmeans.fit_predict(features_scaled)\n",
        "\n",
        "        # Visualize clustering results\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Plot silhouette scores\n",
        "        axes[0].plot(k_range, silhouette_scores, 'bo-')\n",
        "        axes[0].set_xlabel('Number of Clusters')\n",
        "        axes[0].set_ylabel('Silhouette Score')\n",
        "        axes[0].set_title('Optimal Number of Clusters')\n",
        "        axes[0].axvline(x=optimal_k, color='r', linestyle='--', label=f'Optimal k={optimal_k}')\n",
        "        axes[0].legend()\n",
        "\n",
        "        # Plot clustering results using PCA\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        pca_result = pca.fit_transform(features_scaled)\n",
        "\n",
        "        scatter = axes[1].scatter(pca_result[:, 0], pca_result[:, 1],\n",
        "                                 c=cluster_labels, cmap='viridis', alpha=0.6)\n",
        "        axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "        axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "        axes[1].set_title('Clustering Results (PCA visualization)')\n",
        "        plt.colorbar(scatter, ax=axes[1])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.features_path}/clustering_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Create cluster analysis report\n",
        "        cluster_report = pd.DataFrame({\n",
        "            'file': data['metadata']['file'],\n",
        "            'label': data['metadata']['label'],\n",
        "            'category': data['metadata']['category'],\n",
        "            'cluster': cluster_labels\n",
        "        })\n",
        "\n",
        "        cluster_report.to_csv(f'{self.features_path}/cluster_analysis.csv', index=False)\n",
        "\n",
        "        # Print cluster statistics\n",
        "        print(\"\\nCluster Analysis Results:\")\n",
        "        print(f\"Optimal number of clusters: {optimal_k}\")\n",
        "        print(f\"Silhouette score: {silhouette_scores[optimal_k-2]:.3f}\")\n",
        "\n",
        "        print(\"\\nCluster distribution by label:\")\n",
        "        cluster_label_crosstab = pd.crosstab(cluster_report['cluster'], cluster_report['label'])\n",
        "        print(cluster_label_crosstab)\n",
        "\n",
        "        return cluster_report\n",
        "\n",
        "    def comprehensive_feature_analysis(self, data):\n",
        "        \"\"\"Comprehensive analysis of all extracted features\"\"\"\n",
        "\n",
        "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "\n",
        "        # 1. Feature importance analysis (using correlation with labels)\n",
        "        if data['egemaps_features'] is not None:\n",
        "            # Convert labels to numeric for correlation\n",
        "            label_mapping = {label: i for i, label in enumerate(data['metadata']['label'].unique())}\n",
        "            numeric_labels = [label_mapping[label] for label in data['metadata']['label']]\n",
        "\n",
        "            # Calculate correlation between each feature and the labels\n",
        "            correlations = []\n",
        "            for i in range(data['egemaps_features'].shape[1]):\n",
        "                corr = np.corrcoef(data['egemaps_features'][:, i], numeric_labels)[0, 1]\n",
        "                correlations.append(abs(corr))\n",
        "\n",
        "            # Plot top 20 most correlated features\n",
        "            top_features = np.argsort(correlations)[-20:]\n",
        "            axes[0, 0].barh(range(len(top_features)), [correlations[i] for i in top_features])\n",
        "            axes[0, 0].set_yticks(range(len(top_features)))\n",
        "            axes[0, 0].set_yticklabels([f'Feature {i}' for i in top_features])\n",
        "            axes[0, 0].set_xlabel('Absolute Correlation with Labels')\n",
        "            axes[0, 0].set_title('Top 20 Most Discriminative Features')\n",
        "\n",
        "        # 2. Distribution of transcript lengths by category\n",
        "        transcripts = data['transcripts']\n",
        "        text_lengths = [len(t.split()) if isinstance(t, str) and t != \"Unable to transcribe\" else 0\n",
        "                       for t in transcripts['transcript']]\n",
        "\n",
        "        length_df = pd.DataFrame({\n",
        "            'length': text_lengths,\n",
        "            'label': transcripts['label'],\n",
        "            'category': transcripts['category']\n",
        "        })\n",
        "\n",
        "        sns.violinplot(data=length_df, x='category', y='length', hue='label', ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Transcript Length Distribution')\n",
        "        axes[0, 1].set_ylabel('Number of Words')\n",
        "\n",
        "        # 3. Feature stability analysis (coefficient of variation)\n",
        "        if data['egemaps_features'] is not None:\n",
        "            cv_values = []\n",
        "            for i in range(data['egemaps_features'].shape[1]):\n",
        "                feature_values = data['egemaps_features'][:, i]\n",
        "                cv = np.std(feature_values) / np.mean(feature_values) if np.mean(feature_values) != 0 else 0\n",
        "                cv_values.append(cv)\n",
        "\n",
        "            axes[1, 0].hist(cv_values, bins=30, alpha=0.7, color='skyblue')\n",
        "            axes[1, 0].set_xlabel('Coefficient of Variation')\n",
        "            axes[1, 0].set_ylabel('Number of Features')\n",
        "            axes[1, 0].set_title('Feature Stability Distribution')\n",
        "            axes[1, 0].axvline(x=np.mean(cv_values), color='red', linestyle='--', label=f'Mean CV: {np.mean(cv_values):.2f}')\n",
        "            axes[1, 0].legend()\n",
        "\n",
        "        # 4. Semantic similarity heatmap\n",
        "        # Create TF-IDF vectors for transcripts\n",
        "        valid_transcripts = [t for t in transcripts['transcript']\n",
        "                           if isinstance(t, str) and t != \"Unable to transcribe\" and len(t.strip()) > 0]\n",
        "\n",
        "        if len(valid_transcripts) > 1:\n",
        "            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
        "            tfidf_matrix = vectorizer.fit_transform(valid_transcripts)\n",
        "\n",
        "            # Calculate similarity matrix\n",
        "            similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
        "\n",
        "            # Plot heatmap (sample if too large)\n",
        "            if similarity_matrix.shape[0] > 50:\n",
        "                indices = np.random.choice(similarity_matrix.shape[0], 50, replace=False)\n",
        "                similarity_matrix = similarity_matrix[np.ix_(indices, indices)]\n",
        "\n",
        "            sns.heatmap(similarity_matrix, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
        "            axes[1, 1].set_title('Transcript Semantic Similarity Matrix')\n",
        "\n",
        "        # 5. Audio-text feature correlation\n",
        "        if data['spectral_features'] is not None:\n",
        "            # Calculate text-based features\n",
        "            text_features = []\n",
        "            for transcript in transcripts['transcript']:\n",
        "                if isinstance(transcript, str) and transcript != \"Unable to transcribe\":\n",
        "                    blob = TextBlob(transcript)\n",
        "                    text_features.append({\n",
        "                        'word_count': len(transcript.split()),\n",
        "                        'char_count': len(transcript),\n",
        "                        'sentence_count': len(blob.sentences),\n",
        "                        'polarity': blob.sentiment.polarity,\n",
        "                        'subjectivity': blob.sentiment.subjectivity\n",
        "                    })\n",
        "                else:\n",
        "                    text_features.append({\n",
        "                        'word_count': 0,\n",
        "                        'char_count': 0,\n",
        "                        'sentence_count': 0,\n",
        "                        'polarity': 0,\n",
        "                        'subjectivity': 0\n",
        "                    })\n",
        "\n",
        "            text_df = pd.DataFrame(text_features)\n",
        "\n",
        "            # Calculate correlations between audio and text features\n",
        "            audio_text_corr = []\n",
        "            for i in range(min(5, data['spectral_features'].shape[1])):  # First 5 audio features\n",
        "                for text_col in text_df.columns:\n",
        "                    corr = np.corrcoef(data['spectral_features'][:, i], text_df[text_col])[0, 1]\n",
        "                    audio_text_corr.append({\n",
        "                        'audio_feature': f'Audio_{i}',\n",
        "                        'text_feature': text_col,\n",
        "                        'correlation': corr\n",
        "                    })\n",
        "\n",
        "            corr_df = pd.DataFrame(audio_text_corr)\n",
        "            corr_pivot = corr_df.pivot(index='audio_feature', columns='text_feature', values='correlation')\n",
        "\n",
        "            sns.heatmap(corr_pivot, annot=True, cmap='coolwarm', center=0, ax=axes[2, 0])\n",
        "            axes[2, 0].set_title('Audio-Text Feature Correlations')\n",
        "\n",
        "        # 6. Classification performance simulation\n",
        "        if data['egemaps_features'] is not None:\n",
        "            from sklearn.model_selection import cross_val_score\n",
        "            from sklearn.ensemble import RandomForestClassifier\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "            # Prepare data for classification\n",
        "            X = data['egemaps_features']\n",
        "            y = data['metadata']['label']\n",
        "\n",
        "            # Encode labels\n",
        "            le = LabelEncoder()\n",
        "            y_encoded = le.fit_transform(y)\n",
        "\n",
        "            # Perform cross-validation\n",
        "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "            cv_scores = cross_val_score(rf, X, y_encoded, cv=5, scoring='accuracy')\n",
        "\n",
        "            # Plot cross-validation scores\n",
        "            axes[2, 1].bar(range(1, 6), cv_scores, alpha=0.7, color='lightcoral')\n",
        "            axes[2, 1].axhline(y=np.mean(cv_scores), color='navy', linestyle='--',\n",
        "                              label=f'Mean CV Score: {np.mean(cv_scores):.3f}')\n",
        "            axes[2, 1].set_xlabel('Fold')\n",
        "            axes[2, 1].set_ylabel('Accuracy')\n",
        "            axes[2, 1].set_title('Cross-Validation Performance')\n",
        "            axes[2, 1].legend()\n",
        "            axes[2, 1].set_ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.features_path}/comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_analysis_report(self, data):\n",
        "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
        "\n",
        "        report = []\n",
        "        report.append(\"# Speech Dataset Analysis Report\\n\")\n",
        "        report.append(f\"Generated on: {pd.Timestamp.now()}\\n\")\n",
        "        report.append(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "        # Dataset overview\n",
        "        report.append(\"## Dataset Overview\\n\")\n",
        "        report.append(f\"- Total files processed: {len(data['metadata'])}\\n\")\n",
        "        report.append(f\"- Categories: {data['metadata']['category'].unique()}\\n\")\n",
        "        report.append(f\"- Labels: {data['metadata']['label'].unique()}\\n\")\n",
        "\n",
        "        label_counts = data['metadata']['label'].value_counts()\n",
        "        report.append(\"\\n### Label Distribution:\\n\")\n",
        "        for label, count in label_counts.items():\n",
        "            report.append(f\"- {label}: {count} files\\n\")\n",
        "\n",
        "        # Transcript analysis\n",
        "        report.append(\"\\n## Transcript Analysis\\n\")\n",
        "        transcripts = data['transcripts']\n",
        "\n",
        "        # Count successful transcriptions\n",
        "        successful_transcripts = sum(1 for t in transcripts['transcript']\n",
        "                                   if isinstance(t, str) and t != \"Unable to transcribe\")\n",
        "        report.append(f\"- Successful transcriptions: {successful_transcripts}/{len(transcripts)}\\n\")\n",
        "\n",
        "        # Average transcript length\n",
        "        text_lengths = [len(t.split()) if isinstance(t, str) and t != \"Unable to transcribe\" else 0\n",
        "                       for t in transcripts['transcript']]\n",
        "        report.append(f\"- Average transcript length: {np.mean(text_lengths):.1f} words\\n\")\n",
        "\n",
        "        # Feature analysis\n",
        "        report.append(\"\\n## Feature Analysis\\n\")\n",
        "        if data['egemaps_features'] is not None:\n",
        "            report.append(f\"- eGeMAPS features: {data['egemaps_features'].shape[1]} dimensions\\n\")\n",
        "            report.append(f\"- Feature matrix shape: {data['egemaps_features'].shape}\\n\")\n",
        "\n",
        "        if data['spectral_features'] is not None:\n",
        "            report.append(f\"- Spectral features: {data['spectral_features'].shape[1]} dimensions\\n\")\n",
        "\n",
        "        if data['logmel_features'] is not None:\n",
        "            report.append(f\"- Log-mel features: {data['logmel_features'].shape[1]} dimensions\\n\")\n",
        "\n",
        "        # Sentiment analysis summary\n",
        "        sentiment_df = self.analyze_text_sentiment(transcripts)\n",
        "        report.append(\"\\n## Sentiment Analysis Summary\\n\")\n",
        "        report.append(f\"- Average polarity: {sentiment_df['polarity'].mean():.3f}\\n\")\n",
        "        report.append(f\"- Average subjectivity: {sentiment_df['subjectivity'].mean():.3f}\\n\")\n",
        "\n",
        "        # Group-wise sentiment analysis\n",
        "        for label in sentiment_df['label'].unique():\n",
        "            label_sentiment = sentiment_df[sentiment_df['label'] == label]\n",
        "            report.append(f\"- {label} - Polarity: {label_sentiment['polarity'].mean():.3f}, \"\n",
        "                         f\"Subjectivity: {label_sentiment['subjectivity'].mean():.3f}\\n\")\n",
        "\n",
        "        # Recommendations\n",
        "        report.append(\"\\n## Recommendations\\n\")\n",
        "        report.append(\"1. **Feature Selection**: Consider using feature selection techniques to identify the most discriminative features.\\n\")\n",
        "        report.append(\"2. **Data Augmentation**: Given the limited dataset size, consider audio augmentation techniques.\\n\")\n",
        "        report.append(\"3. **Deep Learning**: Explore deep learning approaches for better feature representation.\\n\")\n",
        "        report.append(\"4. **Multimodal Learning**: Combine audio and text features for improved classification.\\n\")\n",
        "        report.append(\"5. **Cross-validation**: Use proper cross-validation techniques for robust model evaluation.\\n\")\n",
        "\n",
        "        # Save report\n",
        "        with open(f'{self.features_path}/analysis_report.txt', 'w') as f:\n",
        "            f.writelines(report)\n",
        "\n",
        "        print(\"Analysis report generated and saved!\")\n",
        "        print(\"\".join(report))\n",
        "\n",
        "        return report\n",
        "\n",
        "# Usage example:\n",
        "print(\"Enhanced Speech Analyzer ready!\")\n",
        "print(\"To use the enhanced analyzer:\")\n",
        "print(\"1. First run the main analysis script\")\n",
        "print(\"2. Then use the following code:\")\n",
        "print(\"\"\"\n",
        "# Initialize enhanced analyzer\n",
        "enhanced_analyzer = EnhancedSpeechAnalyzer(FEATURES_SAVE_PATH, TRANSCRIPTS_SAVE_PATH)\n",
        "\n",
        "# Load saved data\n",
        "data = enhanced_analyzer.load_saved_data()\n",
        "\n",
        "if data is not None:\n",
        "    # Create word clouds\n",
        "    enhanced_analyzer.create_word_clouds(data['transcripts'])\n",
        "\n",
        "    # Create interactive dashboard\n",
        "    enhanced_analyzer.create_interactive_dashboard(data)\n",
        "\n",
        "    # Perform clustering analysis\n",
        "    cluster_report = enhanced_analyzer.clustering_analysis(data)\n",
        "\n",
        "    # Comprehensive feature analysis\n",
        "    enhanced_analyzer.comprehensive_feature_analysis(data)\n",
        "\n",
        "    # Generate analysis report\n",
        "    enhanced_analyzer.generate_analysis_report(data)\n",
        "else:\n",
        "    print(\"Could not load saved data. Please run the main analysis first.\")\n",
        "\"\"\")"
      ]
    }
  ]
}