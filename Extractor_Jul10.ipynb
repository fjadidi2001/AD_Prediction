{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1aBgNCWh5xbIcUTl7jKnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Extractor_Jul10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile opensmile speechbrain transformers torch openai-whisper\n",
        "!pip install pandas numpy matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import whisper\n",
        "import opensmile\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "class ADReSSoAnalyzer:\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Speech\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content\"\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "\n",
        "        # Initialize feature extractors\n",
        "        self.smile = opensmile.Smile(\n",
        "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "            feature_level=opensmile.FeatureLevel.Functionals,\n",
        "        )\n",
        "\n",
        "        # Initialize Whisper for transcription\n",
        "        self.whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "        # Initialize Wav2Vec2\n",
        "        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "        # Initialize BERT\n",
        "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Get all audio files from the dataset\"\"\"\n",
        "        audio_files = {\n",
        "            'diagnosis_ad': [],\n",
        "            'diagnosis_cn': [],\n",
        "            'progression_decline': [],\n",
        "            'progression_no_decline': [],\n",
        "            'progression_test': []\n",
        "        }\n",
        "\n",
        "        # Diagnosis files - extracted-diagnosis-train\n",
        "        diag_ad_path = f\"{self.base_path}/extracted-diagnosis-train/audio/ad\"\n",
        "        diag_cn_path = f\"{self.base_path}/extracted-diagnosis-train/audio/cn\"\n",
        "\n",
        "        if os.path.exists(diag_ad_path):\n",
        "            audio_files['diagnosis_ad'] = [f\"{diag_ad_path}/{f}\" for f in os.listdir(diag_ad_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(diag_cn_path):\n",
        "            audio_files['diagnosis_cn'] = [f\"{diag_cn_path}/{f}\" for f in os.listdir(diag_cn_path) if f.endswith('.wav')]\n",
        "\n",
        "        # Progression files - extracted-progression-train\n",
        "        prog_decline_path = f\"{self.base_path}/extracted-progression-train/audio/decline\"\n",
        "        prog_no_decline_path = f\"{self.base_path}/extracted-progression-train/audio/no-decline\"\n",
        "\n",
        "        if os.path.exists(prog_decline_path):\n",
        "            audio_files['progression_decline'] = [f\"{prog_decline_path}/{f}\" for f in os.listdir(prog_decline_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(prog_no_decline_path):\n",
        "            audio_files['progression_no_decline'] = [f\"{prog_no_decline_path}/{f}\" for f in os.listdir(prog_no_decline_path) if f.endswith('.wav')]\n",
        "\n",
        "        # Progression test files - extracted-progression-test\n",
        "        prog_test_path = f\"{self.base_path}/extracted-progression-test/audio\"\n",
        "\n",
        "        if os.path.exists(prog_test_path):\n",
        "            audio_files['progression_test'] = [f\"{prog_test_path}/{f}\" for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def extract_acoustic_features(self, audio_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract all acoustic features from audio file\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Load audio - resample to 16kHz for Wav2Vec2 compatibility\n",
        "            y, sr = librosa.load(audio_path, sr=16000)  # Force 16kHz sampling rate\n",
        "\n",
        "            # 1. eGeMAPS features using openSMILE\n",
        "            try:\n",
        "                features['egemaps'] = self.smile.process_file(audio_path).values.flatten()\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: eGeMAPS extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['egemaps'] = np.zeros(88)  # Default eGeMAPS feature size\n",
        "\n",
        "            # 2. MFCC features\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.mean(mfccs, axis=1),\n",
        "                    'std': np.std(mfccs, axis=1),\n",
        "                    'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                    'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: MFCC extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.zeros(13),\n",
        "                    'std': np.zeros(13),\n",
        "                    'delta': np.zeros(13),\n",
        "                    'delta2': np.zeros(13)\n",
        "                }\n",
        "\n",
        "            # 3. Log-mel spectrogram\n",
        "            try:\n",
        "                mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "                log_mel = librosa.power_to_db(mel_spec)\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.mean(log_mel, axis=1),\n",
        "                    'std': np.std(log_mel, axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Log-mel extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.zeros(80),\n",
        "                    'std': np.zeros(80)\n",
        "                }\n",
        "\n",
        "            # 4. Wav2Vec2 features - with proper sampling rate handling\n",
        "            try:\n",
        "                # Ensure sampling rate is exactly 16000 Hz for Wav2Vec2\n",
        "                if len(y) == 0:\n",
        "                    raise ValueError(\"Empty audio signal\")\n",
        "\n",
        "                input_values = self.wav2vec_processor(\n",
        "                    y,\n",
        "                    sampling_rate=16000,  # Explicitly set to 16000\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Wav2Vec2 extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['wav2vec2'] = np.zeros(768)  # Default Wav2Vec2 feature size\n",
        "\n",
        "            # 5. Additional prosodic features\n",
        "            try:\n",
        "                # Handle potential issues with F0 extraction\n",
        "                f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "                f0_clean = f0[f0 > 0]  # Remove unvoiced frames\n",
        "\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                    'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                    'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                    'duration': len(y) / sr\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Prosodic feature extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                    'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                    'duration': 0.0\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
        "            features = None\n",
        "\n",
        "        return features\n",
        "\n",
        "    def show_acoustic_features(self, sample_file: str):\n",
        "        \"\"\"Display acoustic features for a sample file\"\"\"\n",
        "        features = self.extract_acoustic_features(sample_file)\n",
        "\n",
        "        if features is None:\n",
        "            print(f\"Could not extract features from {sample_file}\")\n",
        "            return\n",
        "\n",
        "        print(f\"=== Acoustic Features for {os.path.basename(sample_file)} ===\\n\")\n",
        "\n",
        "        # eGeMAPS\n",
        "        print(f\"1. eGeMAPS Features: {len(features['egemaps'])} features\")\n",
        "        print(f\"   Shape: {features['egemaps'].shape}\")\n",
        "        print(f\"   Sample values: {features['egemaps'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # MFCCs\n",
        "        print(\"2. MFCC Features:\")\n",
        "        print(f\"   Mean: {features['mfccs']['mean'].shape} - {features['mfccs']['mean'][:5]}\")\n",
        "        print(f\"   Std: {features['mfccs']['std'].shape} - {features['mfccs']['std'][:5]}\")\n",
        "        print(f\"   Delta: {features['mfccs']['delta'].shape} - {features['mfccs']['delta'][:5]}\")\n",
        "        print(f\"   Delta-Delta: {features['mfccs']['delta2'].shape} - {features['mfccs']['delta2'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Log-mel\n",
        "        print(\"3. Log-Mel Spectrogram Features:\")\n",
        "        print(f\"   Mean: {features['log_mel']['mean'].shape} - {features['log_mel']['mean'][:5]}\")\n",
        "        print(f\"   Std: {features['log_mel']['std'].shape} - {features['log_mel']['std'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Wav2Vec2\n",
        "        print(f\"4. Wav2Vec2 Features: {features['wav2vec2'].shape}\")\n",
        "        print(f\"   Sample values: {features['wav2vec2'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Prosodic\n",
        "        print(\"5. Prosodic Features:\")\n",
        "        for key, value in features['prosodic'].items():\n",
        "            print(f\"   {key}: {value:.4f}\")\n",
        "        print()\n",
        "\n",
        "    def extract_transcripts(self, audio_files: Dict[str, List[str]]) -> Dict[str, str]:\n",
        "        \"\"\"Extract transcripts using Whisper\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(\"Extracting transcripts...\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "            for file_path in files:\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    print(f\"  Transcribing {filename}...\")\n",
        "\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "                    transcript_text = result[\"text\"].strip()\n",
        "\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': transcript_text,\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def save_transcripts(self, transcripts: Dict[str, str]):\n",
        "        \"\"\"Save transcripts to files\"\"\"\n",
        "        os.makedirs(f\"{self.output_path}/transcripts\", exist_ok=True)\n",
        "\n",
        "        # Save individual transcript files\n",
        "        for key, data in transcripts.items():\n",
        "            filename = f\"{key}_transcript.txt\"\n",
        "            filepath = f\"{self.output_path}/transcripts/{filename}\"\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(data['transcript'])\n",
        "\n",
        "        # Save consolidated JSON\n",
        "        with open(f\"{self.output_path}/transcripts/all_transcripts.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save as pickle for easy loading\n",
        "        with open(f\"{self.output_path}/transcripts/transcripts.pkl\", 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "\n",
        "        print(f\"Transcripts saved to {self.output_path}/transcripts/\")\n",
        "\n",
        "    def create_transcript_table(self, transcripts: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"Create a DataFrame with transcript information\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for key, info in transcripts.items():\n",
        "            data.append({\n",
        "                'File_ID': key,\n",
        "                'Category': info['category'],\n",
        "                'Filename': info['filename'],\n",
        "                'Transcript_Length': len(info['transcript']),\n",
        "                'Word_Count': len(info['transcript'].split()) if info['transcript'] else 0,\n",
        "                'Language': info.get('language', 'N/A'),\n",
        "                'Segments': info.get('segments', 'N/A'),\n",
        "                'Has_Error': 'error' in info,\n",
        "                'Transcript_Preview': info['transcript'][:100] + \"...\" if len(info['transcript']) > 100 else info['transcript']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Save the table\n",
        "        df.to_csv(f\"{self.output_path}/transcript_summary.csv\", index=False)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def extract_linguistic_features(self, transcripts: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract linguistic features for BERT preparation\"\"\"\n",
        "        linguistic_features = {}\n",
        "\n",
        "        print(\"Extracting linguistic features...\")\n",
        "\n",
        "        for key, data in transcripts.items():\n",
        "            transcript = data['transcript']\n",
        "\n",
        "            if not transcript:\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': '',\n",
        "                    'word_count': 0,\n",
        "                    'sentence_count': 0,\n",
        "                    'avg_word_length': 0,\n",
        "                    'bert_tokens': [],\n",
        "                    'bert_input_ids': [],\n",
        "                    'bert_attention_mask': []\n",
        "                }\n",
        "                continue\n",
        "\n",
        "            # Basic linguistic features\n",
        "            words = transcript.split()\n",
        "            sentences = transcript.split('.')\n",
        "\n",
        "            # BERT tokenization\n",
        "            bert_encoding = self.bert_tokenizer(\n",
        "                transcript,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            linguistic_features[key] = {\n",
        "                'raw_text': transcript,\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'bert_tokens': self.bert_tokenizer.tokenize(transcript),\n",
        "                'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                'bert_encoding': bert_encoding\n",
        "            }\n",
        "\n",
        "        # Save linguistic features\n",
        "        with open(f\"{self.output_path}/linguistic_features.pkl\", 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "\n",
        "        return linguistic_features\n",
        "\n",
        "    def verify_dataset_structure(self):\n",
        "        \"\"\"Verify that the dataset structure matches expectations\"\"\"\n",
        "        print(\"=== Dataset Structure Verification ===\\n\")\n",
        "\n",
        "        expected_paths = [\n",
        "            f\"{self.base_path}/extracted-diagnosis-train/audio/ad\",\n",
        "            f\"{self.base_path}/extracted-diagnosis-train/audio/cn\",\n",
        "            f\"{self.base_path}/extracted-progression-train/audio/decline\",\n",
        "            f\"{self.base_path}/extracted-progression-train/audio/no-decline\",\n",
        "            f\"{self.base_path}/extracted-progression-test/audio\"\n",
        "        ]\n",
        "\n",
        "        for path in expected_paths:\n",
        "            if os.path.exists(path):\n",
        "                wav_files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
        "                print(f\"✓ {path}: {len(wav_files)} .wav files found\")\n",
        "            else:\n",
        "                print(f\"✗ {path}: Directory not found\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"=== ADReSSo21 Speech Analysis Pipeline ===\\n\")\n",
        "\n",
        "        # Step 0: Verify dataset structure\n",
        "        self.verify_dataset_structure()\n",
        "\n",
        "        # Step 1: Get audio files\n",
        "        print(\"Step 1: Getting audio files...\")\n",
        "        audio_files = self.get_audio_files()\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        print(f\"Found {total_files} audio files across all categories\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"  {category}: {len(files)} files\")\n",
        "\n",
        "        if total_files == 0:\n",
        "            print(\"No audio files found. Please check the dataset path.\")\n",
        "            return\n",
        "\n",
        "        # Step 2: Show acoustic features for sample files\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 2: Demonstrating acoustic features...\")\n",
        "\n",
        "        # Show features for one file from each category that has files\n",
        "        for category, files in audio_files.items():\n",
        "            if files:\n",
        "                print(f\"\\nShowing features for {category}:\")\n",
        "                self.show_acoustic_features(files[0])\n",
        "                break  # Just show one example to avoid too much output\n",
        "\n",
        "        # Step 3: Extract transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 3: Extracting transcripts...\")\n",
        "        transcripts = self.extract_transcripts(audio_files)\n",
        "\n",
        "        # Step 4: Save transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 4: Saving transcripts...\")\n",
        "        self.save_transcripts(transcripts)\n",
        "\n",
        "        # Step 5: Create transcript table\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 5: Creating transcript table...\")\n",
        "        transcript_df = self.create_transcript_table(transcripts)\n",
        "\n",
        "        print(\"Transcript Summary Table:\")\n",
        "        print(transcript_df.to_string(index=False))\n",
        "\n",
        "        # Step 6: Extract linguistic features for BERT\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 6: Extracting linguistic features for BERT...\")\n",
        "        linguistic_features = self.extract_linguistic_features(transcripts)\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "        print(f\"Results saved to: {self.output_path}\")\n",
        "        print(\"\\nOutput files:\")\n",
        "        print(f\"  - Transcripts: {self.output_path}/transcripts/\")\n",
        "        print(f\"  - Transcript summary: {self.output_path}/transcript_summary.csv\")\n",
        "        print(f\"  - Linguistic features: {self.output_path}/linguistic_features.pkl\")\n",
        "\n",
        "        return {\n",
        "            'audio_files': audio_files,\n",
        "            'transcripts': transcripts,\n",
        "            'transcript_df': transcript_df,\n",
        "            'linguistic_features': linguistic_features\n",
        "        }\n",
        "\n",
        "# Usage example:\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the analyzer with your dataset path\n",
        "    analyzer = ADReSSoAnalyzer(base_path=\"/content/drive/MyDrive/Speech\")\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    results = analyzer.run_complete_pipeline()\n",
        "\n",
        "    # You can also run individual steps if needed:\n",
        "    # analyzer.verify_dataset_structure()\n",
        "    # audio_files = analyzer.get_audio_files()\n",
        "    # transcripts = analyzer.extract_transcripts(audio_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM36ytOQNemq",
        "outputId": "4395e813-ccbb-46aa-9b68-770947e5844b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSrELWasQ5SG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}