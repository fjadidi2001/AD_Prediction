{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoPGOTRktmsaoUob6LwlAt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Jul16_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwXqBVYNiGMm",
        "outputId": "34d82982-ccda-4cac-f667-5281ee548858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Collecting audobject>=0.6.1 (from opensmile)\n",
            "  Downloading audobject-0.7.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting audinterface>=0.7.0 (from opensmile)\n",
            "  Downloading audinterface-1.3.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Collecting audeer>=2.1.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audeer-2.2.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audformat-1.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audiofile-1.5.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audmath-1.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting asttokens>=2.0.0 (from audobject>=0.6.1->opensmile)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (8.7.0)\n",
            "Collecting oyaml (from audobject>=0.6.1->opensmile)\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting iso639-lang (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso639_lang-2.6.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting iso3166 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.23.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.17.0)\n",
            "Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audinterface-1.3.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audobject-0.7.12-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading audeer-2.2.2-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audformat-1.3.2-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audiofile-1.5.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audmath-1.4.2-py3-none-any.whl (23 kB)\n",
            "Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading iso639_lang-2.6.1-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.9/324.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=d17c410da5147d5de691718ee34d4cffcda6a91fc7f8d73d7ceb4bd2de54fa4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/d2/9a/801b5cc5b2a1af2e280089b71c326711a682fc1d50ea29d0ed\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: ruamel.yaml.clib, oyaml, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, iso639-lang, iso3166, audresample, audmath, audeer, asttokens, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, audobject, nvidia-cusolver-cu12, hyperpyyaml, audiofile, audformat, openai-whisper, audinterface, speechbrain, opensmile\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed asttokens-3.0.0 audeer-2.2.2 audformat-1.3.2 audinterface-1.3.1 audiofile-1.5.1 audmath-1.4.2 audobject-0.7.12 audresample-1.3.4 hyperpyyaml-1.2.2 iso3166-2.1.1 iso639-lang-2.6.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625 opensmile-2.5.1 oyaml-1.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.7.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch-geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install librosa soundfile opensmile speechbrain transformers torch openai-whisper\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import opensmile\n",
        "import whisper\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor, Wav2Vec2Model,\n",
        "    BertTokenizer, BertModel,\n",
        "    ViTModel, ViTFeatureExtractor\n",
        ")\n",
        "\n",
        "# Graph networks\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "\n",
        "# ML utilities\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Visualization\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ADReSSoAnalyzer:\n",
        "    \"\"\"Complete ADReSSo analysis pipeline with error handling and checkpoints\"\"\"\n",
        "\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content/drive/MyDrive/ADReSSo_Results\"\n",
        "        self.checkpoint_path = f\"{self.output_path}/checkpoints\"\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(self.output_path, exist_ok=True)\n",
        "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
        "        os.makedirs(f\"{self.output_path}/visualizations\", exist_ok=True)\n",
        "\n",
        "        # Initialize containers\n",
        "        self.audio_files = {}\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "        self.linguistic_features = {}\n",
        "\n",
        "        # Initialize models\n",
        "        self.initialize_models()\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize all required models\"\"\"\n",
        "        print(\"Initializing models...\")\n",
        "\n",
        "        try:\n",
        "            # Initialize openSMILE\n",
        "            self.smile = opensmile.Smile(\n",
        "                feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "                feature_level=opensmile.FeatureLevel.Functionals,\n",
        "            )\n",
        "            print(\"✓ OpenSMILE initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ OpenSMILE initialization failed: {e}\")\n",
        "            self.smile = None\n",
        "\n",
        "        try:\n",
        "            # Initialize Whisper\n",
        "            self.whisper_model = whisper.load_model(\"base\")\n",
        "            print(\"✓ Whisper model loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Whisper initialization failed: {e}\")\n",
        "            self.whisper_model = None\n",
        "\n",
        "        try:\n",
        "            # Initialize Wav2Vec2\n",
        "            self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "            self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "            print(\"✓ Wav2Vec2 models loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Wav2Vec2 initialization failed: {e}\")\n",
        "            self.wav2vec_processor = None\n",
        "            self.wav2vec_model = None\n",
        "\n",
        "        try:\n",
        "            # Initialize BERT\n",
        "            self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "            print(\"✓ BERT models loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ BERT initialization failed: {e}\")\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "\n",
        "    def save_checkpoint(self, data: Any, filename: str, step: str):\n",
        "        \"\"\"Save checkpoint data\"\"\"\n",
        "        filepath = f\"{self.checkpoint_path}/{filename}\"\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    pickle.dump(data, f)\n",
        "            elif filename.endswith('.json'):\n",
        "                with open(filepath, 'w') as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            elif filename.endswith('.csv'):\n",
        "                if isinstance(data, pd.DataFrame):\n",
        "                    data.to_csv(filepath, index=False)\n",
        "                else:\n",
        "                    pd.DataFrame(data).to_csv(filepath, index=False)\n",
        "\n",
        "            print(f\"✓ Checkpoint saved: {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Failed to save checkpoint {filename}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_checkpoint(self, filename: str):\n",
        "        \"\"\"Load checkpoint data\"\"\"\n",
        "        filepath = f\"{self.checkpoint_path}/{filename}\"\n",
        "\n",
        "        if not os.path.exists(filepath):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            elif filename.endswith('.json'):\n",
        "                with open(filepath, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            elif filename.endswith('.csv'):\n",
        "                return pd.read_csv(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Failed to load checkpoint {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def step_1_get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Step 1: Get all audio files from the dataset\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 1: GETTING AUDIO FILES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step1_audio_files.json\"\n",
        "        audio_files = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if audio_files is not None:\n",
        "            print(\"✓ Loaded audio files from checkpoint\")\n",
        "            self.audio_files = audio_files\n",
        "            return audio_files\n",
        "\n",
        "        audio_files = {\n",
        "            'diagnosis_ad': [],\n",
        "            'diagnosis_cn': [],\n",
        "            'progression_decline': [],\n",
        "            'progression_no_decline': [],\n",
        "            'progression_test': []\n",
        "        }\n",
        "\n",
        "        # Define paths\n",
        "        paths = {\n",
        "            'diagnosis_ad': f\"{self.base_path}/diagnosis/train/audio/ad\",\n",
        "            'diagnosis_cn': f\"{self.base_path}/diagnosis/train/audio/cn\",\n",
        "            'progression_decline': f\"{self.base_path}/progression/train/audio/decline\",\n",
        "            'progression_no_decline': f\"{self.base_path}/progression/train/audio/no_decline\",\n",
        "            'progression_test': f\"{self.base_path}/progression/test-dist/audio\"\n",
        "        }\n",
        "\n",
        "        # Collect files\n",
        "        for category, path in paths.items():\n",
        "            if os.path.exists(path):\n",
        "                files = [f\"{path}/{f}\" for f in os.listdir(path) if f.endswith('.wav')]\n",
        "                audio_files[category] = files\n",
        "                print(f\"✓ Found {len(files)} files in {category}\")\n",
        "            else:\n",
        "                print(f\"⚠ Path not found: {path}\")\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        print(f\"\\nTotal audio files found: {total_files}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(audio_files, checkpoint_file, \"step1\")\n",
        "        self.audio_files = audio_files\n",
        "\n",
        "        # Visualize file distribution\n",
        "        self.visualize_file_distribution(audio_files)\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def visualize_file_distribution(self, audio_files: Dict[str, List[str]]):\n",
        "        \"\"\"Visualize audio file distribution\"\"\"\n",
        "        categories = list(audio_files.keys())\n",
        "        counts = [len(files) for files in audio_files.values()]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bars = plt.bar(categories, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "        plt.title('Audio File Distribution by Category', fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('Category', fontsize=12)\n",
        "        plt.ylabel('Number of Files', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, count in zip(bars, counts):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                    str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/file_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_2_extract_acoustic_features(self, limit_per_category: int = None):\n",
        "        \"\"\"Step 2: Extract acoustic features from audio files\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 2: EXTRACTING ACOUSTIC FEATURES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step2_acoustic_features.pkl\"\n",
        "        features = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if features is not None:\n",
        "            print(\"✓ Loaded acoustic features from checkpoint\")\n",
        "            self.features = features\n",
        "            return features\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        for category, files in self.audio_files.items():\n",
        "            if not files:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "\n",
        "            # Limit files if specified\n",
        "            if limit_per_category:\n",
        "                files = files[:limit_per_category]\n",
        "\n",
        "            for file_path in tqdm(files, desc=f\"Extracting features for {category}\"):\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    file_key = f\"{category}_{filename}\"\n",
        "\n",
        "                    # Extract features\n",
        "                    file_features = self.extract_acoustic_features_from_file(file_path)\n",
        "\n",
        "                    if file_features is not None:\n",
        "                        features[file_key] = {\n",
        "                            'file_path': file_path,\n",
        "                            'category': category,\n",
        "                            'filename': filename,\n",
        "                            **file_features\n",
        "                        }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠ Error processing {filename}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        print(f\"\\n✓ Extracted features from {len(features)} files\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(features, checkpoint_file, \"step2\")\n",
        "        self.features = features\n",
        "\n",
        "        # Visualize features\n",
        "        self.visualize_acoustic_features(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_acoustic_features_from_file(self, audio_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features from a single audio file\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                return None\n",
        "\n",
        "            # 1. eGeMAPS features\n",
        "            if self.smile is not None:\n",
        "                try:\n",
        "                    egemaps = self.smile.process_file(audio_path).values.flatten()\n",
        "                    features['egemaps'] = egemaps\n",
        "                except Exception as e:\n",
        "                    features['egemaps'] = np.zeros(88)\n",
        "            else:\n",
        "                features['egemaps'] = np.zeros(88)\n",
        "\n",
        "            # 2. MFCC features\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.mean(mfccs, axis=1),\n",
        "                    'std': np.std(mfccs, axis=1),\n",
        "                    'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                    'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.zeros(13), 'std': np.zeros(13),\n",
        "                    'delta': np.zeros(13), 'delta2': np.zeros(13)\n",
        "                }\n",
        "\n",
        "            # 3. Mel spectrogram\n",
        "            try:\n",
        "                mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "                log_mel = librosa.power_to_db(mel_spec)\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.mean(log_mel, axis=1),\n",
        "                    'std': np.std(log_mel, axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.zeros(80), 'std': np.zeros(80)\n",
        "                }\n",
        "\n",
        "            # 4. Wav2Vec2 features\n",
        "            if self.wav2vec_processor is not None and self.wav2vec_model is not None:\n",
        "                try:\n",
        "                    input_values = self.wav2vec_processor(\n",
        "                        y, sampling_rate=16000, return_tensors=\"pt\"\n",
        "                    ).input_values\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                    features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "                except Exception as e:\n",
        "                    features['wav2vec2'] = np.zeros(768)\n",
        "            else:\n",
        "                features['wav2vec2'] = np.zeros(768)\n",
        "\n",
        "            # 5. Prosodic features\n",
        "            try:\n",
        "                f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "                f0_clean = f0[f0 > 0]\n",
        "\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                    'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                    'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                    'duration': len(y) / sr\n",
        "                }\n",
        "            except Exception as e:\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                    'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                    'duration': 0.0\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio file: {e}\")\n",
        "            return None\n",
        "\n",
        "        return features\n",
        "\n",
        "    def visualize_acoustic_features(self, features: Dict[str, Any]):\n",
        "        \"\"\"Visualize acoustic features\"\"\"\n",
        "        if not features:\n",
        "            return\n",
        "\n",
        "        # Sample file for visualization\n",
        "        sample_key = list(features.keys())[0]\n",
        "        sample_features = features[sample_key]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Acoustic Features Visualization - {sample_key}', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # eGeMAPS\n",
        "        axes[0, 0].plot(sample_features['egemaps'][:20])\n",
        "        axes[0, 0].set_title('eGeMAPS Features (first 20)')\n",
        "        axes[0, 0].set_xlabel('Feature Index')\n",
        "        axes[0, 0].set_ylabel('Value')\n",
        "\n",
        "        # MFCC\n",
        "        mfcc_mean = sample_features['mfccs']['mean']\n",
        "        axes[0, 1].plot(mfcc_mean, marker='o')\n",
        "        axes[0, 1].set_title('MFCC Mean')\n",
        "        axes[0, 1].set_xlabel('MFCC Coefficient')\n",
        "        axes[0, 1].set_ylabel('Value')\n",
        "\n",
        "        # Mel spectrogram\n",
        "        mel_mean = sample_features['log_mel']['mean']\n",
        "        axes[0, 2].plot(mel_mean)\n",
        "        axes[0, 2].set_title('Log-Mel Spectrogram Mean')\n",
        "        axes[0, 2].set_xlabel('Mel Bin')\n",
        "        axes[0, 2].set_ylabel('Value')\n",
        "\n",
        "        # Wav2Vec2\n",
        "        axes[1, 0].plot(sample_features['wav2vec2'][:50])\n",
        "        axes[1, 0].set_title('Wav2Vec2 Features (first 50)')\n",
        "        axes[1, 0].set_xlabel('Feature Index')\n",
        "        axes[1, 0].set_ylabel('Value')\n",
        "\n",
        "        # Prosodic features\n",
        "        prosodic = sample_features['prosodic']\n",
        "        prosodic_names = list(prosodic.keys())\n",
        "        prosodic_values = list(prosodic.values())\n",
        "\n",
        "        axes[1, 1].bar(prosodic_names, prosodic_values)\n",
        "        axes[1, 1].set_title('Prosodic Features')\n",
        "        axes[1, 1].set_ylabel('Value')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Feature distribution by category\n",
        "        categories = {}\n",
        "        for key, feature_data in features.items():\n",
        "            category = feature_data['category']\n",
        "            if category not in categories:\n",
        "                categories[category] = []\n",
        "            categories[category].append(feature_data['prosodic']['duration'])\n",
        "\n",
        "        for category, durations in categories.items():\n",
        "            axes[1, 2].hist(durations, alpha=0.7, label=category, bins=20)\n",
        "\n",
        "        axes[1, 2].set_title('Duration Distribution by Category')\n",
        "        axes[1, 2].set_xlabel('Duration (seconds)')\n",
        "        axes[1, 2].set_ylabel('Frequency')\n",
        "        axes[1, 2].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/acoustic_features.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_3_extract_transcripts(self, limit_per_category: int = None):\n",
        "        \"\"\"Step 3: Extract transcripts using Whisper\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 3: EXTRACTING TRANSCRIPTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step3_transcripts.json\"\n",
        "        transcripts = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if transcripts is not None:\n",
        "            print(\"✓ Loaded transcripts from checkpoint\")\n",
        "            self.transcripts = transcripts\n",
        "            return transcripts\n",
        "\n",
        "        if self.whisper_model is None:\n",
        "            print(\"⚠ Whisper model not available, skipping transcript extraction\")\n",
        "            return {}\n",
        "\n",
        "        transcripts = {}\n",
        "\n",
        "        for category, files in self.audio_files.items():\n",
        "            if not files:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "\n",
        "            # Limit files if specified\n",
        "            if limit_per_category:\n",
        "                files = files[:limit_per_category]\n",
        "\n",
        "            for file_path in tqdm(files, desc=f\"Transcribing {category}\"):\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    file_key = f\"{category}_{filename}\"\n",
        "\n",
        "                    # Transcribe\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "\n",
        "                    transcripts[file_key] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': result[\"text\"].strip(),\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠ Error transcribing {filename}: {e}\")\n",
        "                    transcripts[file_key] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        print(f\"\\n✓ Extracted transcripts from {len(transcripts)} files\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(transcripts, checkpoint_file, \"step3\")\n",
        "        self.transcripts = transcripts\n",
        "\n",
        "        # Visualize transcripts\n",
        "        self.visualize_transcripts(transcripts)\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def visualize_transcripts(self, transcripts: Dict[str, Any]):\n",
        "        \"\"\"Visualize transcript statistics\"\"\"\n",
        "        if not transcripts:\n",
        "            return\n",
        "\n",
        "        # Prepare data\n",
        "        data = []\n",
        "        for key, info in transcripts.items():\n",
        "            transcript = info.get('transcript', '')\n",
        "            data.append({\n",
        "                'category': info['category'],\n",
        "                'word_count': len(transcript.split()) if transcript else 0,\n",
        "                'char_count': len(transcript),\n",
        "                'has_error': 'error' in info\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Transcript Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Word count distribution\n",
        "        df.boxplot(column='word_count', by='category', ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Word Count Distribution by Category')\n",
        "        axes[0, 0].set_ylabel('Word Count')\n",
        "\n",
        "        # Character count distribution\n",
        "        df.boxplot(column='char_count', by='category', ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Character Count Distribution by Category')\n",
        "        axes[0, 1].set_ylabel('Character Count')\n",
        "\n",
        "        # Error rate by category\n",
        "        error_rate = df.groupby('category')['has_error'].mean()\n",
        "        axes[1, 0].bar(error_rate.index, error_rate.values)\n",
        "        axes[1, 0].set_title('Error Rate by Category')\n",
        "        axes[1, 0].set_ylabel('Error Rate')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Average metrics by category\n",
        "        avg_metrics = df.groupby('category')[['word_count', 'char_count']].mean()\n",
        "        avg_metrics.plot(kind='bar', ax=axes[1, 1])\n",
        "        axes[1, 1].set_title('Average Metrics by Category')\n",
        "        axes[1, 1].set_ylabel('Count')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/transcript_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_4_extract_linguistic_features(self):\n",
        "        \"\"\"Step 4: Extract linguistic features for BERT\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4: EXTRACTING LINGUISTIC FEATURES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step4_linguistic_features.pkl\"\n",
        "        linguistic_features = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if linguistic_features is not None:\n",
        "            print(\"✓ Loaded linguistic features from checkpoint\")\n",
        "            self.linguistic_features = linguistic_features\n",
        "            return linguistic_features\n",
        "\n",
        "        if self.bert_tokenizer is None:\n",
        "            print(\"⚠ BERT tokenizer not available, skipping linguistic feature extraction\")\n",
        "            return {}\n",
        "\n",
        "        linguistic_features = {}\n",
        "\n",
        "        print(\"Processing transcripts for linguistic features...\")\n",
        "\n",
        "        for key, data in tqdm(self.transcripts.items(), desc=\"Extracting linguistic features\"):\n",
        "            transcript = data.get('transcript', '')\n",
        "\n",
        "            if not transcript:\n",
        "                linguistic_features[key] = self.create_empty_linguistic_features()\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Basic linguistic features\n",
        "                words = transcript.split()\n",
        "                sentences = [s.strip() for s in transcript.split('.') if s.strip()]\n",
        "\n",
        "                # BERT tokenization\n",
        "                bert_encoding = self.bert_tokenizer(\n",
        "                    transcript,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    max_length=512,\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': transcript,\n",
        "                    'word_count': len(words),\n",
        "                    'sentence_count': len(sentences),\n",
        "                    'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                    'unique_words': len(set(words)),\n",
        "                    'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                    'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                    'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                    'category': data['category']\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Error processing {key}: {e}\")\n",
        "                linguistic_features[key] = self.create_empty_linguistic_features()\n",
        "\n",
        "        print(f\"\\n✓ Extracted linguistic features from {len(linguistic_features)} files\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(linguistic_features, checkpoint_file, \"step4\")\n",
        "        self.linguistic_features = linguistic_features\n",
        "\n",
        "        # Visualize linguistic features\n",
        "        self.visualize_linguistic_features(linguistic_features)\n",
        "\n",
        "        return linguistic_features\n",
        "\n",
        "    def create_empty_linguistic_features(self):\n",
        "        \"\"\"Create empty linguistic features structure\"\"\"\n",
        "        return {\n",
        "            'raw_text': '',\n",
        "            'word_count': 0,\n",
        "            'sentence_count': 0,\n",
        "            'avg_word_length': 0,\n",
        "            'unique_words': 0,\n",
        "            'lexical_diversity': 0,\n",
        "            'bert_input_ids': [0] * 512,\n",
        "            'bert_attention_mask': [0] * 512,\n",
        "            'category': 'unknown'\n",
        "        }\n",
        "\n",
        "    def visualize_linguistic_features(self, linguistic_features: Dict[str, Any]):\n",
        "        \"\"\"Visualize linguistic features\"\"\"\n",
        "        if not linguistic_features:\n",
        "            return\n",
        "\n",
        "        # Prepare data\n",
        "        data = []\n",
        "        for key, features in linguistic_features.items():\n",
        "            data.append({\n",
        "                'key': key,\n",
        "                'category': features['category'],\n",
        "                'word_count': features['word_count'],\n",
        "                'sentence_count': features['sentence_count'],\n",
        "                'avg_word_length': features['avg_word_length'],\n",
        "                'unique_words': features['unique_words'],\n",
        "                'lexical_diversity': features['lexical_diversity']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Linguistic Features Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Metrics by category\n",
        "        metrics = ['word_count', 'sentence_count', 'avg_word_length', 'unique_words', 'lexical_diversity']\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            row = i // 3\n",
        "            col = i % 3\n",
        "\n",
        "            if row < 2 and col < 3:\n",
        "                df.boxplot(column=metric, by='category', ax=axes[row, col])\n",
        "                axes[row, col].set_title(f'{metric.replace(\"_\", \" \").title()} by Category')\n",
        "                axes[row, col].set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "\n",
        "        # Correlation heatmap\n",
        "        numeric_df = df.select_dtypes(include=[np.number])\n",
        "        correlation_matrix = numeric_df.corr()\n",
        "\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n",
        "        axes[1, 2].set_title('Feature Correlation Matrix')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/linguistic_features.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def run_complete_pipeline(self, limit_per_category: int = None):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"ADRESSO21 COMPLETE ANALYSIS PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Step 1: Get audio files\n",
        "        results['audio_files'] = self.step_1_get_audio_files()\n",
        "\n",
        "        # Step 2: Extract acoustic features\n",
        "        results['acoustic_features'] = self.step_2_extract_acoustic_features(limit_per_category)\n",
        "\n",
        "        # Step 3: Extract transcripts\n",
        "        results['transcripts'] = self.step_3_extract_transcripts(limit_per_category)\n",
        "\n",
        "        # Step 4: Extract linguistic features\n",
        "        results['linguistic_features'] = self.step_4_extract_linguistic_features()\n",
        "\n",
        "        # Generate final summary\n",
        "        self.generate_final_summary(results)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Results saved to: {self.output_path}\")\n",
        "        print(f\"Checkpoints saved to: {self.checkpoint_path}\")\n",
        "        print(f\"Visualizations saved to: {self.output_path}/visualizations\")\n",
        "\n",
        "        return results\n",
        ""
      ],
      "metadata": {
        "id": "ovqJXJ7Ljyi4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_summary(self, results: Dict[str, Any]):\n",
        "        \"\"\"Generate final summary report\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"GENERATING FINAL SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        summary = {\n",
        "            'total_audio_files': sum(len(files) for files in results['audio_files'].values()),\n",
        "            'processed_features': len(results['acoustic_features']),\n",
        "            'transcripts_extracted': len(results['transcripts']),\n",
        "            'linguistic_features_extracted': len(results['linguistic_features']),\n",
        "            'categories': list(results['audio_files'].keys()),\n",
        "            'feature_dimensions': self.get_feature_dimensions(),\n",
        "            'processing_errors': self.count_processing_errors(results)\n",
        "        }\n",
        "\n",
        "        # Save summary\n",
        "        self.save_checkpoint(summary, \"final_summary.json\", \"summary\")\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n📊 FINAL SUMMARY REPORT:\")\n",
        "        print(\"-\" * 40)\n",
        "        for key, value in summary.items():\n",
        "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def get_feature_dimensions(self):\n",
        "        \"\"\"Get dimensions of extracted features\"\"\"\n",
        "        if not self.features:\n",
        "            return {}\n",
        "\n",
        "        sample_key = list(self.features.keys())[0]\n",
        "        sample_features = self.features[sample_key]\n",
        "\n",
        "        dimensions = {\n",
        "            'egemaps': len(sample_features.get('egemaps', [])),\n",
        "            'mfcc_mean': len(sample_features.get('mfccs', {}).get('mean', [])),\n",
        "            'log_mel_mean': len(sample_features.get('log_mel', {}).get('mean', [])),\n",
        "            'wav2vec2': len(sample_features.get('wav2vec2', [])),\n",
        "            'prosodic': len(sample_features.get('prosodic', {}))\n",
        "        }\n",
        "\n",
        "        return dimensions\n",
        "\n",
        "    def count_processing_errors(self, results: Dict[str, Any]):\n",
        "        \"\"\"Count processing errors across all steps\"\"\"\n",
        "        errors = {\n",
        "            'acoustic_errors': 0,\n",
        "            'transcript_errors': 0,\n",
        "            'linguistic_errors': 0\n",
        "        }\n",
        "\n",
        "        # Count transcript errors\n",
        "        for key, data in results['transcripts'].items():\n",
        "            if 'error' in data:\n",
        "                errors['transcript_errors'] += 1\n",
        "\n",
        "        # Count linguistic errors (empty features)\n",
        "        for key, data in results['linguistic_features'].items():\n",
        "            if data.get('word_count', 0) == 0 and data.get('raw_text', ''):\n",
        "                errors['linguistic_errors'] += 1\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def step_5_create_multimodal_dataset(self):\n",
        "        \"\"\"Step 5: Create unified multimodal dataset\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 5: CREATING MULTIMODAL DATASET\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step5_multimodal_dataset.pkl\"\n",
        "        dataset = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if dataset is not None:\n",
        "            print(\"✓ Loaded multimodal dataset from checkpoint\")\n",
        "            return dataset\n",
        "\n",
        "        dataset = []\n",
        "\n",
        "        print(\"Creating unified multimodal dataset...\")\n",
        "\n",
        "        # Get common keys across all feature types\n",
        "        common_keys = set(self.features.keys()) & set(self.transcripts.keys()) & set(self.linguistic_features.keys())\n",
        "\n",
        "        for key in tqdm(common_keys, desc=\"Creating multimodal samples\"):\n",
        "            try:\n",
        "                # Get features\n",
        "                acoustic_features = self.features[key]\n",
        "                transcript_data = self.transcripts[key]\n",
        "                linguistic_features = self.linguistic_features[key]\n",
        "\n",
        "                # Create unified sample\n",
        "                sample = {\n",
        "                    'id': key,\n",
        "                    'category': acoustic_features['category'],\n",
        "                    'file_path': acoustic_features['file_path'],\n",
        "                    'filename': acoustic_features['filename'],\n",
        "\n",
        "                    # Acoustic features\n",
        "                    'acoustic': self.flatten_acoustic_features(acoustic_features),\n",
        "\n",
        "                    # Text features\n",
        "                    'transcript': transcript_data.get('transcript', ''),\n",
        "                    'linguistic': linguistic_features,\n",
        "\n",
        "                    # Labels\n",
        "                    'diagnosis_label': 1 if 'ad' in acoustic_features['category'] else 0,\n",
        "                    'progression_label': 1 if 'decline' in acoustic_features['category'] else 0,\n",
        "                }\n",
        "\n",
        "                dataset.append(sample)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Error creating sample for {key}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n✓ Created multimodal dataset with {len(dataset)} samples\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(dataset, checkpoint_file, \"step5\")\n",
        "\n",
        "        # Visualize dataset\n",
        "        self.visualize_multimodal_dataset(dataset)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def flatten_acoustic_features(self, features: Dict[str, Any]):\n",
        "        \"\"\"Flatten acoustic features into a single vector\"\"\"\n",
        "        flattened = []\n",
        "\n",
        "        # eGeMAPS\n",
        "        if 'egemaps' in features:\n",
        "            flattened.extend(features['egemaps'])\n",
        "\n",
        "        # MFCC\n",
        "        if 'mfccs' in features:\n",
        "            mfcc_data = features['mfccs']\n",
        "            flattened.extend(mfcc_data.get('mean', []))\n",
        "            flattened.extend(mfcc_data.get('std', []))\n",
        "            flattened.extend(mfcc_data.get('delta', []))\n",
        "            flattened.extend(mfcc_data.get('delta2', []))\n",
        "\n",
        "        # Log-Mel\n",
        "        if 'log_mel' in features:\n",
        "            mel_data = features['log_mel']\n",
        "            flattened.extend(mel_data.get('mean', []))\n",
        "            flattened.extend(mel_data.get('std', []))\n",
        "\n",
        "        # Wav2Vec2\n",
        "        if 'wav2vec2' in features:\n",
        "            flattened.extend(features['wav2vec2'])\n",
        "\n",
        "        # Prosodic\n",
        "        if 'prosodic' in features:\n",
        "            prosodic_data = features['prosodic']\n",
        "            flattened.extend(list(prosodic_data.values()))\n",
        "\n",
        "        return np.array(flattened, dtype=np.float32)\n",
        "\n",
        "    def visualize_multimodal_dataset(self, dataset: List[Dict[str, Any]]):\n",
        "        \"\"\"Visualize multimodal dataset statistics\"\"\"\n",
        "        if not dataset:\n",
        "            return\n",
        "\n",
        "        # Prepare data\n",
        "        categories = [sample['category'] for sample in dataset]\n",
        "        diagnosis_labels = [sample['diagnosis_label'] for sample in dataset]\n",
        "        progression_labels = [sample['progression_label'] for sample in dataset]\n",
        "        acoustic_dims = [len(sample['acoustic']) for sample in dataset]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Multimodal Dataset Overview', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Category distribution\n",
        "        category_counts = pd.Series(categories).value_counts()\n",
        "        axes[0, 0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "        axes[0, 0].set_title('Category Distribution')\n",
        "\n",
        "        # Label distributions\n",
        "        axes[0, 1].bar(['Control', 'AD'], [sum(1-np.array(diagnosis_labels)), sum(diagnosis_labels)])\n",
        "        axes[0, 1].set_title('Diagnosis Label Distribution')\n",
        "        axes[0, 1].set_ylabel('Count')\n",
        "\n",
        "        # Acoustic feature dimensions\n",
        "        axes[1, 0].hist(acoustic_dims, bins=20, edgecolor='black')\n",
        "        axes[1, 0].set_title('Acoustic Feature Dimensions')\n",
        "        axes[1, 0].set_xlabel('Dimension')\n",
        "        axes[1, 0].set_ylabel('Count')\n",
        "\n",
        "        # Sample acoustic features correlation\n",
        "        if len(dataset) > 0:\n",
        "            sample_acoustic = dataset[0]['acoustic'][:50]  # First 50 features\n",
        "            axes[1, 1].plot(sample_acoustic, marker='o', markersize=3)\n",
        "            axes[1, 1].set_title('Sample Acoustic Features (first 50)')\n",
        "            axes[1, 1].set_xlabel('Feature Index')\n",
        "            axes[1, 1].set_ylabel('Value')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/multimodal_dataset.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_6_create_graph_structures(self, dataset: List[Dict[str, Any]]):\n",
        "        \"\"\"Step 6: Create graph structures for graph neural networks\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 6: CREATING GRAPH STRUCTURES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step6_graph_structures.pkl\"\n",
        "        graph_data = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if graph_data is not None:\n",
        "            print(\"✓ Loaded graph structures from checkpoint\")\n",
        "            return graph_data\n",
        "\n",
        "        graph_data = []\n",
        "\n",
        "        print(\"Creating graph structures...\")\n",
        "\n",
        "        for sample in tqdm(dataset, desc=\"Creating graphs\"):\n",
        "            try:\n",
        "                # Create feature-based graph\n",
        "                acoustic_features = sample['acoustic']\n",
        "\n",
        "                # Create nodes (features as nodes)\n",
        "                num_nodes = min(len(acoustic_features), 100)  # Limit for computational efficiency\n",
        "                node_features = acoustic_features[:num_nodes].reshape(-1, 1)\n",
        "\n",
        "                # Create edges based on feature similarity\n",
        "                edges = self.create_feature_similarity_edges(acoustic_features[:num_nodes])\n",
        "\n",
        "                # Create PyTorch Geometric data object\n",
        "                graph = Data(\n",
        "                    x=torch.tensor(node_features, dtype=torch.float),\n",
        "                    edge_index=torch.tensor(edges, dtype=torch.long).t().contiguous(),\n",
        "                    y=torch.tensor([sample['diagnosis_label']], dtype=torch.long),\n",
        "                    sample_id=sample['id']\n",
        "                )\n",
        "\n",
        "                graph_data.append(graph)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Error creating graph for {sample['id']}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n✓ Created {len(graph_data)} graph structures\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(graph_data, checkpoint_file, \"step6\")\n",
        "\n",
        "        # Visualize graphs\n",
        "        self.visualize_graph_structures(graph_data[:5])  # Visualize first 5 graphs\n",
        "\n",
        "        return graph_data\n",
        "\n",
        "    def create_feature_similarity_edges(self, features: np.ndarray, threshold: float = 0.7):\n",
        "        \"\"\"Create edges based on feature similarity\"\"\"\n",
        "        edges = []\n",
        "\n",
        "        # Create similarity matrix\n",
        "        similarity_matrix = np.corrcoef(features.reshape(1, -1), features.reshape(1, -1))\n",
        "\n",
        "        # Create edges for similar features\n",
        "        for i in range(len(features)):\n",
        "            for j in range(i+1, len(features)):\n",
        "                if abs(features[i] - features[j]) < threshold:\n",
        "                    edges.append([i, j])\n",
        "                    edges.append([j, i])  # Undirected graph\n",
        "\n",
        "        # If no edges, create a simple chain\n",
        "        if not edges:\n",
        "            for i in range(len(features)-1):\n",
        "                edges.append([i, i+1])\n",
        "                edges.append([i+1, i])\n",
        "\n",
        "        return edges\n",
        "\n",
        "    def visualize_graph_structures(self, graph_data: List[Data]):\n",
        "        \"\"\"Visualize graph structures\"\"\"\n",
        "        if not graph_data:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, min(3, len(graph_data)), figsize=(15, 5))\n",
        "        if len(graph_data) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        fig.suptitle('Graph Structure Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "        for i, graph in enumerate(graph_data[:3]):\n",
        "            ax = axes[i] if len(graph_data) > 1 else axes[0]\n",
        "\n",
        "            # Convert to NetworkX for visualization\n",
        "            G = nx.Graph()\n",
        "            edge_index = graph.edge_index.numpy()\n",
        "\n",
        "            # Add nodes\n",
        "            for node in range(graph.x.shape[0]):\n",
        "                G.add_node(node)\n",
        "\n",
        "            # Add edges\n",
        "            for edge in edge_index.T:\n",
        "                G.add_edge(edge[0], edge[1])\n",
        "\n",
        "            # Draw graph\n",
        "            pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "            nx.draw(G, pos, ax=ax, node_size=30, node_color='lightblue',\n",
        "                   edge_color='gray', alpha=0.7, with_labels=False)\n",
        "\n",
        "            ax.set_title(f'Graph {i+1}\\nNodes: {graph.x.shape[0]}, Edges: {graph.edge_index.shape[1]}')\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/graph_structures.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_7_prepare_training_data(self, dataset: List[Dict[str, Any]]):\n",
        "        \"\"\"Step 7: Prepare data for training\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 7: PREPARING TRAINING DATA\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step7_training_data.pkl\"\n",
        "        training_data = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if training_data is not None:\n",
        "            print(\"✓ Loaded training data from checkpoint\")\n",
        "            return training_data\n",
        "\n",
        "        print(\"Preparing training data...\")\n",
        "\n",
        "        # Separate by task\n",
        "        diagnosis_data = []\n",
        "        progression_data = []\n",
        "\n",
        "        for sample in dataset:\n",
        "            category = sample['category']\n",
        "\n",
        "            if 'diagnosis' in category:\n",
        "                diagnosis_data.append(sample)\n",
        "            elif 'progression' in category:\n",
        "                progression_data.append(sample)\n",
        "\n",
        "        # Prepare diagnosis task data\n",
        "        diagnosis_train_data = self.prepare_task_data(diagnosis_data, 'diagnosis')\n",
        "\n",
        "        # Prepare progression task data\n",
        "        progression_train_data = self.prepare_task_data(progression_data, 'progression')\n",
        "\n",
        "        training_data = {\n",
        "            'diagnosis': diagnosis_train_data,\n",
        "            'progression': progression_train_data,\n",
        "            'combined': dataset\n",
        "        }\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(training_data, checkpoint_file, \"step7\")\n",
        "\n",
        "        # Visualize training data\n",
        "        self.visualize_training_data(training_data)\n",
        "\n",
        "        return training_data\n",
        "\n",
        "    def prepare_task_data(self, data: List[Dict[str, Any]], task: str):\n",
        "        \"\"\"Prepare data for specific task\"\"\"\n",
        "        if not data:\n",
        "            return None\n",
        "\n",
        "        # Extract features and labels\n",
        "        X_acoustic = np.array([sample['acoustic'] for sample in data])\n",
        "        X_linguistic = np.array([sample['linguistic']['bert_input_ids'] for sample in data])\n",
        "\n",
        "        if task == 'diagnosis':\n",
        "            y = np.array([sample['diagnosis_label'] for sample in data])\n",
        "        else:\n",
        "            y = np.array([sample['progression_label'] for sample in data])\n",
        "\n",
        "        # Split data\n",
        "        X_acoustic_train, X_acoustic_test, X_ling_train, X_ling_test, y_train, y_test = train_test_split(\n",
        "            X_acoustic, X_linguistic, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Scale acoustic features\n",
        "        scaler = StandardScaler()\n",
        "        X_acoustic_train_scaled = scaler.fit_transform(X_acoustic_train)\n",
        "        X_acoustic_test_scaled = scaler.transform(X_acoustic_test)\n",
        "\n",
        "        return {\n",
        "            'X_acoustic_train': X_acoustic_train_scaled,\n",
        "            'X_acoustic_test': X_acoustic_test_scaled,\n",
        "            'X_linguistic_train': X_ling_train,\n",
        "            'X_linguistic_test': X_ling_test,\n",
        "            'y_train': y_train,\n",
        "            'y_test': y_test,\n",
        "            'scaler': scaler,\n",
        "            'sample_ids': [sample['id'] for sample in data]\n",
        "        }\n",
        "\n",
        "    def visualize_training_data(self, training_data: Dict[str, Any]):\n",
        "        \"\"\"Visualize training data splits\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Training Data Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "        for i, (task, data) in enumerate(training_data.items()):\n",
        "            if task == 'combined' or data is None:\n",
        "                continue\n",
        "\n",
        "            row = i // 2\n",
        "            col = i % 2\n",
        "\n",
        "            # Class distribution\n",
        "            train_dist = pd.Series(data['y_train']).value_counts()\n",
        "            test_dist = pd.Series(data['y_test']).value_counts()\n",
        "\n",
        "            x = np.arange(len(train_dist))\n",
        "            width = 0.35\n",
        "\n",
        "            axes[row, col].bar(x - width/2, train_dist.values, width, label='Train', alpha=0.8)\n",
        "            axes[row, col].bar(x + width/2, test_dist.values, width, label='Test', alpha=0.8)\n",
        "\n",
        "            axes[row, col].set_title(f'{task.title()} Task - Class Distribution')\n",
        "            axes[row, col].set_xlabel('Class')\n",
        "            axes[row, col].set_ylabel('Count')\n",
        "            axes[row, col].set_xticks(x)\n",
        "            axes[row, col].set_xticklabels(train_dist.index)\n",
        "            axes[row, col].legend()\n",
        "\n",
        "        # Feature distribution\n",
        "        if training_data['diagnosis'] is not None:\n",
        "            diagnosis_features = training_data['diagnosis']['X_acoustic_train']\n",
        "            axes[1, 0].hist(diagnosis_features[:, 0], bins=30, alpha=0.7, label='Feature 1')\n",
        "            axes[1, 0].hist(diagnosis_features[:, 1], bins=30, alpha=0.7, label='Feature 2')\n",
        "            axes[1, 0].set_title('Diagnosis: Feature Distribution')\n",
        "            axes[1, 0].set_xlabel('Feature Value')\n",
        "            axes[1, 0].set_ylabel('Frequency')\n",
        "            axes[1, 0].legend()\n",
        "\n",
        "        # Dataset size comparison\n",
        "        sizes = []\n",
        "        labels = []\n",
        "        for task, data in training_data.items():\n",
        "            if task != 'combined' and data is not None:\n",
        "                sizes.append(len(data['y_train']) + len(data['y_test']))\n",
        "                labels.append(task.title())\n",
        "\n",
        "        if sizes:\n",
        "            axes[1, 1].bar(labels, sizes, color=['skyblue', 'lightgreen'])\n",
        "            axes[1, 1].set_title('Dataset Sizes by Task')\n",
        "            axes[1, 1].set_ylabel('Number of Samples')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/training_data.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_8_train_models(self, training_data: Dict[str, Any]):\n",
        "        \"\"\"Step 8: Train machine learning models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 8: TRAINING MODELS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step8_trained_models.pkl\"\n",
        "        trained_models = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if trained_models is not None:\n",
        "            print(\"✓ Loaded trained models from checkpoint\")\n",
        "            return trained_models\n",
        "\n",
        "        trained_models = {}\n",
        "\n",
        "        # Train models for each task\n",
        "        for task_name, task_data in training_data.items():\n",
        "            if task_name == 'combined' or task_data is None:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nTraining models for {task_name} task...\")\n",
        "\n",
        "            # Train different model types\n",
        "            task_models = {}\n",
        "\n",
        "            # 1. Simple Neural Network\n",
        "            task_models['neural_network'] = self.train_neural_network(task_data, task_name)\n",
        "\n",
        "            # 2. Multimodal Fusion Model\n",
        "            task_models['multimodal_fusion'] = self.train_multimodal_fusion(task_data, task_name)\n",
        "\n",
        "            trained_models[task_name] = task_models\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(trained_models, checkpoint_file, \"step8\")\n",
        "\n",
        "        # Visualize model performance\n",
        "        self.visualize_model_performance(trained_models)\n",
        "\n",
        "        return trained_models\n",
        "\n",
        "    def train_neural_network(self, data: Dict[str, Any], task_name: str):\n",
        "        \"\"\"Train a simple neural network\"\"\"\n",
        "        print(f\"  Training Neural Network for {task_name}...\")\n",
        "\n",
        "        # Create model\n",
        "        input_dim = data['X_acoustic_train'].shape[1]\n",
        "        model = self.create_neural_network(input_dim)\n",
        "\n",
        "        # Prepare data\n",
        "        X_train = torch.tensor(data['X_acoustic_train'], dtype=torch.float32)\n",
        "        y_train = torch.tensor(data['y_train'], dtype=torch.long)\n",
        "        X_test = torch.tensor(data['X_acoustic_test'], dtype=torch.float32)\n",
        "        y_test = torch.tensor(data['y_test'], dtype=torch.long)\n",
        "\n",
        "        # Training setup\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for epoch in range(100):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_train)\n",
        "            loss = criterion(outputs, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_test)\n",
        "            test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "            test_accuracy = accuracy_score(y_test.numpy(), test_predictions.numpy())\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'accuracy': test_accuracy,\n",
        "            'predictions': test_predictions.numpy(),\n",
        "            'true_labels': y_test.numpy()\n",
        "        }\n",
        "\n",
        "    def create_neural_network(self, input_dim: int):\n",
        "        \"\"\"Create a simple neural network\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)  # Binary classification\n",
        "        )\n",
        "\n",
        "    def train_multimodal_fusion(self, data: Dict[str, Any], task_name: str):\n",
        "        \"\"\"Train multimodal fusion model\"\"\"\n",
        "        print(f\"  Training Multimodal Fusion for {task_name}...\")\n",
        "\n",
        "        # Create model\n",
        "        acoustic_dim = data['X_acoustic_train'].shape[1]\n",
        "        linguistic_dim = data['X_linguistic_train'].shape[1]\n",
        "        model = self.create_multimodal_fusion_model(acoustic_dim, linguistic_dim)\n",
        "\n",
        "        # Prepare data\n",
        "        X_acoustic_train = torch.tensor(data['X_acoustic_train'], dtype=torch.float32)\n",
        "        X_linguistic_train = torch.tensor(data['X_linguistic_train'], dtype=torch.float32)\n",
        "        y_train = torch.tensor(data['y_train'], dtype=torch.long)\n",
        "\n",
        "        X_acoustic_test = torch.tensor(data['X_acoustic_test'], dtype=torch.float32)\n",
        "        X_linguistic_test = torch.tensor(data['X_linguistic_test'], dtype=torch.float32)\n",
        "        y_test = torch.tensor(data['y_test'], dtype=torch.long)\n",
        "\n",
        "        # Training setup\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for epoch in range(100):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_acoustic_train, X_linguistic_train)\n",
        "            loss = criterion(outputs, y_train)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_acoustic_test, X_linguistic_test)\n",
        "            test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "            test_accuracy = accuracy_score(y_test.numpy(), test_predictions.numpy())\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'accuracy': test_accuracy,\n",
        "            'predictions': test_predictions.numpy(),\n",
        "            'true_labels': y_test.numpy()\n",
        "        }\n",
        "\n",
        "    def create_multimodal_fusion_model(self, acoustic_dim: int, linguistic_dim: int):\n",
        "        \"\"\"Create multimodal fusion model\"\"\"\n",
        "\n",
        "        class MultimodalFusionModel(nn.Module):\n",
        "            def __init__(self, acoustic_dim, linguistic_dim):\n",
        "                super().__init__()\n",
        "\n",
        "                # Acoustic branch\n",
        "                self.acoustic_branch = nn.Sequential(\n",
        "                    nn.Linear(acoustic_dim, 256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3),\n",
        "                    nn.Linear(256, 128)\n",
        "                )\n",
        "\n",
        "                # Linguistic branch\n",
        "                self.linguistic_branch = nn.Sequential(\n",
        "                    nn.Linear(linguistic_dim, 256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3),\n",
        "                    nn.Linear(256, 128)\n",
        "                )\n",
        "\n",
        "                # Fusion layer\n",
        "                self.fusion = nn.Sequential(\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3),\n",
        "                    nn.Linear(128, 64),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(64, 2)\n",
        "                )\n",
        "\n",
        "            def forward(self, acoustic, linguistic):\n",
        "                acoustic_features = self.acoustic_branch(acoustic)\n",
        "                linguistic_features = self.linguistic_branch(linguistic)\n",
        "\n",
        "                # Concatenate features\n",
        "                fused_features = torch.cat([acoustic_features, linguistic_features], dim=1)\n",
        "\n",
        "                return self.fusion(fused_features)\n",
        "\n",
        "        return MultimodalFusionModel(acoustic_dim, linguistic_dim)\n",
        "\n",
        "    def visualize_model_performance(self, trained_models: Dict[str, Any]):\n",
        "        \"\"\"Visualize model performance\"\"\"\n",
        "        if not trained_models:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Model Performance Visualization', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Performance comparison\n",
        "        tasks = []\n",
        "        models = []\n",
        "        accuracies = []\n",
        "\n",
        "        for task_name, task_models in trained_models.items():\n",
        "            for model_name, model_info in task_models.items():\n",
        "                tasks.append(task_name)\n",
        "                models.append(model_name)\n",
        "                accuracies.append(model_info['accuracy'])\n",
        "\n",
        "        # Accuracy comparison\n",
        "        performance_df = pd.DataFrame({\n",
        "            'Task': tasks,\n",
        "            'Model': models,\n",
        "            'Accuracy': accuracies\n",
        "        })\n",
        "\n",
        "        # Group by task and model\n",
        "        pivot_df = performance_df.pivot(index='Task', columns='Model', values='Accuracy')\n",
        "        pivot_df.plot(kind='bar', ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Model Accuracy Comparison')\n",
        "        axes[0, 0].set_ylabel('Accuracy')\n",
        "        axes[0, 0].set_xlabel('Task')\n",
        "        axes[0, 0].legend(title='Model')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Confusion matrix for best model\n",
        "        best_task = max(trained_models.keys(), key=lambda x: max(model['accuracy'] for model in trained_models[x].values()))\n",
        "        best_model_name = max(trained_models[best_task].keys(), key=lambda x: trained_models[best_task][x]['accuracy'])\n",
        "        best_model = trained_models[best_task][best_model_name]\n",
        "\n",
        "        cm = confusion_matrix(best_model['true_labels'], best_model['predictions'])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
        "        axes[0, 1].set_title(f'Confusion Matrix - {best_task} ({best_model_name})')\n",
        "        axes[0, 1].set_ylabel('True Label')\n",
        "        axes[0, 1].set_xlabel('Predicted Label')\n",
        "\n",
        "        # Model comparison\n",
        "        model_names = list(set(models))\n",
        "        avg_accuracies = [performance_df[performance_df['Model'] == model]['Accuracy'].mean() for model in model_names]\n",
        "\n",
        "        axes[1, 0].bar(model_names, avg_accuracies, color=['skyblue', 'lightgreen'])\n",
        "        axes[1, 0].set_title('Average Model Performance')\n",
        "        axes[1, 0].set_ylabel('Average Accuracy')\n",
        "        axes[1, 0].set_xlabel('Model Type')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Task difficulty comparison\n",
        "        task_names = list(set(tasks))\n",
        "        avg_task_accuracies = [performance_df[performance_df['Task'] == task]['Accuracy'].mean() for task in task_names]\n",
        "\n",
        "        axes[1, 1].bar(task_names, avg_task_accuracies, color=['coral', 'lightblue'])\n",
        "        axes[1, 1].set_title('Task Difficulty Comparison')\n",
        "        axes[1, 1].set_ylabel('Average Accuracy')\n",
        "        axes[1, 1].set_xlabel('Task')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/model_performance.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def run_extended_pipeline(self, limit_per_category: int = None):\n",
        "        \"\"\"Run the complete extended analysis pipeline\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"ADRESSO21 EXTENDED ANALYSIS PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Run basic pipeline\n",
        "        basic_results = self.run_complete_pipeline(limit_per_category)\n",
        "\n",
        "        # Extended steps\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"RUNNING EXTENDED STEPS\")\n",
        "        print(\"=\"*60)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "6PiKByc7lh5d",
        "outputId": "b1e0ff96-3f04-4717-d5e8-739bdcdcb2ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 28)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    def get_feature_dimensions(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    }
  ]
}