{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHkffzK/fOffufC6l6V6UB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Jul16_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwXqBVYNiGMm",
        "outputId": "34d82982-ccda-4cac-f667-5281ee548858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Collecting audobject>=0.6.1 (from opensmile)\n",
            "  Downloading audobject-0.7.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting audinterface>=0.7.0 (from opensmile)\n",
            "  Downloading audinterface-1.3.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Collecting audeer>=2.1.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audeer-2.2.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audformat-1.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audiofile-1.5.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audmath-1.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting asttokens>=2.0.0 (from audobject>=0.6.1->opensmile)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (8.7.0)\n",
            "Collecting oyaml (from audobject>=0.6.1->opensmile)\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting iso639-lang (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso639_lang-2.6.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting iso3166 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (18.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.23.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.17.0)\n",
            "Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audinterface-1.3.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audobject-0.7.12-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading audeer-2.2.2-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audformat-1.3.2-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audiofile-1.5.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audmath-1.4.2-py3-none-any.whl (23 kB)\n",
            "Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading iso639_lang-2.6.1-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.9/324.9 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=d17c410da5147d5de691718ee34d4cffcda6a91fc7f8d73d7ceb4bd2de54fa4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/d2/9a/801b5cc5b2a1af2e280089b71c326711a682fc1d50ea29d0ed\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: ruamel.yaml.clib, oyaml, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, iso639-lang, iso3166, audresample, audmath, audeer, asttokens, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, audobject, nvidia-cusolver-cu12, hyperpyyaml, audiofile, audformat, openai-whisper, audinterface, speechbrain, opensmile\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed asttokens-3.0.0 audeer-2.2.2 audformat-1.3.2 audinterface-1.3.1 audiofile-1.5.1 audmath-1.4.2 audobject-0.7.12 audresample-1.3.4 hyperpyyaml-1.2.2 iso3166-2.1.1 iso639-lang-2.6.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20250625 opensmile-2.5.1 oyaml-1.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.7.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp->torch-geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install librosa soundfile opensmile speechbrain transformers torch openai-whisper\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Any, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Audio processing\n",
        "import librosa\n",
        "import opensmile\n",
        "import whisper\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor, Wav2Vec2Model,\n",
        "    BertTokenizer, BertModel,\n",
        "    ViTModel, ViTFeatureExtractor\n",
        ")\n",
        "\n",
        "# Graph networks\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "\n",
        "# ML utilities\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Visualization\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ADReSSoAnalyzer:\n",
        "    \"\"\"Complete ADReSSo analysis pipeline with error handling and checkpoints\"\"\"\n",
        "\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content/drive/MyDrive/ADReSSo_Results\"\n",
        "        self.checkpoint_path = f\"{self.output_path}/checkpoints\"\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(self.output_path, exist_ok=True)\n",
        "        os.makedirs(self.checkpoint_path, exist_ok=True)\n",
        "        os.makedirs(f\"{self.output_path}/visualizations\", exist_ok=True)\n",
        "\n",
        "        # Initialize containers\n",
        "        self.audio_files = {}\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "        self.linguistic_features = {}\n",
        "\n",
        "        # Initialize models\n",
        "        self.initialize_models()\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize all required models\"\"\"\n",
        "        print(\"Initializing models...\")\n",
        "\n",
        "        try:\n",
        "            # Initialize openSMILE\n",
        "            self.smile = opensmile.Smile(\n",
        "                feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "                feature_level=opensmile.FeatureLevel.Functionals,\n",
        "            )\n",
        "            print(\"✓ OpenSMILE initialized\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ OpenSMILE initialization failed: {e}\")\n",
        "            self.smile = None\n",
        "\n",
        "        try:\n",
        "            # Initialize Whisper\n",
        "            self.whisper_model = whisper.load_model(\"base\")\n",
        "            print(\"✓ Whisper model loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Whisper initialization failed: {e}\")\n",
        "            self.whisper_model = None\n",
        "\n",
        "        try:\n",
        "            # Initialize Wav2Vec2\n",
        "            self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "            self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "            print(\"✓ Wav2Vec2 models loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Wav2Vec2 initialization failed: {e}\")\n",
        "            self.wav2vec_processor = None\n",
        "            self.wav2vec_model = None\n",
        "\n",
        "        try:\n",
        "            # Initialize BERT\n",
        "            self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "            print(\"✓ BERT models loaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ BERT initialization failed: {e}\")\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "\n",
        "    def save_checkpoint(self, data: Any, filename: str, step: str):\n",
        "        \"\"\"Save checkpoint data\"\"\"\n",
        "        filepath = f\"{self.checkpoint_path}/{filename}\"\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    pickle.dump(data, f)\n",
        "            elif filename.endswith('.json'):\n",
        "                with open(filepath, 'w') as f:\n",
        "                    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "            elif filename.endswith('.csv'):\n",
        "                if isinstance(data, pd.DataFrame):\n",
        "                    data.to_csv(filepath, index=False)\n",
        "                else:\n",
        "                    pd.DataFrame(data).to_csv(filepath, index=False)\n",
        "\n",
        "            print(f\"✓ Checkpoint saved: {filename}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Failed to save checkpoint {filename}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_checkpoint(self, filename: str):\n",
        "        \"\"\"Load checkpoint data\"\"\"\n",
        "        filepath = f\"{self.checkpoint_path}/{filename}\"\n",
        "\n",
        "        if not os.path.exists(filepath):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.pkl'):\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            elif filename.endswith('.json'):\n",
        "                with open(filepath, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            elif filename.endswith('.csv'):\n",
        "                return pd.read_csv(filepath)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Failed to load checkpoint {filename}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def step_1_get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Step 1: Get all audio files from the dataset\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 1: GETTING AUDIO FILES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step1_audio_files.json\"\n",
        "        audio_files = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if audio_files is not None:\n",
        "            print(\"✓ Loaded audio files from checkpoint\")\n",
        "            self.audio_files = audio_files\n",
        "            return audio_files\n",
        "\n",
        "        audio_files = {\n",
        "            'diagnosis_ad': [],\n",
        "            'diagnosis_cn': [],\n",
        "            'progression_decline': [],\n",
        "            'progression_no_decline': [],\n",
        "            'progression_test': []\n",
        "        }\n",
        "\n",
        "        # Define paths\n",
        "        paths = {\n",
        "            'diagnosis_ad': f\"{self.base_path}/diagnosis/train/audio/ad\",\n",
        "            'diagnosis_cn': f\"{self.base_path}/diagnosis/train/audio/cn\",\n",
        "            'progression_decline': f\"{self.base_path}/progression/train/audio/decline\",\n",
        "            'progression_no_decline': f\"{self.base_path}/progression/train/audio/no_decline\",\n",
        "            'progression_test': f\"{self.base_path}/progression/test-dist/audio\"\n",
        "        }\n",
        "\n",
        "        # Collect files\n",
        "        for category, path in paths.items():\n",
        "            if os.path.exists(path):\n",
        "                files = [f\"{path}/{f}\" for f in os.listdir(path) if f.endswith('.wav')]\n",
        "                audio_files[category] = files\n",
        "                print(f\"✓ Found {len(files)} files in {category}\")\n",
        "            else:\n",
        "                print(f\"⚠ Path not found: {path}\")\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        print(f\"\\nTotal audio files found: {total_files}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(audio_files, checkpoint_file, \"step1\")\n",
        "        self.audio_files = audio_files\n",
        "\n",
        "        # Visualize file distribution\n",
        "        self.visualize_file_distribution(audio_files)\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def visualize_file_distribution(self, audio_files: Dict[str, List[str]]):\n",
        "        \"\"\"Visualize audio file distribution\"\"\"\n",
        "        categories = list(audio_files.keys())\n",
        "        counts = [len(files) for files in audio_files.values()]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bars = plt.bar(categories, counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "        plt.title('Audio File Distribution by Category', fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('Category', fontsize=12)\n",
        "        plt.ylabel('Number of Files', fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, count in zip(bars, counts):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                    str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/file_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_2_extract_acoustic_features(self, limit_per_category: int = None):\n",
        "        \"\"\"Step 2: Extract acoustic features from audio files\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 2: EXTRACTING ACOUSTIC FEATURES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step2_acoustic_features.pkl\"\n",
        "        features = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if features is not None:\n",
        "            print(\"✓ Loaded acoustic features from checkpoint\")\n",
        "            self.features = features\n",
        "            return features\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        for category, files in self.audio_files.items():\n",
        "            if not files:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "\n",
        "            # Limit files if specified\n",
        "            if limit_per_category:\n",
        "                files = files[:limit_per_category]\n",
        "\n",
        "            for file_path in tqdm(files, desc=f\"Extracting features for {category}\"):\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    file_key = f\"{category}_{filename}\"\n",
        "\n",
        "                    # Extract features\n",
        "                    file_features = self.extract_acoustic_features_from_file(file_path)\n",
        "\n",
        "                    if file_features is not None:\n",
        "                        features[file_key] = {\n",
        "                            'file_path': file_path,\n",
        "                            'category': category,\n",
        "                            'filename': filename,\n",
        "                            **file_features\n",
        "                        }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠ Error processing {filename}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        print(f\"\\n✓ Extracted features from {len(features)} files\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(features, checkpoint_file, \"step2\")\n",
        "        self.features = features\n",
        "\n",
        "        # Visualize features\n",
        "        self.visualize_acoustic_features(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_acoustic_features_from_file(self, audio_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features from a single audio file\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                return None\n",
        "\n",
        "            # 1. eGeMAPS features\n",
        "            if self.smile is not None:\n",
        "                try:\n",
        "                    egemaps = self.smile.process_file(audio_path).values.flatten()\n",
        "                    features['egemaps'] = egemaps\n",
        "                except Exception as e:\n",
        "                    features['egemaps'] = np.zeros(88)\n",
        "            else:\n",
        "                features['egemaps'] = np.zeros(88)\n",
        "\n",
        "            # 2. MFCC features\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.mean(mfccs, axis=1),\n",
        "                    'std': np.std(mfccs, axis=1),\n",
        "                    'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                    'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.zeros(13), 'std': np.zeros(13),\n",
        "                    'delta': np.zeros(13), 'delta2': np.zeros(13)\n",
        "                }\n",
        "\n",
        "            # 3. Mel spectrogram\n",
        "            try:\n",
        "                mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "                log_mel = librosa.power_to_db(mel_spec)\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.mean(log_mel, axis=1),\n",
        "                    'std': np.std(log_mel, axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.zeros(80), 'std': np.zeros(80)\n",
        "                }\n",
        "\n",
        "            # 4. Wav2Vec2 features\n",
        "            if self.wav2vec_processor is not None and self.wav2vec_model is not None:\n",
        "                try:\n",
        "                    input_values = self.wav2vec_processor(\n",
        "                        y, sampling_rate=16000, return_tensors=\"pt\"\n",
        "                    ).input_values\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                    features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "                except Exception as e:\n",
        "                    features['wav2vec2'] = np.zeros(768)\n",
        "            else:\n",
        "                features['wav2vec2'] = np.zeros(768)\n",
        "\n",
        "            # 5. Prosodic features\n",
        "            try:\n",
        "                f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "                f0_clean = f0[f0 > 0]\n",
        "\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                    'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                    'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                    'duration': len(y) / sr\n",
        "                }\n",
        "            except Exception as e:\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                    'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                    'duration': 0.0\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio file: {e}\")\n",
        "            return None\n",
        "\n",
        "        return features\n",
        "\n",
        "    def visualize_acoustic_features(self, features: Dict[str, Any]):\n",
        "        \"\"\"Visualize acoustic features\"\"\"\n",
        "        if not features:\n",
        "            return\n",
        "\n",
        "        # Sample file for visualization\n",
        "        sample_key = list(features.keys())[0]\n",
        "        sample_features = features[sample_key]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Acoustic Features Visualization - {sample_key}', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # eGeMAPS\n",
        "        axes[0, 0].plot(sample_features['egemaps'][:20])\n",
        "        axes[0, 0].set_title('eGeMAPS Features (first 20)')\n",
        "        axes[0, 0].set_xlabel('Feature Index')\n",
        "        axes[0, 0].set_ylabel('Value')\n",
        "\n",
        "        # MFCC\n",
        "        mfcc_mean = sample_features['mfccs']['mean']\n",
        "        axes[0, 1].plot(mfcc_mean, marker='o')\n",
        "        axes[0, 1].set_title('MFCC Mean')\n",
        "        axes[0, 1].set_xlabel('MFCC Coefficient')\n",
        "        axes[0, 1].set_ylabel('Value')\n",
        "\n",
        "        # Mel spectrogram\n",
        "        mel_mean = sample_features['log_mel']['mean']\n",
        "        axes[0, 2].plot(mel_mean)\n",
        "        axes[0, 2].set_title('Log-Mel Spectrogram Mean')\n",
        "        axes[0, 2].set_xlabel('Mel Bin')\n",
        "        axes[0, 2].set_ylabel('Value')\n",
        "\n",
        "        # Wav2Vec2\n",
        "        axes[1, 0].plot(sample_features['wav2vec2'][:50])\n",
        "        axes[1, 0].set_title('Wav2Vec2 Features (first 50)')\n",
        "        axes[1, 0].set_xlabel('Feature Index')\n",
        "        axes[1, 0].set_ylabel('Value')\n",
        "\n",
        "        # Prosodic features\n",
        "        prosodic = sample_features['prosodic']\n",
        "        prosodic_names = list(prosodic.keys())\n",
        "        prosodic_values = list(prosodic.values())\n",
        "\n",
        "        axes[1, 1].bar(prosodic_names, prosodic_values)\n",
        "        axes[1, 1].set_title('Prosodic Features')\n",
        "        axes[1, 1].set_ylabel('Value')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Feature distribution by category\n",
        "        categories = {}\n",
        "        for key, feature_data in features.items():\n",
        "            category = feature_data['category']\n",
        "            if category not in categories:\n",
        "                categories[category] = []\n",
        "            categories[category].append(feature_data['prosodic']['duration'])\n",
        "\n",
        "        for category, durations in categories.items():\n",
        "            axes[1, 2].hist(durations, alpha=0.7, label=category, bins=20)\n",
        "\n",
        "        axes[1, 2].set_title('Duration Distribution by Category')\n",
        "        axes[1, 2].set_xlabel('Duration (seconds)')\n",
        "        axes[1, 2].set_ylabel('Frequency')\n",
        "        axes[1, 2].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/acoustic_features.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_3_extract_transcripts(self, limit_per_category: int = None):\n",
        "        \"\"\"Step 3: Extract transcripts using Whisper\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 3: EXTRACTING TRANSCRIPTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step3_transcripts.json\"\n",
        "        transcripts = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if transcripts is not None:\n",
        "            print(\"✓ Loaded transcripts from checkpoint\")\n",
        "            self.transcripts = transcripts\n",
        "            return transcripts\n",
        "\n",
        "        if self.whisper_model is None:\n",
        "            print(\"⚠ Whisper model not available, skipping transcript extraction\")\n",
        "            return {}\n",
        "\n",
        "        transcripts = {}\n",
        "\n",
        "        for category, files in self.audio_files.items():\n",
        "            if not files:\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "\n",
        "            # Limit files if specified\n",
        "            if limit_per_category:\n",
        "                files = files[:limit_per_category]\n",
        "\n",
        "            for file_path in tqdm(files, desc=f\"Transcribing {category}\"):\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    file_key = f\"{category}_{filename}\"\n",
        "\n",
        "                    # Transcribe\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "\n",
        "                    transcripts[file_key] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': result[\"text\"].strip(),\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠ Error transcribing {filename}: {e}\")\n",
        "                    transcripts[file_key] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        print(f\"\\n✓ Extracted transcripts from {len(transcripts)} files\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(transcripts, checkpoint_file, \"step3\")\n",
        "        self.transcripts = transcripts\n",
        "\n",
        "        # Visualize transcripts\n",
        "        self.visualize_transcripts(transcripts)\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def visualize_transcripts(self, transcripts: Dict[str, Any]):\n",
        "        \"\"\"Visualize transcript statistics\"\"\"\n",
        "        if not transcripts:\n",
        "            return\n",
        "\n",
        "        # Prepare data\n",
        "        data = []\n",
        "        for key, info in transcripts.items():\n",
        "            transcript = info.get('transcript', '')\n",
        "            data.append({\n",
        "                'category': info['category'],\n",
        "                'word_count': len(transcript.split()) if transcript else 0,\n",
        "                'char_count': len(transcript),\n",
        "                'has_error': 'error' in info\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Transcript Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Word count distribution\n",
        "        df.boxplot(column='word_count', by='category', ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Word Count Distribution by Category')\n",
        "        axes[0, 0].set_ylabel('Word Count')\n",
        "\n",
        "        # Character count distribution\n",
        "        df.boxplot(column='char_count', by='category', ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Character Count Distribution by Category')\n",
        "        axes[0, 1].set_ylabel('Character Count')\n",
        "\n",
        "        # Error rate by category\n",
        "        error_rate = df.groupby('category')['has_error'].mean()\n",
        "        axes[1, 0].bar(error_rate.index, error_rate.values)\n",
        "        axes[1, 0].set_title('Error Rate by Category')\n",
        "        axes[1, 0].set_ylabel('Error Rate')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Average metrics by category\n",
        "        avg_metrics = df.groupby('category')[['word_count', 'char_count']].mean()\n",
        "        avg_metrics.plot(kind='bar', ax=axes[1, 1])\n",
        "        axes[1, 1].set_title('Average Metrics by Category')\n",
        "        axes[1, 1].set_ylabel('Count')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/transcript_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def step_4_extract_linguistic_features(self):\n",
        "        \"\"\"Step 4: Extract linguistic features for BERT\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 4: EXTRACTING LINGUISTIC FEATURES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Check if checkpoint exists\n",
        "        checkpoint_file = \"step4_linguistic_features.pkl\"\n",
        "        linguistic_features = self.load_checkpoint(checkpoint_file)\n",
        "\n",
        "        if linguistic_features is not None:\n",
        "            print(\"✓ Loaded linguistic features from checkpoint\")\n",
        "            self.linguistic_features = linguistic_features\n",
        "            return linguistic_features\n",
        "\n",
        "        if self.bert_tokenizer is None:\n",
        "            print(\"⚠ BERT tokenizer not available, skipping linguistic feature extraction\")\n",
        "            return {}\n",
        "\n",
        "        linguistic_features = {}\n",
        "\n",
        "        print(\"Processing transcripts for linguistic features...\")\n",
        "\n",
        "        for key, data in tqdm(self.transcripts.items(), desc=\"Extracting linguistic features\"):\n",
        "            transcript = data.get('transcript', '')\n",
        "\n",
        "            if not transcript:\n",
        "                linguistic_features[key] = self.create_empty_linguistic_features()\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Basic linguistic features\n",
        "                words = transcript.split()\n",
        "                sentences = [s.strip() for s in transcript.split('.') if s.strip()]\n",
        "\n",
        "                # BERT tokenization\n",
        "                bert_encoding = self.bert_tokenizer(\n",
        "                    transcript,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    max_length=512,\n",
        "                    return_tensors='pt'\n",
        "                )\n",
        "\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': transcript,\n",
        "                    'word_count': len(words),\n",
        "                    'sentence_count': len(sentences),\n",
        "                    'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                    'unique_words': len(set(words)),\n",
        "                    'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                    'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                    'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                    'category': data['category']\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Error processing {key}: {e}\")\n",
        "                linguistic_features[key] = self.create_empty_linguistic_features()\n",
        "\n",
        "        print(f\"\\n✓ Extracted linguistic features from {len(linguistic_features)} files\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        self.save_checkpoint(linguistic_features, checkpoint_file, \"step4\")\n",
        "        self.linguistic_features = linguistic_features\n",
        "\n",
        "        # Visualize linguistic features\n",
        "        self.visualize_linguistic_features(linguistic_features)\n",
        "\n",
        "        return linguistic_features\n",
        "\n",
        "    def create_empty_linguistic_features(self):\n",
        "        \"\"\"Create empty linguistic features structure\"\"\"\n",
        "        return {\n",
        "            'raw_text': '',\n",
        "            'word_count': 0,\n",
        "            'sentence_count': 0,\n",
        "            'avg_word_length': 0,\n",
        "            'unique_words': 0,\n",
        "            'lexical_diversity': 0,\n",
        "            'bert_input_ids': [0] * 512,\n",
        "            'bert_attention_mask': [0] * 512,\n",
        "            'category': 'unknown'\n",
        "        }\n",
        "\n",
        "    def visualize_linguistic_features(self, linguistic_features: Dict[str, Any]):\n",
        "        \"\"\"Visualize linguistic features\"\"\"\n",
        "        if not linguistic_features:\n",
        "            return\n",
        "\n",
        "        # Prepare data\n",
        "        data = []\n",
        "        for key, features in linguistic_features.items():\n",
        "            data.append({\n",
        "                'key': key,\n",
        "                'category': features['category'],\n",
        "                'word_count': features['word_count'],\n",
        "                'sentence_count': features['sentence_count'],\n",
        "                'avg_word_length': features['avg_word_length'],\n",
        "                'unique_words': features['unique_words'],\n",
        "                'lexical_diversity': features['lexical_diversity']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Linguistic Features Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Metrics by category\n",
        "        metrics = ['word_count', 'sentence_count', 'avg_word_length', 'unique_words', 'lexical_diversity']\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            row = i // 3\n",
        "            col = i % 3\n",
        "\n",
        "            if row < 2 and col < 3:\n",
        "                df.boxplot(column=metric, by='category', ax=axes[row, col])\n",
        "                axes[row, col].set_title(f'{metric.replace(\"_\", \" \").title()} by Category')\n",
        "                axes[row, col].set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "\n",
        "        # Correlation heatmap\n",
        "        numeric_df = df.select_dtypes(include=[np.number])\n",
        "        correlation_matrix = numeric_df.corr()\n",
        "\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n",
        "        axes[1, 2].set_title('Feature Correlation Matrix')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.output_path}/visualizations/linguistic_features.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def run_complete_pipeline(self, limit_per_category: int = None):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"ADRESSO21 COMPLETE ANALYSIS PIPELINE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Step 1: Get audio files\n",
        "        results['audio_files'] = self.step_1_get_audio_files()\n",
        "\n",
        "        # Step 2: Extract acoustic features\n",
        "        results['acoustic_features'] = self.step_2_extract_acoustic_features(limit_per_category)\n",
        "\n",
        "        # Step 3: Extract transcripts\n",
        "        results['transcripts'] = self.step_3_extract_transcripts(limit_per_category)\n",
        "\n",
        "        # Step 4: Extract linguistic features\n",
        "        results['linguistic_features'] = self.step_4_extract_linguistic_features()\n",
        "\n",
        "        # Generate final summary\n",
        "        self.generate_final_summary(results)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Results saved to: {self.output_path}\")\n",
        "        print(f\"Checkpoints saved to: {self.checkpoint_path}\")\n",
        "        print(f\"Visualizations saved to: {self.output_path}/visualizations\")\n",
        "\n",
        "        return results\n",
        ""
      ],
      "metadata": {
        "id": "ovqJXJ7Ljyi4"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}