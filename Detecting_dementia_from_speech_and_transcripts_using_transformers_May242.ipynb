{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May242.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your ADReSSo21-diagnosis-train.tgz file to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9iStLhAapOO",
        "outputId": "34c4d83d-b5a2-4e70-cf20-29069c485636"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dementia Detection from Speech and Transcripts using Transformers\n",
        "# Complete Implementation for Google Colab\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: INSTALL REQUIRED PACKAGES AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "!pip install transformers torch torchvision torchaudio librosa pandas scikit-learn matplotlib seaborn numpy\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import librosa.display\n",
        "from transformers import BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: DATA EXTRACTION AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class DataExtractor:\n",
        "    def __init__(self, archive_path, extract_to):\n",
        "        self.archive_path = archive_path\n",
        "        self.extract_to = extract_to\n",
        "\n",
        "    def extract_archive(self):\n",
        "        \"\"\"Extract the ADReSSo21-diagnosis-train.tgz archive\"\"\"\n",
        "        print(\"Extracting archive...\")\n",
        "        with tarfile.open(self.archive_path, 'r:gz') as tar:\n",
        "            tar.extractall(self.extract_to)\n",
        "        print(f\"Archive extracted to {self.extract_to}\")\n",
        "\n",
        "    def collect_data(self):\n",
        "        \"\"\"Collect data from extracted files\"\"\"\n",
        "        base_path = os.path.join(self.extract_to, \"ADReSSo21/diagnosis/train\")\n",
        "\n",
        "        # Paths for audio and segmentation files\n",
        "        audio_ad_path = os.path.join(base_path, \"audio/ad\")\n",
        "        audio_cn_path = os.path.join(base_path, \"audio/cn\")\n",
        "        seg_ad_path = os.path.join(base_path, \"segmentation/ad\")\n",
        "        seg_cn_path = os.path.join(base_path, \"segmentation/cn\")\n",
        "\n",
        "        data_samples = []\n",
        "\n",
        "        # Collect AD samples\n",
        "        if os.path.exists(audio_ad_path) and os.path.exists(seg_ad_path):\n",
        "            for audio_file in os.listdir(audio_ad_path):\n",
        "                if audio_file.endswith('.wav'):\n",
        "                    participant_id = audio_file.replace('.wav', '')\n",
        "                    seg_file = f\"{participant_id}.csv\"\n",
        "\n",
        "                    if os.path.exists(os.path.join(seg_ad_path, seg_file)):\n",
        "                        data_samples.append({\n",
        "                            'audio_path': os.path.join(audio_ad_path, audio_file),\n",
        "                            'transcript_path': os.path.join(seg_ad_path, seg_file),\n",
        "                            'label': 1,  # AD = 1\n",
        "                            'participant_id': participant_id,\n",
        "                            'class_name': 'ad'\n",
        "                        })\n",
        "\n",
        "        # Collect CN (Control) samples\n",
        "        if os.path.exists(audio_cn_path) and os.path.exists(seg_cn_path):\n",
        "            for audio_file in os.listdir(audio_cn_path):\n",
        "                if audio_file.endswith('.wav'):\n",
        "                    participant_id = audio_file.replace('.wav', '')\n",
        "                    seg_file = f\"{participant_id}.csv\"\n",
        "\n",
        "                    if os.path.exists(os.path.join(seg_cn_path, seg_file)):\n",
        "                        data_samples.append({\n",
        "                            'audio_path': os.path.join(audio_cn_path, audio_file),\n",
        "                            'transcript_path': os.path.join(seg_cn_path, seg_file),\n",
        "                            'label': 0,  # CN = 0\n",
        "                            'participant_id': participant_id,\n",
        "                            'class_name': 'cn'\n",
        "                        })\n",
        "\n",
        "        print(f\"Collected {len(data_samples)} samples\")\n",
        "        ad_count = sum(1 for s in data_samples if s['label'] == 1)\n",
        "        cn_count = sum(1 for s in data_samples if s['label'] == 0)\n",
        "        print(f\"AD samples: {ad_count}, CN samples: {cn_count}\")\n",
        "\n",
        "        return data_samples\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: AUDIO FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, n_mels=224, n_fft=2048, hop_length=1024, target_length=224):\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.target_length = target_length\n",
        "\n",
        "    def extract_log_mel_spectrogram(self, audio_path):\n",
        "        \"\"\"Extract Log-Mel spectrogram with delta and delta-delta features\"\"\"\n",
        "        try:\n",
        "            # Load audio file\n",
        "            y, sr = librosa.load(audio_path, sr=22050)\n",
        "\n",
        "            # Extract Mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=y, sr=sr, n_mels=self.n_mels,\n",
        "                n_fft=self.n_fft, hop_length=self.hop_length\n",
        "            )\n",
        "\n",
        "            # Convert to log scale\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            # Calculate delta and delta-delta features\n",
        "            delta = librosa.feature.delta(log_mel_spec)\n",
        "            delta_delta = librosa.feature.delta(log_mel_spec, order=2)\n",
        "\n",
        "            # Stack to create 3-channel image\n",
        "            features = np.stack([log_mel_spec, delta, delta_delta], axis=0)\n",
        "\n",
        "            # Resize to target length (for ViT input)\n",
        "            if features.shape[2] > self.target_length:\n",
        "                features = features[:, :, :self.target_length]\n",
        "            elif features.shape[2] < self.target_length:\n",
        "                # Pad with zeros\n",
        "                pad_width = self.target_length - features.shape[2]\n",
        "                features = np.pad(features, ((0, 0), (0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}\")\n",
        "            return np.zeros((3, self.n_mels, self.target_length))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: TRANSCRIPT PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class TranscriptProcessor:\n",
        "    def __init__(self, tokenizer_name='bert-base-uncased', max_length=512):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def extract_transcript_from_csv(self, csv_path):\n",
        "        \"\"\"Extract transcript text from segmentation CSV\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "\n",
        "            # The CSV might contain different column names\n",
        "            # Common patterns: 'transcript', 'text', 'utterance', etc.\n",
        "            text_columns = ['transcript', 'text', 'utterance', 'speech', 'content']\n",
        "            transcript_text = \"\"\n",
        "\n",
        "            for col in text_columns:\n",
        "                if col in df.columns:\n",
        "                    transcript_text = \" \".join(df[col].dropna().astype(str))\n",
        "                    break\n",
        "\n",
        "            # If no text column found, try to construct from available data\n",
        "            if not transcript_text and 'speaker' in df.columns:\n",
        "                # Sometimes transcript is distributed across rows\n",
        "                participant_rows = df[df['speaker'].str.contains('participant|patient|PAR', case=False, na=False)]\n",
        "                if not participant_rows.empty and len(df.columns) > 3:\n",
        "                    # Try the last column as it might contain text\n",
        "                    last_col = df.columns[-1]\n",
        "                    transcript_text = \" \".join(participant_rows[last_col].dropna().astype(str))\n",
        "\n",
        "            # Fallback: use all non-numeric content\n",
        "            if not transcript_text:\n",
        "                text_parts = []\n",
        "                for col in df.columns:\n",
        "                    if df[col].dtype == 'object':\n",
        "                        text_parts.extend(df[col].dropna().astype(str).tolist())\n",
        "                transcript_text = \" \".join(text_parts)\n",
        "\n",
        "            return transcript_text if transcript_text else \"No transcript available\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing transcript {csv_path}: {e}\")\n",
        "            return \"Error reading transcript\"\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Tokenize text using BERT tokenizer\"\"\"\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'token_type_ids': encoding['token_type_ids'].squeeze()\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class DementiaDataset(Dataset):\n",
        "    def __init__(self, data_samples, audio_extractor, transcript_processor):\n",
        "        self.data_samples = data_samples\n",
        "        self.audio_extractor = audio_extractor\n",
        "        self.transcript_processor = transcript_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        # Extract audio features\n",
        "        audio_features = self.audio_extractor.extract_log_mel_spectrogram(sample['audio_path'])\n",
        "        audio_features = torch.FloatTensor(audio_features)\n",
        "\n",
        "        # Extract transcript features\n",
        "        transcript_text = self.transcript_processor.extract_transcript_from_csv(sample['transcript_path'])\n",
        "        transcript_tokens = self.transcript_processor.tokenize_text(transcript_text)\n",
        "\n",
        "        return {\n",
        "            'audio_features': audio_features,\n",
        "            'input_ids': transcript_tokens['input_ids'],\n",
        "            'attention_mask': transcript_tokens['attention_mask'],\n",
        "            'token_type_ids': transcript_tokens['token_type_ids'],\n",
        "            'label': torch.LongTensor([sample['label']]),\n",
        "            'participant_id': sample['participant_id']\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CrossModalAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=8, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        attended, _ = self.attention(query, key, value)\n",
        "        return self.norm(attended + query)\n",
        "\n",
        "class GatedMultimodalUnit(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(GatedMultimodalUnit, self).__init__()\n",
        "        self.linear_text = nn.Linear(input_size, input_size)\n",
        "        self.linear_audio = nn.Linear(input_size, input_size)\n",
        "        self.linear_gate = nn.Linear(input_size * 2, input_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, text_features, audio_features):\n",
        "        h_text = self.tanh(self.linear_text(text_features))\n",
        "        h_audio = self.tanh(self.linear_audio(audio_features))\n",
        "\n",
        "        concat_features = torch.cat([h_text, h_audio], dim=-1)\n",
        "        gate = self.sigmoid(self.linear_gate(concat_features))\n",
        "\n",
        "        gated_features = gate * h_text + (1 - gate) * h_audio\n",
        "        return gated_features\n",
        "\n",
        "class MultimodalDementiaClassifier(nn.Module):\n",
        "    def __init__(self, fusion_method='crossmodal', bert_model='bert-base-uncased',\n",
        "                 vit_model='google/vit-base-patch16-224-in21k', num_classes=2):\n",
        "        super(MultimodalDementiaClassifier, self).__init__()\n",
        "\n",
        "        self.fusion_method = fusion_method\n",
        "\n",
        "        # Text encoder (BERT)\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.bert_hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        # Audio encoder (ViT)\n",
        "        self.vit = ViTModel.from_pretrained(vit_model)\n",
        "        self.vit_hidden_size = self.vit.config.hidden_size\n",
        "\n",
        "        # Projection layers to match dimensions\n",
        "        self.text_projection = nn.Linear(self.bert_hidden_size, 512)\n",
        "        self.audio_projection = nn.Linear(self.vit_hidden_size, 512)\n",
        "\n",
        "        # Fusion layers\n",
        "        if fusion_method == 'concatenation':\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(1024, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "        elif fusion_method == 'gmu':\n",
        "            self.gmu = GatedMultimodalUnit(512)\n",
        "            self.classifier = nn.Linear(512, num_classes)\n",
        "        elif fusion_method == 'crossmodal':\n",
        "            self.cross_attention_text_to_audio = CrossModalAttention(512)\n",
        "            self.cross_attention_audio_to_text = CrossModalAttention(512)\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(1024, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, audio_features):\n",
        "        # Text encoding\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        text_features = bert_outputs.pooler_output  # [CLS] token\n",
        "        text_features = self.text_projection(text_features)\n",
        "\n",
        "        # Audio encoding\n",
        "        # Reshape audio features for ViT (batch_size, channels, height, width)\n",
        "        batch_size = audio_features.size(0)\n",
        "        audio_features = audio_features.view(batch_size, 3, 224, 224)\n",
        "\n",
        "        vit_outputs = self.vit(pixel_values=audio_features)\n",
        "        audio_features = vit_outputs.pooler_output\n",
        "        audio_features = self.audio_projection(audio_features)\n",
        "\n",
        "        # Fusion\n",
        "        if self.fusion_method == 'concatenation':\n",
        "            fused_features = torch.cat([text_features, audio_features], dim=-1)\n",
        "            logits = self.classifier(fused_features)\n",
        "\n",
        "        elif self.fusion_method == 'gmu':\n",
        "            fused_features = self.gmu(text_features, audio_features)\n",
        "            logits = self.classifier(fused_features)\n",
        "\n",
        "        elif self.fusion_method == 'crossmodal':\n",
        "            # Add sequence dimension for attention\n",
        "            text_seq = text_features.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
        "            audio_seq = audio_features.unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
        "\n",
        "            # Cross-modal attention\n",
        "            text_attended = self.cross_attention_text_to_audio(text_seq, audio_seq, audio_seq)\n",
        "            audio_attended = self.cross_attention_audio_to_text(audio_seq, text_seq, text_seq)\n",
        "\n",
        "            # Concatenate and classify\n",
        "            fused_features = torch.cat([\n",
        "                text_attended.squeeze(1),\n",
        "                audio_attended.squeeze(1)\n",
        "            ], dim=-1)\n",
        "            logits = self.classifier(fused_features)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "class DementiaTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', patience=3, factor=0.5\n",
        "        )\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            token_type_ids = batch['token_type_ids'].to(self.device)\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = self.model(input_ids, attention_mask, token_type_ids, audio_features)\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(self.train_loader)\n",
        "\n",
        "    def validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                token_type_ids = batch['token_type_ids'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                logits = self.model(input_ids, attention_mask, token_type_ids, audio_features)\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Get predictions\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "        return avg_loss, accuracy, all_predictions, all_labels\n",
        "\n",
        "    def train(self, num_epochs=20, early_stopping_patience=6):\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            # Train\n",
        "            train_loss = self.train_epoch()\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_accuracy, val_predictions, val_labels = self.validate_epoch()\n",
        "\n",
        "            # Update scheduler\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Store metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}\")\n",
        "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
        "        return val_predictions, val_labels\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Loss plot\n",
        "        ax1.plot(self.train_losses, label='Train Loss')\n",
        "        ax1.plot(self.val_losses, label='Val Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "\n",
        "        # Accuracy plot\n",
        "        ax2.plot(self.val_accuracies, label='Val Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.set_title('Validation Accuracy')\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: EVALUATION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(y_true, y_pred, class_names=['CN', 'AD']):\n",
        "    \"\"\"Comprehensive evaluation of model performance\"\"\"\n",
        "\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision_per_class = precision_score(y_true, y_pred, average=None)\n",
        "    recall_per_class = recall_score(y_true, y_pred, average=None)\n",
        "    f1_per_class = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"MODEL EVALUATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Overall Precision: {precision:.4f}\")\n",
        "    print(f\"Overall Recall: {recall:.4f}\")\n",
        "    print(f\"Overall F1-Score: {f1:.4f}\")\n",
        "    print(\"\\nPer-Class Metrics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name} - Precision: {precision_per_class[i]:.4f}, \"\n",
        "              f\"Recall: {recall_per_class[i]:.4f}, F1: {f1_per_class[i]:.4f}\")\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'precision_per_class': precision_per_class,\n",
        "        'recall_per_class': recall_per_class,\n",
        "        'f1_per_class': f1_per_class\n",
        "    }\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: MAIN EXECUTION PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"Starting Dementia Detection Pipeline...\")\n",
        "\n",
        "    # Configuration\n",
        "    ARCHIVE_PATH = \"/content/ADReSSo21-diagnosis-train.tgz\"  # Update this path\n",
        "    EXTRACT_TO = \"/content/ADReSSo_extracted\"\n",
        "    BATCH_SIZE = 8\n",
        "    NUM_EPOCHS = 20\n",
        "    FUSION_METHOD = 'crossmodal'  # Options: 'concatenation', 'gmu', 'crossmodal'\n",
        "\n",
        "    # Step 1: Extract and collect data\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 1: DATA EXTRACTION AND COLLECTION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if os.path.exists(ARCHIVE_PATH):\n",
        "        extractor = DataExtractor(ARCHIVE_PATH, EXTRACT_TO)\n",
        "\n",
        "        if not os.path.exists(EXTRACT_TO):\n",
        "            extractor.extract_archive()\n",
        "\n",
        "        data_samples = extractor.collect_data()\n",
        "    else:\n",
        "        print(f\"Archive not found at {ARCHIVE_PATH}\")\n",
        "        print(\"Please upload the ADReSSo21-diagnosis-train.tgz file to Colab\")\n",
        "        return\n",
        "\n",
        "    if len(data_samples) == 0:\n",
        "        print(\"No data samples found. Please check the archive and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Initialize processors\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 2: INITIALIZING PROCESSORS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    audio_extractor = AudioFeatureExtractor()\n",
        "    transcript_processor = TranscriptProcessor()\n",
        "\n",
        "    # Step 3: Split data\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 3: DATA SPLITTING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    train_samples, val_samples = train_test_split(\n",
        "        data_samples, test_size=0.35, random_state=42,\n",
        "        stratify=[s['label'] for s in data_samples]\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_samples)}\")\n",
        "    print(f\"Validation samples: {len(val_samples)}\")\n",
        "\n",
        "    # Step 4: Create datasets and dataloaders\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 4: CREATING DATASETS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    train_dataset = DementiaDataset(train_samples, audio_extractor, transcript_processor)\n",
        "    val_dataset = DementiaDataset(val_samples, audio_extractor, transcript_processor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(\"Datasets created successfully!\")\n",
        "\n",
        "    # Step 5: Initialize model\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 5: INITIALIZING MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model = MultimodalDementiaClassifier(fusion_method=FUSION_METHOD)\n",
        "    print(f\"Model initialized with {FUSION_METHOD} fusion method\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Step 6: Training\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 6: TRAINING\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    trainer = DementiaTrainer(model, train_loader, val_loader, device)\n",
        "    val_predictions, val_labels = trainer.train(num_epochs=NUM_EPOCHS)\n",
        "\n",
        "    # Step 7: Evaluation\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 7: FINAL EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    metrics = evaluate_model(val_labels, val_predictions)\n",
        "\n",
        "    # Step 8: Visualizations\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 8: VISUALIZATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    trainer.plot_training_history()\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "    return model, trainer, metrics\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: COMPARISON OF DIFFERENT FUSION METHODS\n",
        "# ============================================================================\n",
        "\n",
        "def compare_fusion_methods():\n",
        "    \"\"\"Compare different fusion methods\"\"\"\n",
        "    print(\"Comparing different fusion methods...\")\n",
        "\n",
        "    fusion_methods = ['concatenation', 'gmu', 'crossmodal']\n",
        "    results = {}\n",
        "\n",
        "    for method in fusion_methods:\n",
        "        print(f\"\\nTraining with {method} fusion...\")\n",
        "\n",
        "        # Initialize new model\n",
        "        model = MultimodalDementiaClassifier(fusion_method=method)\n",
        "        trainer = DementiaTrainer(model, train_loader, val_loader, device)\n",
        "\n",
        "        # Train with fewer epochs for comparison\n",
        "        val_predictions, val_labels = trainer.train(num_epochs=10)\n",
        "\n",
        "        # Evaluate\n",
        "        metrics = evaluate_model(val_labels, val_predictions)\n",
        "        results[method] = metrics\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FUSION METHOD COMPARISON\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for method, metrics in results.items():\n",
        "        print(f\"{method.upper()}:\")\n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
        "        print()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the main pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Upload your ADReSSo21-diagnosis-train.tgz file to Colab first\n",
        "    main()\n",
        "\n",
        "    # Uncomment to compare fusion methods\n",
        "    # compare_fusion_methods()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF66lka_aW4u",
        "outputId": "02f4c3e5-d7bf-4443-dc1e-7bfb0dc25fdf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Using device: cuda\n",
            "Starting Dementia Detection Pipeline...\n",
            "\n",
            "==================================================\n",
            "STEP 1: DATA EXTRACTION AND COLLECTION\n",
            "==================================================\n",
            "Archive not found at /content/ADReSSo21-diagnosis-train.tgz\n",
            "Please upload the ADReSSo21-diagnosis-train.tgz file to Colab\n"
          ]
        }
      ]
    }
  ]
}