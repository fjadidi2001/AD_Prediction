{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM60hWAUUCtb86Fc8vAHd0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/ADReSSo21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBXVhfF-wLZg",
        "outputId": "56caccd4-d10d-4749-a4a7-dc4426e477f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ADReSSo21 transcript extraction...\n",
            "Extracting datasets...\n",
            "⚠ Warning: ADReSSo21-progression-train.tgz not found at /drive/MyDrive/Voice/ADReSSo21-progression-train.tgz\n",
            "⚠ Warning: ADReSSo21-progression-test.tgz not found at /drive/MyDrive/Voice/ADReSSo21-progression-test.tgz\n",
            "⚠ Warning: ADReSSo21-diagnosis-train.tgz not found at /drive/MyDrive/Voice/ADReSSo21-diagnosis-train.tgz\n",
            "\n",
            "==================================================\n",
            "EXTRACTING PROGRESSION TRANSCRIPTS\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EXTRACTING DIAGNOSIS TRANSCRIPTS\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EXTRACTION COMPLETE!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "class ADReSSo21TranscriptExtractor:\n",
        "    def __init__(self, base_path=\"/drive/MyDrive/Voice/\"):\n",
        "        self.base_path = base_path\n",
        "        self.datasets = {\n",
        "            'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "            'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "            'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "        }\n",
        "        self.extracted_path = os.path.join(base_path, \"extracted\")\n",
        "\n",
        "    def extract_datasets(self):\n",
        "        \"\"\"Extract all tgz files to the extracted directory\"\"\"\n",
        "        print(\"Extracting datasets...\")\n",
        "\n",
        "        # Create extraction directory\n",
        "        os.makedirs(self.extracted_path, exist_ok=True)\n",
        "\n",
        "        for dataset_name, filename in self.datasets.items():\n",
        "            file_path = os.path.join(self.base_path, filename)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"Extracting {filename}...\")\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=self.extracted_path)\n",
        "                print(f\"✓ {filename} extracted successfully\")\n",
        "            else:\n",
        "                print(f\"⚠ Warning: {filename} not found at {file_path}\")\n",
        "\n",
        "    def find_csv_files(self, directory):\n",
        "        \"\"\"Recursively find all CSV files in a directory\"\"\"\n",
        "        csv_files = []\n",
        "        for root, dirs, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv'):\n",
        "                    csv_files.append(os.path.join(root, file))\n",
        "        return csv_files\n",
        "\n",
        "    def read_transcript_csv(self, csv_path):\n",
        "        \"\"\"Read and process a single CSV transcript file\"\"\"\n",
        "        try:\n",
        "            # Try different encodings as CSV files might have different encodings\n",
        "            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_path, encoding=encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "            else:\n",
        "                print(f\"⚠ Could not read {csv_path} with any encoding\")\n",
        "                return None\n",
        "\n",
        "            # Get filename without extension for ID\n",
        "            file_id = os.path.splitext(os.path.basename(csv_path))[0]\n",
        "\n",
        "            # Add file info\n",
        "            df['file_id'] = file_id\n",
        "            df['file_path'] = csv_path\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {csv_path}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def extract_progression_transcripts(self):\n",
        "        \"\"\"Extract transcripts from progression datasets\"\"\"\n",
        "        transcripts = {\n",
        "            'train': {'decline': [], 'no_decline': []},\n",
        "            'test': []\n",
        "        }\n",
        "\n",
        "        # Process training data\n",
        "        train_path = os.path.join(self.extracted_path, \"ADReSSo21/progression/train/segmentation\")\n",
        "\n",
        "        if os.path.exists(train_path):\n",
        "            # Decline cases\n",
        "            decline_path = os.path.join(train_path, \"decline\")\n",
        "            if os.path.exists(decline_path):\n",
        "                csv_files = self.find_csv_files(decline_path)\n",
        "                print(f\"Found {len(csv_files)} CSV files in decline directory\")\n",
        "\n",
        "                for csv_file in csv_files:\n",
        "                    df = self.read_transcript_csv(csv_file)\n",
        "                    if df is not None:\n",
        "                        df['label'] = 'decline'\n",
        "                        transcripts['train']['decline'].append(df)\n",
        "\n",
        "            # No decline cases\n",
        "            no_decline_path = os.path.join(train_path, \"no_decline\")\n",
        "            if os.path.exists(no_decline_path):\n",
        "                csv_files = self.find_csv_files(no_decline_path)\n",
        "                print(f\"Found {len(csv_files)} CSV files in no_decline directory\")\n",
        "\n",
        "                for csv_file in csv_files:\n",
        "                    df = self.read_transcript_csv(csv_file)\n",
        "                    if df is not None:\n",
        "                        df['label'] = 'no_decline'\n",
        "                        transcripts['train']['no_decline'].append(df)\n",
        "\n",
        "        # Process test data\n",
        "        test_path = os.path.join(self.extracted_path, \"ADReSSo21/progression/test-dist/segmentation\")\n",
        "\n",
        "        if os.path.exists(test_path):\n",
        "            csv_files = self.find_csv_files(test_path)\n",
        "            print(f\"Found {len(csv_files)} CSV files in test directory\")\n",
        "\n",
        "            for csv_file in csv_files:\n",
        "                df = self.read_transcript_csv(csv_file)\n",
        "                if df is not None:\n",
        "                    df['label'] = 'test'\n",
        "                    transcripts['test'].append(df)\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def extract_diagnosis_transcripts(self):\n",
        "        \"\"\"Extract transcripts from diagnosis dataset\"\"\"\n",
        "        transcripts = {'ad': [], 'cn': []}\n",
        "\n",
        "        base_path = os.path.join(self.extracted_path, \"ADReSSo21/diagnosis/train/segmentation\")\n",
        "\n",
        "        if os.path.exists(base_path):\n",
        "            # AD (Alzheimer's Disease) cases\n",
        "            ad_path = os.path.join(base_path, \"ad\")\n",
        "            if os.path.exists(ad_path):\n",
        "                csv_files = self.find_csv_files(ad_path)\n",
        "                print(f\"Found {len(csv_files)} CSV files in AD directory\")\n",
        "\n",
        "                for csv_file in csv_files:\n",
        "                    df = self.read_transcript_csv(csv_file)\n",
        "                    if df is not None:\n",
        "                        df['label'] = 'ad'\n",
        "                        transcripts['ad'].append(df)\n",
        "\n",
        "            # CN (Cognitive Normal) cases\n",
        "            cn_path = os.path.join(base_path, \"cn\")\n",
        "            if os.path.exists(cn_path):\n",
        "                csv_files = self.find_csv_files(cn_path)\n",
        "                print(f\"Found {len(csv_files)} CSV files in CN directory\")\n",
        "\n",
        "                for csv_file in csv_files:\n",
        "                    df = self.read_transcript_csv(csv_file)\n",
        "                    if df is not None:\n",
        "                        df['label'] = 'cn'\n",
        "                        transcripts['cn'].append(df)\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def combine_and_save_transcripts(self, transcripts, dataset_name):\n",
        "        \"\"\"Combine transcript dataframes and save to CSV\"\"\"\n",
        "        all_transcripts = []\n",
        "\n",
        "        if dataset_name == 'progression':\n",
        "            # Combine training data\n",
        "            for label in ['decline', 'no_decline']:\n",
        "                if transcripts['train'][label]:\n",
        "                    combined = pd.concat(transcripts['train'][label], ignore_index=True)\n",
        "                    all_transcripts.append(combined)\n",
        "\n",
        "            # Combine test data\n",
        "            if transcripts['test']:\n",
        "                combined_test = pd.concat(transcripts['test'], ignore_index=True)\n",
        "                all_transcripts.append(combined_test)\n",
        "\n",
        "        elif dataset_name == 'diagnosis':\n",
        "            # Combine AD and CN data\n",
        "            for label in ['ad', 'cn']:\n",
        "                if transcripts[label]:\n",
        "                    combined = pd.concat(transcripts[label], ignore_index=True)\n",
        "                    all_transcripts.append(combined)\n",
        "\n",
        "        if all_transcripts:\n",
        "            final_df = pd.concat(all_transcripts, ignore_index=True)\n",
        "\n",
        "            # Save to CSV\n",
        "            output_path = os.path.join(self.base_path, f\"{dataset_name}_transcripts.csv\")\n",
        "            final_df.to_csv(output_path, index=False)\n",
        "            print(f\"✓ Saved {len(final_df)} transcript records to {output_path}\")\n",
        "\n",
        "            return final_df\n",
        "\n",
        "        return None\n",
        "\n",
        "    def display_sample_data(self, df, dataset_name):\n",
        "        \"\"\"Display sample data and statistics\"\"\"\n",
        "        print(f\"\\n=== {dataset_name.upper()} DATASET SUMMARY ===\")\n",
        "        print(f\"Total records: {len(df)}\")\n",
        "\n",
        "        if 'label' in df.columns:\n",
        "            print(\"\\nLabel distribution:\")\n",
        "            print(df['label'].value_counts())\n",
        "\n",
        "        print(f\"\\nColumns: {list(df.columns)}\")\n",
        "\n",
        "        print(f\"\\nSample data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Show some transcript samples if available\n",
        "        text_columns = [col for col in df.columns if 'text' in col.lower() or 'transcript' in col.lower() or 'word' in col.lower()]\n",
        "        if text_columns:\n",
        "            print(f\"\\nSample transcript content from column '{text_columns[0]}':\")\n",
        "            for i, text in enumerate(df[text_columns[0]].dropna().head(3)):\n",
        "                print(f\"Sample {i+1}: {str(text)[:200]}...\")\n",
        "\n",
        "    def run_extraction(self):\n",
        "        \"\"\"Main method to run the complete extraction process\"\"\"\n",
        "        print(\"Starting ADReSSo21 transcript extraction...\")\n",
        "\n",
        "        # Extract datasets\n",
        "        self.extract_datasets()\n",
        "\n",
        "        # Extract progression transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTING PROGRESSION TRANSCRIPTS\")\n",
        "        print(\"=\"*50)\n",
        "        progression_transcripts = self.extract_progression_transcripts()\n",
        "        progression_df = self.combine_and_save_transcripts(progression_transcripts, 'progression')\n",
        "\n",
        "        if progression_df is not None:\n",
        "            self.display_sample_data(progression_df, 'progression')\n",
        "\n",
        "        # Extract diagnosis transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTING DIAGNOSIS TRANSCRIPTS\")\n",
        "        print(\"=\"*50)\n",
        "        diagnosis_transcripts = self.extract_diagnosis_transcripts()\n",
        "        diagnosis_df = self.combine_and_save_transcripts(diagnosis_transcripts, 'diagnosis')\n",
        "\n",
        "        if diagnosis_df is not None:\n",
        "            self.display_sample_data(diagnosis_df, 'diagnosis')\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTION COMPLETE!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return progression_df, diagnosis_df\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize extractor\n",
        "    extractor = ADReSSo21TranscriptExtractor()\n",
        "\n",
        "    # Run extraction\n",
        "    progression_df, diagnosis_df = extractor.run_extraction()\n",
        "\n",
        "    # Optional: Access individual datasets\n",
        "    # You can also use these methods individually:\n",
        "    # extractor.extract_datasets()\n",
        "    # progression_transcripts = extractor.extract_progression_transcripts()\n",
        "    # diagnosis_transcripts = extractor.extract_diagnosis_transcripts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple step-by-step transcript extraction for ADReSSo21 dataset\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "\n",
        "# Set your paths\n",
        "BASE_PATH = \"/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/drive/MyDrive/Voice/extracted/\"\n",
        "\n",
        "# Step 1: Extract the datasets\n",
        "def extract_all_datasets():\n",
        "    \"\"\"Extract all three tgz files\"\"\"\n",
        "    datasets = [\n",
        "        'ADReSSo21-progression-train.tgz',\n",
        "        'ADReSSo21-progression-test.tgz',\n",
        "        'ADReSSo21-diagnosis-train.tgz'\n",
        "    ]\n",
        "\n",
        "    os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "\n",
        "    for dataset in datasets:\n",
        "        file_path = os.path.join(BASE_PATH, dataset)\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"Extracting {dataset}...\")\n",
        "            with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=EXTRACT_PATH)\n",
        "            print(f\"✓ Done\")\n",
        "        else:\n",
        "            print(f\"⚠ {dataset} not found\")\n",
        "\n",
        "# Step 2: Read all CSV files from a directory\n",
        "def read_all_csvs_from_directory(directory_path, label=None):\n",
        "    \"\"\"Read all CSV files from a directory and combine them\"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    if not os.path.exists(directory_path):\n",
        "        print(f\"Directory not found: {directory_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    csv_files = []\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                csv_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(csv_files)} CSV files in {directory_path}\")\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        try:\n",
        "            # Try multiple encodings\n",
        "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_file, encoding=encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            # Add metadata\n",
        "            df['file_id'] = os.path.splitext(os.path.basename(csv_file))[0]\n",
        "            df['file_path'] = csv_file\n",
        "            if label:\n",
        "                df['label'] = label\n",
        "\n",
        "            all_data.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {csv_file}: {e}\")\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Step 3: Extract transcripts step by step\n",
        "\n",
        "print(\"Step 1: Extracting datasets...\")\n",
        "extract_all_datasets()\n",
        "\n",
        "print(\"\\nStep 2: Reading progression training data...\")\n",
        "# Progression training - decline cases\n",
        "decline_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/train/segmentation/decline/\")\n",
        "decline_df = read_all_csvs_from_directory(decline_path, label='decline')\n",
        "\n",
        "# Progression training - no decline cases\n",
        "no_decline_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/train/segmentation/no_decline/\")\n",
        "no_decline_df = read_all_csvs_from_directory(no_decline_path, label='no_decline')\n",
        "\n",
        "# Combine progression training data\n",
        "if not decline_df.empty and not no_decline_df.empty:\n",
        "    progression_train_df = pd.concat([decline_df, no_decline_df], ignore_index=True)\n",
        "elif not decline_df.empty:\n",
        "    progression_train_df = decline_df\n",
        "elif not no_decline_df.empty:\n",
        "    progression_train_df = no_decline_df\n",
        "else:\n",
        "    progression_train_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Progression training data: {len(progression_train_df)} records\")\n",
        "\n",
        "print(\"\\nStep 3: Reading progression test data...\")\n",
        "# Progression test data\n",
        "test_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/test-dist/segmentation/\")\n",
        "progression_test_df = read_all_csvs_from_directory(test_path, label='test')\n",
        "print(f\"Progression test data: {len(progression_test_df)} records\")\n",
        "\n",
        "print(\"\\nStep 4: Reading diagnosis data...\")\n",
        "# Diagnosis - AD cases\n",
        "ad_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/segmentation/ad/\")\n",
        "ad_df = read_all_csvs_from_directory(ad_path, label='ad')\n",
        "\n",
        "# Diagnosis - CN cases\n",
        "cn_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/segmentation/cn/\")\n",
        "cn_df = read_all_csvs_from_directory(cn_path, label='cn')\n",
        "\n",
        "# Combine diagnosis data\n",
        "if not ad_df.empty and not cn_df.empty:\n",
        "    diagnosis_df = pd.concat([ad_df, cn_df], ignore_index=True)\n",
        "elif not ad_df.empty:\n",
        "    diagnosis_df = ad_df\n",
        "elif not cn_df.empty:\n",
        "    diagnosis_df = cn_df\n",
        "else:\n",
        "    diagnosis_df = pd.DataFrame()\n",
        "\n",
        "print(f\"Diagnosis data: {len(diagnosis_df)} records\")\n",
        "\n",
        "print(\"\\nStep 5: Saving results...\")\n",
        "\n",
        "# Save progression data\n",
        "if not progression_train_df.empty:\n",
        "    progression_train_df.to_csv(os.path.join(BASE_PATH, \"progression_train_transcripts.csv\"), index=False)\n",
        "    print(\"✓ Saved progression_train_transcripts.csv\")\n",
        "\n",
        "if not progression_test_df.empty:\n",
        "    progression_test_df.to_csv(os.path.join(BASE_PATH, \"progression_test_transcripts.csv\"), index=False)\n",
        "    print(\"✓ Saved progression_test_transcripts.csv\")\n",
        "\n",
        "# Save diagnosis data\n",
        "if not diagnosis_df.empty:\n",
        "    diagnosis_df.to_csv(os.path.join(BASE_PATH, \"diagnosis_transcripts.csv\"), index=False)\n",
        "    print(\"✓ Saved diagnosis_transcripts.csv\")\n",
        "\n",
        "print(\"\\nStep 6: Summary\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Display summaries\n",
        "if not progression_train_df.empty:\n",
        "    print(f\"\\nProgression Training Data:\")\n",
        "    print(f\"Total records: {len(progression_train_df)}\")\n",
        "    print(f\"Columns: {list(progression_train_df.columns)}\")\n",
        "    if 'label' in progression_train_df.columns:\n",
        "        print(\"Label distribution:\")\n",
        "        print(progression_train_df['label'].value_counts())\n",
        "    print(\"\\nSample data:\")\n",
        "    print(progression_train_df.head(3))\n",
        "\n",
        "if not progression_test_df.empty:\n",
        "    print(f\"\\nProgression Test Data:\")\n",
        "    print(f\"Total records: {len(progression_test_df)}\")\n",
        "    print(f\"Columns: {list(progression_test_df.columns)}\")\n",
        "    print(\"\\nSample data:\")\n",
        "    print(progression_test_df.head(3))\n",
        "\n",
        "if not diagnosis_df.empty:\n",
        "    print(f\"\\nDiagnosis Data:\")\n",
        "    print(f\"Total records: {len(diagnosis_df)}\")\n",
        "    print(f\"Columns: {list(diagnosis_df.columns)}\")\n",
        "    if 'label' in diagnosis_df.columns:\n",
        "        print(\"Label distribution:\")\n",
        "        print(diagnosis_df['label'].value_counts())\n",
        "    print(\"\\nSample data:\")\n",
        "    print(diagnosis_df.head(3))\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXTRACTION COMPLETE!\")\n",
        "print(\"Your transcript files are saved in:\", BASE_PATH)\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foH5BMW0xqBn",
        "outputId": "1a2fa1fd-6800-4f45-c5c8-633fe72e2242"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Extracting datasets...\n",
            "⚠ ADReSSo21-progression-train.tgz not found\n",
            "⚠ ADReSSo21-progression-test.tgz not found\n",
            "⚠ ADReSSo21-diagnosis-train.tgz not found\n",
            "\n",
            "Step 2: Reading progression training data...\n",
            "Directory not found: /drive/MyDrive/Voice/extracted/ADReSSo21/progression/train/segmentation/decline/\n",
            "Directory not found: /drive/MyDrive/Voice/extracted/ADReSSo21/progression/train/segmentation/no_decline/\n",
            "Progression training data: 0 records\n",
            "\n",
            "Step 3: Reading progression test data...\n",
            "Directory not found: /drive/MyDrive/Voice/extracted/ADReSSo21/progression/test-dist/segmentation/\n",
            "Progression test data: 0 records\n",
            "\n",
            "Step 4: Reading diagnosis data...\n",
            "Directory not found: /drive/MyDrive/Voice/extracted/ADReSSo21/diagnosis/train/segmentation/ad/\n",
            "Directory not found: /drive/MyDrive/Voice/extracted/ADReSSo21/diagnosis/train/segmentation/cn/\n",
            "Diagnosis data: 0 records\n",
            "\n",
            "Step 5: Saving results...\n",
            "\n",
            "Step 6: Summary\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "EXTRACTION COMPLETE!\n",
            "Your transcript files are saved in: /drive/MyDrive/Voice/\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4B7Zj3PzRLv",
        "outputId": "e8dd2f42-8163-4376-c524-725ab01bfc09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.11/dist-packages (3.14.3)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-by-Step Audio Transcript Extractor for ADReSSo21 Dataset\n",
        "# This script will:\n",
        "# 1. Mount Google Drive\n",
        "# 2. Extract dataset files\n",
        "# 3. Find all WAV files\n",
        "# 4. Extract transcripts from audio using speech recognition\n",
        "# 5. Save organized transcripts\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: MOUNT GOOGLE DRIVE\n",
        "print(\"\\nSTEP 1: Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"⚠ Not running in Colab or Drive already mounted\")\n",
        "\n",
        "# STEP 2: INSTALL REQUIRED PACKAGES\n",
        "print(\"\\nSTEP 2: Installing required packages...\")\n",
        "print(\"Installing speech recognition and audio processing libraries...\")\n",
        "\n",
        "# Uncomment these lines to install packages (run once)\n",
        "# !pip install SpeechRecognition\n",
        "# !pip install pydub\n",
        "# !pip install librosa\n",
        "# !pip install soundfile\n",
        "# !apt-get install -y ffmpeg\n",
        "\n",
        "print(\"✓ Packages ready (make sure to install them first)\")\n",
        "\n",
        "# STEP 3: SET UP PATHS AND CONFIGURATION\n",
        "print(\"\\nSTEP 3: Setting up paths and configuration...\")\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Voice/extracted/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Voice/transcripts/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "datasets = {\n",
        "    'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "    'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "    'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "}\n",
        "\n",
        "print(f\"✓ Base path: {BASE_PATH}\")\n",
        "print(f\"✓ Extract path: {EXTRACT_PATH}\")\n",
        "print(f\"✓ Output path: {OUTPUT_PATH}\")\n",
        "\n",
        "# STEP 4: EXTRACT DATASET FILES\n",
        "print(\"\\nSTEP 4: Extracting dataset files...\")\n",
        "\n",
        "def extract_datasets():\n",
        "    \"\"\"Extract all tgz files\"\"\"\n",
        "    for dataset_name, filename in datasets.items():\n",
        "        file_path = os.path.join(BASE_PATH, filename)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"  Extracting {filename}...\")\n",
        "            try:\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=EXTRACT_PATH)\n",
        "                print(f\"  ✓ {filename} extracted successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error extracting {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"  ⚠ {filename} not found at {file_path}\")\n",
        "\n",
        "extract_datasets()\n",
        "\n",
        "# STEP 5: FIND ALL WAV FILES\n",
        "print(\"\\nSTEP 5: Finding all WAV files...\")\n",
        "\n",
        "def find_wav_files():\n",
        "    \"\"\"Find all WAV files and organize by dataset and label\"\"\"\n",
        "    wav_files = {\n",
        "        'progression_train': {'decline': [], 'no_decline': []},\n",
        "        'progression_test': [],\n",
        "        'diagnosis_train': {'ad': [], 'cn': []}\n",
        "    }\n",
        "\n",
        "    # Progression training files\n",
        "    prog_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/train/audio/\")\n",
        "\n",
        "    # Decline cases\n",
        "    decline_path = os.path.join(prog_train_base, \"decline/\")\n",
        "    if os.path.exists(decline_path):\n",
        "        decline_wavs = [f for f in os.listdir(decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['decline'] = [os.path.join(decline_path, f) for f in decline_wavs]\n",
        "        print(f\"  Found {len(decline_wavs)} decline WAV files\")\n",
        "\n",
        "    # No decline cases\n",
        "    no_decline_path = os.path.join(prog_train_base, \"no_decline/\")\n",
        "    if os.path.exists(no_decline_path):\n",
        "        no_decline_wavs = [f for f in os.listdir(no_decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['no_decline'] = [os.path.join(no_decline_path, f) for f in no_decline_wavs]\n",
        "        print(f\"  Found {len(no_decline_wavs)} no_decline WAV files\")\n",
        "\n",
        "    # Progression test files\n",
        "    prog_test_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/test-dist/audio/\")\n",
        "    if os.path.exists(prog_test_path):\n",
        "        test_wavs = [f for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_test'] = [os.path.join(prog_test_path, f) for f in test_wavs]\n",
        "        print(f\"  Found {len(test_wavs)} test WAV files\")\n",
        "\n",
        "    # Diagnosis training files\n",
        "    diag_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/audio/\")\n",
        "\n",
        "    # AD cases\n",
        "    ad_path = os.path.join(diag_train_base, \"ad/\")\n",
        "    if os.path.exists(ad_path):\n",
        "        ad_wavs = [f for f in os.listdir(ad_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['ad'] = [os.path.join(ad_path, f) for f in ad_wavs]\n",
        "        print(f\"  Found {len(ad_wavs)} AD WAV files\")\n",
        "\n",
        "    # CN cases\n",
        "    cn_path = os.path.join(diag_train_base, \"cn/\")\n",
        "    if os.path.exists(cn_path):\n",
        "        cn_wavs = [f for f in os.listdir(cn_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['cn'] = [os.path.join(cn_path, f) for f in cn_wavs]\n",
        "        print(f\"  Found {len(cn_wavs)} CN WAV files\")\n",
        "\n",
        "    return wav_files\n",
        "\n",
        "wav_files = find_wav_files()\n",
        "\n",
        "# STEP 6: AUDIO PREPROCESSING FUNCTIONS\n",
        "print(\"\\nSTEP 6: Setting up audio preprocessing...\")\n",
        "\n",
        "def preprocess_audio(audio_path, target_sr=16000):\n",
        "    \"\"\"Preprocess audio file for speech recognition\"\"\"\n",
        "    try:\n",
        "        # Load audio with librosa\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "\n",
        "        # Normalize audio\n",
        "        audio = librosa.util.normalize(audio)\n",
        "\n",
        "        # Remove silence\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)\n",
        "\n",
        "        return audio_trimmed, target_sr\n",
        "    except Exception as e:\n",
        "        print(f\"    Error preprocessing {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def convert_to_wav_if_needed(audio_path):\n",
        "    \"\"\"Convert audio to WAV format if needed\"\"\"\n",
        "    try:\n",
        "        if not audio_path.endswith('.wav'):\n",
        "            # Convert using pydub\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            wav_path = audio_path.rsplit('.', 1)[0] + '_converted.wav'\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            return wav_path\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"    Error converting {audio_path}: {e}\")\n",
        "        return audio_path\n",
        "\n",
        "# STEP 7: SPEECH RECOGNITION FUNCTION\n",
        "print(\"\\nSTEP 7: Setting up speech recognition...\")\n",
        "\n",
        "def extract_transcript_from_audio(audio_path, method='google'):\n",
        "    \"\"\"Extract transcript from audio file using speech recognition\"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    try:\n",
        "        # Convert to WAV if needed\n",
        "        wav_path = convert_to_wav_if_needed(audio_path)\n",
        "\n",
        "        # Preprocess audio\n",
        "        audio_data, sr_rate = preprocess_audio(wav_path, target_sr=16000)\n",
        "\n",
        "        if audio_data is None:\n",
        "            return None, \"Preprocessing failed\"\n",
        "\n",
        "        # Save preprocessed audio temporarily\n",
        "        temp_wav = audio_path.replace('.wav', '_temp.wav')\n",
        "        sf.write(temp_wav, audio_data, sr_rate)\n",
        "\n",
        "        # Use speech recognition\n",
        "        with sr.AudioFile(temp_wav) as source:\n",
        "            # Adjust for ambient noise\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio = recognizer.listen(source)\n",
        "\n",
        "        # Try different recognition methods\n",
        "        transcript = None\n",
        "        error_msg = \"\"\n",
        "\n",
        "        if method == 'google':\n",
        "            try:\n",
        "                transcript = recognizer.recognize_google(audio)\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg = \"Google Speech Recognition could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg = f\"Google Speech Recognition error: {e}\"\n",
        "\n",
        "        # Fallback to other methods if Google fails\n",
        "        if transcript is None:\n",
        "            try:\n",
        "                transcript = recognizer.recognize_sphinx(audio)\n",
        "                method = 'sphinx'\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg += \"; Sphinx could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg += f\"; Sphinx error: {e}\"\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "\n",
        "        if transcript:\n",
        "            return transcript.strip(), method\n",
        "        else:\n",
        "            return None, error_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error processing audio: {str(e)}\"\n",
        "\n",
        "# STEP 8: PROCESS ALL AUDIO FILES AND EXTRACT TRANSCRIPTS\n",
        "print(\"\\nSTEP 8: Processing audio files and extracting transcripts...\")\n",
        "print(\"This may take a while depending on the number and length of audio files...\")\n",
        "\n",
        "def process_audio_files(wav_files):\n",
        "    \"\"\"Process all audio files and extract transcripts\"\"\"\n",
        "    all_transcripts = []\n",
        "\n",
        "    # Process progression training data\n",
        "    print(\"\\n  Processing progression training data...\")\n",
        "    for label in ['decline', 'no_decline']:\n",
        "        files = wav_files['progression_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'progression_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    # Process progression test data\n",
        "    print(\"\\n  Processing progression test data...\")\n",
        "    files = wav_files['progression_test']\n",
        "    print(f\"    Processing {len(files)} test files...\")\n",
        "\n",
        "    for i, audio_path in enumerate(files):\n",
        "        print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "        transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "        all_transcripts.append({\n",
        "            'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "            'file_path': audio_path,\n",
        "            'dataset': 'progression_test',\n",
        "            'label': 'test',\n",
        "            'transcript': transcript,\n",
        "            'recognition_method': method_or_error if transcript else None,\n",
        "            'error': None if transcript else method_or_error,\n",
        "            'success': transcript is not None\n",
        "        })\n",
        "\n",
        "    # Process diagnosis training data\n",
        "    print(\"\\n  Processing diagnosis training data...\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        files = wav_files['diagnosis_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'diagnosis_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    return all_transcripts\n",
        "\n",
        "# Process all files\n",
        "transcripts = process_audio_files(wav_files)\n",
        "\n",
        "# STEP 9: SAVE RESULTS\n",
        "print(\"\\nSTEP 9: Saving transcription results...\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(transcripts)\n",
        "\n",
        "# Save complete results\n",
        "complete_output = os.path.join(OUTPUT_PATH, \"all_transcripts.csv\")\n",
        "df.to_csv(complete_output, index=False)\n",
        "print(f\"✓ Saved complete results to: {complete_output}\")\n",
        "\n",
        "# Save successful transcripts only\n",
        "successful_df = df[df['success'] == True].copy()\n",
        "success_output = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "successful_df.to_csv(success_output, index=False)\n",
        "print(f\"✓ Saved successful transcripts to: {success_output}\")\n",
        "\n",
        "# Save by dataset\n",
        "datasets_to_save = df['dataset'].unique()\n",
        "for dataset in datasets_to_save:\n",
        "    dataset_df = df[df['dataset'] == dataset].copy()\n",
        "    dataset_output = os.path.join(OUTPUT_PATH, f\"{dataset}_transcripts.csv\")\n",
        "    dataset_df.to_csv(dataset_output, index=False)\n",
        "    print(f\"✓ Saved {dataset} transcripts to: {dataset_output}\")\n",
        "\n",
        "# STEP 10: DISPLAY SUMMARY STATISTICS\n",
        "print(\"\\nSTEP 10: Summary Statistics\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_files = len(df)\n",
        "successful = len(successful_df)\n",
        "failed = total_files - successful\n",
        "\n",
        "print(f\"Total audio files processed: {total_files}\")\n",
        "print(f\"Successful transcriptions: {successful} ({successful/total_files*100:.1f}%)\")\n",
        "print(f\"Failed transcriptions: {failed} ({failed/total_files*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nDataset breakdown:\")\n",
        "for dataset in df['dataset'].unique():\n",
        "    dataset_total = len(df[df['dataset'] == dataset])\n",
        "    dataset_success = len(df[(df['dataset'] == dataset) & (df['success'] == True)])\n",
        "    print(f\"  {dataset}: {dataset_success}/{dataset_total} successful ({dataset_success/dataset_total*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nLabel distribution (successful transcripts only):\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['label'].value_counts())\n",
        "\n",
        "print(f\"\\nRecognition methods used:\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['recognition_method'].value_counts())\n",
        "\n",
        "# Show sample transcripts\n",
        "print(f\"\\nSample successful transcripts:\")\n",
        "sample_transcripts = successful_df['transcript'].dropna().head(3)\n",
        "for i, transcript in enumerate(sample_transcripts):\n",
        "    print(f\"  Sample {i+1}: {transcript[:200]}...\")\n",
        "\n",
        "# Show common errors\n",
        "print(f\"\\nMost common errors:\")\n",
        "error_df = df[df['success'] == False]\n",
        "if not error_df.empty:\n",
        "    error_counts = error_df['error'].value_counts().head(5)\n",
        "    for error, count in error_counts.items():\n",
        "        print(f\"  {error}: {count} files\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRANSCRIPT EXTRACTION COMPLETE!\")\n",
        "print(f\"All results saved in: {OUTPUT_PATH}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56RZ8BRdy4pM",
        "outputId": "74b8629c-9a46-42a7-e130-826d00c7e54d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\n",
            "============================================================\n",
            "\n",
            "STEP 1: Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted successfully!\n",
            "\n",
            "STEP 2: Installing required packages...\n",
            "Installing speech recognition and audio processing libraries...\n",
            "✓ Packages ready (make sure to install them first)\n",
            "\n",
            "STEP 3: Setting up paths and configuration...\n",
            "✓ Base path: /content/drive/MyDrive/Voice/\n",
            "✓ Extract path: /content/drive/MyDrive/Voice/extracted/\n",
            "✓ Output path: /content/drive/MyDrive/Voice/transcripts/\n",
            "\n",
            "STEP 4: Extracting dataset files...\n",
            "  Extracting ADReSSo21-progression-train.tgz...\n",
            "  ✓ ADReSSo21-progression-train.tgz extracted successfully\n",
            "  Extracting ADReSSo21-progression-test.tgz...\n"
          ]
        }
      ]
    }
  ]
}