{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOy2t3sXyaBHIuhrJ8LPFM+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/ADReSSo21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4B7Zj3PzRLv",
        "outputId": "2cfa641e-1a59-468a-d566-e0a2456d4367"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SpeechRecognition) (4.13.2)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.14.3 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step-by-Step Audio Transcript Extractor for ADReSSo21 Dataset\n",
        "# This script will:\n",
        "# 1. Mount Google Drive\n",
        "# 2. Extract dataset files\n",
        "# 3. Find all WAV files\n",
        "# 4. Extract transcripts from audio using speech recognition\n",
        "# 5. Save organized transcripts\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: MOUNT GOOGLE DRIVE\n",
        "print(\"\\nSTEP 1: Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"✓ Google Drive mounted successfully!\")\n",
        "except:\n",
        "    print(\"⚠ Not running in Colab or Drive already mounted\")\n",
        "\n",
        "# STEP 2: INSTALL REQUIRED PACKAGES\n",
        "print(\"\\nSTEP 2: Installing required packages...\")\n",
        "print(\"Installing speech recognition and audio processing libraries...\")\n",
        "\n",
        "# Install packages (run once)\n",
        "!pip install SpeechRecognition\n",
        "!pip install pydub\n",
        "!pip install librosa\n",
        "!pip install soundfile\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "print(\"✓ Packages ready (make sure to install them first)\")\n",
        "\n",
        "# STEP 3: SET UP PATHS AND CONFIGURATION\n",
        "print(\"\\nSTEP 3: Setting up paths and configuration...\")\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/Voice/\"\n",
        "EXTRACT_PATH = \"/content/drive/MyDrive/Voice/extracted/\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Voice/transcripts/\"\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(EXTRACT_PATH, exist_ok=True)\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "datasets = {\n",
        "    'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "    'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "    'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "}\n",
        "\n",
        "print(f\"✓ Base path: {BASE_PATH}\")\n",
        "print(f\"✓ Extract path: {EXTRACT_PATH}\")\n",
        "print(f\"✓ Output path: {OUTPUT_PATH}\")\n",
        "\n",
        "# STEP 4: EXTRACT DATASET FILES\n",
        "print(\"\\nSTEP 4: Extracting dataset files...\")\n",
        "\n",
        "def extract_datasets():\n",
        "    \"\"\"Extract all tgz files\"\"\"\n",
        "    for dataset_name, filename in datasets.items():\n",
        "        file_path = os.path.join(BASE_PATH, filename)\n",
        "\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"  Extracting {filename}...\")\n",
        "            try:\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=EXTRACT_PATH)\n",
        "                print(f\"  ✓ {filename} extracted successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠ Error extracting {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"  ⚠ {filename} not found at {file_path}\")\n",
        "\n",
        "extract_datasets()\n",
        "\n",
        "# STEP 5: FIND ALL WAV FILES\n",
        "print(\"\\nSTEP 5: Finding all WAV files...\")\n",
        "\n",
        "def find_wav_files():\n",
        "    \"\"\"Find all WAV files and organize by dataset and label\"\"\"\n",
        "    wav_files = {\n",
        "        'progression_train': {'decline': [], 'no_decline': []},\n",
        "        'progression_test': [],\n",
        "        'diagnosis_train': {'ad': [], 'cn': []}\n",
        "    }\n",
        "\n",
        "    # Progression training files\n",
        "    prog_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/train/audio/\")\n",
        "\n",
        "    # Decline cases\n",
        "    decline_path = os.path.join(prog_train_base, \"decline/\")\n",
        "    if os.path.exists(decline_path):\n",
        "        decline_wavs = [f for f in os.listdir(decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['decline'] = [os.path.join(decline_path, f) for f in decline_wavs]\n",
        "        print(f\"  Found {len(decline_wavs)} decline WAV files\")\n",
        "\n",
        "    # No decline cases\n",
        "    no_decline_path = os.path.join(prog_train_base, \"no_decline/\")\n",
        "    if os.path.exists(no_decline_path):\n",
        "        no_decline_wavs = [f for f in os.listdir(no_decline_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_train']['no_decline'] = [os.path.join(no_decline_path, f) for f in no_decline_wavs]\n",
        "        print(f\"  Found {len(no_decline_wavs)} no_decline WAV files\")\n",
        "\n",
        "    # Progression test files\n",
        "    prog_test_path = os.path.join(EXTRACT_PATH, \"ADReSSo21/progression/test-dist/audio/\")\n",
        "    if os.path.exists(prog_test_path):\n",
        "        test_wavs = [f for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "        wav_files['progression_test'] = [os.path.join(prog_test_path, f) for f in test_wavs]\n",
        "        print(f\"  Found {len(test_wavs)} test WAV files\")\n",
        "\n",
        "    # Diagnosis training files\n",
        "    diag_train_base = os.path.join(EXTRACT_PATH, \"ADReSSo21/diagnosis/train/audio/\")\n",
        "\n",
        "    # AD cases\n",
        "    ad_path = os.path.join(diag_train_base, \"ad/\")\n",
        "    if os.path.exists(ad_path):\n",
        "        ad_wavs = [f for f in os.listdir(ad_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['ad'] = [os.path.join(ad_path, f) for f in ad_wavs]\n",
        "        print(f\"  Found {len(ad_wavs)} AD WAV files\")\n",
        "\n",
        "    # CN cases\n",
        "    cn_path = os.path.join(diag_train_base, \"cn/\")\n",
        "    if os.path.exists(cn_path):\n",
        "        cn_wavs = [f for f in os.listdir(cn_path) if f.endswith('.wav')]\n",
        "        wav_files['diagnosis_train']['cn'] = [os.path.join(cn_path, f) for f in cn_wavs]\n",
        "        print(f\"  Found {len(cn_wavs)} CN WAV files\")\n",
        "\n",
        "    return wav_files\n",
        "\n",
        "wav_files = find_wav_files()\n",
        "\n",
        "# STEP 6: AUDIO PREPROCESSING FUNCTIONS\n",
        "print(\"\\nSTEP 6: Setting up audio preprocessing...\")\n",
        "\n",
        "def preprocess_audio(audio_path, target_sr=16000):\n",
        "    \"\"\"Preprocess audio file for speech recognition\"\"\"\n",
        "    try:\n",
        "        # Load audio with librosa\n",
        "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
        "\n",
        "        # Normalize audio\n",
        "        audio = librosa.util.normalize(audio)\n",
        "\n",
        "        # Remove silence\n",
        "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=20)\n",
        "\n",
        "        return audio_trimmed, target_sr\n",
        "    except Exception as e:\n",
        "        print(f\"    Error preprocessing {audio_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def convert_to_wav_if_needed(audio_path):\n",
        "    \"\"\"Convert audio to WAV format if needed\"\"\"\n",
        "    try:\n",
        "        if not audio_path.endswith('.wav'):\n",
        "            # Convert using pydub\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "            wav_path = audio_path.rsplit('.', 1)[0] + '_converted.wav'\n",
        "            audio.export(wav_path, format=\"wav\")\n",
        "            return wav_path\n",
        "        return audio_path\n",
        "    except Exception as e:\n",
        "        print(f\"    Error converting {audio_path}: {e}\")\n",
        "        return audio_path\n",
        "\n",
        "# STEP 7: SPEECH RECOGNITION FUNCTION\n",
        "print(\"\\nSTEP 7: Setting up speech recognition...\")\n",
        "\n",
        "def extract_transcript_from_audio(audio_path, method='google'):\n",
        "    \"\"\"Extract transcript from audio file using speech recognition\"\"\"\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    try:\n",
        "        # Convert to WAV if needed\n",
        "        wav_path = convert_to_wav_if_needed(audio_path)\n",
        "\n",
        "        # Preprocess audio\n",
        "        audio_data, sr_rate = preprocess_audio(wav_path, target_sr=16000)\n",
        "\n",
        "        if audio_data is None:\n",
        "            return None, \"Preprocessing failed\"\n",
        "\n",
        "        # Save preprocessed audio temporarily\n",
        "        temp_wav = audio_path.replace('.wav', '_temp.wav')\n",
        "        sf.write(temp_wav, audio_data, sr_rate)\n",
        "\n",
        "        # Use speech recognition\n",
        "        with sr.AudioFile(temp_wav) as source:\n",
        "            # Adjust for ambient noise\n",
        "            recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio = recognizer.listen(source)\n",
        "\n",
        "        # Try different recognition methods\n",
        "        transcript = None\n",
        "        error_msg = \"\"\n",
        "\n",
        "        if method == 'google':\n",
        "            try:\n",
        "                transcript = recognizer.recognize_google(audio)\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg = \"Google Speech Recognition could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg = f\"Google Speech Recognition error: {e}\"\n",
        "\n",
        "        # Fallback to other methods if Google fails\n",
        "        if transcript is None:\n",
        "            try:\n",
        "                transcript = recognizer.recognize_sphinx(audio)\n",
        "                method = 'sphinx'\n",
        "            except sr.UnknownValueError:\n",
        "                error_msg += \"; Sphinx could not understand audio\"\n",
        "            except sr.RequestError as e:\n",
        "                error_msg += f\"; Sphinx error: {e}\"\n",
        "\n",
        "        # Clean up temporary file\n",
        "        if os.path.exists(temp_wav):\n",
        "            os.remove(temp_wav)\n",
        "\n",
        "        if transcript:\n",
        "            return transcript.strip(), method\n",
        "        else:\n",
        "            return None, error_msg\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error processing audio: {str(e)}\"\n",
        "\n",
        "# STEP 8: PROCESS ALL AUDIO FILES AND EXTRACT TRANSCRIPTS\n",
        "print(\"\\nSTEP 8: Processing audio files and extracting transcripts...\")\n",
        "print(\"This may take a while depending on the number and length of audio files...\")\n",
        "\n",
        "def process_audio_files(wav_files):\n",
        "    \"\"\"Process all audio files and extract transcripts\"\"\"\n",
        "    all_transcripts = []\n",
        "\n",
        "    # Process progression training data\n",
        "    print(\"\\n  Processing progression training data...\")\n",
        "    for label in ['decline', 'no_decline']:\n",
        "        files = wav_files['progression_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'progression_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    # Process progression test data\n",
        "    print(\"\\n  Processing progression test data...\")\n",
        "    files = wav_files['progression_test']\n",
        "    print(f\"    Processing {len(files)} test files...\")\n",
        "\n",
        "    for i, audio_path in enumerate(files):\n",
        "        print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "        transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "        all_transcripts.append({\n",
        "            'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "            'file_path': audio_path,\n",
        "            'dataset': 'progression_test',\n",
        "            'label': 'test',\n",
        "            'transcript': transcript,\n",
        "            'recognition_method': method_or_error if transcript else None,\n",
        "            'error': None if transcript else method_or_error,\n",
        "            'success': transcript is not None\n",
        "        })\n",
        "\n",
        "    # Process diagnosis training data\n",
        "    print(\"\\n  Processing diagnosis training data...\")\n",
        "    for label in ['ad', 'cn']:\n",
        "        files = wav_files['diagnosis_train'][label]\n",
        "        print(f\"    Processing {len(files)} {label} files...\")\n",
        "\n",
        "        for i, audio_path in enumerate(files):\n",
        "            print(f\"      Processing {i+1}/{len(files)}: {os.path.basename(audio_path)}\")\n",
        "\n",
        "            transcript, method_or_error = extract_transcript_from_audio(audio_path)\n",
        "\n",
        "            all_transcripts.append({\n",
        "                'file_id': os.path.splitext(os.path.basename(audio_path))[0],\n",
        "                'file_path': audio_path,\n",
        "                'dataset': 'diagnosis_train',\n",
        "                'label': label,\n",
        "                'transcript': transcript,\n",
        "                'recognition_method': method_or_error if transcript else None,\n",
        "                'error': None if transcript else method_or_error,\n",
        "                'success': transcript is not None\n",
        "            })\n",
        "\n",
        "    return all_transcripts\n",
        "\n",
        "# Process all files\n",
        "transcripts = process_audio_files(wav_files)\n",
        "\n",
        "# STEP 9: SAVE RESULTS\n",
        "print(\"\\nSTEP 9: Saving transcription results...\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(transcripts)\n",
        "\n",
        "# Save complete results\n",
        "complete_output = os.path.join(OUTPUT_PATH, \"all_transcripts.csv\")\n",
        "df.to_csv(complete_output, index=False)\n",
        "print(f\"✓ Saved complete results to: {complete_output}\")\n",
        "\n",
        "# Save successful transcripts only\n",
        "successful_df = df[df['success'] == True].copy()\n",
        "success_output = os.path.join(OUTPUT_PATH, \"successful_transcripts.csv\")\n",
        "successful_df.to_csv(success_output, index=False)\n",
        "print(f\"✓ Saved successful transcripts to: {success_output}\")\n",
        "\n",
        "# Save by dataset\n",
        "datasets_to_save = df['dataset'].unique()\n",
        "for dataset in datasets_to_save:\n",
        "    dataset_df = df[df['dataset'] == dataset].copy()\n",
        "    dataset_output = os.path.join(OUTPUT_PATH, f\"{dataset}_transcripts.csv\")\n",
        "    dataset_df.to_csv(dataset_output, index=False)\n",
        "    print(f\"✓ Saved {dataset} transcripts to: {dataset_output}\")\n",
        "\n",
        "# STEP 10: DISPLAY SUMMARY STATISTICS\n",
        "print(\"\\nSTEP 10: Summary Statistics\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_files = len(df)\n",
        "successful = len(successful_df)\n",
        "failed = total_files - successful\n",
        "\n",
        "print(f\"Total audio files processed: {total_files}\")\n",
        "print(f\"Successful transcriptions: {successful} ({successful/total_files*100:.1f}%)\")\n",
        "print(f\"Failed transcriptions: {failed} ({failed/total_files*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nDataset breakdown:\")\n",
        "for dataset in df['dataset'].unique():\n",
        "    dataset_total = len(df[df['dataset'] == dataset])\n",
        "    dataset_success = len(df[(df['dataset'] == dataset) & (df['success'] == True)])\n",
        "    print(f\"  {dataset}: {dataset_success}/{dataset_total} successful ({dataset_success/dataset_total*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nLabel distribution (successful transcripts only):\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['label'].value_counts())\n",
        "\n",
        "print(f\"\\nRecognition methods used:\")\n",
        "if not successful_df.empty:\n",
        "    print(successful_df['recognition_method'].value_counts())\n",
        "\n",
        "# Show sample transcripts\n",
        "print(f\"\\nSample successful transcripts:\")\n",
        "sample_transcripts = successful_df['transcript'].dropna().head(3)\n",
        "for i, transcript in enumerate(sample_transcripts):\n",
        "    print(f\"  Sample {i+1}: {transcript[:200]}...\")\n",
        "\n",
        "# Show common errors\n",
        "print(f\"\\nMost common errors:\")\n",
        "error_df = df[df['success'] == False]\n",
        "if not error_df.empty:\n",
        "    error_counts = error_df['error'].value_counts().head(5)\n",
        "    for error, count in error_counts.items():\n",
        "        print(f\"  {error}: {count} files\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRANSCRIPT EXTRACTION COMPLETE!\")\n",
        "print(f\"All results saved in: {OUTPUT_PATH}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56RZ8BRdy4pM",
        "outputId": "59419cb2-f3c8-41e2-f915-5a84cd8c4a69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ADReSSo21 AUDIO TRANSCRIPT EXTRACTOR\n",
            "============================================================\n",
            "\n",
            "STEP 1: Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "✓ Google Drive mounted successfully!\n",
            "\n",
            "STEP 2: Installing required packages...\n",
            "Installing speech recognition and audio processing libraries...\n",
            "✓ Packages ready (make sure to install them first)\n",
            "\n",
            "STEP 3: Setting up paths and configuration...\n",
            "✓ Base path: /content/drive/MyDrive/Voice/\n",
            "✓ Extract path: /content/drive/MyDrive/Voice/extracted/\n",
            "✓ Output path: /content/drive/MyDrive/Voice/transcripts/\n",
            "\n",
            "STEP 4: Extracting dataset files...\n",
            "  Extracting ADReSSo21-progression-train.tgz...\n",
            "  ✓ ADReSSo21-progression-train.tgz extracted successfully\n",
            "  Extracting ADReSSo21-progression-test.tgz...\n",
            "  ✓ ADReSSo21-progression-test.tgz extracted successfully\n",
            "  Extracting ADReSSo21-diagnosis-train.tgz...\n",
            "  ✓ ADReSSo21-diagnosis-train.tgz extracted successfully\n",
            "\n",
            "STEP 5: Finding all WAV files...\n",
            "  Found 15 decline WAV files\n",
            "  Found 58 no_decline WAV files\n",
            "  Found 32 test WAV files\n",
            "  Found 87 AD WAV files\n",
            "  Found 79 CN WAV files\n",
            "\n",
            "STEP 6: Setting up audio preprocessing...\n",
            "\n",
            "STEP 7: Setting up speech recognition...\n",
            "\n",
            "STEP 8: Processing audio files and extracting transcripts...\n",
            "This may take a while depending on the number and length of audio files...\n",
            "\n",
            "  Processing progression training data...\n",
            "    Processing 15 decline files...\n",
            "      Processing 1/15: adrsp055.wav\n",
            "      Processing 2/15: adrsp003.wav\n",
            "      Processing 3/15: adrsp266.wav\n",
            "      Processing 4/15: adrsp300.wav\n",
            "      Processing 5/15: adrsp320.wav\n",
            "      Processing 6/15: adrsp313.wav\n",
            "      Processing 7/15: adrsp179.wav\n",
            "      Processing 8/15: adrsp357.wav\n",
            "      Processing 9/15: adrsp051.wav\n",
            "      Processing 10/15: adrsp101.wav\n",
            "      Processing 11/15: adrsp326.wav\n",
            "      Processing 12/15: adrsp127.wav\n",
            "      Processing 13/15: adrsp276.wav\n",
            "      Processing 14/15: adrsp209.wav\n",
            "      Processing 15/15: adrsp318.wav\n",
            "    Processing 58 no_decline files...\n",
            "      Processing 1/58: adrsp196.wav\n",
            "      Processing 2/58: adrsp137.wav\n",
            "      Processing 3/58: adrsp130.wav\n",
            "      Processing 4/58: adrsp349.wav\n",
            "      Processing 5/58: adrsp198.wav\n",
            "      Processing 6/58: adrsp321.wav\n",
            "      Processing 7/58: adrsp136.wav\n",
            "      Processing 8/58: adrsp024.wav\n",
            "      Processing 9/58: adrsp007.wav\n",
            "      Processing 10/58: adrsp382.wav\n",
            "      Processing 11/58: adrsp043.wav\n",
            "      Processing 12/58: adrsp019.wav\n",
            "      Processing 13/58: adrsp333.wav\n",
            "      Processing 14/58: adrsp056.wav\n",
            "      Processing 15/58: adrsp042.wav\n",
            "      Processing 16/58: adrsp310.wav\n",
            "      Processing 17/58: adrsp377.wav\n",
            "      Processing 18/58: adrsp363.wav\n",
            "      Processing 19/58: adrsp028.wav\n",
            "      Processing 20/58: adrsp350.wav\n",
            "      Processing 21/58: adrsp096.wav\n",
            "      Processing 22/58: adrsp052.wav\n",
            "      Processing 23/58: adrsp204.wav\n",
            "      Processing 24/58: adrsp380.wav\n",
            "      Processing 25/58: adrsp109.wav\n",
            "      Processing 26/58: adrsp255.wav\n",
            "      Processing 27/58: adrsp157.wav\n",
            "      Processing 28/58: adrsp306.wav\n",
            "      Processing 29/58: adrsp197.wav\n",
            "      Processing 30/58: adrsp031.wav\n",
            "      Processing 31/58: adrsp368.wav\n",
            "      Processing 32/58: adrsp032.wav\n",
            "      Processing 33/58: adrsp091.wav\n",
            "      Processing 34/58: adrsp344.wav\n",
            "      Processing 35/58: adrsp124.wav\n",
            "      Processing 36/58: adrsp195.wav\n",
            "      Processing 37/58: adrsp253.wav\n",
            "      Processing 38/58: adrsp251.wav\n",
            "      Processing 39/58: adrsp039.wav\n",
            "      Processing 40/58: adrsp001.wav\n",
            "      Processing 41/58: adrsp041.wav\n",
            "      Processing 42/58: adrsp384.wav\n",
            "      Processing 43/58: adrsp207.wav\n",
            "      Processing 44/58: adrsp379.wav\n",
            "      Processing 45/58: adrsp324.wav\n",
            "      Processing 46/58: adrsp177.wav\n",
            "      Processing 47/58: adrsp148.wav\n",
            "      Processing 48/58: adrsp023.wav\n",
            "      Processing 49/58: adrsp359.wav\n",
            "      Processing 50/58: adrsp122.wav\n",
            "      Processing 51/58: adrsp200.wav\n",
            "      Processing 52/58: adrsp030.wav\n",
            "      Processing 53/58: adrsp319.wav\n",
            "      Processing 54/58: adrsp378.wav\n",
            "      Processing 55/58: adrsp193.wav\n",
            "      Processing 56/58: adrsp128.wav\n",
            "      Processing 57/58: adrsp161.wav\n",
            "      Processing 58/58: adrsp192.wav\n",
            "\n",
            "  Processing progression test data...\n",
            "    Processing 32 test files...\n",
            "      Processing 1/32: adrspt20.wav\n",
            "      Processing 2/32: adrspt15.wav\n",
            "      Processing 3/32: adrspt4.wav\n",
            "      Processing 4/32: adrspt28.wav\n",
            "      Processing 5/32: adrspt16.wav\n",
            "      Processing 6/32: adrspt27.wav\n",
            "      Processing 7/32: adrspt9.wav\n",
            "      Processing 8/32: adrspt10.wav\n",
            "      Processing 9/32: adrspt13.wav\n",
            "      Processing 10/32: adrspt26.wav\n",
            "      Processing 11/32: adrspt23.wav\n",
            "      Processing 12/32: adrspt31.wav\n",
            "      Processing 13/32: adrspt14.wav\n",
            "      Processing 14/32: adrspt6.wav\n",
            "      Processing 15/32: adrspt12.wav\n",
            "      Processing 16/32: adrspt32.wav\n",
            "      Processing 17/32: adrspt21.wav\n",
            "      Processing 18/32: adrspt1.wav\n",
            "      Processing 19/32: adrspt29.wav\n",
            "      Processing 20/32: adrspt30.wav\n",
            "      Processing 21/32: adrspt3.wav\n",
            "      Processing 22/32: adrspt8.wav\n",
            "      Processing 23/32: adrspt19.wav\n",
            "      Processing 24/32: adrspt18.wav\n",
            "      Processing 25/32: adrspt25.wav\n",
            "      Processing 26/32: adrspt2.wav\n",
            "      Processing 27/32: adrspt24.wav\n",
            "      Processing 28/32: adrspt11.wav\n",
            "      Processing 29/32: adrspt17.wav\n",
            "      Processing 30/32: adrspt22.wav\n",
            "      Processing 31/32: adrspt7.wav\n",
            "      Processing 32/32: adrspt5.wav\n",
            "\n",
            "  Processing diagnosis training data...\n",
            "    Processing 87 ad files...\n",
            "      Processing 1/87: adrso047.wav\n",
            "      Processing 2/87: adrso128.wav\n",
            "      Processing 3/87: adrso045.wav\n",
            "      Processing 4/87: adrso110.wav\n",
            "      Processing 5/87: adrso036.wav\n",
            "      Processing 6/87: adrso189.wav\n",
            "      Processing 7/87: adrso093.wav\n",
            "      Processing 8/87: adrso112.wav\n",
            "      Processing 9/87: adrso205.wav\n",
            "      Processing 10/87: adrso089.wav\n",
            "      Processing 11/87: adrso060.wav\n",
            "      Processing 12/87: adrso232.wav\n",
            "      Processing 13/87: adrso075.wav\n",
            "      Processing 14/87: adrso063.wav\n",
            "      Processing 15/87: adrso106.wav\n",
            "      Processing 16/87: adrso202.wav\n",
            "      Processing 17/87: adrso043.wav\n",
            "      Processing 18/87: adrso206.wav\n",
            "      Processing 19/87: adrso039.wav\n",
            "      Processing 20/87: adrso109.wav\n",
            "      Processing 21/87: adrso126.wav\n",
            "      Processing 22/87: adrso071.wav\n",
            "      Processing 23/87: adrso209.wav\n",
            "      Processing 24/87: adrso244.wav\n",
            "      Processing 25/87: adrso228.wav\n",
            "      Processing 26/87: adrso122.wav\n",
            "      Processing 27/87: adrso116.wav\n",
            "      Processing 28/87: adrso141.wav\n",
            "      Processing 29/87: adrso130.wav\n",
            "      Processing 30/87: adrso248.wav\n",
            "      Processing 31/87: adrso070.wav\n",
            "      Processing 32/87: adrso055.wav\n",
            "      Processing 33/87: adrso222.wav\n",
            "      Processing 34/87: adrso190.wav\n",
            "      Processing 35/87: adrso215.wav\n",
            "      Processing 36/87: adrso223.wav\n",
            "      Processing 37/87: adrso192.wav\n",
            "      Processing 38/87: adrso236.wav\n",
            "      Processing 39/87: adrso234.wav\n",
            "      Processing 40/87: adrso059.wav\n",
            "      Processing 41/87: adrso098.wav\n",
            "      Processing 42/87: adrso090.wav\n",
            "      Processing 43/87: adrso250.wav\n",
            "      Processing 44/87: adrso025.wav\n",
            "      Processing 45/87: adrso031.wav\n",
            "      Processing 46/87: adrso197.wav\n",
            "      Processing 47/87: adrso224.wav\n",
            "      Processing 48/87: adrso074.wav\n",
            "      Processing 49/87: adrso049.wav\n",
            "      Processing 50/87: adrso211.wav\n",
            "      Processing 51/87: adrso229.wav\n",
            "      Processing 52/87: adrso138.wav\n",
            "      Processing 53/87: adrso123.wav\n",
            "      Processing 54/87: adrso027.wav\n",
            "      Processing 55/87: adrso072.wav\n",
            "      Processing 56/87: adrso056.wav\n",
            "      Processing 57/87: adrso068.wav\n",
            "      Processing 58/87: adrso054.wav\n",
            "      Processing 59/87: adrso187.wav\n",
            "      Processing 60/87: adrso078.wav\n",
            "      Processing 61/87: adrso053.wav\n",
            "      Processing 62/87: adrso200.wav\n",
            "      Processing 63/87: adrso249.wav\n",
            "      Processing 64/87: adrso028.wav\n",
            "      Processing 65/87: adrso245.wav\n",
            "      Processing 66/87: adrso216.wav\n",
            "      Processing 67/87: adrso092.wav\n",
            "      Processing 68/87: adrso220.wav\n",
            "      Processing 69/87: adrso134.wav\n",
            "      Processing 70/87: adrso142.wav\n",
            "      Processing 71/87: adrso198.wav\n",
            "      Processing 72/87: adrso077.wav\n",
            "      Processing 73/87: adrso024.wav\n",
            "      Processing 74/87: adrso212.wav\n",
            "      Processing 75/87: adrso046.wav\n",
            "      Processing 76/87: adrso035.wav\n",
            "      Processing 77/87: adrso233.wav\n",
            "      Processing 78/87: adrso247.wav\n",
            "      Processing 79/87: adrso033.wav\n",
            "      Processing 80/87: adrso125.wav\n",
            "      Processing 81/87: adrso188.wav\n",
            "      Processing 82/87: adrso237.wav\n",
            "      Processing 83/87: adrso032.wav\n",
            "      Processing 84/87: adrso253.wav\n",
            "      Processing 85/87: adrso218.wav\n",
            "      Processing 86/87: adrso144.wav\n",
            "      Processing 87/87: adrso246.wav\n",
            "    Processing 79 cn files...\n",
            "      Processing 1/79: adrso173.wav\n",
            "      Processing 2/79: adrso015.wav\n",
            "      Processing 3/79: adrso307.wav\n",
            "      Processing 4/79: adrso283.wav\n",
            "      Processing 5/79: adrso167.wav\n",
            "      Processing 6/79: adrso168.wav\n",
            "      Processing 7/79: adrso172.wav\n",
            "      Processing 8/79: adrso292.wav\n",
            "      Processing 9/79: adrso316.wav\n",
            "      Processing 10/79: adrso162.wav\n",
            "      Processing 11/79: adrso296.wav\n",
            "      Processing 12/79: adrso278.wav\n",
            "      Processing 13/79: adrso300.wav\n",
            "      Processing 14/79: adrso291.wav\n",
            "      Processing 15/79: adrso169.wav\n",
            "      Processing 16/79: adrso178.wav\n",
            "      Processing 17/79: adrso165.wav\n",
            "      Processing 18/79: adrso262.wav\n",
            "      Processing 19/79: adrso177.wav\n",
            "      Processing 20/79: adrso265.wav\n",
            "      Processing 21/79: adrso014.wav\n",
            "      Processing 22/79: adrso261.wav\n",
            "      Processing 23/79: adrso268.wav\n",
            "      Processing 24/79: adrso021.wav\n",
            "      Processing 25/79: adrso156.wav\n",
            "      Processing 26/79: adrso310.wav\n",
            "      Processing 27/79: adrso016.wav\n",
            "      Processing 28/79: adrso148.wav\n",
            "      Processing 29/79: adrso302.wav\n",
            "      Processing 30/79: adrso308.wav\n",
            "      Processing 31/79: adrso018.wav\n",
            "      Processing 32/79: adrso309.wav\n",
            "      Processing 33/79: adrso180.wav\n",
            "      Processing 34/79: adrso298.wav\n",
            "      Processing 35/79: adrso154.wav\n",
            "      Processing 36/79: adrso273.wav\n",
            "      Processing 37/79: adrso259.wav\n",
            "      Processing 38/79: adrso151.wav\n",
            "      Processing 39/79: adrso159.wav\n",
            "      Processing 40/79: adrso267.wav\n",
            "      Processing 41/79: adrso274.wav\n",
            "      Processing 42/79: adrso019.wav\n",
            "      Processing 43/79: adrso153.wav\n",
            "      Processing 44/79: adrso023.wav\n",
            "      Processing 45/79: adrso012.wav\n",
            "      Processing 46/79: adrso280.wav\n",
            "      Processing 47/79: adrso002.wav\n",
            "      Processing 48/79: adrso266.wav\n",
            "      Processing 49/79: adrso022.wav\n",
            "      Processing 50/79: adrso007.wav\n",
            "      Processing 51/79: adrso152.wav\n",
            "      Processing 52/79: adrso276.wav\n",
            "      Processing 53/79: adrso260.wav\n",
            "      Processing 54/79: adrso005.wav\n",
            "      Processing 55/79: adrso017.wav\n",
            "      Processing 56/79: adrso299.wav\n",
            "      Processing 57/79: adrso157.wav\n",
            "      Processing 58/79: adrso182.wav\n",
            "      Processing 59/79: adrso008.wav\n",
            "      Processing 60/79: adrso161.wav\n",
            "      Processing 61/79: adrso263.wav\n",
            "      Processing 62/79: adrso257.wav\n",
            "      Processing 63/79: adrso164.wav\n",
            "      Processing 64/79: adrso270.wav\n",
            "      Processing 65/79: adrso289.wav\n",
            "      Processing 66/79: adrso264.wav\n",
            "      Processing 67/79: adrso277.wav\n",
            "      Processing 68/79: adrso160.wav\n",
            "      Processing 69/79: adrso286.wav\n",
            "      Processing 70/79: adrso003.wav\n",
            "      Processing 71/79: adrso186.wav\n",
            "      Processing 72/79: adrso285.wav\n",
            "      Processing 73/79: adrso170.wav\n",
            "      Processing 74/79: adrso183.wav\n",
            "      Processing 75/79: adrso281.wav\n",
            "      Processing 76/79: adrso315.wav\n",
            "      Processing 77/79: adrso010.wav\n",
            "      Processing 78/79: adrso312.wav\n",
            "      Processing 79/79: adrso158.wav\n",
            "\n",
            "STEP 9: Saving transcription results...\n",
            "✓ Saved complete results to: /content/drive/MyDrive/Voice/transcripts/all_transcripts.csv\n",
            "✓ Saved successful transcripts to: /content/drive/MyDrive/Voice/transcripts/successful_transcripts.csv\n",
            "✓ Saved progression_train transcripts to: /content/drive/MyDrive/Voice/transcripts/progression_train_transcripts.csv\n",
            "✓ Saved progression_test transcripts to: /content/drive/MyDrive/Voice/transcripts/progression_test_transcripts.csv\n",
            "✓ Saved diagnosis_train transcripts to: /content/drive/MyDrive/Voice/transcripts/diagnosis_train_transcripts.csv\n",
            "\n",
            "STEP 10: Summary Statistics\n",
            "==================================================\n",
            "Total audio files processed: 271\n",
            "Successful transcriptions: 155 (57.2%)\n",
            "Failed transcriptions: 116 (42.8%)\n",
            "\n",
            "Dataset breakdown:\n",
            "  progression_train: 42/73 successful (57.5%)\n",
            "  progression_test: 16/32 successful (50.0%)\n",
            "  diagnosis_train: 97/166 successful (58.4%)\n",
            "\n",
            "Label distribution (successful transcripts only):\n",
            "label\n",
            "ad            51\n",
            "cn            46\n",
            "no_decline    34\n",
            "test          16\n",
            "decline        8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Recognition methods used:\n",
            "recognition_method\n",
            "google    155\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample successful transcripts:\n",
            "  Sample 1: you can start now...\n",
            "  Sample 2: is in 1 minute time I want you to name as many...\n",
            "  Sample 3: cat dog giraffe...\n",
            "\n",
            "Most common errors:\n",
            "  Google Speech Recognition could not understand audio; Sphinx error: missing PocketSphinx module: ensure that PocketSphinx is set up correctly.: 116 files\n",
            "\n",
            "============================================================\n",
            "TRANSCRIPT EXTRACTION COMPLETE!\n",
            "All results saved in: /content/drive/MyDrive/Voice/transcripts/\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Audio-Text AD Classification Pipeline\n",
        "# This script combines audio feature extraction, BERT processing, and DARTS classification\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For BERT processing\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPREHENSIVE AD CLASSIFICATION PIPELINE\")\n",
        "print(\"Audio Features + BERT + DARTS Architecture\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "IuuPR--FzKPd",
        "outputId": "037d4448-81b7-4c6a-cdfa-6ed596332a90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPREHENSIVE AD CLASSIFICATION PIPELINE\n",
            "Audio Features + BERT + DARTS Architecture\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 1: ADVANCED AUDIO FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self, sr=16000, n_mfcc=13, n_fft=2048, hop_length=512):\n",
        "        self.sr = sr\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def extract_mfcc_features(self, audio_path):\n",
        "        \"\"\"Extract comprehensive MFCC features including deltas\"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            # Extract MFCC features\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc,\n",
        "                                       n_fft=self.n_fft, hop_length=self.hop_length)\n",
        "\n",
        "            # Extract delta features (first derivative)\n",
        "            delta_mfccs = librosa.feature.delta(mfccs)\n",
        "\n",
        "            # Extract delta-delta features (second derivative)\n",
        "            delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "            # Combine all MFCC features\n",
        "            combined_mfccs = np.concatenate([mfccs, delta_mfccs, delta2_mfccs], axis=0)\n",
        "\n",
        "            # Statistical features for each coefficient\n",
        "            features = {}\n",
        "\n",
        "            # Mean, std, min, max for each coefficient\n",
        "            features['mfcc_mean'] = np.mean(combined_mfccs, axis=1)\n",
        "            features['mfcc_std'] = np.std(combined_mfccs, axis=1)\n",
        "            features['mfcc_min'] = np.min(combined_mfccs, axis=1)\n",
        "            features['mfcc_max'] = np.max(combined_mfccs, axis=1)\n",
        "            features['mfcc_median'] = np.median(combined_mfccs, axis=1)\n",
        "            features['mfcc_skew'] = self._calculate_skewness(combined_mfccs)\n",
        "            features['mfcc_kurtosis'] = self._calculate_kurtosis(combined_mfccs)\n",
        "\n",
        "            return features, combined_mfccs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting MFCC from {audio_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def extract_spectral_features(self, audio_path):\n",
        "        \"\"\"Extract spectral features\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # Spectral centroid\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "            features['spectral_centroid_mean'] = np.mean(spectral_centroid)\n",
        "            features['spectral_centroid_std'] = np.std(spectral_centroid)\n",
        "\n",
        "            # Spectral bandwidth\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "            features['spectral_bandwidth_mean'] = np.mean(spectral_bandwidth)\n",
        "            features['spectral_bandwidth_std'] = np.std(spectral_bandwidth)\n",
        "\n",
        "            # Spectral rolloff\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "            features['spectral_rolloff_mean'] = np.mean(spectral_rolloff)\n",
        "            features['spectral_rolloff_std'] = np.std(spectral_rolloff)\n",
        "\n",
        "            # Zero crossing rate\n",
        "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "            features['zcr_mean'] = np.mean(zcr)\n",
        "            features['zcr_std'] = np.std(zcr)\n",
        "\n",
        "            # Chroma features\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            features['chroma_mean'] = np.mean(chroma, axis=1)\n",
        "            features['chroma_std'] = np.std(chroma, axis=1)\n",
        "\n",
        "            # Tempo\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            features['tempo'] = tempo\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting spectral features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_prosodic_features(self, audio_path):\n",
        "        \"\"\"Extract prosodic features (pitch, energy, etc.)\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # Fundamental frequency (pitch)\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'),\n",
        "                                                       fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "            # Remove NaN values\n",
        "            f0_clean = f0[~np.isnan(f0)]\n",
        "            if len(f0_clean) > 0:\n",
        "                features['f0_mean'] = np.mean(f0_clean)\n",
        "                features['f0_std'] = np.std(f0_clean)\n",
        "                features['f0_min'] = np.min(f0_clean)\n",
        "                features['f0_max'] = np.max(f0_clean)\n",
        "                features['f0_range'] = np.max(f0_clean) - np.min(f0_clean)\n",
        "            else:\n",
        "                features.update({\n",
        "                    'f0_mean': 0, 'f0_std': 0, 'f0_min': 0,\n",
        "                    'f0_max': 0, 'f0_range': 0\n",
        "                })\n",
        "\n",
        "            # RMS energy\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "            features['rms_mean'] = np.mean(rms)\n",
        "            features['rms_std'] = np.std(rms)\n",
        "\n",
        "            # Spectral contrast\n",
        "            contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            features['contrast_mean'] = np.mean(contrast, axis=1)\n",
        "            features['contrast_std'] = np.std(contrast, axis=1)\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting prosodic features from {audio_path}: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _calculate_skewness(self, data):\n",
        "        \"\"\"Calculate skewness for each row\"\"\"\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1  # Avoid division by zero\n",
        "        normalized = (data - mean) / std\n",
        "        skewness = np.mean(normalized**3, axis=1)\n",
        "        return skewness\n",
        "\n",
        "    def _calculate_kurtosis(self, data):\n",
        "        \"\"\"Calculate kurtosis for each row\"\"\"\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        std[std == 0] = 1  # Avoid division by zero\n",
        "        normalized = (data - mean) / std\n",
        "        kurtosis = np.mean(normalized**4, axis=1) - 3\n",
        "        return kurtosis\n",
        "\n",
        "    def extract_all_features(self, audio_path):\n",
        "        \"\"\"Extract all audio features\"\"\"\n",
        "        all_features = {}\n",
        "\n",
        "        # MFCC features\n",
        "        mfcc_features, mfcc_matrix = self.extract_mfcc_features(audio_path)\n",
        "        if mfcc_features:\n",
        "            all_features.update(mfcc_features)\n",
        "\n",
        "        # Spectral features\n",
        "        spectral_features = self.extract_spectral_features(audio_path)\n",
        "        all_features.update(spectral_features)\n",
        "\n",
        "        # Prosodic features\n",
        "        prosodic_features = self.extract_prosodic_features(audio_path)\n",
        "        all_features.update(prosodic_features)\n",
        "\n",
        "        # Flatten nested arrays\n",
        "        flattened_features = {}\n",
        "        for key, value in all_features.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                if value.ndim == 1:\n",
        "                    for i, v in enumerate(value):\n",
        "                        flattened_features[f\"{key}_{i}\"] = v\n",
        "                else:\n",
        "                    flattened_features[key] = np.mean(value)\n",
        "            else:\n",
        "                flattened_features[key] = value\n",
        "\n",
        "        return flattened_features, mfcc_matrix\n"
      ],
      "metadata": {
        "id": "2bTxcZLizRWu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# PART 2: BERT TEXT PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class BERTTextProcessor:\n",
        "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        if pd.isna(text) or text is None:\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to string and clean\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove special characters but keep spaces and basic punctuation\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\!\\?]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def extract_bert_features(self, text):\n",
        "        \"\"\"Extract BERT embeddings from text\"\"\"\n",
        "        try:\n",
        "            # Preprocess text\n",
        "            clean_text = self.preprocess_text(text)\n",
        "\n",
        "            if not clean_text:\n",
        "                # Return zero vector for empty text\n",
        "                return np.zeros(768)\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                clean_text,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Extract features\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "                # Use [CLS] token embedding (first token)\n",
        "                cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
        "\n",
        "                # Also compute mean pooling of all tokens\n",
        "                attention_mask = inputs['attention_mask']\n",
        "                token_embeddings = outputs.last_hidden_state\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "                sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "                mean_embedding = sum_embeddings / sum_mask\n",
        "                mean_embedding = mean_embedding.squeeze()\n",
        "\n",
        "                # Combine CLS and mean pooling\n",
        "                combined_embedding = (cls_embedding + mean_embedding) / 2\n",
        "\n",
        "                return combined_embedding.numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting BERT features: {e}\")\n",
        "            return np.zeros(768)\n",
        "\n",
        "    def extract_linguistic_features(self, text):\n",
        "        \"\"\"Extract basic linguistic features\"\"\"\n",
        "        try:\n",
        "            clean_text = self.preprocess_text(text)\n",
        "\n",
        "            if not clean_text:\n",
        "                return {\n",
        "                    'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                    'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0\n",
        "                }\n",
        "\n",
        "            words = clean_text.split()\n",
        "            sentences = clean_text.split('.')\n",
        "\n",
        "            features = {\n",
        "                'word_count': len(words),\n",
        "                'char_count': len(clean_text),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'question_count': clean_text.count('?'),\n",
        "                'exclamation_count': clean_text.count('!')\n",
        "            }\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting linguistic features: {e}\")\n",
        "            return {'word_count': 0, 'char_count': 0, 'avg_word_length': 0,\n",
        "                   'sentence_count': 0, 'question_count': 0, 'exclamation_count': 0}\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRNEytjwzVGs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 3: IMPROVED DARTS ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class ImprovedDARTSCell(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_ops=8):\n",
        "        super(ImprovedDARTSCell, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Ensure dimensions match for operations\n",
        "        if input_dim != output_dim:\n",
        "            self.projection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "        # Define possible operations with proper dimensionality\n",
        "        self.operations = nn.ModuleList([\n",
        "            nn.Identity(),  # Skip connection\n",
        "            nn.ReLU(),      # Activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU()),  # Linear + ReLU\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.Tanh()),  # Linear + Tanh\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Dropout(0.1)),  # With dropout\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim // 2), nn.ReLU(), nn.Linear(output_dim // 2, output_dim)),  # Bottleneck\n",
        "            nn.Sequential(nn.LayerNorm(output_dim), nn.ReLU()),  # Layer norm + activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Linear(output_dim, output_dim))  # Deep linear\n",
        "        ])\n",
        "\n",
        "        # Architecture parameters (alpha) - learnable weights for each operation\n",
        "        self.alpha = nn.Parameter(torch.randn(len(self.operations)))\n",
        "\n",
        "        # Temperature parameter for gumbel softmax (learnable)\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to correct dimension\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Apply Gumbel Softmax for differentiable architecture search\n",
        "        if self.training:\n",
        "            # Use Gumbel Softmax during training\n",
        "            gumbel_weights = F.gumbel_softmax(self.alpha, tau=self.temperature, hard=False)\n",
        "        else:\n",
        "            # Use regular softmax during evaluation\n",
        "            gumbel_weights = F.softmax(self.alpha / self.temperature, dim=0)\n",
        "\n",
        "        # Apply operations\n",
        "        outputs = []\n",
        "        for op in self.operations:\n",
        "            try:\n",
        "                out = op(x)\n",
        "                outputs.append(out)\n",
        "            except Exception as e:\n",
        "                # Fallback to identity if operation fails\n",
        "                outputs.append(x)\n",
        "\n",
        "        # Weighted combination of all operations\n",
        "        result = sum(w * out for w, out in zip(gumbel_weights, outputs))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_selected_operation(self):\n",
        "        \"\"\"Get the operation with highest weight (for inference)\"\"\"\n",
        "        selected_idx = torch.argmax(self.alpha).item()\n",
        "        return selected_idx, self.operations[selected_idx]\n",
        "\n",
        "class MultimodalDARTSClassifier(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=256, num_classes=2):\n",
        "        super(MultimodalDARTSClassifier, self).__init__()\n",
        "\n",
        "        # Audio processing branch with DARTS\n",
        "        self.audio_projection = nn.Sequential(\n",
        "            nn.Linear(audio_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.audio_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Text processing branch with DARTS\n",
        "        self.text_projection = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.text_darts_cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(2)\n",
        "        ])\n",
        "\n",
        "        # Cross-modal attention\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "\n",
        "        # Fusion and classification\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        # Process audio through DARTS\n",
        "        audio_x = self.audio_projection(audio_features)\n",
        "        for cell in self.audio_darts_cells:\n",
        "            residual = audio_x\n",
        "            audio_x = cell(audio_x)\n",
        "            audio_x = audio_x + residual  # Residual connection\n",
        "\n",
        "        # Process text through DARTS\n",
        "        text_x = self.text_projection(text_features)\n",
        "        for cell in self.text_darts_cells:\n",
        "            residual = text_x\n",
        "            text_x = cell(text_x)\n",
        "            text_x = text_x + residual  # Residual connection\n",
        "\n",
        "        # Cross-modal attention\n",
        "        audio_attended, _ = self.cross_attention(\n",
        "            audio_x.unsqueeze(1), text_x.unsqueeze(1), text_x.unsqueeze(1)\n",
        "        )\n",
        "        audio_attended = audio_attended.squeeze(1)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([audio_attended, text_x], dim=1)\n",
        "        fused = self.fusion(fused)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_architecture_info(self):\n",
        "        \"\"\"Get information about the learned architecture\"\"\"\n",
        "        arch_info = {}\n",
        "\n",
        "        # Audio DARTS info\n",
        "        for i, cell in enumerate(self.audio_darts_cells):\n",
        "            selected_idx, _ = cell.get_selected_operation()\n",
        "            arch_info[f'audio_cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "\n",
        "        # Text DARTS info\n",
        "        for i, cell in enumerate(self.text_darts_cells):\n",
        "            selected_idx, _ = cell.get_selected_operation()\n",
        "            arch_info[f'text_cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "\n",
        "        return arch_info\n"
      ],
      "metadata": {
        "id": "xlcNZpa1zZVJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PART 4: DATASET AND TRAINING UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "class ADDataset(Dataset):\n",
        "    def __init__(self, audio_features, text_features, labels):\n",
        "        self.audio_features = torch.FloatTensor(audio_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.audio_features[idx], self.text_features[idx], self.labels[idx]\n",
        "\n",
        "def visualize_features(audio_features, text_features, labels, feature_names=None):\n",
        "    \"\"\"Visualize feature distributions\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Audio feature distribution\n",
        "    axes[0,0].hist(audio_features[labels==0][:, 0], alpha=0.7, label='Control', bins=30)\n",
        "    axes[0,0].hist(audio_features[labels==1][:, 0], alpha=0.7, label='AD', bins=30)\n",
        "    axes[0,0].set_title('Audio Feature Distribution (First Feature)')\n",
        "    axes[0,0].legend()\n",
        "\n",
        "    # Text feature distribution\n",
        "    axes[0,1].hist(text_features[labels==0][:, 0], alpha=0.7, label='Control', bins=30)\n",
        "    axes[0,1].hist(text_features[labels==1][:, 0], alpha=0.7, label='AD', bins=30)\n",
        "    axes[0,1].set_title('Text Feature Distribution (First Feature)')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # Feature correlation heatmap (subset)\n",
        "    subset_audio = audio_features[:, :min(10, audio_features.shape[1])]\n",
        "    corr_matrix = np.corrcoef(subset_audio.T)\n",
        "    sns.heatmap(corr_matrix, ax=axes[1,0], cmap='coolwarm', center=0)\n",
        "    axes[1,0].set_title('Audio Feature Correlation (Subset)')\n",
        "\n",
        "    # Label distribution\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    axes[1,1].bar(['Control', 'AD'], counts)\n",
        "    axes[1,1].set_title('Label Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(train_losses, val_losses, val_accuracies):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Training and validation loss\n",
        "    axes[0].plot(train_losses, label='Training Loss')\n",
        "    axes[0].plot(val_losses, label='Validation Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Validation accuracy\n",
        "    axes[1].plot(val_accuracies, label='Validation Accuracy', color='green')\n",
        "    axes[1].set_title('Validation Accuracy')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Loss difference\n",
        "    loss_diff = np.array(val_losses) - np.array(train_losses)\n",
        "    axes[2].plot(loss_diff, label='Val - Train Loss', color='red')\n",
        "    axes[2].set_title('Overfitting Monitor')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Loss Difference')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n",
        "    \"\"\"Train the DARTS model\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Training on device: {device}\")\n",
        "    print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (audio_batch, text_batch, labels_batch) in enumerate(train_loader):\n",
        "            audio_batch, text_batch, labels_batch = audio_batch.to(device), text_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(audio_batch, text_batch)\n",
        "            loss = criterion(outputs, labels_batch)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels_batch.size(0)\n",
        "            train_correct += (predicted == labels_batch).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for audio_batch, text_batch, labels_batch in val_loader:\n",
        "                audio_batch, text_batch, labels_batch = audio_batch.to(device), text_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "                outputs = model(audio_batch, text_batch)\n",
        "                loss = criterion(outputs, labels_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels_batch.size(0)\n",
        "                val_correct += (predicted == labels_batch).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%')\n",
        "            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "            print(f'  LR: {current_lr:.6f}, Patience: {patience_counter}/15')\n",
        "            print()\n",
        "\n",
        "        # Early stopping condition\n",
        "        if patience_counter >= 15:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader, class_names=['Control', 'AD']):\n",
        "    \"\"\"Evaluate the trained model and return comprehensive metrics\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for audio_batch, text_batch, labels_batch in test_loader:\n",
        "            audio_batch, text_batch, labels_batch = audio_batch.to(device), text_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "            outputs = model(audio_batch, text_batch)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels_batch.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "    # AUC score\n",
        "    all_probabilities = np.array(all_probabilities)\n",
        "    if all_probabilities.shape[1] == 2:\n",
        "        auc_score = roc_auc_score(all_labels, all_probabilities[:, 1])\n",
        "    else:\n",
        "        auc_score = 0\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(all_labels, all_predictions, target_names=class_names)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Print results\n",
        "    print(\"=\"*50)\n",
        "    print(\"MODEL EVALUATION RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"AUC Score: {auc_score:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # Return metrics for further analysis\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'auc_score': auc_score,\n",
        "        'predictions': all_predictions,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probabilities,\n",
        "        'confusion_matrix': cm,\n",
        "        'classification_report': report\n",
        "    }"
      ],
      "metadata": {
        "id": "2SgZdh7FonhH"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}