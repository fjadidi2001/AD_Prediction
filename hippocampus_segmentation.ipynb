{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPd9wOEvEHdN2PH+4QUbbF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/hippocampus_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Rnz7jM6FGD"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Constants\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "# Seeding\n",
        "seed = 2019\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Define dataset paths (update these based on your folder structure)\n",
        "data = \"./seghipp0/images\"\n",
        "data_left = \"./seghipp0/masks/left\"\n",
        "data_right = \"./seghipp0/masks/right\"\n",
        "\n",
        "# Load image and mask file paths\n",
        "train_data = []\n",
        "for dirName, _, fileList in sorted(os.walk(data)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower():\n",
        "            train_data.append(os.path.join(dirName, filename))\n",
        "\n",
        "mask_left = []\n",
        "for dirName, _, fileList in sorted(os.walk(data_left)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower():\n",
        "            mask_left.append(os.path.join(dirName, filename))\n",
        "\n",
        "mask_right = []\n",
        "for dirName, _, fileList in sorted(os.walk(data_right)):\n",
        "    for filename in fileList:\n",
        "        if \".jpg\" in filename.lower():\n",
        "            mask_right.append(os.path.join(dirName, filename))\n",
        "\n",
        "# Initialize arrays for training data\n",
        "X_train = np.zeros((len(train_data), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
        "Y_train = np.zeros((len(train_data), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
        "\n",
        "# Load and preprocess images\n",
        "for file_index in tqdm(range(len(train_data))):\n",
        "    img = imread(train_data[file_index])\n",
        "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n",
        "    img = img / 255.0\n",
        "    X_train[file_index] = img\n",
        "\n",
        "# Load and preprocess masks\n",
        "for n in tqdm(range(len(mask_right))):\n",
        "    maskl = imread(mask_left[n], as_gray=True)\n",
        "    maskr = imread(mask_right[n], as_gray=True)\n",
        "    mask = np.maximum(maskl, maskr)\n",
        "    mask = resize(mask, (IMG_HEIGHT, IMG_WIDTH, 1), mode='constant', preserve_range=True)\n",
        "    mask = mask / 255.0\n",
        "    Y_train[n] = mask\n",
        "\n",
        "# Data Generator\n",
        "class DataGen(keras.utils.Sequence):\n",
        "    def __init__(self, ids, path, batch_size=8, image_size=128):\n",
        "        self.ids = ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __load__(self, id_name):\n",
        "        image_path = os.path.join(self.path, \"images\", id_name + \".jpg\")\n",
        "        mask_path = os.path.join(self.path, \"masks\", id_name + \".jpg\")\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, (self.image_size, self.image_size))\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "        image = image / 255.0\n",
        "        mask = mask / 255.0\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if (index + 1) * self.batch_size > len(self.ids):\n",
        "            self.batch_size = len(self.ids) - index * self.batch_size\n",
        "\n",
        "        files_batch = self.ids[index * self.batch_size : (index + 1) * self.batch_size]\n",
        "\n",
        "        image = []\n",
        "        mask = []\n",
        "\n",
        "        for id_name in files_batch:\n",
        "            _img, _mask = self.__load__(id_name)\n",
        "            image.append(_img)\n",
        "            mask.append(_mask)\n",
        "\n",
        "        image = np.array(image)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.ids) / float(self.batch_size)))\n",
        "\n",
        "# Load train.csv (assuming it's in the dataset folder)\n",
        "dataset_path = \"./seghipp0/\"\n",
        "train_csv = pd.read_csv(dataset_path + \"train.csv\")\n",
        "train_ids = train_csv[\"id\"].values\n",
        "\n",
        "image_size = 128\n",
        "batch_size = 16\n",
        "val_data_size = 9\n",
        "\n",
        "valid_ids = train_ids[:val_data_size]\n",
        "train_ids = train_ids[val_data_size:]\n",
        "\n",
        "train_path = dataset_path\n",
        "gen = DataGen(train_ids, train_path, batch_size=batch_size, image_size=image_size)\n",
        "x, y = gen.__getitem__(0)\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "# Visualize a random sample\n",
        "r = random.randint(0, len(X_train) - 1)\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "ax = fig.add_subplot(1, 2, 1)\n",
        "ax.imshow(X_train[r])\n",
        "ax = fig.add_subplot(1, 2, 2)\n",
        "ax.imshow(np.reshape(Y_train[r] * 255, (image_size, image_size)), cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "# Define ResUNet model\n",
        "def bn_act(x, act=True):\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    if act:\n",
        "        x = keras.layers.Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = bn_act(x)\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
        "    return conv\n",
        "\n",
        "def stem(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    conv = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
        "    conv = conv_block(conv, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([conv, shortcut])\n",
        "    return output\n",
        "\n",
        "def residual_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n",
        "    res = conv_block(x, filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
        "    res = conv_block(res, filters, kernel_size=kernel_size, padding=padding, strides=1)\n",
        "\n",
        "    shortcut = keras.layers.Conv2D(filters, kernel_size=(1, 1), padding=padding, strides=strides)(x)\n",
        "    shortcut = bn_act(shortcut, act=False)\n",
        "\n",
        "    output = keras.layers.Add()([shortcut, res])\n",
        "    return output\n",
        "\n",
        "def upsample_concat_block(x, xskip):\n",
        "    u = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    c = keras.layers.Concatenate()([u, xskip])\n",
        "    return c\n",
        "\n",
        "def ResUNet():\n",
        "    f = [16, 32, 64, 128, 256]\n",
        "    inputs = keras.layers.Input((image_size, image_size, 3))\n",
        "\n",
        "    # Encoder\n",
        "    e0 = inputs\n",
        "    e1 = stem(e0, f[0])\n",
        "    e2 = residual_block(e1, f[1], strides=2)\n",
        "    e3 = residual_block(e2, f[2], strides=2)\n",
        "    e4 = residual_block(e3, f[3], strides=2)\n",
        "    e5 = residual_block(e4, f[4], strides=2)\n",
        "\n",
        "    # Bridge\n",
        "    b0 = conv_block(e5, f[4], strides=1)\n",
        "    b1 = conv_block(b0, f[4], strides=1)\n",
        "\n",
        "    # Decoder\n",
        "    u1 = upsample_concat_block(b1, e4)\n",
        "    d1 = residual_block(u1, f[4])\n",
        "\n",
        "    u2 = upsample_concat_block(d1, e3)\n",
        "    d2 = residual_block(u2, f[3])\n",
        "\n",
        "    u3 = upsample_concat_block(d2, e2)\n",
        "    d3 = residual_block(u3, f[2])\n",
        "\n",
        "    u4 = upsample_concat_block(d3, e1)\n",
        "    d4 = residual_block(u4, f[1])\n",
        "\n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define Dice coefficient and loss\n",
        "smooth = 1.\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred_f = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "# Build and compile model\n",
        "model = ResUNet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Optionally use Dice loss:\n",
        "# model.compile(optimizer='adam', loss=dice_coef_loss, metrics=[dice_coef])\n",
        "model.summary()\n",
        "\n",
        "# Train model\n",
        "results = model.fit(X_train, Y_train, validation_split=0.1, shuffle=True, batch_size=16, epochs=10)\n",
        "\n",
        "# Train with Data Generator\n",
        "train_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n",
        "\n",
        "train_steps = len(train_ids) // batch_size\n",
        "valid_steps = len(valid_ids) // batch_size\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "model.fit(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps,\n",
        "          validation_steps=valid_steps, epochs=epochs)\n",
        "\n",
        "# Save model weights\n",
        "model.save_weights(\"ResUNet.h5\")\n",
        "\n",
        "# Visualize predictions\n",
        "print(\"\\n      Ground Truth            Predicted Value\")\n",
        "for i in range(1, 5):\n",
        "    x, y = valid_gen.__getitem__(i)\n",
        "    result = model.predict(x)\n",
        "    result = result > 0.4\n",
        "\n",
        "    for j in range(len(result)):\n",
        "        fig = plt.figure()\n",
        "        fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "        ax = fig.add_subplot(1, 2, 1)\n",
        "        ax.imshow(np.reshape(y[j] * 255, (image_size, image_size)), cmap=\"gray\")\n",
        "        ax = fig.add_subplot(1, 2, 2)\n",
        "        ax.imshow(np.reshape(result[j] * 255, (image_size, image_size)), cmap=\"gray\")\n",
        "        plt.show()"
      ]
    }
  ]
}