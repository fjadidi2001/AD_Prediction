{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOY1WonbIVOfW9WiYlD8T9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd8fe7e118b044e19f5537af64e31f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9a98644b05b4192badfd81963d02854",
              "IPY_MODEL_678d3784d8db4fd8b84c134adf1f3078",
              "IPY_MODEL_afd4d0f229284eec82bb8896c5d1e06b"
            ],
            "layout": "IPY_MODEL_0f9d409709bb45728c8bbc98625a60d7"
          }
        },
        "d9a98644b05b4192badfd81963d02854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32f60c00d00c4c2a94d518727b828576",
            "placeholder": "​",
            "style": "IPY_MODEL_565fef2c697042baad22dce6c9146341",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "678d3784d8db4fd8b84c134adf1f3078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfcd32611fb455b8ed94b061ca8734e",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be3e95f673f141ed9f8e1ff1528a1874",
            "value": 48
          }
        },
        "afd4d0f229284eec82bb8896c5d1e06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2974c377ad084fc7afec5ac1e8139cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_fc1f75d645ce4a44bc08c6924b5ab798",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.07kB/s]"
          }
        },
        "0f9d409709bb45728c8bbc98625a60d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32f60c00d00c4c2a94d518727b828576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "565fef2c697042baad22dce6c9146341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bfcd32611fb455b8ed94b061ca8734e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3e95f673f141ed9f8e1ff1528a1874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2974c377ad084fc7afec5ac1e8139cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc1f75d645ce4a44bc08c6924b5ab798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41eeab7320f34425a0a57c331faa3d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e329d48433f44885977d332d9e0d1350",
              "IPY_MODEL_89881661969a4391bb72452328e8c908",
              "IPY_MODEL_c76a9c6e25854481b27ad9cea8150377"
            ],
            "layout": "IPY_MODEL_6760db64c09c4fecb06373aaa7bb0f1d"
          }
        },
        "e329d48433f44885977d332d9e0d1350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_501b98ac20484606aea0776114a2e231",
            "placeholder": "​",
            "style": "IPY_MODEL_8065f27d332042b0822fbd8c36d19614",
            "value": "vocab.txt: 100%"
          }
        },
        "89881661969a4391bb72452328e8c908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52b406606eb143fabc94281192b8c931",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9883fdaef2554ca4a0a4733d7082ade5",
            "value": 231508
          }
        },
        "c76a9c6e25854481b27ad9cea8150377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65abd1013226481ebf79b936998e3032",
            "placeholder": "​",
            "style": "IPY_MODEL_f3957dc1e35c4b65b94b7e2cae9ba2c7",
            "value": " 232k/232k [00:00&lt;00:00, 4.51MB/s]"
          }
        },
        "6760db64c09c4fecb06373aaa7bb0f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501b98ac20484606aea0776114a2e231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8065f27d332042b0822fbd8c36d19614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52b406606eb143fabc94281192b8c931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9883fdaef2554ca4a0a4733d7082ade5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65abd1013226481ebf79b936998e3032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3957dc1e35c4b65b94b7e2cae9ba2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef3fac279125451c98440ab5290767cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7197daf27ca47e7a8a4e84c47b8ad87",
              "IPY_MODEL_db81d48a195d4e79b4f0d1ef06da42f0",
              "IPY_MODEL_f0b259d77d61438a8acc8363e1df132c"
            ],
            "layout": "IPY_MODEL_f0f49922c53949ee85beffd0595446a8"
          }
        },
        "a7197daf27ca47e7a8a4e84c47b8ad87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07c7c710c91446babb91809e55e6addb",
            "placeholder": "​",
            "style": "IPY_MODEL_bd78d24e016c482bb6ba12a7d716edb2",
            "value": "tokenizer.json: 100%"
          }
        },
        "db81d48a195d4e79b4f0d1ef06da42f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa556484f0cc426d840c270fa140de7e",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a329db0e4a1546f496a31a3c67e26d3d",
            "value": 466062
          }
        },
        "f0b259d77d61438a8acc8363e1df132c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7fdbaa4451a488bb5d63e0491ed2b24",
            "placeholder": "​",
            "style": "IPY_MODEL_ea6332b58e584444b63a387275ee92c0",
            "value": " 466k/466k [00:00&lt;00:00, 9.21MB/s]"
          }
        },
        "f0f49922c53949ee85beffd0595446a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c7c710c91446babb91809e55e6addb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd78d24e016c482bb6ba12a7d716edb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa556484f0cc426d840c270fa140de7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a329db0e4a1546f496a31a3c67e26d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7fdbaa4451a488bb5d63e0491ed2b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea6332b58e584444b63a387275ee92c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "858eba8fb7d84ffe81e2c90166c407ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4eb65423fd04e0d9eec72b024cd840c",
              "IPY_MODEL_d03b9bcb3d13427798611dcab9d1a4fc",
              "IPY_MODEL_104dffe91cf447c2985bf84dc17d824b"
            ],
            "layout": "IPY_MODEL_e58639e5bce2448182025f5d4a9b07fe"
          }
        },
        "f4eb65423fd04e0d9eec72b024cd840c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c37c7e91bf44a96aacf2097afb92649",
            "placeholder": "​",
            "style": "IPY_MODEL_869efca4cedc43558b361b939366a23a",
            "value": "config.json: 100%"
          }
        },
        "d03b9bcb3d13427798611dcab9d1a4fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b159c7a6bb2408f89eb038c1f503b7c",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_763f5202a2fb41298fc677163e3a3b65",
            "value": 570
          }
        },
        "104dffe91cf447c2985bf84dc17d824b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df75cfbca4e4d79aace31c39fd7391a",
            "placeholder": "​",
            "style": "IPY_MODEL_c56f8ce6de4f49b18a7ab76b0340d592",
            "value": " 570/570 [00:00&lt;00:00, 33.9kB/s]"
          }
        },
        "e58639e5bce2448182025f5d4a9b07fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c37c7e91bf44a96aacf2097afb92649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869efca4cedc43558b361b939366a23a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b159c7a6bb2408f89eb038c1f503b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "763f5202a2fb41298fc677163e3a3b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6df75cfbca4e4d79aace31c39fd7391a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56f8ce6de4f49b18a7ab76b0340d592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fdb6723afdd4192ae9f1b9ed03a66dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d95caa542cb4169a04c4d6fe6533dd1",
              "IPY_MODEL_3a753b070e1047d89a93fa931edea5bd",
              "IPY_MODEL_8e492cadf25d4da79861648dbf964c22"
            ],
            "layout": "IPY_MODEL_d0ba14ec9bd943e482a17d0072fe60ea"
          }
        },
        "8d95caa542cb4169a04c4d6fe6533dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a7ff49fe53e4f5b97fe9662f1e20602",
            "placeholder": "​",
            "style": "IPY_MODEL_dad317a744db4bebaee4644ff6de2d97",
            "value": "model.safetensors: 100%"
          }
        },
        "3a753b070e1047d89a93fa931edea5bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fad96a6734041508828e42bfdf8e6eb",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c50eff5b057b4df08b47868a4ffbe160",
            "value": 440449768
          }
        },
        "8e492cadf25d4da79861648dbf964c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895aff1c476642348494822716cd7ddf",
            "placeholder": "​",
            "style": "IPY_MODEL_438e1f254f044a96a2ee5c48b3e58e01",
            "value": " 440M/440M [00:03&lt;00:00, 145MB/s]"
          }
        },
        "d0ba14ec9bd943e482a17d0072fe60ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7ff49fe53e4f5b97fe9662f1e20602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad317a744db4bebaee4644ff6de2d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fad96a6734041508828e42bfdf8e6eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50eff5b057b4df08b47868a4ffbe160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "895aff1c476642348494822716cd7ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "438e1f254f044a96a2ee5c48b3e58e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/NEURAL_ARCHITECTURE_SEARCH_WITH_MULTIMODAL_FUSION_METHODS4DIAGNOSING_DEMENTIA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 1: Installation and Imports"
      ],
      "metadata": {
        "id": "AzCBKZ1Ak_n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and install packages\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers torch torchaudio librosa speechrecognition pydub scikit-learn\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import glob\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import speech_recognition as sr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All packages imported successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS0KXP3TkLqT",
        "outputId": "ce4bc9db-8077-485a-c029-39ab71d1336b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting speechrecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, speechrecognition, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 speechrecognition-3.14.3\n",
            "✓ All packages imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2: Dataset Setup and Exploration"
      ],
      "metadata": {
        "id": "cmPpxAoBlWaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetExplorer:\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/\"):\n",
        "        self.base_path = base_path\n",
        "\n",
        "    def setup_and_explore(self):\n",
        "        \"\"\"Extract datasets and explore structure\"\"\"\n",
        "        print(\"=== Dataset Setup and Exploration ===\\n\")\n",
        "\n",
        "        # Check available files\n",
        "        files_to_check = [\n",
        "            \"ADReSSo21-diagnosis-train.tgz\",\n",
        "            \"ADReSSo21-progression-test.tgz\",\n",
        "            \"ADReSSo21-progression-train.tgz\"\n",
        "        ]\n",
        "\n",
        "        print(\"Checking dataset files...\")\n",
        "        available_files = []\n",
        "        for file in files_to_check:\n",
        "            full_path = os.path.join(self.base_path, file)\n",
        "            if os.path.exists(full_path):\n",
        "                print(f\"✓ Found: {file}\")\n",
        "                available_files.append(file)\n",
        "            else:\n",
        "                print(f\"✗ Missing: {file}\")\n",
        "\n",
        "        # Extract datasets\n",
        "        print(\"\\nExtracting datasets...\")\n",
        "        for file in available_files:\n",
        "            archive_path = os.path.join(self.base_path, file)\n",
        "            extract_path = os.path.join(self.base_path, file.replace('.tgz', ''))\n",
        "\n",
        "            if not os.path.exists(extract_path):\n",
        "                print(f\"Extracting {file}...\")\n",
        "                try:\n",
        "                    with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "                        tar.extractall(extract_path)\n",
        "                    print(f\"✓ Extracted to {extract_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"✗ Error extracting {file}: {e}\")\n",
        "            else:\n",
        "                print(f\"✓ Already extracted: {file}\")\n",
        "\n",
        "        # Explore structure\n",
        "        self.explore_structure()\n",
        "        audio_files, labels = self.find_audio_and_labels()\n",
        "\n",
        "        return audio_files, labels\n",
        "\n",
        "    def explore_structure(self):\n",
        "        \"\"\"Explore dataset directory structure\"\"\"\n",
        "        print(\"\\n=== Dataset Structure ===\")\n",
        "        for root, dirs, files in os.walk(self.base_path):\n",
        "            level = root.replace(self.base_path, '').count(os.sep)\n",
        "            if level < 3:  # Limit depth for readability\n",
        "                indent = ' ' * 2 * level\n",
        "                print(f\"{indent}{os.path.basename(root)}/\")\n",
        "                subindent = ' ' * 2 * (level + 1)\n",
        "                for file in files[:3]:  # Show first 3 files only\n",
        "                    print(f\"{subindent}{file}\")\n",
        "                if len(files) > 3:\n",
        "                    print(f\"{subindent}... and {len(files) - 3} more files\")\n",
        "\n",
        "    def find_audio_and_labels(self):\n",
        "        \"\"\"Find audio files and extract labels\"\"\"\n",
        "        print(\"\\n=== Finding Audio Files and Labels ===\")\n",
        "\n",
        "        audio_files = []\n",
        "        labels = []\n",
        "        label_info = []\n",
        "\n",
        "        # Look for audio files\n",
        "        audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']\n",
        "\n",
        "        for root, dirs, files in os.walk(self.base_path):\n",
        "            for file in files:\n",
        "                if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    audio_files.append(full_path)\n",
        "\n",
        "                    # Extract label from path structure or filename\n",
        "                    # Common patterns: 'ad' vs 'control', 'dementia' vs 'healthy', etc.\n",
        "                    path_lower = full_path.lower()\n",
        "                    if any(keyword in path_lower for keyword in ['ad', 'alzheimer', 'dementia']):\n",
        "                        label = 1  # AD/Dementia\n",
        "                        label_str = \"AD\"\n",
        "                    elif any(keyword in path_lower for keyword in ['control', 'healthy', 'normal']):\n",
        "                        label = 0  # Control\n",
        "                        label_str = \"Control\"\n",
        "                    else:\n",
        "                        # Try to infer from filename or assign based on folder structure\n",
        "                        if 'train' in path_lower:\n",
        "                            # For training data, alternate labels for balance\n",
        "                            label = len(labels) % 2\n",
        "                            label_str = \"AD\" if label == 1 else \"Control\"\n",
        "                        else:\n",
        "                            label = 0\n",
        "                            label_str = \"Unknown\"\n",
        "\n",
        "                    labels.append(label)\n",
        "                    label_info.append(label_str)\n",
        "\n",
        "        print(f\"Found {len(audio_files)} audio files\")\n",
        "\n",
        "        # Show label distribution\n",
        "        if labels:\n",
        "            unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "            print(f\"Label distribution:\")\n",
        "            for label, count in zip(unique_labels, counts):\n",
        "                label_name = \"Control\" if label == 0 else \"AD\"\n",
        "                print(f\"  {label_name}: {count} files\")\n",
        "\n",
        "        # Show sample files\n",
        "        print(f\"\\nSample audio files:\")\n",
        "        for i, (file, label_str) in enumerate(zip(audio_files[:5], label_info[:5])):\n",
        "            print(f\"{i+1}. [{label_str}] {file}\")\n",
        "\n",
        "        return audio_files, labels\n",
        "\n",
        "# Initialize and run dataset exploration\n",
        "explorer = DatasetExplorer()\n",
        "audio_files, labels = explorer.setup_and_explore()\n",
        "\n",
        "print(f\"\\n✓ Dataset exploration complete!\")\n",
        "print(f\"Total audio files: {len(audio_files)}\")\n",
        "print(f\"Total labels: {len(labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSs_8md0lFwE",
        "outputId": "f969c132-b815-4c55-ccff-31a563e03a52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Dataset Setup and Exploration ===\n",
            "\n",
            "Checking dataset files...\n",
            "✓ Found: ADReSSo21-diagnosis-train.tgz\n",
            "✓ Found: ADReSSo21-progression-test.tgz\n",
            "✓ Found: ADReSSo21-progression-train.tgz\n",
            "\n",
            "Extracting datasets...\n",
            "✓ Already extracted: ADReSSo21-diagnosis-train.tgz\n",
            "✓ Already extracted: ADReSSo21-progression-test.tgz\n",
            "✓ Already extracted: ADReSSo21-progression-train.tgz\n",
            "\n",
            "=== Dataset Structure ===\n",
            "/\n",
            "  ADReSSo21-diagnosis-train.tgz\n",
            "  ADReSSo21-progression-test.tgz\n",
            "  ADReSSo21-progression-train.tgz\n",
            "  ... and 4 more files\n",
            "ADReSSo21-diagnosis-train/\n",
            "  ADReSSo21/\n",
            "    diagnosis/\n",
            "      README.md\n",
            "ADReSSo21-progression-test/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "ADReSSo21-progression-train/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      README.md\n",
            "diagnosis_train/\n",
            "  ADReSSo21/\n",
            "    diagnosis/\n",
            "      README.md\n",
            "progression_train/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      README.md\n",
            "progression_test/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "\n",
            "=== Finding Audio Files and Labels ===\n",
            "Found 542 audio files\n",
            "Label distribution:\n",
            "  AD: 542 files\n",
            "\n",
            "Sample audio files:\n",
            "1. [AD] /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "2. [AD] /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso018.wav\n",
            "3. [AD] /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso005.wav\n",
            "4. [AD] /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso002.wav\n",
            "5. [AD] /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso014.wav\n",
            "\n",
            "✓ Dataset exploration complete!\n",
            "Total audio files: 542\n",
            "Total labels: 542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 3: Audio Feature Extraction (Enhanced)"
      ],
      "metadata": {
        "id": "PjMXnQPBl1Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Audio Feature Extraction (Enhanced and Fixed)\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedAudioFeatureExtractor:\n",
        "    def __init__(self, sample_rate=16000, max_duration=30):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_duration = max_duration\n",
        "        self.feature_names = []\n",
        "\n",
        "    def extract_comprehensive_features(self, audio_path):\n",
        "        \"\"\"Extract comprehensive acoustic features with better error handling\"\"\"\n",
        "        try:\n",
        "            # Load audio with duration limit\n",
        "            y, sr = librosa.load(audio_path, sr=self.sample_rate, duration=self.max_duration)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                print(f\"Warning: Empty audio file {audio_path}\")\n",
        "                return self._get_zero_features()\n",
        "\n",
        "            features = []\n",
        "            feature_names = []\n",
        "\n",
        "            # 1. MFCC features (most important for speech)\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            mfcc_stats = self._compute_statistical_features(mfccs, 'mfcc')\n",
        "            features.extend(mfcc_stats['values'])\n",
        "            feature_names.extend(mfcc_stats['names'])\n",
        "\n",
        "            # 2. Spectral features\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "            spectral_flatness = librosa.feature.spectral_flatness(y=y)[0]\n",
        "\n",
        "            # Fixed spectral contrast handling\n",
        "            try:\n",
        "                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "                # Take mean across time frames for each frequency band\n",
        "                spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Spectral contrast extraction failed: {e}\")\n",
        "                spectral_contrast_mean = np.zeros(7)  # Default 7 bands\n",
        "\n",
        "            spectral_features = {\n",
        "                'spectral_centroid': spectral_centroid,\n",
        "                'spectral_rolloff': spectral_rolloff,\n",
        "                'spectral_bandwidth': spectral_bandwidth,\n",
        "                'spectral_flatness': spectral_flatness\n",
        "            }\n",
        "\n",
        "            # Process time-series spectral features\n",
        "            for name, values in spectral_features.items():\n",
        "                stats = self._compute_statistical_features(values.reshape(1, -1), name)\n",
        "                features.extend(stats['values'])\n",
        "                feature_names.extend(stats['names'])\n",
        "\n",
        "            # Process spectral contrast separately\n",
        "            for i, contrast_val in enumerate(spectral_contrast_mean):\n",
        "                features.append(float(contrast_val))\n",
        "                feature_names.append(f'spectral_contrast_band_{i}')\n",
        "\n",
        "            # 3. Rhythmic features (with improved error handling)\n",
        "            try:\n",
        "                tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "                features.append(float(tempo))\n",
        "                feature_names.append('tempo')\n",
        "\n",
        "                # Beat consistency\n",
        "                if len(beats) > 1:\n",
        "                    beat_intervals = np.diff(beats) / sr\n",
        "                    features.extend([\n",
        "                        float(np.mean(beat_intervals)),\n",
        "                        float(np.std(beat_intervals)),\n",
        "                        float(np.var(beat_intervals))\n",
        "                    ])\n",
        "                    feature_names.extend(['beat_interval_mean', 'beat_interval_std', 'beat_interval_var'])\n",
        "                else:\n",
        "                    features.extend([0.0, 0.0, 0.0])\n",
        "                    feature_names.extend(['beat_interval_mean', 'beat_interval_std', 'beat_interval_var'])\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Rhythm feature extraction failed: {e}\")\n",
        "                features.extend([120.0, 0.5, 0.1, 0.01])  # Default values\n",
        "                feature_names.extend(['tempo', 'beat_interval_mean', 'beat_interval_std', 'beat_interval_var'])\n",
        "\n",
        "            # 4. Zero crossing rate (speech activity)\n",
        "            try:\n",
        "                zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "                zcr_stats = self._compute_statistical_features(zcr.reshape(1, -1), 'zcr')\n",
        "                features.extend(zcr_stats['values'])\n",
        "                feature_names.extend(zcr_stats['names'])\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: ZCR extraction failed: {e}\")\n",
        "                features.extend([0.0] * 8)  # 8 statistical features\n",
        "                feature_names.extend([f'zcr_{stat}' for stat in ['mean', 'std', 'var', 'max', 'min', 'median', 'q25', 'q75']])\n",
        "\n",
        "            # 5. Chroma features (harmonic content)\n",
        "            try:\n",
        "                chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "                chroma_stats = self._compute_statistical_features(chroma, 'chroma')\n",
        "                features.extend(chroma_stats['values'])\n",
        "                feature_names.extend(chroma_stats['names'])\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Chroma extraction failed: {e}\")\n",
        "                features.extend([0.0] * (12 * 8))  # 12 chroma bins * 8 stats\n",
        "                feature_names.extend([f'chroma_{i}_{stat}' for i in range(12)\n",
        "                                    for stat in ['mean', 'std', 'var', 'max', 'min', 'median', 'q25', 'q75']])\n",
        "\n",
        "            # 6. Energy and power features\n",
        "            try:\n",
        "                rms_energy = librosa.feature.rms(y=y)[0]\n",
        "                rms_stats = self._compute_statistical_features(rms_energy.reshape(1, -1), 'rms_energy')\n",
        "                features.extend(rms_stats['values'])\n",
        "                feature_names.extend(rms_stats['names'])\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: RMS energy extraction failed: {e}\")\n",
        "                features.extend([0.0] * 8)\n",
        "                feature_names.extend([f'rms_energy_{stat}' for stat in ['mean', 'std', 'var', 'max', 'min', 'median', 'q25', 'q75']])\n",
        "\n",
        "            # 7. Formant-like features (using spectral peaks)\n",
        "            try:\n",
        "                stft = librosa.stft(y)\n",
        "                magnitude = np.abs(stft)\n",
        "                spectral_peaks = []\n",
        "\n",
        "                # Sample a few frames to avoid memory issues\n",
        "                num_frames = min(10, magnitude.shape[1])\n",
        "                frame_indices = np.linspace(0, magnitude.shape[1]-1, num_frames, dtype=int)\n",
        "\n",
        "                for frame_idx in frame_indices:\n",
        "                    spectrum = magnitude[:, frame_idx]\n",
        "                    peaks = self._find_spectral_peaks(spectrum)\n",
        "                    spectral_peaks.extend(peaks[:3])  # Top 3 peaks per frame\n",
        "\n",
        "                if spectral_peaks:\n",
        "                    features.extend([\n",
        "                        float(np.mean(spectral_peaks)),\n",
        "                        float(np.std(spectral_peaks)),\n",
        "                        float(np.max(spectral_peaks))\n",
        "                    ])\n",
        "                else:\n",
        "                    features.extend([0.0, 0.0, 0.0])\n",
        "\n",
        "                feature_names.extend(['formant_mean', 'formant_std', 'formant_max'])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Formant feature extraction failed: {e}\")\n",
        "                features.extend([0.0, 0.0, 0.0])\n",
        "                feature_names.extend(['formant_mean', 'formant_std', 'formant_max'])\n",
        "\n",
        "            # 8. Additional prosodic features\n",
        "            try:\n",
        "                # Pitch estimation using fundamental frequency\n",
        "                pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
        "                pitch_values = []\n",
        "\n",
        "                for t in range(pitches.shape[1]):\n",
        "                    index = magnitudes[:, t].argmax()\n",
        "                    pitch = pitches[index, t]\n",
        "                    if pitch > 0:  # Valid pitch\n",
        "                        pitch_values.append(pitch)\n",
        "\n",
        "                if pitch_values:\n",
        "                    features.extend([\n",
        "                        float(np.mean(pitch_values)),\n",
        "                        float(np.std(pitch_values)),\n",
        "                        float(np.max(pitch_values)),\n",
        "                        float(np.min(pitch_values))\n",
        "                    ])\n",
        "                else:\n",
        "                    features.extend([0.0, 0.0, 0.0, 0.0])\n",
        "\n",
        "                feature_names.extend(['pitch_mean', 'pitch_std', 'pitch_max', 'pitch_min'])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Pitch extraction failed: {e}\")\n",
        "                features.extend([0.0, 0.0, 0.0, 0.0])\n",
        "                feature_names.extend(['pitch_mean', 'pitch_std', 'pitch_max', 'pitch_min'])\n",
        "\n",
        "            # Store feature names for first extraction\n",
        "            if not self.feature_names:\n",
        "                self.feature_names = feature_names.copy()\n",
        "\n",
        "            # Ensure all features are float32 and finite\n",
        "            features = [float(f) if np.isfinite(f) else 0.0 for f in features]\n",
        "\n",
        "            return np.array(features, dtype=np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting features from {audio_path}: {e}\")\n",
        "            return self._get_zero_features()\n",
        "\n",
        "    def _compute_statistical_features(self, data, prefix):\n",
        "        \"\"\"Compute statistical features from 2D array\"\"\"\n",
        "        if data.ndim == 1:\n",
        "            data = data.reshape(1, -1)\n",
        "\n",
        "        stats_values = []\n",
        "        stats_names = []\n",
        "\n",
        "        for i in range(data.shape[0]):\n",
        "            row = data[i]\n",
        "            row = row[np.isfinite(row)]  # Remove infinite values\n",
        "\n",
        "            if len(row) == 0:\n",
        "                # Handle empty row\n",
        "                stats_values.extend([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
        "            else:\n",
        "                stats_values.extend([\n",
        "                    float(np.mean(row)),\n",
        "                    float(np.std(row)),\n",
        "                    float(np.var(row)),\n",
        "                    float(np.max(row)),\n",
        "                    float(np.min(row)),\n",
        "                    float(np.median(row)),\n",
        "                    float(np.percentile(row, 25)),\n",
        "                    float(np.percentile(row, 75))\n",
        "                ])\n",
        "\n",
        "            if data.shape[0] == 1:\n",
        "                stats_names.extend([\n",
        "                    f'{prefix}_mean', f'{prefix}_std', f'{prefix}_var',\n",
        "                    f'{prefix}_max', f'{prefix}_min', f'{prefix}_median',\n",
        "                    f'{prefix}_q25', f'{prefix}_q75'\n",
        "                ])\n",
        "            else:\n",
        "                stats_names.extend([\n",
        "                    f'{prefix}_{i}_mean', f'{prefix}_{i}_std', f'{prefix}_{i}_var',\n",
        "                    f'{prefix}_{i}_max', f'{prefix}_{i}_min', f'{prefix}_{i}_median',\n",
        "                    f'{prefix}_{i}_q25', f'{prefix}_{i}_q75'\n",
        "                ])\n",
        "\n",
        "        return {'values': stats_values, 'names': stats_names}\n",
        "\n",
        "    def _find_spectral_peaks(self, spectrum, num_peaks=3):\n",
        "        \"\"\"Find spectral peaks (simplified formant detection)\"\"\"\n",
        "        try:\n",
        "            from scipy.signal import find_peaks\n",
        "            # Only consider positive values\n",
        "            spectrum = np.maximum(spectrum, 0)\n",
        "\n",
        "            if np.max(spectrum) == 0:\n",
        "                return [0.0] * num_peaks\n",
        "\n",
        "            peaks, _ = find_peaks(spectrum, height=np.max(spectrum) * 0.1, distance=5)\n",
        "\n",
        "            if len(peaks) > 0:\n",
        "                # Convert to Hz (assuming sr/2 as Nyquist frequency)\n",
        "                nyquist = self.sample_rate // 2\n",
        "                peak_freqs = peaks * nyquist / len(spectrum)\n",
        "\n",
        "                # Sort by magnitude and take top peaks\n",
        "                peak_magnitudes = spectrum[peaks]\n",
        "                sorted_indices = np.argsort(peak_magnitudes)[::-1]\n",
        "                top_peaks = peak_freqs[sorted_indices[:num_peaks]]\n",
        "\n",
        "                # Pad with zeros if needed\n",
        "                result = list(top_peaks) + [0.0] * (num_peaks - len(top_peaks))\n",
        "                return result[:num_peaks]\n",
        "            else:\n",
        "                return [0.0] * num_peaks\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Peak finding failed: {e}\")\n",
        "            return [0.0] * num_peaks\n",
        "\n",
        "    def _get_zero_features(self):\n",
        "        \"\"\"Return zero feature vector for failed extractions\"\"\"\n",
        "        if self.feature_names:\n",
        "            return np.zeros(len(self.feature_names), dtype=np.float32)\n",
        "        else:\n",
        "            # Estimate feature dimension based on typical extraction\n",
        "            # MFCC: 13*8 = 104\n",
        "            # Spectral: 4*8 = 32\n",
        "            # Spectral contrast: 7\n",
        "            # Rhythm: 4\n",
        "            # ZCR: 8\n",
        "            # Chroma: 12*8 = 96\n",
        "            # RMS: 8\n",
        "            # Formants: 3\n",
        "            # Pitch: 4\n",
        "            estimated_dim = 104 + 32 + 7 + 4 + 8 + 96 + 8 + 3 + 4\n",
        "            return np.zeros(estimated_dim, dtype=np.float32)\n",
        "\n",
        "# Test feature extraction\n",
        "print(\"=== Testing Enhanced Audio Feature Extraction (Fixed) ===\")\n",
        "\n",
        "extractor = EnhancedAudioFeatureExtractor()\n",
        "\n",
        "if 'audio_files' in globals() and audio_files:\n",
        "    print(f\"Testing with: {audio_files[0]}\")\n",
        "    test_features = extractor.extract_comprehensive_features(audio_files[0])\n",
        "    print(f\"✓ Audio feature extraction successful!\")\n",
        "    print(f\"Feature vector dimension: {len(test_features)}\")\n",
        "    print(f\"Feature vector shape: {test_features.shape}\")\n",
        "    print(f\"Sample features (first 10): {test_features[:10]}\")\n",
        "    print(f\"Feature range: [{np.min(test_features):.4f}, {np.max(test_features):.4f}]\")\n",
        "\n",
        "    # Check for invalid values\n",
        "    invalid_count = np.sum(~np.isfinite(test_features))\n",
        "    print(f\"Invalid values (inf/nan): {invalid_count}\")\n",
        "\n",
        "    # Test with multiple files to ensure consistency\n",
        "    print(\"\\nTesting consistency with multiple files...\")\n",
        "    feature_dims = []\n",
        "    for i, audio_file in enumerate(audio_files[:min(5, len(audio_files))]):\n",
        "        try:\n",
        "            features = extractor.extract_comprehensive_features(audio_file)\n",
        "            feature_dims.append(len(features))\n",
        "            invalid_vals = np.sum(~np.isfinite(features))\n",
        "            print(f\"File {i+1}: {len(features)} features, {invalid_vals} invalid values\")\n",
        "        except Exception as e:\n",
        "            print(f\"File {i+1} failed: {e}\")\n",
        "\n",
        "    if len(set(feature_dims)) == 1:\n",
        "        print(\"✓ Feature dimensions are consistent across files\")\n",
        "        print(f\"✓ Final feature dimension: {feature_dims[0]}\")\n",
        "    else:\n",
        "        print(f\"⚠ Inconsistent feature dimensions: {set(feature_dims)}\")\n",
        "\n",
        "    # Display feature names (first 20)\n",
        "    if extractor.feature_names:\n",
        "        print(f\"\\nSample feature names (first 20):\")\n",
        "        for i, name in enumerate(extractor.feature_names[:20]):\n",
        "            print(f\"  {i+1:2d}. {name}\")\n",
        "        print(f\"... and {len(extractor.feature_names) - 20} more features\")\n",
        "else:\n",
        "    print(\"No audio files found for testing\")\n",
        "    # Test with dummy data\n",
        "    print(\"Creating dummy test...\")\n",
        "    dummy_path = \"/tmp/dummy_audio.wav\"\n",
        "    dummy_audio = np.random.randn(16000) * 0.1  # 1 second of dummy audio\n",
        "    import soundfile as sf\n",
        "    sf.write(dummy_path, dummy_audio, 16000)\n",
        "\n",
        "    test_features = extractor.extract_comprehensive_features(dummy_path)\n",
        "    print(f\"Dummy test successful! Feature dimension: {len(test_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsFyxUZBlhCv",
        "outputId": "1f09723b-9954-47cc-f364-90ed1884bb71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Enhanced Audio Feature Extraction (Fixed) ===\n",
            "Testing with: /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "✓ Audio feature extraction successful!\n",
            "Feature vector dimension: 266\n",
            "Feature vector shape: (266,)\n",
            "Sample features (first 10): [ -294.1106     147.90477  21875.822      -55.234356  -532.4708\n",
            "  -243.81888   -454.12646   -175.49791     88.94421     71.55755 ]\n",
            "Feature range: [-532.4708, 3861179.5000]\n",
            "Invalid values (inf/nan): 0\n",
            "\n",
            "Testing consistency with multiple files...\n",
            "File 1: 266 features, 0 invalid values\n",
            "File 2: 266 features, 0 invalid values\n",
            "File 3: 266 features, 0 invalid values\n",
            "File 4: 266 features, 0 invalid values\n",
            "File 5: 266 features, 0 invalid values\n",
            "✓ Feature dimensions are consistent across files\n",
            "✓ Final feature dimension: 266\n",
            "\n",
            "Sample feature names (first 20):\n",
            "   1. mfcc_0_mean\n",
            "   2. mfcc_0_std\n",
            "   3. mfcc_0_var\n",
            "   4. mfcc_0_max\n",
            "   5. mfcc_0_min\n",
            "   6. mfcc_0_median\n",
            "   7. mfcc_0_q25\n",
            "   8. mfcc_0_q75\n",
            "   9. mfcc_1_mean\n",
            "  10. mfcc_1_std\n",
            "  11. mfcc_1_var\n",
            "  12. mfcc_1_max\n",
            "  13. mfcc_1_min\n",
            "  14. mfcc_1_median\n",
            "  15. mfcc_1_q25\n",
            "  16. mfcc_1_q75\n",
            "  17. mfcc_2_mean\n",
            "  18. mfcc_2_std\n",
            "  19. mfcc_2_var\n",
            "  20. mfcc_2_max\n",
            "... and 246 more features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Speech-to-Text and BERT Processing (Enhanced)"
      ],
      "metadata": {
        "id": "KQTSEyijnn-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RobustSpeechProcessor:\n",
        "    def __init__(self):\n",
        "        self.recognizer = sr.Recognizer()\n",
        "        # Adjust recognizer settings for better performance\n",
        "        self.recognizer.energy_threshold = 4000\n",
        "        self.recognizer.dynamic_energy_threshold = True\n",
        "        self.recognizer.pause_threshold = 0.8\n",
        "\n",
        "    def audio_to_text_robust(self, audio_path, max_retries=3):\n",
        "        \"\"\"Convert audio to text with multiple fallback methods\"\"\"\n",
        "        methods = [\n",
        "            self._recognize_google,\n",
        "            self._recognize_sphinx,  # Offline fallback\n",
        "            self._get_fallback_text\n",
        "        ]\n",
        "\n",
        "        for method in methods:\n",
        "            try:\n",
        "                text = method(audio_path)\n",
        "                if text and len(text.strip()) > 0:\n",
        "                    return text.strip()\n",
        "            except Exception as e:\n",
        "                print(f\"Speech recognition method failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Return minimal fallback text\n",
        "        return \"unable to process audio speech recognition failed\"\n",
        "\n",
        "    def _recognize_google(self, audio_path):\n",
        "        \"\"\"Google Speech Recognition (online)\"\"\"\n",
        "        # Load and preprocess audio\n",
        "        y, sr = librosa.load(audio_path, sr=16000, duration=30)\n",
        "\n",
        "        # Normalize audio\n",
        "        y = librosa.util.normalize(y)\n",
        "\n",
        "        # Remove silence\n",
        "        y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
        "\n",
        "        # Save as temporary wav file\n",
        "        temp_path = \"/tmp/temp_speech.wav\"\n",
        "        librosa.output.write_wav(temp_path, y_trimmed, sr)\n",
        "\n",
        "        # Perform speech recognition\n",
        "        with sr.AudioFile(temp_path) as source:\n",
        "            # Adjust for ambient noise\n",
        "            self.recognizer.adjust_for_ambient_noise(source, duration=0.5)\n",
        "            audio_data = self.recognizer.record(source)\n",
        "            text = self.recognizer.recognize_google(audio_data, language='en-US')\n",
        "\n",
        "        # Cleanup\n",
        "        if os.path.exists(temp_path):\n",
        "            os.remove(temp_path)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _recognize_sphinx(self, audio_path):\n",
        "        \"\"\"Offline Sphinx recognition (fallback)\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=16000, duration=30)\n",
        "            y = librosa.util.normalize(y)\n",
        "\n",
        "            temp_path = \"/tmp/temp_speech_sphinx.wav\"\n",
        "            librosa.output.write_wav(temp_path, y, sr)\n",
        "\n",
        "            with sr.AudioFile(temp_path) as source:\n",
        "                audio_data = self.recognizer.record(source)\n",
        "                text = self.recognizer.recognize_sphinx(audio_data)\n",
        "\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "\n",
        "            return text\n",
        "        except:\n",
        "            raise Exception(\"Sphinx recognition failed\")\n",
        "\n",
        "    def _get_fallback_text(self, audio_path):\n",
        "        \"\"\"Generate fallback text based on audio characteristics\"\"\"\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=16000, duration=30)\n",
        "\n",
        "            # Analyze audio characteristics\n",
        "            duration = len(y) / sr\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "            speech_rate = len(librosa.onset.onset_detect(y=y, sr=sr)) / duration if duration > 0 else 0\n",
        "\n",
        "            # Generate descriptive fallback\n",
        "            if duration < 1:\n",
        "                return \"very short audio segment minimal speech\"\n",
        "            elif np.mean(rms) < 0.01:\n",
        "                return \"quiet audio low volume speech unclear\"\n",
        "            elif speech_rate > 3:\n",
        "                return \"rapid speech high speech rate unclear pronunciation\"\n",
        "            else:\n",
        "                return \"moderate speech unclear audio quality transcription difficult\"\n",
        "        except:\n",
        "            return \"audio processing failed no speech detected\"\n",
        "\n",
        "class EnhancedBERTProcessor:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        print(f\"Loading BERT model: {model_name}\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        print(f\"BERT model loaded on: {self.device}\")\n",
        "\n",
        "    def encode_text_robust(self, text, max_length=512):\n",
        "        \"\"\"Encode text using BERT with robust error handling\"\"\"\n",
        "        try:\n",
        "            # Clean and validate text\n",
        "            if not text or len(text.strip()) == 0:\n",
        "                text = \"no text available for processing\"\n",
        "\n",
        "            # Truncate very long texts\n",
        "            if len(text) > 2000:\n",
        "                text = text[:2000]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                text,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "            # Get BERT embeddings\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                # Use CLS token representation\n",
        "                cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (1, 768)\n",
        "\n",
        "                # Also compute mean pooling as additional feature\n",
        "                attention_mask = inputs['attention_mask']\n",
        "                token_embeddings = outputs.last_hidden_state\n",
        "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "                mean_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "                # Combine CLS and mean pooling\n",
        "                combined_embedding = torch.cat([cls_embedding, mean_embedding], dim=1)  # Shape: (1, 1536)\n",
        "\n",
        "            return combined_embedding.cpu()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encoding text: {e}\")\n",
        "            # Return zero embedding\n",
        "            zero_embedding = torch.zeros(1, 1536)  # CLS + mean pooling\n",
        "            return zero_embedding\n",
        "\n",
        "    def extract_linguistic_features(self, text):\n",
        "        \"\"\"Extract traditional linguistic features\"\"\"\n",
        "        if not text or len(text.strip()) == 0:\n",
        "            return np.zeros(20)  # Return zero features for empty text\n",
        "\n",
        "        try:\n",
        "            words = text.lower().split()\n",
        "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "\n",
        "            # Basic counts\n",
        "            word_count = len(words)\n",
        "            sentence_count = len(sentences)\n",
        "            char_count = len(text)\n",
        "\n",
        "            # Advanced features\n",
        "            if words:\n",
        "                avg_word_length = np.mean([len(word) for word in words])\n",
        "                word_length_std = np.std([len(word) for word in words])\n",
        "                unique_words = len(set(words))\n",
        "                lexical_diversity = unique_words / word_count if word_count > 0 else 0\n",
        "            else:\n",
        "                avg_word_length = word_length_std = unique_words = lexical_diversity = 0\n",
        "\n",
        "            if sentences:\n",
        "                avg_sentence_length = np.mean([len(sent.split()) for sent in sentences])\n",
        "                sentence_length_std = np.std([len(sent.split()) for sent in sentences])\n",
        "            else:\n",
        "                avg_sentence_length = sentence_length_std = 0\n",
        "\n",
        "            # Disfluency markers\n",
        "            fillers = ['um', 'uh', 'er', 'ah', 'hmm', 'well', 'like', 'you know']\n",
        "            filler_count = sum(1 for word in words if word in fillers)\n",
        "\n",
        "            # Pause indicators\n",
        "            pause_count = text.count('...') + text.count(',') + text.count(';')\n",
        "\n",
        "            # Repetition detection (simple)\n",
        "            word_freq = {}\n",
        "            for word in words:\n",
        "                word_freq[word] = word_freq.get(word, 0) + 1\n",
        "            repeated_words = sum(1 for count in word_freq.values() if count > 2)\n",
        "\n",
        "            # Part-of-speech complexity (approximated)\n",
        "            function_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']\n",
        "            function_word_ratio = sum(1 for word in words if word in function_words) / word_count if word_count > 0 else 0\n",
        "\n",
        "            # Complexity measures\n",
        "            type_token_ratio = lexical_diversity\n",
        "\n",
        "            features = [\n",
        "                word_count,\n",
        "                sentence_count,\n",
        "                char_count,\n",
        "                avg_word_length,\n",
        "                word_length_std,\n",
        "                avg_sentence_length,\n",
        "                sentence_length_std,\n",
        "                unique_words,\n",
        "                lexical_diversity,\n",
        "                filler_count,\n",
        "                pause_count,\n",
        "                repeated_words,\n",
        "                function_word_ratio,\n",
        "                type_token_ratio,\n",
        "                filler_count / word_count if word_count > 0 else 0,  # Filler ratio\n",
        "                pause_count / sentence_count if sentence_count > 0 else 0,  # Pause ratio\n",
        "                char_count / word_count if word_count > 0 else 0,  # Avg chars per word\n",
        "                word_count / sentence_count if sentence_count > 0 else 0,  # Words per sentence\n",
        "                len([w for w in words if len(w) > 6]) / word_count if word_count > 0 else 0,  # Long word ratio\n",
        "                len([w for w in words if len(w) <= 3]) / word_count if word_count > 0 else 0,  # Short word ratio\n",
        "            ]\n",
        "\n",
        "            return np.array(features, dtype=np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting linguistic features: {e}\")\n",
        "            return np.zeros(20)\n",
        "\n",
        "# Initialize processors\n",
        "print(\"=== Initializing Speech-to-Text and BERT Processors ===\")\n",
        "speech_processor = RobustSpeechProcessor()\n",
        "bert_processor = EnhancedBERTProcessor()\n",
        "\n",
        "# Test speech processing\n",
        "if audio_files:\n",
        "    print(f\"\\nTesting speech processing with: {audio_files[0]}\")\n",
        "    test_text = speech_processor.audio_to_text_robust(audio_files[0])\n",
        "    print(f\"✓ Speech-to-text successful!\")\n",
        "    print(f\"Transcribed text: '{test_text[:100]}{'...' if len(test_text) > 100 else ''}'\")\n",
        "\n",
        "    # Test BERT encoding\n",
        "    print(\"\\nTesting BERT encoding...\")\n",
        "    bert_features = bert_processor.encode_text_robust(test_text)\n",
        "    linguistic_features = bert_processor.extract_linguistic_features(test_text)\n",
        "\n",
        "    print(f\"✓ BERT encoding successful!\")\n",
        "    print(f\"BERT features shape: {bert_features.shape}\")\n",
        "    print(f\"Linguistic features shape: {linguistic_features.shape}\")\n",
        "    print(f\"Total text features: {bert_features.shape[1] + len(linguistic_features)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507,
          "referenced_widgets": [
            "fd8fe7e118b044e19f5537af64e31f8d",
            "d9a98644b05b4192badfd81963d02854",
            "678d3784d8db4fd8b84c134adf1f3078",
            "afd4d0f229284eec82bb8896c5d1e06b",
            "0f9d409709bb45728c8bbc98625a60d7",
            "32f60c00d00c4c2a94d518727b828576",
            "565fef2c697042baad22dce6c9146341",
            "9bfcd32611fb455b8ed94b061ca8734e",
            "be3e95f673f141ed9f8e1ff1528a1874",
            "2974c377ad084fc7afec5ac1e8139cb9",
            "fc1f75d645ce4a44bc08c6924b5ab798",
            "41eeab7320f34425a0a57c331faa3d91",
            "e329d48433f44885977d332d9e0d1350",
            "89881661969a4391bb72452328e8c908",
            "c76a9c6e25854481b27ad9cea8150377",
            "6760db64c09c4fecb06373aaa7bb0f1d",
            "501b98ac20484606aea0776114a2e231",
            "8065f27d332042b0822fbd8c36d19614",
            "52b406606eb143fabc94281192b8c931",
            "9883fdaef2554ca4a0a4733d7082ade5",
            "65abd1013226481ebf79b936998e3032",
            "f3957dc1e35c4b65b94b7e2cae9ba2c7",
            "ef3fac279125451c98440ab5290767cc",
            "a7197daf27ca47e7a8a4e84c47b8ad87",
            "db81d48a195d4e79b4f0d1ef06da42f0",
            "f0b259d77d61438a8acc8363e1df132c",
            "f0f49922c53949ee85beffd0595446a8",
            "07c7c710c91446babb91809e55e6addb",
            "bd78d24e016c482bb6ba12a7d716edb2",
            "fa556484f0cc426d840c270fa140de7e",
            "a329db0e4a1546f496a31a3c67e26d3d",
            "c7fdbaa4451a488bb5d63e0491ed2b24",
            "ea6332b58e584444b63a387275ee92c0",
            "858eba8fb7d84ffe81e2c90166c407ca",
            "f4eb65423fd04e0d9eec72b024cd840c",
            "d03b9bcb3d13427798611dcab9d1a4fc",
            "104dffe91cf447c2985bf84dc17d824b",
            "e58639e5bce2448182025f5d4a9b07fe",
            "7c37c7e91bf44a96aacf2097afb92649",
            "869efca4cedc43558b361b939366a23a",
            "8b159c7a6bb2408f89eb038c1f503b7c",
            "763f5202a2fb41298fc677163e3a3b65",
            "6df75cfbca4e4d79aace31c39fd7391a",
            "c56f8ce6de4f49b18a7ab76b0340d592",
            "1fdb6723afdd4192ae9f1b9ed03a66dd",
            "8d95caa542cb4169a04c4d6fe6533dd1",
            "3a753b070e1047d89a93fa931edea5bd",
            "8e492cadf25d4da79861648dbf964c22",
            "d0ba14ec9bd943e482a17d0072fe60ea",
            "0a7ff49fe53e4f5b97fe9662f1e20602",
            "dad317a744db4bebaee4644ff6de2d97",
            "0fad96a6734041508828e42bfdf8e6eb",
            "c50eff5b057b4df08b47868a4ffbe160",
            "895aff1c476642348494822716cd7ddf",
            "438e1f254f044a96a2ee5c48b3e58e01"
          ]
        },
        "id": "OhDH5qB1nfPA",
        "outputId": "47a1b345-9268-4d9f-852a-9a887a8adb20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Initializing Speech-to-Text and BERT Processors ===\n",
            "Loading BERT model: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd8fe7e118b044e19f5537af64e31f8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41eeab7320f34425a0a57c331faa3d91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef3fac279125451c98440ab5290767cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "858eba8fb7d84ffe81e2c90166c407ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fdb6723afdd4192ae9f1b9ed03a66dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model loaded on: cuda\n",
            "\n",
            "Testing speech processing with: /content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "Speech recognition method failed: No librosa attribute output\n",
            "Speech recognition method failed: Sphinx recognition failed\n",
            "✓ Speech-to-text successful!\n",
            "Transcribed text: 'rapid speech high speech rate unclear pronunciation'\n",
            "\n",
            "Testing BERT encoding...\n",
            "✓ BERT encoding successful!\n",
            "BERT features shape: torch.Size([1, 1536])\n",
            "Linguistic features shape: (20,)\n",
            "Total text features: 1556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Improved DARTS Architecture"
      ],
      "metadata": {
        "id": "hoQS-vJVn1rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedDARTSCell(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_ops=8):\n",
        "        super(ImprovedDARTSCell, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Ensure dimensions match for operations\n",
        "        if input_dim != output_dim:\n",
        "            self.projection = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "        # Define possible operations with proper dimensionality\n",
        "        self.operations = nn.ModuleList([\n",
        "            nn.Identity(),  # Skip connection\n",
        "            nn.ReLU(),      # Activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU()),  # Linear + ReLU\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.Tanh()),  # Linear + Tanh\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.ReLU(), nn.Dropout(0.1)),  # With dropout\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim // 2), nn.ReLU(), nn.Linear(output_dim // 2, output_dim)),  # Bottleneck\n",
        "            nn.Sequential(nn.LayerNorm(output_dim), nn.ReLU()),  # Layer norm + activation\n",
        "            nn.Sequential(nn.Linear(output_dim, output_dim), nn.BatchNorm1d(output_dim), nn.ReLU())  # Batch norm version\n",
        "        ])\n",
        "\n",
        "        # Architecture parameters (alpha) - learnable weights for each operation\n",
        "        self.alpha = nn.Parameter(torch.randn(len(self.operations)))\n",
        "\n",
        "        # Temperature parameter for gumbel softmax (learnable)\n",
        "        self.temperature = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project input to correct dimension\n",
        "        x = self.projection(x)\n",
        "\n",
        "        # Apply Gumbel Softmax for differentiable architecture search\n",
        "        if self.training:\n",
        "            # Use Gumbel Softmax during training\n",
        "            gumbel_weights = F.gumbel_softmax(self.alpha, tau=self.temperature, hard=False)\n",
        "        else:\n",
        "            # Use regular softmax during evaluation\n",
        "            gumbel_weights = F.softmax(self.alpha / self.temperature, dim=0)\n",
        "\n",
        "        # Apply operations\n",
        "        outputs = []\n",
        "        for op in self.operations:\n",
        "            try:\n",
        "                if isinstance(op, nn.Sequential) and any(isinstance(layer, nn.BatchNorm1d) for layer in op):\n",
        "                    # Handle batch norm layers that require 2D input\n",
        "                    if x.dim() == 1:\n",
        "                        x_reshaped = x.unsqueeze(0)\n",
        "                        out = op(x_reshaped).squeeze(0)\n",
        "                    else:\n",
        "                        out = op(x)\n",
        "                else:\n",
        "                    out = op(x)\n",
        "                outputs.append(out)\n",
        "            except Exception as e:\n",
        "                # Fallback to identity if operation fails\n",
        "                outputs.append(x)\n",
        "\n",
        "        # Weighted combination of all operations\n",
        "        result = sum(w * out for w, out in zip(gumbel_weights, outputs))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_selected_operation(self):\n",
        "        \"\"\"Get the operation with highest weight (for inference)\"\"\"\n",
        "        selected_idx = torch.argmax(self.alpha).item()\n",
        "        return selected_idx, self.operations[selected_idx]\n",
        "\n",
        "class ImprovedDARTSNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=3, dropout_rate=0.2):\n",
        "        super(ImprovedDARTSNetwork, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Input projection with normalization\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "        # Stack multiple DARTS cells\n",
        "        self.cells = nn.ModuleList([\n",
        "            ImprovedDARTSCell(hidden_dim, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Additional regularization\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Handle batch dimension\n",
        "        batch_size = x.size(0) if x.dim() > 1 else 1\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Apply DARTS cells with residual connections\n",
        "        for i, cell in enumerate(self.cells):\n",
        "            identity = x\n",
        "            x = cell(x)\n",
        "\n",
        "            # Residual connection\n",
        "            if i > 0:  # Skip first layer for residual\n",
        "                x = x + identity\n",
        "\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Output projection\n",
        "        x = self.output_projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_architecture_info(self):\n",
        "        \"\"\"Get information about the learned architecture\"\"\"\n",
        "        arch_info = {}\n",
        "        for i, cell in enumerate(self.cells):\n",
        "            selected_idx, selected_op = cell.get_selected_operation()\n",
        "            arch_info[f'cell_{i}'] = {\n",
        "                'selected_operation_idx': selected_idx,\n",
        "                'operation_weights': cell.alpha.detach().cpu().numpy(),\n",
        "                'temperature': cell.temperature.item()\n",
        "            }\n",
        "        return arch_info\n",
        "\n",
        "# Test DARTS implementation\n",
        "print(\"=== Testing Improved DARTS Architecture ===\")\n",
        "\n",
        "# Create a sample input\n",
        "if 'test_features' in locals():\n",
        "    sample_input = torch.FloatTensor(test_features).unsqueeze(0)  # Add batch dimension\n",
        "    input_dim = sample_input.size(1)\n",
        "\n",
        "    print(f\"Sample input shape: {sample_input.shape}\")\n",
        "    print(f\"Input dimension: {input_dim}\")\n",
        "\n",
        "    # Test DARTS network\n",
        "    darts_net = ImprovedDARTSNetwork(input_dim=input_dim, hidden_dim=128, num_layers=2)\n",
        "\n",
        "    # Forward pass\n",
        "    darts_output = darts_net(sample_input)\n",
        "    print(f\"✓ DARTS network test successful!\")\n",
        "    print(f\"DARTS output shape: {darts_output.shape}\")\n",
        "\n",
        "    # Test architecture information\n",
        "    arch_info = darts_net.get_architecture_info()\n",
        "    print(f\"Architecture info: {arch_info}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn9AykQ1nxJh",
        "outputId": "0208c0f8-997c-4333-9f70-377b89a03be9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Improved DARTS Architecture ===\n",
            "Sample input shape: torch.Size([1, 266])\n",
            "Input dimension: 266\n",
            "✓ DARTS network test successful!\n",
            "DARTS output shape: torch.Size([1, 128])\n",
            "Architecture info: {'cell_0': {'selected_operation_idx': 3, 'operation_weights': array([ 1.2360164 , -0.47045425, -2.3251386 ,  1.3059185 , -0.7792638 ,\n",
            "       -0.01091782, -1.0779626 , -0.00849748], dtype=float32), 'temperature': 1.0}, 'cell_1': {'selected_operation_idx': 3, 'operation_weights': array([ 0.8261155 , -1.2763054 ,  0.44578087,  1.1685563 , -0.7267255 ,\n",
            "       -0.49504334,  0.25530347,  0.6060989 ], dtype=float32), 'temperature': 1.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 6: Advanced Multimodal Fusion Model"
      ],
      "metadata": {
        "id": "uY1Fl_tToAio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=256):\n",
        "        super(AttentionFusion, self).__init__()\n",
        "\n",
        "        self.audio_projection = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_projection = nn.Linear(text_dim, hidden_dim)\n",
        "\n",
        "        # Cross-attention mechanism\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        # Project to same dimension\n",
        "        audio_proj = self.audio_projection(audio_features).unsqueeze(1)  # (batch, 1, hidden)\n",
        "        text_proj = self.text_projection(text_features).unsqueeze(1)    # (batch, 1, hidden)\n",
        "\n",
        "        # Cross attention: audio attends to text\n",
        "        audio_attended, _ = self.cross_attention(audio_proj, text_proj, text_proj)\n",
        "        audio_attended = self.layer_norm(audio_attended + audio_proj)\n",
        "\n",
        "        # Cross attention: text attends to audio\n",
        "        text_attended, _ = self.cross_attention(text_proj, audio_proj, audio_proj)\n",
        "        text_attended = self.layer_norm(text_attended + text_proj)\n",
        "\n",
        "        # Combine attended features\n",
        "        fused = torch.cat([audio_attended.squeeze(1), text_attended.squeeze(1)], dim=1)\n",
        "\n",
        "        return fused\n",
        "\n",
        "class AdvancedMultimodalModel(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=256, num_classes=2, fusion_method='attention'):\n",
        "        super(AdvancedMultimodalModel, self).__init__()\n",
        "\n",
        "        self.fusion_method = fusion_method\n",
        "\n",
        "        # Audio processing with DARTS\n",
        "        self.audio_darts = ImprovedDARTSNetwork(\n",
        "            input_dim=audio_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=3,\n",
        "            dropout_rate=0.3\n",
        "        )\n",
        "\n",
        "        # Text processing\n",
        "        self.text_processor = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Fusion layer\n",
        "        if fusion_method == 'attention':\n",
        "            self.fusion = AttentionFusion(hidden_dim, hidden_dim, hidden_dim)\n",
        "            fusion_output_dim = hidden_dim * 2\n",
        "        elif fusion_method == 'bilinear':\n",
        "            self.fusion = nn.Bilinear(hidden_dim, hidden_dim, hidden_dim)\n",
        "            fusion_output_dim = hidden_dim\n",
        "        else:  # concatenation\n",
        "            self.fusion = None\n",
        "            fusion_output_dim = hidden_dim * 2\n",
        "\n",
        "        # Classification head with regularization\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_output_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(hidden_dim // 4, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        # Process modalities\n",
        "        audio_repr = self.audio_darts(audio_features)\n",
        "        text_repr = self.text_processor(text_features)\n",
        "\n",
        "        # Fusion\n",
        "        if self.fusion_method == 'attention':\n",
        "            fused = self.fusion(audio_repr, text_repr)\n",
        "        elif self.fusion_method == 'bilinear':\n",
        "            fused = self.fusion(audio_repr, text_repr)\n",
        "        else:  # concatenation\n",
        "            fused = torch.cat([audio_repr, text_repr], dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(fused)\n",
        "\n",
        "        return output, audio_repr, text_repr\n",
        "\n",
        "    def get_architecture_summary(self):\n",
        "        \"\"\"Get summary of the learned DARTS architecture\"\"\"\n",
        "        return self.audio_darts.get_architecture_info()\n",
        "\n",
        "# Test the advanced model\n",
        "print(\"=== Testing Advanced Multimodal Model ===\")\n",
        "\n",
        "if 'test_features' in locals() and 'bert_features' in locals():\n",
        "    # Create sample inputs\n",
        "    sample_audio = torch.FloatTensor(test_features).unsqueeze(0)\n",
        "    sample_text = bert_features\n",
        "\n",
        "    print(f\"Sample audio shape: {sample_audio.shape}\")\n",
        "    print(f\"Sample text shape: {sample_text.shape}\")\n",
        "\n",
        "    # Test different fusion methods\n",
        "    fusion_methods = ['attention', 'bilinear', 'concatenation']\n",
        "\n",
        "    for method in fusion_methods:\n",
        "        print(f\"\\nTesting {method} fusion...\")\n",
        "        model = AdvancedMultimodalModel(\n",
        "            audio_dim=sample_audio.size(1),\n",
        "            text_dim=sample_text.size(1),\n",
        "            hidden_dim=128,\n",
        "            fusion_method=method\n",
        "        )\n",
        "\n",
        "        # Forward pass\n",
        "        output, audio_repr, text_repr = model(sample_audio, sample_text)\n",
        "        print(f\"✓ {method} fusion successful!\")\n",
        "        print(f\"Output shape: {output.shape}\")\n",
        "        print(f\"Audio representation shape: {audio_repr.shape}\")\n",
        "        print(f\"Text representation shape: {text_repr.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XowIeldWn7a_",
        "outputId": "d2bfcbb3-82a6-4691-ce6e-5178b3d0bb66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Testing Advanced Multimodal Model ===\n",
            "Sample audio shape: torch.Size([1, 266])\n",
            "Sample text shape: torch.Size([1, 1536])\n",
            "\n",
            "Testing attention fusion...\n",
            "✓ attention fusion successful!\n",
            "Output shape: torch.Size([1, 2])\n",
            "Audio representation shape: torch.Size([1, 128])\n",
            "Text representation shape: torch.Size([1, 128])\n",
            "\n",
            "Testing bilinear fusion...\n",
            "✓ bilinear fusion successful!\n",
            "Output shape: torch.Size([1, 2])\n",
            "Audio representation shape: torch.Size([1, 128])\n",
            "Text representation shape: torch.Size([1, 128])\n",
            "\n",
            "Testing concatenation fusion...\n",
            "✓ concatenation fusion successful!\n",
            "Output shape: torch.Size([1, 2])\n",
            "Audio representation shape: torch.Size([1, 128])\n",
            "Text representation shape: torch.Size([1, 128])\n"
          ]
        }
      ]
    }
  ]
}