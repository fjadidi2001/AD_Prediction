# [CycleSGAN: A cycle-consistent and semantics-preserving generative adversarial network for unpaired MR-to-CT image synthesis](https://doi.org/10.1016/j.compmedimag.2024.102431)

## [[Runze Wang]]; [[Alexander F. Heimann]]; [[Moritz Tannast]] et al.

## Abstract
CycleGAN has been leveraged to synthesize a CT image from an available MR image after trained on unpaired data. Due to the lack of direct constraints between the synthetic and the input images, CycleGAN cannot guarantee structural consistency and often generates inaccurate mappings that shift the anatomy, which is highly undesirable for downstream clinical applications such as MRI-guided radiotherapy treatment planning and PET/MRI attenuation correction. In this paper, we propose a cycle-consistent and semantics-preserving generative adversarial network, referred as CycleSGAN, for unpaired MR-to-CT image synthesis. Our design features a novel and generic way to incorporate semantic information into CycleGAN. This is done by designing a pair of three-player games within the CycleGAN framework where each three-player game consists of one generator and two discriminators to formulate two distinct types of adversarial learning: appearance adversarial learning and structure adversarial learning. These two types of adversarial learning are alternately trained to ensure both realistic image synthesis and semantic structure preservation. Results on unpaired hip MR-to-CT image synthesis show that our method produces better synthetic CT images in both accuracy and visual quality as compared to other state-of-the-art (SOTA) unpaired MR-to-CT image synthesis methods.

## Key concepts
#claim/adversarial_learning; #adversarial_learning; #computed_tomography; #finding/synthesis; #synthesis

## Quote
> The study presents a novel approach, CycleSGAN, for unpaired MR-to-CT image synthesis, which incorporates semantic information into CycleGAN using a pair of three-player games, resulting in superior performance compared to state-of-the-art methods.


## Figures
![Figure 1. A schematic illustration of our proposed CycleSGAN framework. Blue and red colored blocks and arrows show parts related with MR and CT domains, respectively. Solid arrows denote the data flow and yellow blocks represent the loss functions used in the training process](https://x.scholarcy.com/images/0/CycleSGAN__A_cycle-consistent_and_semantics-preserving_generative_adversarial_network_for_unpaired_MR-to-CT_image_synthesis/img-Figure-000.webp)
![Figure 2. Network architecture for generators and discriminators used in the proposed CycleSGAN. We denote 3D instance normalization, 3D convolution, 3D transpose-convolution and LeakyReLU as IN3D, Conv3D, TConv3D and LReLU, respectively](https://x.scholarcy.com/images/0/CycleSGAN__A_cycle-consistent_and_semantics-preserving_generative_adversarial_network_for_unpaired_MR-to-CT_image_synthesis/img-Figure-001.webp)
![Figure 3. Qualitative results of different image translation methods when evaluated on a testing MR image (the 1st row) from the Fribourg dataset. From the 2nd row to the 6th row, we show the qualitative results of MUNIT, 3D-CycleGAN, 3D MINDCycle, 3D-CycleSeg and CycleSGAN, respectively. Results are visualized in three views: axial view (the left three columns), saggital view (the middle three columns), and coronal view (the right three columns). In each view, the first column presents the CT images synthesized by different methods; In the second column, the ground truth annotation of the input MR image is overlaid onto the synthetic CT images, where red and green color corresponds to the pelvis and the proximal femur, respectively. The third column shows a zoomed view of a local region in the second column](https://x.scholarcy.com/images/0/CycleSGAN__A_cycle-consistent_and_semantics-preserving_generative_adversarial_network_for_unpaired_MR-to-CT_image_synthesis/img-Figure-002.webp)
![Figure 4. Qualitative results of the analytical ablation study on the Fribourg dataset. Top row: an input testing MR image with the corresponding annotation; 2nd–4th rows: results obtained when different types of discriminators were used for training](https://x.scholarcy.com/images/0/CycleSGAN__A_cycle-consistent_and_semantics-preserving_generative_adversarial_network_for_unpaired_MR-to-CT_image_synthesis/img-Figure-003.webp)
## Key points
- The limitations of the study include the requirement of label supervisions to train the model, although the model can take an MR image as input to generate a synthetic CT image without the need of annotation labels at testing stage.
- The study acknowledges that the evaluation was only conducted on hip MR-CT datasets, and further investigation is required to determine whether similar performance can be obtained on datasets of different anatomical regions or acquired with different imaging protocols. The study also acknowledges that CycleSGAN requires label supervisions to train a model for unpaired MR-to-CT image synthesis.
- The study acknowledged the limitations of the present study, including the evaluation being only conducted on hip MR-CT datasets and the requirement of label supervision to train a model.
- The future work includes further investigation of the influence of different backbone generators and discriminators on the performance of CycleSGAN, as well as the exploration of using automatically generated annotations instead of manual annotations.
- The study suggests that CycleSGAN can be applied to datasets of other anatomical regions, such as brain and abdomen. The study also suggests that CycleSGAN can be used for clinical applications, such as radiation treatment planning and multimodal image registration.
- The study suggested that further investigation is required to determine whether similar performance can be obtained when CycleSGAN is applied to datasets of different anatomical regions or to datasets acquired with different imaging protocols.
- The practical applications of the study include the use of synthetic CT images generated by CycleSGAN for radiotherapy treatment planning and positron emission tomography (PET) attenuation correction, which can help to reduce the radiation dose to patients and improve the accuracy of treatment planning.
- The study suggests that CycleSGAN has potential practical applications in clinical settings, including radiation treatment planning and multimodal image registration. The study also suggests that CycleSGAN can be used to facilitate the development of MRI-based radiation treatment planning.
- The study highlighted the importance of the approach for down-stream tasks such as treatment planning and attenuation correction.


## Summary

### Snapshot
The study presents a novel approach, CycleSGAN, for unpaired MR-to-CT image synthesis, which incorporates semantic information into CycleGAN using a pair of three-player games, resulting in superior performance compared to state-of-the-art methods.

### Key findings
The key findings of the study include the development of a novel CycleSGAN method that outperforms other state-of-the-art methods in terms of accuracy and visual quality, and the demonstration that only a limited number of semantic annotations are needed to train the structure adversarial learning.
The study finds that CycleSGAN outperforms multiple state-of-the-art methods, including 3D-CycleGAN, 3D-MINDCycle, and MUNIT, in terms of segmentation score and image-based metrics. The study also finds that CycleSGAN requires limited supervision, with 10 annotations sufficient to learn the label-conditional image generation.
MUNIT exhibited the worst segmentation score (S-score) results while 3D-CycleGAN, which operated on a volume-level rather than on a slice-level as in MUNIT, demonstrated an average Dice Overlap Coefficient (DOC) increase of 6.61% and an average Average Symmetric Surface Distance (ASSD) decrease of 0.48 mm when compared to MUNIT
We developed and validated a new unpaired MR-to-CT image synthesis method
Our approach outperformed multiple state-of-theart methods and demonstrated the value of the three-player game in ensuring both realistic image synthesis and semantic structure preservation, which were of importance for down-stream tasks such as treatment planning and attenuation correction

### Objectives
The objective of the study is to propose a cycle-consistent and semantics-preserving generative adversarial network for unpaired MR-to-CT image synthesis, which can generate synthetic CT images that are both realistic and structurally consistent with the input MR images.

### Methods
The method used in the study is based on the CycleGAN framework, with a pair of three-player games that consist of one generator and two discriminators to formulate two distinct types of adversarial learning: appearance adversarial learning and structure adversarial learning.
The study uses a pair of three-player games within the CycleGAN framework, with each game consisting of one generator and two discriminators. The study evaluates the performance of CycleSGAN using segmentation score and image-based metrics, including DOC, ASSD, MAE, PSNR, and SSIM.
The study used a cycle-consistent and semantics-preserving generative adversarial network, referred to as CycleSGAN, which consists of a pair of three-player games within the CycleGAN framework.

### Results
The results of the study show that the proposed CycleSGAN method outperforms other state-of-the-art methods in terms of accuracy and visual quality, with an increase of 11.28% average DOC and a decrease of 0.98 mm average ASSD when compared to the second-best method.
The study finds that CycleSGAN achieves the best results in terms of DOC and ASSD, with an average DOC of 95.13% and an average ASSD of 0.45 mm. The study also finds that CycleSGAN outperforms other state-of-the-art methods, including 3D-CycleGAN, 3D-MINDCycle, and MUNIT.
The results of the study showed that CycleSGAN outperformed multiple state-of-the-art methods, with the best results obtained when all discriminators were used.

### Conclusions
The conclusions of the study are that the proposed CycleSGAN method is effective for unpaired MR-to-CT image synthesis, and that it can generate synthetic CT images that are both realistic and structurally consistent with the input MR images.
The study concludes that CycleSGAN is a superior approach for unpaired MR-to-CT image synthesis, outperforming state-of-the-art methods. The study also concludes that CycleSGAN requires limited supervision, with 10 annotations sufficient to learn the label-conditional image generation.
The study concluded that CycleSGAN is a novel and effective approach for unpaired MR-to-CT image synthesis, which can ensure both realistic image synthesis and semantic structure preservation with limited label supervision.

## Data analysis
- #method/t_tests

## Findings
- MUNIT exhibited the worst <a title="segmentation score" href="#" class="keyword">S-score</a> results while 3D-CycleGAN, <mark class="fact">which operated on a volume-level</mark> rather than on a slice-level as in MUNIT, demonstrated an average <a title="Dice Overlap Coefficient" href="#" class="keyword">DOC</a> increase of 6.61% and an average <a title="Average Symmetric Surface Distance" href="#" class="keyword">ASSD</a> decrease of 0.48 mm when compared to MUNIT
- CycleSGAN, which only utilized 10 annotations from each modality, achieved the best results in terms of both <a title="Dice Overlap Coefficient" href="#" class="keyword">DOC</a> and <a title="Average Symmetric Surface Distance" href="#" class="keyword">ASSD</a>, with an increase of 11.28% average <a title="Dice Overlap Coefficient" href="#" class="keyword">DOC</a> and a decrease of 0.98 mm average <a title="Average Symmetric Surface Distance" href="#" class="keyword">ASSD</a> when compared to the secondbest method (3D-MINDCycle)
- When only the structure discriminators were used, we observed improved results with an average <a title="Dice Overlap Coefficient" href="#" class="keyword">DOC</a> of 91.84% and an average <a title="Average Symmetric Surface Distance" href="#" class="keyword">ASSD</a> of 0.73 mm
- <mark class="claim">Our approach outperformed multiple state-of-theart methods and demonstrated the value of the three-player game in ensuring both realistic image <a title="synthesis" href="https://en.wikipedia.org/wiki/synthesis" class="keyword">synthesis</a> and semantic structure preservation, <mark class="fact">which were of importance for down-stream tasks</mark> such as treatment planning and attenuation correction</mark>

##  Builds on previous research
- Similarly, after training, we extracted 2D slices along the coronal view from the 3D testing MR images, which were then fed to the trained MUNIT model to get the synthetic CT slices. ==Subsequently, we placed the synthetic 2D slices back into the original 3D space for a volume-level evaluation==; (2) 3D-CycleGAN: CycleGAN ([^Zhu_et+al_2017_a]) is another unsupervised image-to-image translation method based on cycle-consistency.
- We thus chose to use ResNet as the backbone network. In ==this study, we investigated two different backbone generators==: ResNet ([^He_et+al_2016_a]) (referred to as Ours) and U-Net ([^Ronneberger_et+al_2015_a]) (referred to as Ours (U-Net)).

## Differs from previous work
- Thus, [^Cai_et+al_2019_a]) proposed to add simultaneously trained a target-domain segmentor to enhance shape-consistency. ==However, a careful analysis reveals that the same criticism that they raise for cycle-consistency can also be applied here, i==.e., when the translated image with geometric distortion is fed to the target-domain segmentor, the distortion can be recovered without provoking any penalty in the segmentation cost, leading to sub-optimal synthesized images.
- To address the intrinsic ambiguity of cycle-==consistent reconstruction with respect to geometric transformation in CycleGAN, other works introduced shape consistency ([^Cai_et+al_2019_a]; [^Phan_et+al_2023_a]), anatomy regularization ([^Chen_et+al_2020_a]), structural similarity ([^Hiasa_et+al_2018_a]), structure consistency ([^Yang_et+al_2020_a]) and deformation correction layers ([^Wang_et+al_2021_a]). However, a careful analysis reveals that the same criticism that they raise for cycle-consistency can also be applied here, i==.e., when the translated image with geometric distortion is fed to the target-domain segmentor, the distortion can be recovered without provoking any penalty in the segmentation cost, leading to sub-optimal synthesized images. .

## Counterpoint to earlier claims
- The joint distribution is represented as a channel-wise concatenation of images and their corresponding multi-label annotation masks where each voxel has a distinct label. ==Unlike the conventional CycleGAN== ([^Zhu_et+al_2017_a]), which is restricted to the two-player framework, CycleSGAN adopts a three-player game in each domain.

## Contributions
- <mark class="claim">We developed and validated a new unpaired MR-to-CT image synthesis method</mark>. Our method featured a novel and generic way to incorporate semantic information into CycleGAN by designing a pair of three-player game. <mark class="claim">Our approach outperformed multiple state-of-theart methods and demonstrated the value of the three-player game in ensuring both realistic image synthesis and semantic structure preservation, <mark class="fact">which were of importance for down-stream tasks</mark> such as treatment planning and attenuation correction</mark>.

## Limitations
- The limitations of the study include the requirement of label supervisions to train the model, although the model can take an MR image as input to generate a synthetic CT image without the need of annotation labels at testing stage.
- The study acknowledges that the evaluation was only conducted on hip MR-CT datasets, and further investigation is required to determine whether similar performance can be obtained on datasets of different anatomical regions or acquired with different imaging protocols. The study also acknowledges that CycleSGAN requires label supervisions to train a model for unpaired MR-to-CT image synthesis.
- The study acknowledged the limitations of the present study, including the evaluation being only conducted on hip MR-CT datasets and the requirement of label supervision to train a model.

## Future work
- The future work includes further investigation of the influence of different backbone generators and discriminators on the performance of CycleSGAN, as well as the exploration of using automatically generated annotations instead of manual annotations.
- The study suggests that CycleSGAN can be applied to datasets of other anatomical regions, such as brain and abdomen. The study also suggests that CycleSGAN can be used for clinical applications, such as radiation treatment planning and multimodal image registration.
- The study suggested that further investigation is required to determine whether similar performance can be obtained when CycleSGAN is applied to datasets of different anatomical regions or to datasets acquired with different imaging protocols.


## References
[^Abu-Srhan_et+al_2021_a]: Abu-Srhan, A., Almallahi, I., Abushariah, M.A., Mahafza, W., Al-Kadi, O.S., 2021. Paired-unpaired unsupervised attention guided gan with transfer learning for bidirectional brain mr-ct synthesis. Comput. Biol. Med. 136, 104763.  [OA](http://engine.scholarcy.com/oa_version?query=Abu-Srhan%2C%20A.%20Almallahi%2C%20I.%20Abushariah%2C%20M.A.%20Mahafza%2C%20W.%20Paired-unpaired%20unsupervised%20attention%20guided%20gan%20with%20transfer%20learning%20for%20bidirectional%20brain%20mr-ct%20synthesis%202021&author=Abu-Srhan&title=Paired-unpaired%20unsupervised%20attention%20guided%20gan%20with%20transfer%20learning%20for%20bidirectional%20brain%20mr-ct%20synthesis&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Abu-Srhan%2C%20A.%20Almallahi%2C%20I.%20Abushariah%2C%20M.A.%20Mahafza%2C%20W.%20Paired-unpaired%20unsupervised%20attention%20guided%20gan%20with%20transfer%20learning%20for%20bidirectional%20brain%20mr-ct%20synthesis%202021) [Scite](/scite_tallies?query=author%3AAbu-Srhan%2Ctitle%3APaired-unpaired%20unsupervised%20attention%20guided%20gan%20with%20transfer%20learning%20for%20bidirectional%20brain%20mr-ct%20synthesis%2Cyear%3A2021)

[^Burgos_et+al_2015_a]: Burgos, N., Cardoso, M.J., Guerreiro, F., Veiga, C., Modat, M., McClelland, J., Knopf, A.C., Punwani, S., Atkinson, D., Arridge, S.R., et al., 2015. Robust ct synthesis for radiotherapy planning: application to the head and neck region. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October (2015) 5-9, Proceedings, Part II 18.  [OA](https://scholar.google.co.uk/scholar?q=Burgos%2C%20N.%20Cardoso%2C%20M.J.%20Guerreiro%2C%20F.%20Veiga%2C%20C.%20Robust%20ct%20synthesis%20for%20radiotherapy%20planning%3A%20application%20to%20the%20head%20and%20neck%20region%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Burgos%2C%20N.%20Cardoso%2C%20M.J.%20Guerreiro%2C%20F.%20Veiga%2C%20C.%20Robust%20ct%20synthesis%20for%20radiotherapy%20planning%3A%20application%20to%20the%20head%20and%20neck%20region%202015) 

[^Cai_et+al_2019_a]: Cai, J., Zhang, Z., Cui, L., Zheng, Y., Yang, L., 2019. Towards cross-modal organ translation and segmentation: A cycle-and shape-consistent generative adversarial network. Med. Image Anal. 52, 174–184.  [OA](http://engine.scholarcy.com/oa_version?query=Cai%2C%20J.%20Zhang%2C%20Z.%20Cui%2C%20L.%20Zheng%2C%20Y.%20Towards%20cross-modal%20organ%20translation%20and%20segmentation%3A%20A%20cycle-and%20shape-consistent%20generative%20adversarial%20network%202019&author=Cai&title=Towards%20cross-modal%20organ%20translation%20and%20segmentation%3A%20A%20cycle-and%20shape-consistent%20generative%20adversarial%20network&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Cai%2C%20J.%20Zhang%2C%20Z.%20Cui%2C%20L.%20Zheng%2C%20Y.%20Towards%20cross-modal%20organ%20translation%20and%20segmentation%3A%20A%20cycle-and%20shape-consistent%20generative%20adversarial%20network%202019) [Scite](/scite_tallies?query=author%3ACai%2Ctitle%3ATowards%20cross-modal%20organ%20translation%20and%20segmentation%3A%20A%20cycle-and%20shape-consistent%20generative%20adversarial%20network%2Cyear%3A2019)

[^Chartsias_et+al_2019_a]: Chartsias, A., Joyce, T., Papanastasiou, G., Semple, S., Williams, M., Newby, D.E., Dharmakumar, R., Tsaftaris, S.A., 2019. Disentangled representation learning in cardiac image analysis. Med. Image Anal. 58, 101535.  [OA](http://engine.scholarcy.com/oa_version?query=Chartsias%2C%20A.%20Joyce%2C%20T.%20Papanastasiou%2C%20G.%20Semple%2C%20S.%20Disentangled%20representation%20learning%20in%20cardiac%20image%20analysis%202019&author=Chartsias&title=Disentangled%20representation%20learning%20in%20cardiac%20image%20analysis&year=2019) [GScholar](https://scholar.google.co.uk/scholar?q=Chartsias%2C%20A.%20Joyce%2C%20T.%20Papanastasiou%2C%20G.%20Semple%2C%20S.%20Disentangled%20representation%20learning%20in%20cardiac%20image%20analysis%202019) [Scite](/scite_tallies?query=author%3AChartsias%2Ctitle%3ADisentangled%20representation%20learning%20in%20cardiac%20image%20analysis%2Cyear%3A2019)

[^Chen_et+al_2020_a]: Chen, X., Lian, C., Wang, L., Deng, H., Kuang, T., Fung, S., Gateno, J., Yap, P.T., Xia, J.J., Shen, D., 2020. Anatomy-regularized representation learning for cross-modality medical image segmentation. IEEE Trans. Med. Imaging 40, 274–285.  [OA](http://engine.scholarcy.com/oa_version?query=Chen%2C%20X.%20Lian%2C%20C.%20Wang%2C%20L.%20Deng%2C%20H.%20Anatomy-regularized%20representation%20learning%20for%20cross-modality%20medical%20image%20segmentation%202020&author=Chen&title=Anatomy-regularized%20representation%20learning%20for%20cross-modality%20medical%20image%20segmentation&year=2020) [GScholar](https://scholar.google.co.uk/scholar?q=Chen%2C%20X.%20Lian%2C%20C.%20Wang%2C%20L.%20Deng%2C%20H.%20Anatomy-regularized%20representation%20learning%20for%20cross-modality%20medical%20image%20segmentation%202020) [Scite](/scite_tallies?query=author%3AChen%2Ctitle%3AAnatomy-regularized%20representation%20learning%20for%20cross-modality%20medical%20image%20segmentation%2Cyear%3A2020)

[^Ek_et+al_2016_a]: Ççek, Ö., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O., 2016. 3D unet: learning dense volumetric segmentation from sparse annotation. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October (2016) 17-21, Proceedings, Part II 19.  [OA](https://scholar.google.co.uk/scholar?q=%C3%87%C3%A7ek%2C%20%C3%96.%20Abdulkadir%2C%20A.%20Lienkamp%2C%20S.S.%20Brox%2C%20T.%203D%20unet%3A%20learning%20dense%20volumetric%20segmentation%20from%20sparse%20annotation%202016) [GScholar](https://scholar.google.co.uk/scholar?q=%C3%87%C3%A7ek%2C%20%C3%96.%20Abdulkadir%2C%20A.%20Lienkamp%2C%20S.S.%20Brox%2C%20T.%203D%20unet%3A%20learning%20dense%20volumetric%20segmentation%20from%20sparse%20annotation%202016) 

[^Dalmaz_et+al_2022_a]: Dalmaz, O., Yurt, M., Çkur, T., 2022. Resvit: Residual vision transformers for multimodal medical image synthesis. IEEE Trans. Med. Imaging 41, 2598–2614.  [OA](http://engine.scholarcy.com/oa_version?query=Dalmaz%2C%20O.%20Yurt%2C%20M.%20%C3%87kur%2C%20T.%20Resvit%3A%20Residual%20vision%20transformers%20for%20multimodal%20medical%20image%20synthesis%202022&author=Dalmaz&title=Resvit%3A%20Residual%20vision%20transformers%20for%20multimodal%20medical%20image%20synthesis&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Dalmaz%2C%20O.%20Yurt%2C%20M.%20%C3%87kur%2C%20T.%20Resvit%3A%20Residual%20vision%20transformers%20for%20multimodal%20medical%20image%20synthesis%202022) [Scite](/scite_tallies?query=author%3ADalmaz%2Ctitle%3AResvit%3A%20Residual%20vision%20transformers%20for%20multimodal%20medical%20image%20synthesis%2Cyear%3A2022)

[^Edmund_2017_a]: Edmund, J.M., Nyholm, T., 2017. A review of substitute ct generation for mri-only radiation therapy. Radiat. Oncol. 12, 1–15.  [OA](http://engine.scholarcy.com/oa_version?query=Edmund%2C%20J.M.%20Nyholm%2C%20T.%20A%20review%20of%20substitute%20ct%20generation%20for%20mri-only%20radiation%20therapy%202017&author=Edmund&title=A%20review%20of%20substitute%20ct%20generation%20for%20mri-only%20radiation%20therapy&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Edmund%2C%20J.M.%20Nyholm%2C%20T.%20A%20review%20of%20substitute%20ct%20generation%20for%20mri-only%20radiation%20therapy%202017) [Scite](/scite_tallies?query=author%3AEdmund%2Ctitle%3AA%20review%20of%20substitute%20ct%20generation%20for%20mri-only%20radiation%20therapy%2Cyear%3A2017)

[^Ge_et+al_2019_a]: Ge, Y., Wei, D., Xue, Z., Wang, Q., Zhou, X., Zhan, Y., Liao, S., 2019. Unpaired mr to ct synthesis with explicit structural constrained adversarial learning. In: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE, pp. 1096–1099.  [OA](https://scholar.google.co.uk/scholar?q=Ge%2C%20Y.%20Wei%2C%20D.%20Xue%2C%20Z.%20Wang%2C%20Q.%20Unpaired%20mr%20to%20ct%20synthesis%20with%20explicit%20structural%20constrained%20adversarial%20learning%202019) [GScholar](https://scholar.google.co.uk/scholar?q=Ge%2C%20Y.%20Wei%2C%20D.%20Xue%2C%20Z.%20Wang%2C%20Q.%20Unpaired%20mr%20to%20ct%20synthesis%20with%20explicit%20structural%20constrained%20adversarial%20learning%202019) 

[^Gudur_et+al_2014_a]: Gudur, M.S.R., Hara, W., Le, Q.-T., Wang, L., Xing, L., Li, R., 2014. A unifying probabilistic bayesian approach to derive electron density from mri for radiation therapy treatment planning. Phys. Med. Biol. 59, 6595.  [OA](http://engine.scholarcy.com/oa_version?query=Gudur%2C%20M.S.R.%20Hara%2C%20W.%20Le%2C%20Q.-T.%20Wang%2C%20L.%20A%20unifying%20probabilistic%20bayesian%20approach%20to%20derive%20electron%20density%20from%20mri%20for%20radiation%20therapy%20treatment%20planning%202014&author=Gudur&title=A%20unifying%20probabilistic%20bayesian%20approach%20to%20derive%20electron%20density%20from%20mri%20for%20radiation%20therapy%20treatment%20planning&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Gudur%2C%20M.S.R.%20Hara%2C%20W.%20Le%2C%20Q.-T.%20Wang%2C%20L.%20A%20unifying%20probabilistic%20bayesian%20approach%20to%20derive%20electron%20density%20from%20mri%20for%20radiation%20therapy%20treatment%20planning%202014) [Scite](/scite_tallies?query=author%3AGudur%2Ctitle%3AA%20unifying%20probabilistic%20bayesian%20approach%20to%20derive%20electron%20density%20from%20mri%20for%20radiation%20therapy%20treatment%20planning%2Cyear%3A2014)

[^He_et+al_2016_a]: He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 770–778.  [OA](https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016) [GScholar](https://scholar.google.co.uk/scholar?q=He%2C%20K.%20Zhang%2C%20X.%20Ren%2C%20S.%20Sun%2C%20J.%20Deep%20residual%20learning%20for%20image%20recognition%202016) 

[^Hiasa_et+al_2018_a]: Hiasa, Y., Otake, Y., Takao, M., Matsuoka, T., Takashima, K., Carass, A., Prince, J.L., Sugano, N., Sato, Y., 2018. Cross-modality image synthesis from unpaired data using cyclegan. In: International Workshop on Simulation and Synthesis in Medical Imaging. Springer, pp. 31–41.  [OA](https://scholar.google.co.uk/scholar?q=Hiasa%2C%20Y.%20Otake%2C%20Y.%20Takao%2C%20M.%20Matsuoka%2C%20T.%20Cross-modality%20image%20synthesis%20from%20unpaired%20data%20using%20cyclegan%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Hiasa%2C%20Y.%20Otake%2C%20Y.%20Takao%2C%20M.%20Matsuoka%2C%20T.%20Cross-modality%20image%20synthesis%20from%20unpaired%20data%20using%20cyclegan%202018) 

[^Hsu_et+al_2013_a]: Hsu, S.-H., Cao, Y., Huang, K., Feng, M., Balter, J.M., 2013. Investigation of a method for generating synthetic ct models from mri scans of the head and neck for radiation therapy. Phys. Med. Biol. 58, 8419.  [OA](http://engine.scholarcy.com/oa_version?query=Hsu%2C%20S.-H.%20Cao%2C%20Y.%20Huang%2C%20K.%20Feng%2C%20M.%20Investigation%20of%20a%20method%20for%20generating%20synthetic%20ct%20models%20from%20mri%20scans%20of%20the%20head%20and%20neck%20for%20radiation%20therapy%202013&author=Hsu&title=Investigation%20of%20a%20method%20for%20generating%20synthetic%20ct%20models%20from%20mri%20scans%20of%20the%20head%20and%20neck%20for%20radiation%20therapy&year=2013) [GScholar](https://scholar.google.co.uk/scholar?q=Hsu%2C%20S.-H.%20Cao%2C%20Y.%20Huang%2C%20K.%20Feng%2C%20M.%20Investigation%20of%20a%20method%20for%20generating%20synthetic%20ct%20models%20from%20mri%20scans%20of%20the%20head%20and%20neck%20for%20radiation%20therapy%202013) [Scite](/scite_tallies?query=author%3AHsu%2Ctitle%3AInvestigation%20of%20a%20method%20for%20generating%20synthetic%20ct%20models%20from%20mri%20scans%20of%20the%20head%20and%20neck%20for%20radiation%20therapy%2Cyear%3A2013)

[^Hu_et+al_2014_a]: Hu, L., Su, K.-H., Pereira, G.C., Grover, A., Traughber, B., Traughber, M., Muzic, Jr., R.F., 2014. K-space sampling optimization for ultrashort te imaging of cortical bone: Applications in radiation therapy planning and mr-based pet attenuation correction. Med. Phys. 41, 102301.  [OA](http://engine.scholarcy.com/oa_version?query=Hu%2C%20L.%20Su%2C%20K.-H.%20Pereira%2C%20G.C.%20Grover%2C%20A.%20K-space%20sampling%20optimization%20for%20ultrashort%20te%20imaging%20of%20cortical%20bone%3A%20Applications%20in%20radiation%20therapy%20planning%20and%20mr-based%20pet%20attenuation%20correction%202014&author=Hu&title=K-space%20sampling%20optimization%20for%20ultrashort%20te%20imaging%20of%20cortical%20bone%3A%20Applications%20in%20radiation%20therapy%20planning%20and%20mr-based%20pet%20attenuation%20correction&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Hu%2C%20L.%20Su%2C%20K.-H.%20Pereira%2C%20G.C.%20Grover%2C%20A.%20K-space%20sampling%20optimization%20for%20ultrashort%20te%20imaging%20of%20cortical%20bone%3A%20Applications%20in%20radiation%20therapy%20planning%20and%20mr-based%20pet%20attenuation%20correction%202014) [Scite](/scite_tallies?query=author%3AHu%2Ctitle%3AK-space%20sampling%20optimization%20for%20ultrashort%20te%20imaging%20of%20cortical%20bone%3A%20Applications%20in%20radiation%20therapy%20planning%20and%20mr-based%20pet%20attenuation%20correction%2Cyear%3A2014)

[^Huang_et+al_2018_a]: Huang, X., Liu, M.-Y., Belongie, S., Kautz, J., 2018. Multimodal unsupervised image-toimage translation. In: Proceedings of the European Conference on Computer Vision. ECCV, pp. 172–189.  [OA](https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-toimage%20translation%202018) [GScholar](https://scholar.google.co.uk/scholar?q=Huang%2C%20X.%20Liu%2C%20M.-Y.%20Belongie%2C%20S.%20Kautz%2C%20J.%20Multimodal%20unsupervised%20image-toimage%20translation%202018) 

[^Huynh_et+al_2015_a]: Huynh, T., Gao, Y., Kang, J., Wang, L., Zhang, P., Lian, J., Shen, D., 2015. Estimating ct image from mri data using structured random forest and auto-context model. IEEE Trans. Med. Imaging 35, 174–183.  [OA](http://engine.scholarcy.com/oa_version?query=Huynh%2C%20T.%20Gao%2C%20Y.%20Kang%2C%20J.%20Wang%2C%20L.%20Estimating%20ct%20image%20from%20mri%20data%20using%20structured%20random%20forest%20and%20auto-context%20model%202015&author=Huynh&title=Estimating%20ct%20image%20from%20mri%20data%20using%20structured%20random%20forest%20and%20auto-context%20model&year=2015) [GScholar](https://scholar.google.co.uk/scholar?q=Huynh%2C%20T.%20Gao%2C%20Y.%20Kang%2C%20J.%20Wang%2C%20L.%20Estimating%20ct%20image%20from%20mri%20data%20using%20structured%20random%20forest%20and%20auto-context%20model%202015) [Scite](/scite_tallies?query=author%3AHuynh%2Ctitle%3AEstimating%20ct%20image%20from%20mri%20data%20using%20structured%20random%20forest%20and%20auto-context%20model%2Cyear%3A2015)

[^Isensee_et+al_2021_a]: Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H., 2021. Nnu-net: a self-configuring method for deep learning-based biomedical image segmentation. Nature Methods 18, 203–211.  [OA](http://engine.scholarcy.com/oa_version?query=Isensee%2C%20F.%20Jaeger%2C%20P.F.%20Kohl%2C%20S.A.%20Petersen%2C%20J.%20Nnu-net%3A%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation%202021&author=Isensee&title=Nnu-net%3A%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Isensee%2C%20F.%20Jaeger%2C%20P.F.%20Kohl%2C%20S.A.%20Petersen%2C%20J.%20Nnu-net%3A%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation%202021) [Scite](/scite_tallies?query=author%3AIsensee%2Ctitle%3ANnu-net%3A%20a%20self-configuring%20method%20for%20deep%20learning-based%20biomedical%20image%20segmentation%2Cyear%3A2021)

[^Jiang_et+al_2020_a]: Jiang, J., Hu, Y.-C., Tyagi, N., Rimner, A., Lee, N., Deasy, J.O., Berry, S., Veeraraghavan, H., 2020. Psigan: Joint probabilistic segmentation and image distribution matching for unpaired cross-modality adaptation-based mri segmentation. IEEE Trans. Med. Imaging 39, 4071–4084.  [OA](http://engine.scholarcy.com/oa_version?query=Jiang%2C%20J.%20Hu%2C%20Y.-C.%20Tyagi%2C%20N.%20Rimner%2C%20A.%20Psigan%3A%20Joint%20probabilistic%20segmentation%20and%20image%20distribution%20matching%20for%20unpaired%20cross-modality%20adaptation-based%20mri%20segmentation%202020&author=Jiang&title=Psigan%3A%20Joint%20probabilistic%20segmentation%20and%20image%20distribution%20matching%20for%20unpaired%20cross-modality%20adaptation-based%20mri%20segmentation&year=2020) [GScholar](https://scholar.google.co.uk/scholar?q=Jiang%2C%20J.%20Hu%2C%20Y.-C.%20Tyagi%2C%20N.%20Rimner%2C%20A.%20Psigan%3A%20Joint%20probabilistic%20segmentation%20and%20image%20distribution%20matching%20for%20unpaired%20cross-modality%20adaptation-based%20mri%20segmentation%202020) [Scite](/scite_tallies?query=author%3AJiang%2Ctitle%3APsigan%3A%20Joint%20probabilistic%20segmentation%20and%20image%20distribution%20matching%20for%20unpaired%20cross-modality%20adaptation-based%20mri%20segmentation%2Cyear%3A2020)

[^Kingma_2014_a]: Kingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.  [OA](https://arxiv.org/abs/1412.6980)  

[^Ladefoged_et+al_2017_a]: Ladefoged, C.N., Law, I., Anazodo, U., Lawrence, K.S., Izquierdo-Garcia, D., Catana, C., Burgos, N., Cardoso, M.J., Ourselin, S., Hutton, B., et al., 2017. A multi-centre evaluation of eleven clinically feasible brain pet/mri attenuation correction techniques using a large cohort of patients. Neuroimage 147, 346–359.  [OA](http://engine.scholarcy.com/oa_version?query=Ladefoged%2C%20C.N.%20Law%2C%20I.%20Anazodo%2C%20U.%20Lawrence%2C%20K.S.%20A%20multi-centre%20evaluation%20of%20eleven%20clinically%20feasible%20brain%20pet/mri%20attenuation%20correction%20techniques%20using%20a%20large%20cohort%20of%20patients%202017&author=Ladefoged&title=A%20multi-centre%20evaluation%20of%20eleven%20clinically%20feasible%20brain%20pet/mri%20attenuation%20correction%20techniques%20using%20a%20large%20cohort%20of%20patients&year=2017) [GScholar](https://scholar.google.co.uk/scholar?q=Ladefoged%2C%20C.N.%20Law%2C%20I.%20Anazodo%2C%20U.%20Lawrence%2C%20K.S.%20A%20multi-centre%20evaluation%20of%20eleven%20clinically%20feasible%20brain%20pet/mri%20attenuation%20correction%20techniques%20using%20a%20large%20cohort%20of%20patients%202017) [Scite](/scite_tallies?query=author%3ALadefoged%2Ctitle%3AA%20multi-centre%20evaluation%20of%20eleven%20clinically%20feasible%20brain%20pet/mri%20attenuation%20correction%20techniques%20using%20a%20large%20cohort%20of%20patients%2Cyear%3A2017)

[^Li_et+al_2022_a]: Li, C., Xu, K., Zhu, J., Liu, J., Zhang, B., 2022. Triple generative adversarial networks. IEEE Trans. Pattern Anal. Mach. Intell. 44, 9629–9640.  [OA](http://engine.scholarcy.com/oa_version?query=Li%2C%20C.%20Xu%2C%20K.%20Zhu%2C%20J.%20Liu%2C%20J.%20Triple%20generative%20adversarial%20networks%202022&author=Li&title=Triple%20generative%20adversarial%20networks&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Li%2C%20C.%20Xu%2C%20K.%20Zhu%2C%20J.%20Liu%2C%20J.%20Triple%20generative%20adversarial%20networks%202022) [Scite](/scite_tallies?query=author%3ALi%2Ctitle%3ATriple%20generative%20adversarial%20networks%2Cyear%3A2022)

[^Liu_et+al_2022_a]: Liu, X., Sanchez, P., Thermos, S., O’Neil, A.Q., Tsaftaris, S.A., 2022. Learning disentangled representations in the imaging domain. Med. Image Anal. 80, 102516.  [OA](http://engine.scholarcy.com/oa_version?query=Liu%2C%20X.%20Sanchez%2C%20P.%20Thermos%2C%20S.%20O%E2%80%99Neil%2C%20A.Q.%20Learning%20disentangled%20representations%20in%20the%20imaging%20domain%202022&author=Liu&title=Learning%20disentangled%20representations%20in%20the%20imaging%20domain&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Liu%2C%20X.%20Sanchez%2C%20P.%20Thermos%2C%20S.%20O%E2%80%99Neil%2C%20A.Q.%20Learning%20disentangled%20representations%20in%20the%20imaging%20domain%202022) [Scite](/scite_tallies?query=author%3ALiu%2Ctitle%3ALearning%20disentangled%20representations%20in%20the%20imaging%20domain%2Cyear%3A2022)

[^Nie_et+al_2018_a]: Nie, D., Trullo, R., Lian, J., Wang, L., Petitjean, C., Ruan, S., Wang, Q., Shen, D., 2018. Medical image synthesis with deep convolutional adversarial networks. IEEE Trans. Biomed. Eng. 65, 2720–2730.  [OA](http://engine.scholarcy.com/oa_version?query=Nie%2C%20D.%20Trullo%2C%20R.%20Lian%2C%20J.%20Wang%2C%20L.%20Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks%202018&author=Nie&title=Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Nie%2C%20D.%20Trullo%2C%20R.%20Lian%2C%20J.%20Wang%2C%20L.%20Medical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks%202018) [Scite](/scite_tallies?query=author%3ANie%2Ctitle%3AMedical%20image%20synthesis%20with%20deep%20convolutional%20adversarial%20networks%2Cyear%3A2018)

[^Oezbey_et+al_2023_a]: Özbey, M., Dalmaz, O., Dar, S.U., Bedel, H.A., Özturk, Ş., Güngör, A., Çkur, T., 2023. Unsupervised medical image translation with adversarial diffusion models. IEEE Trans. Med. Imaging.  [OA](http://engine.scholarcy.com/oa_version?query=%C3%96zbey%2C%20M.%20Dalmaz%2C%20O.%20Dar%2C%20S.U.%20Bedel%2C%20H.A.%20Unsupervised%20medical%20image%20translation%20with%20adversarial%20diffusion%20models%202023&author=Oezbey&title=Unsupervised%20medical%20image%20translation%20with%20adversarial%20diffusion%20models&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=%C3%96zbey%2C%20M.%20Dalmaz%2C%20O.%20Dar%2C%20S.U.%20Bedel%2C%20H.A.%20Unsupervised%20medical%20image%20translation%20with%20adversarial%20diffusion%20models%202023) [Scite](/scite_tallies?query=author%3AOezbey%2Ctitle%3AUnsupervised%20medical%20image%20translation%20with%20adversarial%20diffusion%20models%2Cyear%3A2023)

[^Phan_et+al_2023_a]: Phan, V.M.H., Liao, Z., Verjans, J.W., To, M.-S., 2023. Structure-preserving synthesis: Maskgan for unpaired mr-ct translation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, pp. 56–65.  [OA](https://scholar.google.co.uk/scholar?q=Phan%2C%20V.M.H.%20Liao%2C%20Z.%20Verjans%2C%20J.W.%20To%2C%20M.-S.%20Structure-preserving%20synthesis%3A%20Maskgan%20for%20unpaired%20mr-ct%20translation%202023) [GScholar](https://scholar.google.co.uk/scholar?q=Phan%2C%20V.M.H.%20Liao%2C%20Z.%20Verjans%2C%20J.W.%20To%2C%20M.-S.%20Structure-preserving%20synthesis%3A%20Maskgan%20for%20unpaired%20mr-ct%20translation%202023) 

[^Reaungamornrat_et+al_2022_a]: Reaungamornrat, S., Sari, H., Catana, C., Kamen, A., 2022. Multimodal image synthesis based on disentanglement representations of anatomical and modality specific features, learned using uncooperative relativistic gan. Med. Image Anal. 80, 102514.  [OA](http://engine.scholarcy.com/oa_version?query=Reaungamornrat%2C%20S.%20Sari%2C%20H.%20Catana%2C%20C.%20Kamen%2C%20A.%20Multimodal%20image%20synthesis%20based%20on%20disentanglement%20representations%20of%20anatomical%20and%20modality%20specific%20features%2C%20learned%20using%20uncooperative%20relativistic%20gan%202022&author=Reaungamornrat&title=Multimodal%20image%20synthesis%20based%20on%20disentanglement%20representations%20of%20anatomical%20and%20modality%20specific%20features%2C%20learned%20using%20uncooperative%20relativistic%20gan&year=2022) [GScholar](https://scholar.google.co.uk/scholar?q=Reaungamornrat%2C%20S.%20Sari%2C%20H.%20Catana%2C%20C.%20Kamen%2C%20A.%20Multimodal%20image%20synthesis%20based%20on%20disentanglement%20representations%20of%20anatomical%20and%20modality%20specific%20features%2C%20learned%20using%20uncooperative%20relativistic%20gan%202022) [Scite](/scite_tallies?query=author%3AReaungamornrat%2Ctitle%3AMultimodal%20image%20synthesis%20based%20on%20disentanglement%20representations%20of%20anatomical%20and%20modality%20specific%20features%2C%20learned%20using%20uncooperative%20relativistic%20gan%2Cyear%3A2022)

[^Ronneberger_et+al_2015_a]: Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October (2015) 5-9, Proceedings, Part III 18.  [OA](https://scholar.google.co.uk/scholar?q=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%202015) [GScholar](https://scholar.google.co.uk/scholar?q=Ronneberger%2C%20O.%20Fischer%2C%20P.%20Brox%2C%20T.%20U-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%202015) 

[^Roy_et+al_2014_a]: Roy, S., Wang, W.-T., Carass, A., Prince, J.L., Butman, J.A., Pham, D.L., 2014. Pet attenuation correction using synthetic ct from ultrashort echo-time mr imaging. J. Nucl. Med. 55, 2071–2077.  [OA](http://engine.scholarcy.com/oa_version?query=Roy%2C%20S.%20Wang%2C%20W.-T.%20Carass%2C%20A.%20Prince%2C%20J.L.%20Pet%20attenuation%20correction%20using%20synthetic%20ct%20from%20ultrashort%20echo-time%20mr%20imaging%202014&author=Roy&title=Pet%20attenuation%20correction%20using%20synthetic%20ct%20from%20ultrashort%20echo-time%20mr%20imaging&year=2014) [GScholar](https://scholar.google.co.uk/scholar?q=Roy%2C%20S.%20Wang%2C%20W.-T.%20Carass%2C%20A.%20Prince%2C%20J.L.%20Pet%20attenuation%20correction%20using%20synthetic%20ct%20from%20ultrashort%20echo-time%20mr%20imaging%202014) [Scite](/scite_tallies?query=author%3ARoy%2Ctitle%3APet%20attenuation%20correction%20using%20synthetic%20ct%20from%20ultrashort%20echo-time%20mr%20imaging%2Cyear%3A2014)

[^Schlemmer_et+al_2008_a]: Schlemmer, H.-P.W., Pichler, B.J., Schmand, M., Burbar, Z., Michel, C., Ladebeck, R., Jattke, K., Townsend, D., Nahmias, C., Jacob, P.K., et al., 2008. Simultaneous mr/pet imaging of the human brain: feasibility study. Radiology 248, 1028.  [OA](http://engine.scholarcy.com/oa_version?query=Schlemmer%2C%20H.-P.W.%20Pichler%2C%20B.J.%20Schmand%2C%20M.%20Burbar%2C%20Z.%20Simultaneous%20mr/pet%20imaging%20of%20the%20human%20brain%3A%20feasibility%20study%202008&author=Schlemmer&title=Simultaneous%20mr/pet%20imaging%20of%20the%20human%20brain%3A%20feasibility%20study&year=2008) [GScholar](https://scholar.google.co.uk/scholar?q=Schlemmer%2C%20H.-P.W.%20Pichler%2C%20B.J.%20Schmand%2C%20M.%20Burbar%2C%20Z.%20Simultaneous%20mr/pet%20imaging%20of%20the%20human%20brain%3A%20feasibility%20study%202008) [Scite](/scite_tallies?query=author%3ASchlemmer%2Ctitle%3ASimultaneous%20mr/pet%20imaging%20of%20the%20human%20brain%3A%20feasibility%20study%2Cyear%3A2008)

[^Sjoelund_et+al_2015_a]: Sjölund, J., Forsberg, D., Andersson, M., Knutsson, H., 2015. Generating patient specific pseudo-ct of the head from mr using atlas-based regression. Phys. Med. Biol. 60, 825.  [OA](http://engine.scholarcy.com/oa_version?query=Sj%C3%B6lund%2C%20J.%20Forsberg%2C%20D.%20Andersson%2C%20M.%20Knutsson%2C%20H.%20Generating%20patient%20specific%20pseudo-ct%20of%20the%20head%20from%20mr%20using%20atlas-based%20regression%202015&author=Sjoelund&title=Generating%20patient%20specific%20pseudo-ct%20of%20the%20head%20from%20mr%20using%20atlas-based%20regression&year=2015) [GScholar](https://scholar.google.co.uk/scholar?q=Sj%C3%B6lund%2C%20J.%20Forsberg%2C%20D.%20Andersson%2C%20M.%20Knutsson%2C%20H.%20Generating%20patient%20specific%20pseudo-ct%20of%20the%20head%20from%20mr%20using%20atlas-based%20regression%202015) [Scite](/scite_tallies?query=author%3ASjoelund%2Ctitle%3AGenerating%20patient%20specific%20pseudo-ct%20of%20the%20head%20from%20mr%20using%20atlas-based%20regression%2Cyear%3A2015)

[^Ulyanov_et+al_2016_a]: Ulyanov, D., Vedaldi, A., Lempitsky, V., 2016. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022.  [OA](https://arxiv.org/abs/1607.08022)  

[^Wang_et+al_2004_a]: Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., 2004. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process. 13, 600–612.  [OA](http://engine.scholarcy.com/oa_version?query=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004&author=Wang&title=Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity&year=2004) [GScholar](https://scholar.google.co.uk/scholar?q=Wang%2C%20Z.%20Bovik%2C%20A.C.%20Sheikh%2C%20H.R.%20Simoncelli%2C%20E.P.%20Image%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%202004) [Scite](/scite_tallies?query=author%3AWang%2Ctitle%3AImage%20quality%20assessment%3A%20from%20error%20visibility%20to%20structural%20similarity%2Cyear%3A2004)

[^Wang_et+al_2023_a]: Wang, J., Wu, Q.J., Pourpanah, F., 2023. Dc-cyclegan: bidirectional ct-to-mr synthesis from unpaired data. Comput. Med. Imaging Graph. 108, 102249.  [OA](http://engine.scholarcy.com/oa_version?query=Wang%2C%20J.%20Wu%2C%20Q.J.%20Pourpanah%2C%20F.%20Dc-cyclegan%3A%20bidirectional%20ct-to-mr%20synthesis%20from%20unpaired%20data%202023&author=Wang&title=Dc-cyclegan%3A%20bidirectional%20ct-to-mr%20synthesis%20from%20unpaired%20data&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Wang%2C%20J.%20Wu%2C%20Q.J.%20Pourpanah%2C%20F.%20Dc-cyclegan%3A%20bidirectional%20ct-to-mr%20synthesis%20from%20unpaired%20data%202023) [Scite](/scite_tallies?query=author%3AWang%2Ctitle%3ADc-cyclegan%3A%20bidirectional%20ct-to-mr%20synthesis%20from%20unpaired%20data%2Cyear%3A2023)

[^Wang_et+al_2021_a]: Wang, C., Yang, G., Papanastasiou, G., Tsaftaris, S.A., Newby, D.E., Gray, C., Macnaught, G., MacGillivray, T.J., 2021. Dicyc: Gan-based deformation invariant cross-domain information fusion for medical image synthesis. Inf. Fusion 67, 147–160.  [OA](http://engine.scholarcy.com/oa_version?query=Wang%2C%20C.%20Yang%2C%20G.%20Papanastasiou%2C%20G.%20Tsaftaris%2C%20S.A.%20Dicyc%3A%20Gan-based%20deformation%20invariant%20cross-domain%20information%20fusion%20for%20medical%20image%20synthesis%202021&author=Wang&title=Dicyc%3A%20Gan-based%20deformation%20invariant%20cross-domain%20information%20fusion%20for%20medical%20image%20synthesis&year=2021) [GScholar](https://scholar.google.co.uk/scholar?q=Wang%2C%20C.%20Yang%2C%20G.%20Papanastasiou%2C%20G.%20Tsaftaris%2C%20S.A.%20Dicyc%3A%20Gan-based%20deformation%20invariant%20cross-domain%20information%20fusion%20for%20medical%20image%20synthesis%202021) [Scite](/scite_tallies?query=author%3AWang%2Ctitle%3ADicyc%3A%20Gan-based%20deformation%20invariant%20cross-domain%20information%20fusion%20for%20medical%20image%20synthesis%2Cyear%3A2021)

[^Wolterink_et+al_2017_a]: Wolterink, J.M., Dinkla, A.M., Savenije, M.H., Seevinck, P.R., van den Berg, C.A., Išum, I., 2017. Deep mr to ct synthesis using unpaired data. In: International Workshop on Simulation and Synthesis in Medical Imaging. Springer, pp. 14–23.  [OA](https://scholar.google.co.uk/scholar?q=Wolterink%2C%20J.M.%20Dinkla%2C%20A.M.%20Savenije%2C%20M.H.%20Seevinck%2C%20P.R.%20Deep%20mr%20to%20ct%20synthesis%20using%20unpaired%20data%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Wolterink%2C%20J.M.%20Dinkla%2C%20A.M.%20Savenije%2C%20M.H.%20Seevinck%2C%20P.R.%20Deep%20mr%20to%20ct%20synthesis%20using%20unpaired%20data%202017) 

[^Xiang_et+al_2018_a]: Xiang, L., Wang, Q., Nie, D., Zhang, L., Jin, X., Qiao, Y., Shen, D., 2018. Deep embedding convolutional neural network for synthesizing ct image from t1-weighted mr image. Med. Image Anal. 47, 31–44.  [OA](http://engine.scholarcy.com/oa_version?query=Xiang%2C%20L.%20Wang%2C%20Q.%20Nie%2C%20D.%20Zhang%2C%20L.%20Deep%20embedding%20convolutional%20neural%20network%20for%20synthesizing%20ct%20image%20from%20t1-weighted%20mr%20image%202018&author=Xiang&title=Deep%20embedding%20convolutional%20neural%20network%20for%20synthesizing%20ct%20image%20from%20t1-weighted%20mr%20image&year=2018) [GScholar](https://scholar.google.co.uk/scholar?q=Xiang%2C%20L.%20Wang%2C%20Q.%20Nie%2C%20D.%20Zhang%2C%20L.%20Deep%20embedding%20convolutional%20neural%20network%20for%20synthesizing%20ct%20image%20from%20t1-weighted%20mr%20image%202018) [Scite](/scite_tallies?query=author%3AXiang%2Ctitle%3ADeep%20embedding%20convolutional%20neural%20network%20for%20synthesizing%20ct%20image%20from%20t1-weighted%20mr%20image%2Cyear%3A2018)

[^Yang_et+al_2020_a]: Yang, H., Sun, J., Carass, A., Zhao, C., Lee, J., Prince, J.L., Xu, Z., 2020. Unsupervised mr-to-ct synthesis using structure-constrained cyclegan. IEEE Trans. Med. Imaging 39, 4249–4261.  [OA](http://engine.scholarcy.com/oa_version?query=Yang%2C%20H.%20Sun%2C%20J.%20Carass%2C%20A.%20Zhao%2C%20C.%20Unsupervised%20mr-to-ct%20synthesis%20using%20structure-constrained%20cyclegan%202020&author=Yang&title=Unsupervised%20mr-to-ct%20synthesis%20using%20structure-constrained%20cyclegan&year=2020) [GScholar](https://scholar.google.co.uk/scholar?q=Yang%2C%20H.%20Sun%2C%20J.%20Carass%2C%20A.%20Zhao%2C%20C.%20Unsupervised%20mr-to-ct%20synthesis%20using%20structure-constrained%20cyclegan%202020) [Scite](/scite_tallies?query=author%3AYang%2Ctitle%3AUnsupervised%20mr-to-ct%20synthesis%20using%20structure-constrained%20cyclegan%2Cyear%3A2020)

[^Zhao_et+al_2017_a]: Zhao, C., Carass, A., Lee, J., Jog, A., Prince, J.L., 2017. A supervoxel based random forest synthesis framework for bidirectional mr/ct synthesis. In: International Workshop on Simulation and Synthesis in Medical Imaging. Springer, pp. 33–40.  [OA](https://scholar.google.co.uk/scholar?q=Zhao%2C%20C.%20Carass%2C%20A.%20Lee%2C%20J.%20Jog%2C%20A.%20A%20supervoxel%20based%20random%20forest%20synthesis%20framework%20for%20bidirectional%20mr/ct%20synthesis%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Zhao%2C%20C.%20Carass%2C%20A.%20Lee%2C%20J.%20Jog%2C%20A.%20A%20supervoxel%20based%20random%20forest%20synthesis%20framework%20for%20bidirectional%20mr/ct%20synthesis%202017) 

[^Zhao_et+al_2020_a]: Zhao, J., Li, D., Kassam, Z., Howey, J., Chong, J., Chen, B., Li, S., 2020. Tripartite-gan: Synthesizing liver contrast-enhanced mri to improve tumor detection. Med. Image Anal. 63, 101667.  [OA](http://engine.scholarcy.com/oa_version?query=Zhao%2C%20J.%20Li%2C%20D.%20Kassam%2C%20Z.%20Howey%2C%20J.%20Tripartite-gan%3A%20Synthesizing%20liver%20contrast-enhanced%20mri%20to%20improve%20tumor%20detection%202020&author=Zhao&title=Tripartite-gan%3A%20Synthesizing%20liver%20contrast-enhanced%20mri%20to%20improve%20tumor%20detection&year=2020) [GScholar](https://scholar.google.co.uk/scholar?q=Zhao%2C%20J.%20Li%2C%20D.%20Kassam%2C%20Z.%20Howey%2C%20J.%20Tripartite-gan%3A%20Synthesizing%20liver%20contrast-enhanced%20mri%20to%20improve%20tumor%20detection%202020) [Scite](/scite_tallies?query=author%3AZhao%2Ctitle%3ATripartite-gan%3A%20Synthesizing%20liver%20contrast-enhanced%20mri%20to%20improve%20tumor%20detection%2Cyear%3A2020)

[^Zhong_et+al_2023_a]: Zhong, L., Chen, Z., Shu, H., Zheng, K., Li, Y., Chen, W., Wu, Y., Ma, J., Feng, Q., Yang, W., 2023. Multi-scale tokens-aware transformer network for multi-region and multi-sequence mr-to-ct synthesis in a single model. IEEE Trans. Med. Imaging.  [OA](http://engine.scholarcy.com/oa_version?query=Zhong%2C%20L.%20Chen%2C%20Z.%20Shu%2C%20H.%20Zheng%2C%20K.%20Multi-scale%20tokens-aware%20transformer%20network%20for%20multi-region%20and%20multi-sequence%20mr-to-ct%20synthesis%20in%20a%20single%20model%202023&author=Zhong&title=Multi-scale%20tokens-aware%20transformer%20network%20for%20multi-region%20and%20multi-sequence%20mr-to-ct%20synthesis%20in%20a%20single%20model&year=2023) [GScholar](https://scholar.google.co.uk/scholar?q=Zhong%2C%20L.%20Chen%2C%20Z.%20Shu%2C%20H.%20Zheng%2C%20K.%20Multi-scale%20tokens-aware%20transformer%20network%20for%20multi-region%20and%20multi-sequence%20mr-to-ct%20synthesis%20in%20a%20single%20model%202023) [Scite](/scite_tallies?query=author%3AZhong%2Ctitle%3AMulti-scale%20tokens-aware%20transformer%20network%20for%20multi-region%20and%20multi-sequence%20mr-to-ct%20synthesis%20in%20a%20single%20model%2Cyear%3A2023)

[^Zhu_et+al_2017_a]: Zhu, J.-Y., Park, T., Isola, P., Efros, A.A., 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2223–2232.  [OA](https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017) [GScholar](https://scholar.google.co.uk/scholar?q=Zhu%2C%20J.-Y.%20Park%2C%20T.%20Isola%2C%20P.%20Efros%2C%20A.A.%20Unpaired%20image-to-image%20translation%20using%20cycle-consistent%20adversarial%20networks%202017) 

