{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6RKt9DFLSmf+26so1rjmz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/CyberAttackDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlowChart"
      ],
      "metadata": {
        "id": "2OYrAS-e0DNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kki9cyway1lD",
        "outputId": "3e055ecc-3ccb-4d7c-ef1e-18ba444ffb4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'flowchart.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "dot = Digraph(comment='Cyber Attack Detection Flowchart')\n",
        "dot.node('A', 'Start')\n",
        "dot.node('B', 'Define Problem & Scope')\n",
        "dot.node('C', 'Collect Dataset')\n",
        "dot.node('D', 'Preprocess Data')\n",
        "dot.node('E', 'Perform EDA')\n",
        "dot.node('F', 'Select Model')\n",
        "dot.node('G', 'Train Model')\n",
        "dot.node('H', 'Evaluate Model')\n",
        "dot.node('I', 'Optimize Model')\n",
        "dot.node('J', 'Deploy Model')\n",
        "dot.node('K', 'Monitor & Retrain')\n",
        "dot.node('L', 'End')\n",
        "\n",
        "dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG', 'GH', 'HJ', 'JK', 'KL'])\n",
        "dot.edge('H', 'I', label='If performance poor')\n",
        "dot.edge('I', 'G', label='Retrain')\n",
        "\n",
        "dot.render('flowchart', format='png', view=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = '/content/drive/MyDrive/network-intrusion-dataset.zip'\n",
        "extract_dir = '/content/cicids2017/'\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# List of CSV files\n",
        "csv_files = [\n",
        "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
        "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
        "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
        "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
        "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
        "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
        "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
        "    'Wednesday-workingHours.pcap_ISCX.csv'\n",
        "]\n",
        "\n",
        "# Step 1: Load and Combine Datasets\n",
        "def load_and_combine_data():\n",
        "    data_frames = []\n",
        "    base_path = os.path.join(extract_dir, 'MachineLearningCVE')\n",
        "    for file in csv_files:\n",
        "        file_path = os.path.join(base_path, file)\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "        data_frames.append(df)\n",
        "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "print(\"Loading and combining datasets...\")\n",
        "df = load_and_combine_data()\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# Step 2: Initial Data Inspection\n",
        "def inspect_data():\n",
        "    print(\"\\nStep 2: Initial Data Inspection\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nColumn names:\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nBasic statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    # Save column names for reference\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.bar(range(len(df.columns)), [1] * len(df.columns))\n",
        "    plt.xticks(range(len(df.columns)), df.columns, rotation=90)\n",
        "    plt.title(\"Dataset Columns\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('columns.png')\n",
        "    plt.close()\n",
        "\n",
        "inspect_data()\n",
        "\n",
        "# Step 3: Check for Missing Values\n",
        "def check_missing_values():\n",
        "    print(\"\\nStep 3: Check for Missing Values\")\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "\n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    missing_values[missing_values > 0].plot(kind='bar')\n",
        "    plt.title(\"Missing Values per Column\")\n",
        "    plt.xlabel(\"Columns\")\n",
        "    plt.ylabel(\"Number of Missing Values\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('missing_values.png')\n",
        "    plt.close()\n",
        "\n",
        "check_missing_values()\n",
        "\n",
        "# Step 4: Handle Missing Values\n",
        "def handle_missing_values():\n",
        "    print(\"\\nStep 4: Handle Missing Values\")\n",
        "    global df\n",
        "    # Replace inf values with NaN\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    # Impute numerical columns with median\n",
        "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
        "    # Verify no missing values remain\n",
        "    print(\"\\nMissing values after imputation:\")\n",
        "    print(df.isnull().sum().sum())\n",
        "\n",
        "handle_missing_values()\n",
        "\n",
        "# Step 5: Analyze Class Distribution\n",
        "def analyze_class_distribution():\n",
        "    print(\"\\nStep 5: Analyze Class Distribution\")\n",
        "    # Map labels to Attack (1) and Non-Attack (0)\n",
        "    df['Label'] = df[' Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
        "    class_counts = df['Label'].value_counts()\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(class_counts)\n",
        "\n",
        "    # Visualize class distribution\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.countplot(x='Label', data=df)\n",
        "    plt.title(\"Class Distribution (0: Non-Attack, 1: Attack)\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.savefig('class_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Detailed attack type distribution\n",
        "    attack_types = df[df['Label'] == 1][' Label'].value_counts()\n",
        "    print(\"\\nAttack types distribution:\")\n",
        "    print(attack_types)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    attack_types.plot(kind='bar')\n",
        "    plt.title(\"Distribution of Attack Types\")\n",
        "    plt.xlabel(\"Attack Type\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attack_types_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "analyze_class_distribution()\n",
        "\n",
        "# Step 6: Feature Correlation Analysis\n",
        "def correlation_analysis():\n",
        "    print(\"\\nStep 6: Feature Correlation Analysis\")\n",
        "    # Select numerical features\n",
        "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    corr_matrix = df[numerical_cols].corr()\n",
        "\n",
        "    # Visualize correlation matrix\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Numerical Features\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "correlation_analysis()\n",
        "\n",
        "# Step 7: Feature Distribution Analysis\n",
        "def feature_distribution():\n",
        "    print(\"\\nStep 7: Feature Distribution Analysis\")\n",
        "    # Select a few key features for visualization\n",
        "    key_features = [' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', ' Flow Bytes/s']\n",
        "    for feature in key_features:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(df[feature], bins=50, kde=True)\n",
        "        plt.title(f\"Distribution of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.savefig(f'distribution_{feature.replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "feature_distribution()\n",
        "\n",
        "# Step 8: Data Preprocessing\n",
        "def preprocess_data():\n",
        "    print(\"\\nStep 8: Data Preprocessing\")\n",
        "    global df\n",
        "    # Drop original label column\n",
        "    df = df.drop(' Label', axis=1)\n",
        "    # Normalize numerical features\n",
        "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    numerical_cols = [col for col in numerical_cols if col != 'Label']  # Exclude Label\n",
        "    scaler = StandardScaler()\n",
        "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "    # Convert features to text for Hugging Face model\n",
        "    def features_to_text(row):\n",
        "        text = \" \".join([f\"{col}:{row[col]}\" for col in numerical_cols])\n",
        "        return text\n",
        "\n",
        "    df['text'] = df.apply(features_to_text, axis=1)\n",
        "    print(\"\\nSample text representation:\")\n",
        "    print(df['text'].iloc[0])\n",
        "\n",
        "    # Save preprocessed dataset\n",
        "    df[['text', 'Label']].to_csv('preprocessed_cicids2017.csv', index=False)\n",
        "    print(\"\\nPreprocessed dataset saved as 'preprocessed_cicids2017.csv'\")\n",
        "\n",
        "preprocess_data()\n",
        "\n",
        "# Step 9: Summary of Preprocessed Data\n",
        "def summarize_preprocessed_data():\n",
        "    print(\"\\nStep 9: Summary of Preprocessed Data\")\n",
        "    print(\"\\nShape of preprocessed dataset:\")\n",
        "    print(df[['text', 'Label']].shape)\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df[['text', 'Label']].head())\n",
        "\n",
        "summarize_preprocessed_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "FUCzavz63imk",
        "outputId": "cca05880-c869-4751-d353-462cff3cd918"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cpu\n",
            "Loading and combining datasets...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/cicids2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-98b9e62e31d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and combining datasets...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_combine_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-98b9e62e31d0>\u001b[0m in \u001b[0;36mload_and_combine_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mdata_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/cicids2017/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'"
          ]
        }
      ]
    }
  ]
}