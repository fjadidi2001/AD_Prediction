{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# config.py\n"
      ],
      "metadata": {
        "id": "JvqOFqpMaVks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4363DPnhaNED"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configuration module for ADReSSo21 Speech Analysis\n",
        "Handles all paths, settings, and system configuration\n",
        "\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "import multiprocessing\n",
        "\n",
        "@dataclass\n",
        "class SystemConfig:\n",
        "    \"\"\"System resource configuration\"\"\"\n",
        "    n_cores: int = min(10, multiprocessing.cpu_count())  # Use available cores, max 10\n",
        "    max_workers: int = 8  # Leave some cores for system\n",
        "    chunk_size: int = 2  # Process files in chunks\n",
        "    memory_limit_gb: int = 30  # Leave 5GB for system from your 35GB\n",
        "\n",
        "@dataclass\n",
        "class PathConfig:\n",
        "    \"\"\"Path configuration for Windows 10\"\"\"\n",
        "    base_path: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\"\n",
        "    output_path: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\output\"\n",
        "\n",
        "    # Diagnosis paths\n",
        "    diagnosis_train_audio_ad: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\"\n",
        "    diagnosis_train_audio_cn: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\"\n",
        "    diagnosis_train_seg_ad: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\ad\"\n",
        "    diagnosis_train_seg_cn: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\cn\"\n",
        "\n",
        "    # Progression train paths\n",
        "    progression_train_audio_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\"\n",
        "    progression_train_audio_no_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\"\n",
        "    progression_train_seg_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\decline\"\n",
        "    progression_train_seg_no_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\no_decline\"\n",
        "\n",
        "    # Progression test paths\n",
        "    progression_test_audio: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\"\n",
        "    progression_test_seg: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\segmentation\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create output directory if it doesn't exist\"\"\"\n",
        "        Path(self.output_path).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"features\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"transcripts\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"models\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"logs\")).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model configuration\"\"\"\n",
        "    whisper_model_size: str = \"base\"  # base, small, medium, large\n",
        "    wav2vec_model: str = \"facebook/wav2vec2-base-960h\"\n",
        "    bert_model: str = \"bert-base-uncased\"\n",
        "    sampling_rate: int = 16000\n",
        "    max_sequence_length: int = 512\n",
        "\n",
        "@dataclass\n",
        "class FeatureConfig:\n",
        "    \"\"\"Feature extraction configuration\"\"\"\n",
        "    n_mfcc: int = 13\n",
        "    n_mels: int = 80\n",
        "    f0_min: float = 50.0\n",
        "    f0_max: float = 300.0\n",
        "    egemaps_feature_count: int = 88\n",
        "    wav2vec_feature_size: int = 768\n",
        "\n",
        "# Global configuration instances\n",
        "SYSTEM_CONFIG = SystemConfig()\n",
        "PATH_CONFIG = PathConfig()\n",
        "MODEL_CONFIG = ModelConfig()\n",
        "FEATURE_CONFIG = FeatureConfig()\n",
        "\n",
        "def get_audio_file_paths() -> Dict[str, List[str]]:\n",
        "    \"\"\"Get all audio file paths organized by category\"\"\"\n",
        "    audio_files = {\n",
        "        'diagnosis_ad': [],\n",
        "        'diagnosis_cn': [],\n",
        "        'progression_decline': [],\n",
        "        'progression_no_decline': [],\n",
        "        'progression_test': []\n",
        "    }\n",
        "\n",
        "    # Helper function to safely get files\n",
        "    def get_wav_files(path: str) -> List[str]:\n",
        "        if os.path.exists(path):\n",
        "            return [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        return []\n",
        "\n",
        "    # Get diagnosis files\n",
        "    audio_files['diagnosis_ad'] = get_wav_files(PATH_CONFIG.diagnosis_train_audio_ad)\n",
        "    audio_files['diagnosis_cn'] = get_wav_files(PATH_CONFIG.diagnosis_train_audio_cn)\n",
        "\n",
        "    # Get progression files\n",
        "    audio_files['progression_decline'] = get_wav_files(PATH_CONFIG.progression_train_audio_decline)\n",
        "    audio_files['progression_no_decline'] = get_wav_files(PATH_CONFIG.progression_train_audio_no_decline)\n",
        "    audio_files['progression_test'] = get_wav_files(PATH_CONFIG.progression_test_audio)\n",
        "\n",
        "    return audio_files\n",
        "\n",
        "def print_system_info():\n",
        "    \"\"\"Print system configuration info\"\"\"\n",
        "    print(\"=== System Configuration ===\")\n",
        "    print(f\"CPU Cores Available: {multiprocessing.cpu_count()}\")\n",
        "    print(f\"Using Cores: {SYSTEM_CONFIG.n_cores}\")\n",
        "    print(f\"Max Workers: {SYSTEM_CONFIG.max_workers}\")\n",
        "    print(f\"Memory Limit: {SYSTEM_CONFIG.memory_limit_gb}GB\")\n",
        "    print(f\"Output Path: {PATH_CONFIG.output_path}\")\n",
        "    print(f\"Whisper Model: {MODEL_CONFIG.whisper_model_size}\")\n",
        "    print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils.py - Utilities Module\n"
      ],
      "metadata": {
        "id": "Cs2VfF3SalFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utilities module for ADReSSo21 Speech Analysis\n",
        "Common utilities, logging, and helper functions\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import logging\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Any, List, Union\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "from config import PATH_CONFIG, SYSTEM_CONFIG\n",
        "\n",
        "def setup_logging(log_level: str = \"INFO\") -> logging.Logger:\n",
        "    \"\"\"Setup logging configuration\"\"\"\n",
        "    log_dir = os.path.join(PATH_CONFIG.output_path, \"logs\")\n",
        "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = os.path.join(log_dir, f\"adresso_analysis_{timestamp}.log\")\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=getattr(logging, log_level.upper()),\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file, encoding='utf-8'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger('ADReSSoAnalyzer')\n",
        "    logger.info(f\"Logging initialized. Log file: {log_file}\")\n",
        "    return logger\n",
        "\n",
        "def monitor_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Monitor current memory usage\"\"\"\n",
        "    memory = psutil.virtual_memory()\n",
        "    return {\n",
        "        'total_gb': memory.total / (1024**3),\n",
        "        'available_gb': memory.available / (1024**3),\n",
        "        'used_gb': memory.used / (1024**3),\n",
        "        'percent_used': memory.percent\n",
        "    }\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Force garbage collection and clear GPU memory if available\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def check_memory_limit(threshold_percent: float = 85.0) -> bool:\n",
        "    \"\"\"Check if memory usage is below threshold\"\"\"\n",
        "    memory_info = monitor_memory_usage()\n",
        "    return memory_info['percent_used'] < threshold_percent\n",
        "\n",
        "def safe_save_pickle(data: Any, filepath: str, logger: logging.Logger = None):\n",
        "    \"\"\"Safely save data as pickle with error handling\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        if logger:\n",
        "            logger.info(f\"Successfully saved pickle: {filepath}\")\n",
        "    except Exception as e:\n",
        "        if logger:\n",
        "            logger.error(f\"Failed to save pickle {filepath}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def safe_load_pickle(filepath: str, logger: logging.Logger = None) -> Any:\n",
        "    \"\"\"Safely load pickle with error handling\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        if logger:\n",
        "            logger.info(f\"Successfully loaded pickle: {filepath}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        if logger:\n",
        "            logger.error(f\"Failed to load pickle {filepath}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def safe_save_json(data: Dict, filepath: str, logger: logging.Logger = None):\n",
        "    \"\"\"Safely save data as JSON with error handling\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False, default=str)\n",
        "        if logger:\n",
        "            logger.info(f\"Successfully saved JSON: {filepath}\")\n",
        "    except Exception as e:\n",
        "        if logger:\n",
        "            logger.error(f\"Failed to save JSON {filepath}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def validate_audio_file(filepath: str) -> bool:\n",
        "    \"\"\"Validate if audio file exists and is readable\"\"\"\n",
        "    return os.path.exists(filepath) and os.path.getsize(filepath) > 0\n",
        "\n",
        "def get_file_info(filepath: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get basic file information\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        return {'exists': False}\n",
        "\n",
        "    stat = os.stat(filepath)\n",
        "    return {\n",
        "        'exists': True,\n",
        "        'size_mb': stat.st_size / (1024**2),\n",
        "        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),\n",
        "        'filename': os.path.basename(filepath),\n",
        "        'extension': os.path.splitext(filepath)[1]\n",
        "    }\n",
        "\n",
        "def create_progress_bar(total: int, desc: str = \"Processing\") -> Any:\n",
        "    \"\"\"Create a progress bar for batch processing\"\"\"\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        return tqdm(total=total, desc=desc, unit=\"files\")\n",
        "    except ImportError:\n",
        "        # Fallback simple counter if tqdm not available\n",
        "        class SimpleProgress:\n",
        "            def __init__(self, total, desc):\n",
        "                self.total = total\n",
        "                self.current = 0\n",
        "                self.desc = desc\n",
        "\n",
        "            def update(self, n=1):\n",
        "                self.current += n\n",
        "                print(f\"\\r{self.desc}: {self.current}/{self.total}\", end=\"\")\n",
        "\n",
        "            def close(self):\n",
        "                print()  # New line\n",
        "\n",
        "        return SimpleProgress(total, desc)\n",
        "\n",
        "def batch_generator(items: List[Any], batch_size: int):\n",
        "    \"\"\"Generate batches from a list of items\"\"\"\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        yield items[i:i + batch_size]\n",
        "\n",
        "def flatten_dict(d: Dict, parent_key: str = '', sep: str = '_') -> Dict:\n",
        "    \"\"\"Flatten nested dictionary\"\"\"\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        elif isinstance(v, np.ndarray):\n",
        "            # Convert numpy arrays to lists for JSON serialization\n",
        "            items.append((new_key, v.tolist() if v.ndim > 0 else float(v)))\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "def create_summary_dataframe(results: Dict[str, Any], save_path: str = None) -> pd.DataFrame:\n",
        "    \"\"\"Create a summary DataFrame from results dictionary\"\"\"\n",
        "    data = []\n",
        "\n",
        "    for key, result in results.items():\n",
        "        if isinstance(result, dict):\n",
        "            # Flatten the result dictionary\n",
        "            flat_result = flatten_dict(result)\n",
        "            flat_result['file_id'] = key\n",
        "            data.append(flat_result)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    if save_path:\n",
        "        df.to_csv(save_path, index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "def log_processing_stats(processed: int, failed: int, total: int, logger: logging.Logger):\n",
        "    \"\"\"Log processing statistics\"\"\"\n",
        "    success_rate = (processed / total * 100) if total > 0 else 0\n",
        "    logger.info(f\"Processing completed: {processed}/{total} successful ({success_rate:.1f}%)\")\n",
        "    if failed > 0:\n",
        "        logger.warning(f\"Failed files: {failed}\")\n",
        "\n",
        "class ProcessingTimer:\n",
        "    \"\"\"Context manager for timing operations\"\"\"\n",
        "\n",
        "    def __init__(self, operation_name: str, logger: logging.Logger = None):\n",
        "        self.operation_name = operation_name\n",
        "        self.logger = logger\n",
        "        self.start_time = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start_time = datetime.now()\n",
        "        if self.logger:\n",
        "            self.logger.info(f\"Starting {self.operation_name}...\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - self.start_time\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.info(f\"Completed {self.operation_name} in {duration}\")\n",
        "\n",
        "        if exc_type:\n",
        "            if self.logger:\n",
        "                self.logger.error(f\"Error in {self.operation_name}: {exc_val}\")\n",
        "\n",
        "def ensure_directory_exists(directory_path: str):\n",
        "    \"\"\"Ensure directory exists, create if not\"\"\"\n",
        "    Path(directory_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_available_models() -> Dict[str, bool]:\n",
        "    \"\"\"Check which models are available/working\"\"\"\n",
        "    models_status = {\n",
        "        'whisper': False,\n",
        "        'wav2vec2': False,\n",
        "        'bert': False,\n",
        "        'opensmile': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        import whisper\n",
        "        models_status['whisper'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "        models_status['wav2vec2'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        from transformers import BertTokenizer, BertModel\n",
        "        models_status['bert'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import opensmile\n",
        "        models_status['opensmile'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    return models_status"
      ],
      "metadata": {
        "id": "2UlUQUdjaiC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# acoustic_features_service.py - Acoustic Features Extraction Service\n"
      ],
      "metadata": {
        "id": "P6QiFmxSbnzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Acoustic Features Service - Microservice for extracting acoustic features\n",
        "Handles eGeMAPS, MFCC, Log-mel, Wav2Vec2, and prosodic features\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import warnings\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Model imports with error handling\n",
        "try:\n",
        "    import opensmile\n",
        "    OPENSMILE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPENSMILE_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "    WAV2VEC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WAV2VEC_AVAILABLE = False\n",
        "\n",
        "from config import MODEL_CONFIG, FEATURE_CONFIG, SYSTEM_CONFIG\n",
        "from utils import setup_logging, monitor_memory_usage, cleanup_memory, safe_save_pickle\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "@dataclass\n",
        "class AcousticFeatures:\n",
        "    \"\"\"Data class to hold acoustic features\"\"\"\n",
        "    egemaps: np.ndarray\n",
        "    mfccs: Dict[str, np.ndarray]\n",
        "    log_mel: Dict[str, np.ndarray]\n",
        "    wav2vec2: np.ndarray\n",
        "    prosodic: Dict[str, float]\n",
        "    extraction_success: Dict[str, bool]\n",
        "\n",
        "class AcousticFeaturesService:\n",
        "    \"\"\"Service for extracting acoustic features from audio files\"\"\"\n",
        "\n",
        "    def __init__(self, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or setup_logging()\n",
        "        self.smile = None\n",
        "        self.wav2vec_processor = None\n",
        "        self.wav2vec_model = None\n",
        "\n",
        "        self._initialize_models()\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        \"\"\"Initialize feature extraction models\"\"\"\n",
        "        self.logger.info(\"Initializing acoustic feature extraction models...\")\n",
        "\n",
        "        # Initialize OpenSMILE for eGeMAPS\n",
        "        if OPENSMILE_AVAILABLE:\n",
        "            try:\n",
        "                self.smile = opensmile.Smile(\n",
        "                    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "                    feature_level=opensmile.FeatureLevel.Functionals,\n",
        "                )\n",
        "                self.logger.info(\"✓ OpenSMILE (eGeMAPS) initialized\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to initialize OpenSMILE: {e}\")\n",
        "                self.smile = None\n",
        "        else:\n",
        "            self.logger.warning(\"OpenSMILE not available - eGeMAPS features will be skipped\")\n",
        "\n",
        "        # Initialize Wav2Vec2\n",
        "        if WAV2VEC_AVAILABLE:\n",
        "            try:\n",
        "                self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_CONFIG.wav2vec_model)\n",
        "                self.wav2vec_model = Wav2Vec2Model.from_pretrained(MODEL_CONFIG.wav2vec_model)\n",
        "                self.logger.info(\"✓ Wav2Vec2 initialized\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to initialize Wav2Vec2: {e}\")\n",
        "                self.wav2vec_processor = None\n",
        "                self.wav2vec_model = None\n",
        "        else:\n",
        "            self.logger.warning(\"Transformers not available - Wav2Vec2 features will be skipped\")\n",
        "\n",
        "    def extract_egemaps_features(self, audio_path: str) -> Tuple[np.ndarray, bool]:\n",
        "        \"\"\"Extract eGeMAPS features using OpenSMILE\"\"\"\n",
        "        try:\n",
        "            if self.smile is None:\n",
        "                return np.zeros(FEATURE_CONFIG.egemaps_feature_count), False\n",
        "\n",
        "            features = self.smile.process_file(audio_path).values.flatten()\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"eGeMAPS extraction failed for {os.path.basename(audio_path)}: {e}\")\n",
        "            return np.zeros(FEATURE_CONFIG.egemaps_feature_count), False\n",
        "\n",
        "    def extract_mfcc_features(self, y: np.ndarray, sr: int) -> Tuple[Dict[str, np.ndarray], bool]:\n",
        "        \"\"\"Extract MFCC features and their derivatives\"\"\"\n",
        "        try:\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=FEATURE_CONFIG.n_mfcc)\n",
        "\n",
        "            features = {\n",
        "                'mean': np.mean(mfccs, axis=1),\n",
        "                'std': np.std(mfccs, axis=1),\n",
        "                'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "            }\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"MFCC extraction failed: {e}\")\n",
        "            default_features = {\n",
        "                'mean': np.zeros(FEATURE_CONFIG.n_mfcc),\n",
        "                'std': np.zeros(FEATURE_CONFIG.n_mfcc),\n",
        "                'delta': np.zeros(FEATURE_CONFIG.n_mfcc),\n",
        "                'delta2': np.zeros(FEATURE_CONFIG.n_mfcc)\n",
        "            }\n",
        "            return default_features, False\n",
        "\n",
        "    def extract_logmel_features(self, y: np.ndarray, sr: int) -> Tuple[Dict[str, np.ndarray], bool]:\n",
        "        \"\"\"Extract log-mel spectrogram features\"\"\"\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=FEATURE_CONFIG.n_mels)\n",
        "            log_mel = librosa.power_to_db(mel_spec)\n",
        "\n",
        "            features = {\n",
        "                'mean': np.mean(log_mel, axis=1),\n",
        "                'std': np.std(log_mel, axis=1)\n",
        "            }\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Log-mel extraction failed: {e}\")\n",
        "            default_features = {\n",
        "                'mean': np.zeros(FEATURE_CONFIG.n_mels),\n",
        "                'std': np.zeros(FEATURE_CONFIG.n_mels)\n",
        "            }\n",
        "            return default_features, False\n",
        "\n",
        "    def extract_wav2vec_features(self, y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n",
        "        \"\"\"Extract Wav2Vec2 features\"\"\"\n",
        "        try:\n",
        "            if self.wav2vec_processor is None or self.wav2vec_model is None:\n",
        "                return np.zeros(FEATURE_CONFIG.wav2vec_feature_size), False\n",
        "\n",
        "            if len(y) == 0:\n",
        "                raise ValueError(\"Empty audio signal\")\n",
        "\n",
        "            # Ensure correct sampling rate\n",
        "            if sr != MODEL_CONFIG.sampling_rate:\n",
        "                y = librosa.resample(y, orig_sr=sr, target_sr=MODEL_CONFIG.sampling_rate)\n",
        "                sr = MODEL_CONFIG.sampling_rate\n",
        "\n",
        "            input_values = self.wav2vec_processor(\n",
        "                y,\n",
        "                sampling_rate=sr,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_values\n",
        "\n",
        "            with torch.no_grad():\n",
        "                wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Wav2Vec2 extraction failed: {e}\")\n",
        "            return np.zeros(FEATURE_CONFIG.wav2vec_feature_size), False\n",
        "\n",
        "    def extract_prosodic_features(self, y: np.ndarray, sr: int) -> Tuple[Dict[str, float], bool]:\n",
        "        \"\"\"Extract prosodic features\"\"\"\n",
        "        try:\n",
        "            # F0 extraction\n",
        "            f0 = librosa.yin(y, fmin=FEATURE_CONFIG.f0_min, fmax=FEATURE_CONFIG.f0_max, sr=sr)\n",
        "            f0_clean = f0[f0 > 0]  # Remove unvoiced frames\n",
        "\n",
        "            # Energy features\n",
        "            rms = librosa.feature.rms(y=y)\n",
        "\n",
        "            # Spectral features\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "            zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
        "\n",
        "            features = {\n",
        "                'f0_mean': float(np.mean(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_std': float(np.std(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_median': float(np.median(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_range': float(np.max(f0_clean) - np.min(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'energy_mean': float(np.mean(rms)),\n",
        "                'energy_std': float(np.std(rms)),\n",
        "                'zero_crossing_rate': float(np.mean(zero_crossing_rate)),\n",
        "                'spectral_centroid': float(np.mean(spectral_centroid)),\n",
        "                'spectral_rolloff': float(np.mean(spectral_rolloff)),\n",
        "                'duration': len(y) / sr,\n",
        "                'voicing_fraction': len(f0_clean) / len(f0) if len(f0) > 0 else 0.0\n",
        "            }\n",
        "\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Prosodic feature extraction failed: {e}\")\n",
        "            default_features = {\n",
        "                'f0_mean': 0.0, 'f0_std': 0.0, 'f0_median': 0.0, 'f0_range': 0.0,\n",
        "                'energy_mean': 0.0, 'energy_std': 0.0, 'zero_crossing_rate': 0.0,\n",
        "                'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                'duration': 0.0, 'voicing_fraction': 0.0\n",
        "            }\n",
        "            return default_features, False\n",
        "\n",
        "    def extract_features_from_file(self, audio_path: str) -> Optional[AcousticFeatures]:\n",
        "        \"\"\"Extract all acoustic features from a single audio file\"\"\"\n",
        "        try:\n",
        "            # Load audio file\n",
        "            y, sr = librosa.load(audio_path, sr=MODEL_CONFIG.sampling_rate)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                self.logger.warning(f\"Empty audio file: {audio_path}\")\n",
        "                return None\n",
        "\n",
        "            # Extract all features\n",
        "            egemaps, egemaps_success = self.extract_egemaps_features(audio_path)\n",
        "            mfccs, mfccs_success = self.extract_mfcc_features(y, sr)\n",
        "            log_mel, logmel_success = self.extract_logmel_features(y, sr)\n",
        "            wav2vec2, wav2vec_success = self.extract_wav2vec_features(y, sr)\n",
        "            prosodic, prosodic_success = self.extract_prosodic_features(y, sr)\n",
        "\n",
        "            features = AcousticFeatures(\n",
        "                egemaps=egemaps,\n",
        "                mfccs=mfccs,\n",
        "                log_mel=log_mel,\n",
        "                wav2vec2=wav2vec2,\n",
        "                prosodic=prosodic,\n",
        "                extraction_success={\n",
        "                    'egemaps': egemaps_success,\n",
        "                    'mfccs': mfccs_success,\n",
        "                    'log_mel': logmel_success,\n",
        "                    'wav2vec2': wav2vec_success,\n",
        "                    'prosodic': prosodic_success\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to extract features from {audio_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_files_batch(self, file_paths: List[str]) -> Dict[str, Optional[AcousticFeatures]]:\n",
        "        \"\"\"Process a batch of audio files\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            filename = os.path.basename(file_path)\n",
        "            try:\n",
        "                features = self.extract_features_from_file(file_path)\n",
        "                results[filename] = features\n",
        "\n",
        "                # Memory cleanup for large batches\n",
        "                if len(results) % 10 == 0:\n",
        "                    cleanup_memory()\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error processing {filename}: {e}\")\n",
        "                results[filename] = None\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_features_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Dict[str, Optional[AcousticFeatures]]]:\n",
        "        \"\"\"Extract features from all audio files using parallel processing\"\"\"\n",
        "        self.logger.info(\"Starting parallel acoustic feature extraction...\")\n",
        "\n",
        "        all_results = {}\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed_files = 0\n",
        "\n",
        "        for category, file_paths in audio_files.items():\n",
        "            if not file_paths:\n",
        "                continue\n",
        "\n",
        "            self.logger.info(f\"Processing {category}: {len(file_paths)} files\")\n",
        "            category_results = {}\n",
        "\n",
        "            # Process files in batches to manage memory\n",
        "            batch_size = SYSTEM_CONFIG.chunk_size\n",
        "            batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
        "\n",
        "            with ProcessPoolExecutor(max_workers=SYSTEM_CONFIG.max_workers) as executor:\n",
        "                # Submit batch jobs\n",
        "                future_to_batch = {\n",
        "                    executor.submit(process_audio_batch_worker, batch): batch\n",
        "                    for batch in batches\n",
        "                }\n",
        "\n",
        "                # Collect results\n",
        "                for future in as_completed(future_to_batch):\n",
        "                    batch = future_to_batch[future]\n",
        "                    try:\n",
        "                        batch_results = future.result()\n",
        "                        category_results.update(batch_results)\n",
        "                        processed_files += len(batch)\n",
        "\n",
        "                        # Log progress\n",
        "                        progress = (processed_files / total_files) * 100\n",
        "                        self.logger.info(f\"Progress: {processed_files}/{total_files} ({progress:.1f}%)\")\n",
        "\n",
        "                        # Check memory usage\n",
        "                        memory_info = monitor_memory_usage()\n",
        "                        if memory_info['percent_used'] > 80:\n",
        "                            self.logger.warning(f\"High memory usage: {memory_info['percent_used']:.1f}%\")\n",
        "                            cleanup_memory()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"Batch processing failed: {e}\")\n",
        "\n",
        "            all_results[category] = category_results\n",
        "            self.logger.info(f\"Completed {category}: {len(category_results)} files processed\")\n",
        "\n",
        "        # Final cleanup\n",
        "        cleanup_memory()\n",
        "        self.logger.info(f\"Acoustic feature extraction completed: {processed_files}/{total_files} files\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def save_features(self, features: Dict[str, Dict[str, Optional[AcousticFeatures]]], output_dir: str):\n",
        "        \"\"\"Save extracted features to disk\"\"\"\n",
        "        self.logger.info(\"Saving acoustic features...\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save features by category\n",
        "        for category, category_features in features.items():\n",
        "            category_path = os.path.join(output_dir, f\"acoustic_features_{category}.pkl\")\n",
        "\n",
        "            # Convert AcousticFeatures objects to dictionaries for serialization\n",
        "            serializable_features = {}\n",
        "            for filename, feature_obj in category_features.items():\n",
        "                if feature_obj is not None:\n",
        "                    serializable_features[filename] = {\n",
        "                        'egemaps': feature_obj.egemaps,\n",
        "                        'mfccs': feature_obj.mfccs,\n",
        "                        'log_mel': feature_obj.log_mel,\n",
        "                        'wav2vec2': feature_obj.wav2vec2,\n",
        "                        'prosodic': feature_obj.prosodic,\n",
        "                        'extraction_success': feature_obj.extraction_success\n",
        "                    }\n",
        "                else:\n",
        "                    serializable_features[filename] = None\n",
        "\n",
        "            safe_save_pickle(serializable_features, category_path, self.logger)\n",
        "\n",
        "        # Save combined features\n",
        "        combined_path = os.path.join(output_dir, \"acoustic_features_all.pkl\")\n",
        "        safe_save_pickle(features, combined_path, self.logger)\n",
        "\n",
        "        self.logger.info(f\"Acoustic features saved to {output_dir}\")\n",
        "\n",
        "def process_audio_batch_worker(file_paths: List[str]) -> Dict[str, Optional[AcousticFeatures]]:\n",
        "    \"\"\"Worker function for parallel processing of audio batches\"\"\"\n",
        "    # Create a new service instance for each worker to avoid sharing model states\n",
        "    service = AcousticFeaturesService()\n",
        "    return service.process_files_batch(file_paths)\n",
        "\n",
        "def demonstrate_acoustic_features(audio_file_path: str, logger: Optional[logging.Logger] = None):\n",
        "    \"\"\"Demonstrate acoustic feature extraction on a single file\"\"\"\n",
        "    if logger is None:\n",
        "        logger = setup_logging()\n",
        "\n",
        "    service = AcousticFeaturesService(logger)\n",
        "\n",
        "    logger.info(f\"Demonstrating acoustic features for: {os.path.basename(audio_file_path)}\")\n",
        "\n",
        "    features = service.extract_features_from_file(audio_file_path)\n",
        "\n",
        "    if features is None:\n",
        "        logger.error(\"Failed to extract features\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n=== Acoustic Features for {os.path.basename(audio_file_path)} ===\\n\")\n",
        "\n",
        "    # eGeMAPS\n",
        "    print(f\"1. eGeMAPS Features: {len(features.egemaps)} features\")\n",
        "    print(f\"   Success: {features.extraction_success['egemaps']}\")\n",
        "    print(f\"   Shape: {features.egemaps.shape}\")\n",
        "    print(f\"   Sample values: {features.egemaps[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # MFCCs\n",
        "    print(\"2. MFCC Features:\")\n",
        "    print(f\"   Success: {features.extraction_success['mfccs']}\")\n",
        "    for key, values in features.mfccs.items():\n",
        "        print(f\"   {key}: {values.shape} - {values[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # Log-mel\n",
        "    print(\"3. Log-Mel Spectrogram Features:\")\n",
        "    print(f\"   Success: {features.extraction_success['log_mel']}\")\n",
        "    for key, values in features.log_mel.items():\n",
        "        print(f\"   {key}: {values.shape} - {values[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # Wav2Vec2\n",
        "    print(f\"4. Wav2Vec2 Features: {features.wav2vec2.shape}\")\n",
        "    print(f\"   Success: {features.extraction_success['wav2vec2']}\")\n",
        "    print(f\"   Sample values: {features.wav2vec2[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # Prosodic\n",
        "    print(\"5. Prosodic Features:\")\n",
        "    print(f\"   Success: {features.extraction_success['prosodic']}\")\n",
        "    for key, value in features.prosodic.items():\n",
        "        print(f\"   {key}: {value:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # Success summary\n",
        "    successful_features = sum(features.extraction_success.values())\n",
        "    total_features = len(features.extraction_success)\n",
        "    print(f\"Feature extraction success: {successful_features}/{total_features}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the service with a single file\n",
        "    from config import get_audio_file_paths\n",
        "\n",
        "    logger = setup_logging()\n",
        "    audio_files = get_audio_file_paths()\n",
        "\n",
        "    # Find first available audio file for demonstration\n",
        "    test_file = None\n",
        "    for category, files in audio_files.items():\n",
        "        if files:\n",
        "            test_file = files[0]\n",
        "            break\n",
        "\n",
        "    if test_file:\n",
        "        demonstrate_acoustic_features(test_file, logger)\n",
        "    else:\n",
        "        logger.error(\"No audio files found for demonstration\")"
      ],
      "metadata": {
        "id": "v1NJ4TsIcDeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transcription_service.py - Speech Transcription Service\n"
      ],
      "metadata": {
        "id": "LIb-e2Zwbvln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transcription Service - Microservice for speech-to-text transcription using Whisper\n",
        "Handles parallel transcription with memory management\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Any\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "# Whisper import with error handling\n",
        "try:\n",
        "    import whisper\n",
        "    WHISPER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WHISPER_AVAILABLE = False\n",
        "\n",
        "from config import MODEL_CONFIG, SYSTEM_CONFIG, PATH_CONFIG\n",
        "from utils import (setup_logging, monitor_memory_usage, cleanup_memory,\n",
        "                   safe_save_json, safe_save_pickle, ProcessingTimer,\n",
        "                   create_progress_bar, validate_audio_file)\n",
        "\n",
        "@dataclass\n",
        "class TranscriptionResult:\n",
        "    \"\"\"Data class for transcription results\"\"\"\n",
        "    file_path: str\n",
        "    category: str\n",
        "    filename: str\n",
        "    transcript: str\n",
        "    language: str\n",
        "    segments: int\n",
        "    duration: float\n",
        "    confidence: float\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "class TranscriptionService:\n",
        "    \"\"\"Service for transcribing audio files using Whisper\"\"\"\n",
        "\n",
        "    def __init__(self, model_size: str = None, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or setup_logging()\n",
        "        self.model_size = model_size or MODEL_CONFIG.whisper_model_size\n",
        "        self.whisper_model = None\n",
        "\n",
        "        self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize Whisper model\"\"\"\n",
        "        if not WHISPER_AVAILABLE:\n",
        "            self.logger.error(\"Whisper not available. Please install: pip install openai-whisper\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Loading Whisper model: {self.model_size}\")\n",
        "            self.whisper_model = whisper.load_model(self.model_size)\n",
        "            self.logger.info(\"✓ Whisper model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load Whisper model: {e}\")\n",
        "            self.whisper_model = None\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path: str, category: str = \"\") -> TranscriptionResult:\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        filename = os.path.basename(audio_path)\n",
        "\n",
        "        # Validate file\n",
        "        if not validate_audio_file(audio_path):\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=\"Invalid or missing audio file\"\n",
        "            )\n",
        "\n",
        "        if self.whisper_model is None:\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=\"Whisper model not available\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            self.logger.debug(f\"Transcribing {filename}...\")\n",
        "\n",
        "            # Transcribe with Whisper\n",
        "            result = self.whisper_model.transcribe(\n",
        "                audio_path,\n",
        "                fp16=False,  # Use fp32 for better compatibility\n",
        "                language=None,  # Auto-detect language\n",
        "                task=\"transcribe\"\n",
        "            )\n",
        "\n",
        "            transcript_text = result[\"text\"].strip()\n",
        "            segments = result.get(\"segments\", [])\n",
        "\n",
        "            # Calculate average confidence if available\n",
        "            confidence = 0.0\n",
        "            if segments:\n",
        "                confidences = [seg.get(\"avg_logprob\", 0) for seg in segments if \"avg_logprob\" in seg]\n",
        "                if confidences:\n",
        "                    # Convert log probabilities to rough confidence scores\n",
        "                    confidence = float(np.mean([np.exp(c) for c in confidences]))\n",
        "\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=transcript_text,\n",
        "                language=result.get(\"language\", \"unknown\"),\n",
        "                segments=len(segments),\n",
        "                duration=result.get(\"duration\", 0.0),\n",
        "                confidence=confidence,\n",
        "                success=True\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Transcription failed for {filename}: {e}\")\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=str(e)\n",
        "            )\n",
        "\n",
        "    def transcribe_files_batch(self, file_paths: List[str], category: str = \"\") -> Dict[str, TranscriptionResult]:\n",
        "        \"\"\"Transcribe a batch of audio files\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            filename = os.path.basename(file_path)\n",
        "\n",
        "            try:\n",
        "                result = self.transcribe_audio_file(file_path, category)\n",
        "                results[filename] = result\n",
        "\n",
        "                # Memory management\n",
        "                if len(results) % 5 == 0:  # More frequent cleanup for transcription\n",
        "                    cleanup_memory()\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error processing {filename}: {e}\")\n",
        "                results[filename] = TranscriptionResult(\n",
        "                    file_path=file_path,\n",
        "                    category=category,\n",
        "                    filename=filename,\n",
        "                    transcript=\"\",\n",
        "                    language=\"\",\n",
        "                    segments=0,\n",
        "                    duration=0.0,\n",
        "                    confidence=0.0,\n",
        "                    success=False,\n",
        "                    error_message=str(e)\n",
        "                )\n",
        "\n",
        "        return results\n",
        "\n",
        "    def transcribe_all_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Dict[str, TranscriptionResult]]:\n",
        "        \"\"\"Transcribe all audio files using parallel processing\"\"\"\n",
        "        self.logger.info(\"Starting parallel transcription...\")\n",
        "\n",
        "        all_results = {}\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed_files = 0\n",
        "\n",
        "        with ProcessingTimer(\"Complete transcription process\", self.logger):\n",
        "            for category, file_paths in audio_files.items():\n",
        "                if not file_paths:\n",
        "                    continue\n",
        "\n",
        "                self.logger.info(f\"Transcribing {category}: {len(file_paths)} files\")\n",
        "                category_results = {}\n",
        "\n",
        "                # Use smaller batches for transcription to manage memory better\n",
        "                batch_size = max(1, SYSTEM_CONFIG.chunk_size // 2)  # Smaller batches\n",
        "                batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
        "\n",
        "                # Use fewer workers for transcription as it's memory intensive\n",
        "                max_workers = min(SYSTEM_CONFIG.max_workers // 2, 4)\n",
        "\n",
        "                with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "                    # Submit batch jobs\n",
        "                    future_to_batch = {\n",
        "                        executor.submit(transcribe_batch_worker, batch, category, self.model_size): batch\n",
        "                        for batch in batches\n",
        "                    }\n",
        "\n",
        "                    # Progress tracking\n",
        "                    progress_bar = create_progress_bar(len(batches), f\"Transcribing {category}\")\n",
        "\n",
        "                    # Collect results\n",
        "                    for future in as_completed(future_to_batch):\n",
        "                        batch = future_to_batch[future]\n",
        "                        try:\n",
        "                            batch_results = future.result()\n",
        "                            category_results.update(batch_results)\n",
        "                            processed_files += len(batch)\n",
        "\n",
        "                            progress_bar.update(1)\n",
        "\n",
        "                            # Memory monitoring\n",
        "                            memory_info = monitor_memory_usage()\n",
        "                            if memory_info['percent_used'] > 75:\n",
        "                                self.logger.warning(f\"High memory usage: {memory_info['percent_used']:.1f}%\")\n",
        "                                cleanup_memory()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            self.logger.error(f\"Batch transcription failed: {e}\")\n",
        "\n",
        "                    progress_bar.close()\n",
        "\n",
        "                all_results[category] = category_results\n",
        "\n",
        "                # Log category completion stats\n",
        "                successful = sum(1 for result in category_results.values() if result.success)\n",
        "                self.logger.info(f\"Completed {category}: {successful}/{len(category_results)} successful\")\n",
        "\n",
        "        # Final cleanup\n",
        "        cleanup_memory()\n",
        "\n",
        "        # Log overall stats\n",
        "        total_successful = sum(\n",
        "            sum(1 for result in category_results.values() if result.success)\n",
        "            for category_results in all_results.values()\n",
        "        )\n",
        "        self.logger.info(f\"Transcription completed: {total_successful}/{processed_files} successful\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def save_transcriptions(self, transcriptions: Dict[str, Dict[str, TranscriptionResult]], output_dir: str):\n",
        "        \"\"\"Save transcriptions in multiple formats\"\"\"\n",
        "        self.logger.info(\"Saving transcriptions...\")\n",
        "\n",
        "        # Create output directories\n",
        "        transcripts_dir = os.path.join(output_dir, \"transcripts\")\n",
        "        os.makedirs(transcripts_dir, exist_ok=True)\n",
        "\n",
        "        all_transcripts = {}\n",
        "        transcript_summary = []\n",
        "\n",
        "        for category, category_results in transcriptions.items():\n",
        "            # Save individual category files\n",
        "            category_transcripts = {}\n",
        "\n",
        "            for filename, result in category_results.items():\n",
        "                # Convert to dictionary for JSON serialization\n",
        "                transcript_dict = {\n",
        "                    'file_path': result.file_path,\n",
        "                    'category': result.category,\n",
        "                    'filename': result.filename,\n",
        "                    'transcript': result.transcript,\n",
        "                    'language': result.language,\n",
        "                    'segments': result.segments,\n",
        "                    'duration': result.duration,\n",
        "                    'confidence': result.confidence,\n",
        "                    'success': result.success,\n",
        "                    'error_message': result.error_message\n",
        "                }\n",
        "\n",
        "                category_transcripts[filename] = transcript_dict\n",
        "                all_transcripts[f\"{category}_{filename}\"] = transcript_dict\n",
        "\n",
        "                # Add to summary\n",
        "                transcript_summary.append({\n",
        "                    'File_ID': f\"{category}_{filename}\",\n",
        "                    'Category': result.category,\n",
        "                    'Filename': result.filename,\n",
        "                    'Success': result.success,\n",
        "                    'Language': result.language,\n",
        "                    'Duration': result.duration,\n",
        "                    'Transcript_Length': len(result.transcript),\n",
        "                    'Word_Count': len(result.transcript.split()) if result.transcript else 0,\n",
        "                    'Segments': result.segments,\n",
        "                    'Confidence': result.confidence,\n",
        "                    'Error': result.error_message if result.error_message else \"\",\n",
        "                    'Transcript_Preview': (result.transcript[:100] + \"...\") if len(result.transcript) > 100 else result.transcript\n",
        "                })\n",
        "\n",
        "                # Save individual transcript file\n",
        "                if result.success and result.transcript:\n",
        "                    transcript_file = os.path.join(transcripts_dir, f\"{category}_{filename}_transcript.txt\")\n",
        "                    try:\n",
        "                        with open(transcript_file, 'w', encoding='utf-8') as f:\n",
        "                            f.write(result.transcript)\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Failed to save individual transcript {transcript_file}: {e}\")\n",
        "\n",
        "            # Save category JSON\n",
        "            category_json_path = os.path.join(transcripts_dir, f\"transcripts_{category}.json\")\n",
        "            safe_save_json(category_transcripts, category_json_path, self.logger)\n",
        "\n",
        "        # Save consolidated files\n",
        "        all_transcripts_path = os.path.join(transcripts_dir, \"all_transcripts.json\")\n",
        "        safe_save_json(all_transcripts, all_transcripts_path, self.logger)\n",
        "\n",
        "        # Save as pickle\n",
        "        transcripts_pickle_path = os.path.join(transcripts_dir, \"transcripts.pkl\")\n",
        "        safe_save_pickle(all_transcripts, transcripts_pickle_path, self.logger)\n",
        "\n",
        "        # Save summary CSV\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            summary_df = pd.DataFrame(transcript_summary)\n",
        "            summary_csv_path = os.path.join(output_dir, \"transcript_summary.csv\")\n",
        "            summary_df.to_csv(summary_csv_path, index=False)\n",
        "            self.logger.info(f\"Transcript summary saved: {summary_csv_path}\")\n",
        "        except ImportError:\n",
        "            self.logger.warning(\"Pandas not available, skipping CSV summary\")\n",
        "\n",
        "        self.logger.info(f\"All transcriptions saved to {output_dir}\")\n",
        "        return all_transcripts\n",
        "\n",
        "def transcribe_batch_worker(file_paths: List[str], category: str, model_size: str) -> Dict[str, TranscriptionResult]:\n",
        "    \"\"\"Worker function for parallel transcription\"\"\"\n",
        "    # Create new service instance for each worker\n",
        "    service = TranscriptionService(model_size=model_size)\n",
        "    return service.transcribe_files_batch(file_paths, category)\n",
        "\n",
        "def demonstrate_transcription(audio_file_path: str, logger: Optional[logging.Logger] = None):\n",
        "    \"\"\"Demonstrate transcription on a single file\"\"\"\n",
        "    if logger is None:\n",
        "        logger = setup_logging()\n",
        "\n",
        "    service = TranscriptionService(logger=logger)\n",
        "\n",
        "    logger.info(f\"Demonstrating transcription for: {os.path.basename(audio_file_path)}\")\n",
        "\n",
        "    result = service.transcribe_audio_file(audio_file_path, \"demo\")\n",
        "\n",
        "    print(f\"\\n=== Transcription Result for {result.filename} ===\")\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Language: {result.language}\")\n",
        "    print(f\"Duration: {result.duration:.2f} seconds\")\n",
        "    print(f\"Segments: {result.segments}\")\n",
        "    print(f\"Confidence: {result.confidence:.3f}\")\n",
        "\n",
        "    if result.success:\n",
        "        print(f\"Transcript ({len(result.transcript)} chars, {len(result.transcript.split())} words):\")\n",
        "        print(f'\"{result.transcript}\"')\n",
        "    else:\n",
        "        print(f\"Error: {result.error_message}\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the service\n",
        "    from config import get_audio_file_paths\n",
        "\n",
        "    logger = setup_logging()\n",
        "    audio_files = get_audio_file_paths()\n",
        "\n",
        "    # Find first available file for demonstration\n",
        "    test_file = None\n",
        "    for category, files in audio_files.items():\n",
        "        if files:\n",
        "            test_file = files[0]\n",
        "            break\n",
        "\n",
        "    if test_file:\n",
        "        demonstrate_transcription(test_file, logger)\n",
        "    else:\n",
        "        logger.error(\"No audio files found for demonstration\")"
      ],
      "metadata": {
        "id": "ODXCIYLbcF5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# linguistic_features_service.py - Linguistic Features Service\n"
      ],
      "metadata": {
        "id": "AlNQFq6Abztw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Linguistic Features Service - Microservice for extracting linguistic features and BERT embeddings\n",
        "Handles text analysis and BERT preprocessing\n",
        "\"\"\"\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# BERT imports with error handling\n",
        "try:\n",
        "    from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
        "    BERT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    BERT_AVAILABLE = False\n",
        "\n",
        "from config import MODEL_CONFIG, SYSTEM_CONFIG\n",
        "from utils import (setup_logging, monitor_memory_usage, cleanup_memory,\n",
        "                   safe_save_pickle, ProcessingTimer, batch_generator)\n",
        "from transcription_service import TranscriptionResult\n",
        "\n",
        "@dataclass\n",
        "class LinguisticFeatures:\n",
        "    \"\"\"Data class for linguistic features\"\"\"\n",
        "    # Basic text statistics\n",
        "    raw_text: str\n",
        "    word_count: int\n",
        "    sentence_count: int\n",
        "    char_count: int\n",
        "    avg_word_length: float\n",
        "    avg_sentence_length: float\n",
        "\n",
        "    # Vocabulary features\n",
        "    unique_words: int\n",
        "    lexical_diversity: float\n",
        "    function_words_ratio: float\n",
        "    content_words_ratio: float\n",
        "\n",
        "    # Syntactic features\n",
        "    noun_ratio: float\n",
        "    verb_ratio: float\n",
        "    adjective_ratio: float\n",
        "    pronoun_ratio: float\n",
        "\n",
        "    # Semantic complexity\n",
        "    syllable_count: int\n",
        "    avg_syllables_per_word: float\n",
        "    complex_words_ratio: float  # Words with 3+ syllables\n",
        "\n",
        "    # Discourse features\n",
        "    repetition_ratio: float\n",
        "    pause_indicators: int\n",
        "    filler_words: int\n",
        "\n",
        "    # BERT features\n",
        "    bert_tokens: List[str]\n",
        "    bert_input_ids: List[int]\n",
        "    bert_attention_mask: List[int]\n",
        "    bert_embeddings: Optional[np.ndarray]\n",
        "\n",
        "    # Processing metadata\n",
        "    processing_success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "class LinguisticFeaturesService:\n",
        "    \"\"\"Service for extracting linguistic features from transcripts\"\"\"\n",
        "\n",
        "    def __init__(self, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or setup_logging()\n",
        "        self.bert_tokenizer = None\n",
        "        self.bert_model = None\n",
        "\n",
        "        # Define word lists for analysis\n",
        "        self.function_words = self._load_function_words()\n",
        "        self.filler_words = {'um', 'uh', 'er', 'ah', 'hmm', 'well', 'like', 'you know', 'sort of', 'kind of'}\n",
        "        self.pause_indicators = {'[pause]', '[silence]', '...', '--'}\n",
        "\n",
        "        # Simple POS tag mappings for basic analysis\n",
        "        self.noun_patterns = {'NN', 'NNS', 'NNP', 'NNPS'}\n",
        "        self.verb_patterns = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
        "        self.adjective_patterns = {'JJ', 'JJR', 'JJS'}\n",
        "        self.pronoun_patterns = {'PRP', 'PRP$', 'WP', 'WP$'}\n",
        "\n",
        "        self._initialize_bert()\n",
        "\n",
        "    def _initialize_bert(self):\n",
        "        \"\"\"Initialize BERT model for embeddings\"\"\"\n",
        "        if not BERT_AVAILABLE:\n",
        "            self.logger.warning(\"Transformers not available - BERT features will be limited\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Loading BERT model: {MODEL_CONFIG.bert_model}\")\n",
        "            self.bert_tokenizer = BertTokenizer.from_pretrained(MODEL_CONFIG.bert_model)\n",
        "            self.bert_model = BertModel.from_pretrained(MODEL_CONFIG.bert_model)\n",
        "            self.bert_model.eval()  # Set to evaluation mode\n",
        "            self.logger.info(\"✓ BERT model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load BERT model: {e}\")\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "\n",
        "    def _load_function_words(self) -> set:\n",
        "        \"\"\"Load common function words\"\"\"\n",
        "        function_words = {\n",
        "            # Articles\n",
        "            'a', 'an', 'the',\n",
        "            # Prepositions\n",
        "            'in', 'on', 'at', 'by', 'to', 'from', 'of', 'with', 'about', 'into', 'through', 'during',\n",
        "            'before', 'after', 'above', 'below', 'over', 'under', 'between', 'among', 'against',\n",
        "            # Conjunctions\n",
        "            'and', 'or', 'but', 'nor', 'for', 'yet', 'so', 'because', 'since', 'although', 'while',\n",
        "            'if', 'unless', 'until', 'when', 'where', 'how', 'why',\n",
        "            # Pronouns\n",
        "            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "            'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours', 'ours', 'theirs',\n",
        "            'this', 'that', 'these', 'those', 'what', 'which', 'who', 'whom', 'whose',\n",
        "            # Auxiliary verbs\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',\n",
        "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "            'will', 'would', 'shall', 'should', 'may', 'might', 'can', 'could', 'must',\n",
        "            # Others\n",
        "            'not', 'no', 'yes', 'there', 'here'\n",
        "        }\n",
        "        return function_words\n",
        "\n",
        "    def _count_syllables(self, word: str) -> int:\n",
        "        \"\"\"Count syllables in a word (simple heuristic)\"\"\"\n",
        "        word = word.lower().strip(\".,!?;:\")\n",
        "        if not word:\n",
        "            return 0\n",
        "\n",
        "        # Remove silent 'e' at the end\n",
        "        if word.endswith('e') and len(word) > 1:\n",
        "            word = word[:-1]\n",
        "\n",
        "        # Count vowel groups\n",
        "        vowels = \"aeiouy\"\n",
        "        syllable_count = 0\n",
        "        prev_was_vowel = False\n",
        "\n",
        "        for char in word:\n",
        "            is_vowel = char in vowels\n",
        "            if is_vowel and not prev_was_vowel:\n",
        "                syllable_count += 1\n",
        "            prev_was_vowel = is_vowel\n",
        "\n",
        "        # Every word has at least one syllable\n",
        "        return max(1, syllable_count)\n",
        "\n",
        "    def _extract_basic_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract basic text statistics\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'word_count': 0,\n",
        "                'sentence_count': 0,\n",
        "                'char_count': 0,\n",
        "                'avg_word_length': 0.0,\n",
        "                'avg_sentence_length': 0.0\n",
        "            }\n",
        "\n",
        "        # Clean text\n",
        "        clean_text = text.strip()\n",
        "\n",
        "        # Word analysis\n",
        "        words = clean_text.split()\n",
        "        word_count = len(words)\n",
        "\n",
        "        # Sentence analysis - improved sentence detection\n",
        "        sentences = re.split(r'[.!?]+', clean_text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        sentence_count = len(sentences)\n",
        "\n",
        "        # Character count (excluding spaces)\n",
        "        char_count = len(clean_text.replace(' ', ''))\n",
        "\n",
        "        # Averages\n",
        "        avg_word_length = np.mean([len(word) for word in words]) if words else 0.0\n",
        "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            'word_count': word_count,\n",
        "            'sentence_count': sentence_count,\n",
        "            'char_count': char_count,\n",
        "            'avg_word_length': avg_word_length,\n",
        "            'avg_sentence_length': avg_sentence_length\n",
        "        }\n",
        "\n",
        "    def _extract_vocabulary_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract vocabulary and lexical features\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'unique_words': 0,\n",
        "                'lexical_diversity': 0.0,\n",
        "                'function_words_ratio': 0.0,\n",
        "                'content_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        # Tokenize and clean words\n",
        "        words = text.lower().split()\n",
        "        words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]\n",
        "\n",
        "        if not words:\n",
        "            return {\n",
        "                'unique_words': 0,\n",
        "                'lexical_diversity': 0.0,\n",
        "                'function_words_ratio': 0.0,\n",
        "                'content_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        unique_words = len(set(words))\n",
        "        lexical_diversity = unique_words / len(words)\n",
        "\n",
        "        # Function vs content words\n",
        "        function_word_count = sum(1 for word in words if word in self.function_words)\n",
        "        content_word_count = len(words) - function_word_count\n",
        "\n",
        "        function_words_ratio = function_word_count / len(words)\n",
        "        content_words_ratio = content_word_count / len(words)\n",
        "\n",
        "        return {\n",
        "            'unique_words': unique_words,\n",
        "            'lexical_diversity': lexical_diversity,\n",
        "            'function_words_ratio': function_words_ratio,\n",
        "            'content_words_ratio': content_words_ratio\n",
        "        }\n",
        "\n",
        "    def _extract_syntactic_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract syntactic features (simplified POS analysis)\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'noun_ratio': 0.0,\n",
        "                'verb_ratio': 0.0,\n",
        "                'adjective_ratio': 0.0,\n",
        "                'pronoun_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        words = text.lower().split()\n",
        "        words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]\n",
        "\n",
        "        if not words:\n",
        "            return {\n",
        "                'noun_ratio': 0.0,\n",
        "                'verb_ratio': 0.0,\n",
        "                'adjective_ratio': 0.0,\n",
        "                'pronoun_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        # Simple heuristic-based POS tagging\n",
        "        noun_count = 0\n",
        "        verb_count = 0\n",
        "        adjective_count = 0\n",
        "        pronoun_count = 0\n",
        "\n",
        "        # Common verb endings and forms\n",
        "        verb_endings = {'ed', 'ing', 'es', 's'}\n",
        "        common_verbs = {'is', 'are', 'was', 'were', 'have', 'has', 'had', 'do', 'does', 'did',\n",
        "                       'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n",
        "\n",
        "        # Common adjective endings\n",
        "        adj_endings = {'ly', 'ful', 'less', 'ous', 'ive', 'able', 'ible'}\n",
        "\n",
        "        # Common pronouns (already in function words, but specific ones)\n",
        "        pronouns = {'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "                   'my', 'your', 'his', 'her', 'its', 'our', 'their', 'this', 'that', 'these', 'those'}\n",
        "\n",
        "        for word in words:\n",
        "            # Check pronouns first\n",
        "            if word in pronouns:\n",
        "                pronoun_count += 1\n",
        "            # Check common verbs\n",
        "            elif word in common_verbs or any(word.endswith(ending) for ending in verb_endings if len(word) > 3):\n",
        "                verb_count += 1\n",
        "            # Check adjectives (simple heuristic)\n",
        "            elif any(word.endswith(ending) for ending in adj_endings):\n",
        "                adjective_count += 1\n",
        "            # Default to noun if capitalized or doesn't match other patterns\n",
        "            else:\n",
        "                noun_count += 1\n",
        "\n",
        "        total_words = len(words)\n",
        "        return {\n",
        "            'noun_ratio': noun_count / total_words,\n",
        "            'verb_ratio': verb_count / total_words,\n",
        "            'adjective_ratio': adjective_count / total_words,\n",
        "            'pronoun_ratio': pronoun_count / total_words\n",
        "        }\n",
        "\n",
        "    def _extract_semantic_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract semantic complexity features\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'syllable_count': 0,\n",
        "                'avg_syllables_per_word': 0.0,\n",
        "                'complex_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        words = text.split()\n",
        "        words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]\n",
        "\n",
        "        if not words:\n",
        "            return {\n",
        "                'syllable_count': 0,\n",
        "                'avg_syllables_per_word': 0.0,\n",
        "                'complex_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        syllable_counts = [self._count_syllables(word) for word in words]\n",
        "        total_syllables = sum(syllable_counts)\n",
        "        complex_words = sum(1 for count in syllable_counts if count >= 3)\n",
        "\n",
        "        return {\n",
        "            'syllable_count': total_syllables,\n",
        "            'avg_syllables_per_word': total_syllables / len(words),\n",
        "            'complex_words_ratio': complex_words / len(words)\n",
        "        }\n",
        "\n",
        "    def _extract_discourse_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract discourse and disfluency features\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'repetition_ratio': 0.0,\n",
        "                'pause_indicators': 0,\n",
        "                'filler_words': 0\n",
        "            }\n",
        "\n",
        "        # Count pause indicators\n",
        "        pause_count = 0\n",
        "        text_lower = text.lower()\n",
        "        for indicator in self.pause_indicators:\n",
        "            pause_count += text_lower.count(indicator)\n",
        "\n",
        "        # Count filler words\n",
        "        words = text.lower().split()\n",
        "        filler_count = 0\n",
        "        for filler in self.filler_words:\n",
        "            if ' ' in filler:  # Multi-word fillers\n",
        "                filler_count += text_lower.count(filler)\n",
        "            else:  # Single word fillers\n",
        "                filler_count += words.count(filler)\n",
        "\n",
        "        # Calculate repetition ratio (simple word repetition)\n",
        "        word_counts = Counter(words)\n",
        "        repeated_words = sum(count - 1 for count in word_counts.values() if count > 1)\n",
        "        repetition_ratio = repeated_words / len(words) if words else 0.0\n",
        "\n",
        "        return {\n",
        "            'repetition_ratio': repetition_ratio,\n",
        "            'pause_indicators': pause_count,\n",
        "            'filler_words': filler_count\n",
        "        }\n",
        "\n",
        "    def _extract_bert_features(self, text: str, max_length: int = 512) -> Dict[str, Any]:\n",
        "        \"\"\"Extract BERT tokens and embeddings\"\"\"\n",
        "        if not BERT_AVAILABLE or not self.bert_tokenizer:\n",
        "            return {\n",
        "                'bert_tokens': [],\n",
        "                'bert_input_ids': [],\n",
        "                'bert_attention_mask': [],\n",
        "                'bert_embeddings': None\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Tokenize text\n",
        "            encoded = self.bert_tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Get tokens for analysis\n",
        "            tokens = self.bert_tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
        "\n",
        "            # Get embeddings if model is available\n",
        "            embeddings = None\n",
        "            if self.bert_model:\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.bert_model(**encoded)\n",
        "                    # Use [CLS] token embedding as sentence representation\n",
        "                    embeddings = outputs.last_hidden_state[0][0].cpu().numpy()\n",
        "\n",
        "            return {\n",
        "                'bert_tokens': tokens,\n",
        "                'bert_input_ids': encoded['input_ids'][0].tolist(),\n",
        "                'bert_attention_mask': encoded['attention_mask'][0].tolist(),\n",
        "                'bert_embeddings': embeddings\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting BERT features: {e}\")\n",
        "            return {\n",
        "                'bert_tokens': [],\n",
        "                'bert_input_ids': [],\n",
        "                'bert_attention_mask': [],\n",
        "                'bert_embeddings': None\n",
        "            }\n",
        "\n",
        "    def extract_features(self, text: str) -> LinguisticFeatures:\n",
        "        \"\"\"Extract all linguistic features from text\"\"\"\n",
        "        try:\n",
        "            if not text or not text.strip():\n",
        "                self.logger.warning(\"Empty text provided for feature extraction\")\n",
        "                return self._create_empty_features(text, \"Empty text provided\")\n",
        "\n",
        "            self.logger.info(f\"Extracting linguistic features from text ({len(text)} characters)\")\n",
        "\n",
        "            with ProcessingTimer() as timer:\n",
        "                # Extract different feature categories\n",
        "                basic_features = self._extract_basic_features(text)\n",
        "                vocabulary_features = self._extract_vocabulary_features(text)\n",
        "                syntactic_features = self._extract_syntactic_features(text)\n",
        "                semantic_features = self._extract_semantic_features(text)\n",
        "                discourse_features = self._extract_discourse_features(text)\n",
        "                bert_features = self._extract_bert_features(text)\n",
        "\n",
        "                # Create LinguisticFeatures object\n",
        "                features = LinguisticFeatures(\n",
        "                    raw_text=text,\n",
        "                    processing_success=True,\n",
        "                    **basic_features,\n",
        "                    **vocabulary_features,\n",
        "                    **syntactic_features,\n",
        "                    **semantic_features,\n",
        "                    **discourse_features,\n",
        "                    **bert_features\n",
        "                )\n",
        "\n",
        "            self.logger.info(f\"✓ Feature extraction completed in {timer.elapsed:.2f}s\")\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting linguistic features: {e}\")\n",
        "            return self._create_empty_features(text, str(e))\n",
        "\n",
        "    def _create_empty_features(self, text: str, error_message: str) -> LinguisticFeatures:\n",
        "        \"\"\"Create empty features object for error cases\"\"\"\n",
        "        return LinguisticFeatures(\n",
        "            raw_text=text or \"\",\n",
        "            word_count=0,\n",
        "            sentence_count=0,\n",
        "            char_count=0,\n",
        "            avg_word_length=0.0,\n",
        "            avg_sentence_length=0.0,\n",
        "            unique_words=0,\n",
        "            lexical_diversity=0.0,\n",
        "            function_words_ratio=0.0,\n",
        "            content_words_ratio=0.0,\n",
        "            noun_ratio=0.0,\n",
        "            verb_ratio=0.0,\n",
        "            adjective_ratio=0.0,\n",
        "            pronoun_ratio=0.0,\n",
        "            syllable_count=0,\n",
        "            avg_syllables_per_word=0.0,\n",
        "            complex_words_ratio=0.0,\n",
        "            repetition_ratio=0.0,\n",
        "            pause_indicators=0,\n",
        "            filler_words=0,\n",
        "            bert_tokens=[],\n",
        "            bert_input_ids=[],\n",
        "            bert_attention_mask=[],\n",
        "            bert_embeddings=None,\n",
        "            processing_success=False,\n",
        "            error_message=error_message\n",
        "        )\n",
        "\n",
        "    def process_transcription_result(self, result: TranscriptionResult) -> LinguisticFeatures:\n",
        "        \"\"\"Process a TranscriptionResult to extract linguistic features\"\"\"\n",
        "        try:\n",
        "            if not result.success or not result.text:\n",
        "                return self._create_empty_features(\n",
        "                    result.text or \"\",\n",
        "                    f\"Transcription failed: {result.error_message}\"\n",
        "                )\n",
        "\n",
        "            return self.extract_features(result.text)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing transcription result: {e}\")\n",
        "            return self._create_empty_features(\"\", str(e))\n",
        "\n",
        "    def batch_process_texts(self, texts: List[str], batch_size: int = 32) -> List[LinguisticFeatures]:\n",
        "        \"\"\"Process multiple texts in batches\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Processing {len(texts)} texts in batches of {batch_size}\")\n",
        "            results = []\n",
        "\n",
        "            with ProcessingTimer() as timer:\n",
        "                for batch in batch_generator(texts, batch_size):\n",
        "                    batch_results = []\n",
        "                    for text in batch:\n",
        "                        features = self.extract_features(text)\n",
        "                        batch_results.append(features)\n",
        "                    results.extend(batch_results)\n",
        "\n",
        "                    # Memory cleanup between batches\n",
        "                    cleanup_memory()\n",
        "\n",
        "            self.logger.info(f\"✓ Batch processing completed in {timer.elapsed:.2f}s\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in batch processing: {e}\")\n",
        "            return [self._create_empty_features(text, str(e)) for text in texts]\n",
        "\n",
        "    def get_feature_summary(self, features: LinguisticFeatures) -> Dict[str, Any]:\n",
        "        \"\"\"Get a summary of extracted features\"\"\"\n",
        "        if not features.processing_success:\n",
        "            return {\n",
        "                'status': 'failed',\n",
        "                'error': features.error_message,\n",
        "                'text_length': len(features.raw_text)\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'basic_stats': {\n",
        "                'words': features.word_count,\n",
        "                'sentences': features.sentence_count,\n",
        "                'characters': features.char_count,\n",
        "                'avg_word_length': round(features.avg_word_length, 2),\n",
        "                'avg_sentence_length': round(features.avg_sentence_length, 2)\n",
        "            },\n",
        "            'vocabulary': {\n",
        "                'unique_words': features.unique_words,\n",
        "                'lexical_diversity': round(features.lexical_diversity, 3),\n",
        "                'function_words_ratio': round(features.function_words_ratio, 3),\n",
        "                'content_words_ratio': round(features.content_words_ratio, 3)\n",
        "            },\n",
        "            'complexity': {\n",
        "                'avg_syllables_per_word': round(features.avg_syllables_per_word, 2),\n",
        "                'complex_words_ratio': round(features.complex_words_ratio, 3),\n",
        "                'total_syllables': features.syllable_count\n",
        "            },\n",
        "            'discourse': {\n",
        "                'repetition_ratio': round(features.repetition_ratio, 3),\n",
        "                'pause_indicators': features.pause_indicators,\n",
        "                'filler_words': features.filler_words\n",
        "            },\n",
        "            'bert_available': features.bert_embeddings is not None,\n",
        "            'bert_tokens_count': len(features.bert_tokens)\n",
        "        }\n",
        "\n",
        "    def save_features(self, features: LinguisticFeatures, filepath: str) -> bool:\n",
        "        \"\"\"Save linguistic features to file\"\"\"\n",
        "        try:\n",
        "            return safe_save_pickle(features, filepath)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving features: {e}\")\n",
        "            return False\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        try:\n",
        "            if self.bert_model:\n",
        "                del self.bert_model\n",
        "            if self.bert_tokenizer:\n",
        "                del self.bert_tokenizer\n",
        "            cleanup_memory()\n",
        "            self.logger.info(\"✓ Linguistic features service cleanup completed\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during cleanup: {e}\")\n",
        "\n",
        "# Factory function for easy service creation\n",
        "def create_linguistic_features_service(logger: Optional[logging.Logger] = None) -> LinguisticFeaturesService:\n",
        "    \"\"\"Factory function to create a LinguisticFeaturesService instance\"\"\"\n",
        "    return LinguisticFeaturesService(logger=logger)"
      ],
      "metadata": {
        "id": "-6Rgd_dsdAl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data_Manager_Service.py"
      ],
      "metadata": {
        "id": "O6S6OhuGfu4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Manager Service - Handles ADReSSo21 dataset loading and file management\n",
        "Optimized for Windows 10 with parallel processing\n",
        "\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pickle\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "from config import SYSTEM_CONFIG, MODEL_CONFIG\n",
        "from utils import setup_logging, ProcessingTimer, safe_save_pickle, cleanup_memory\n",
        "\n",
        "@dataclass\n",
        "class AudioFile:\n",
        "    \"\"\"Data class for audio file information\"\"\"\n",
        "    file_path: str\n",
        "    filename: str\n",
        "    category: str\n",
        "    label: str\n",
        "    dataset_type: str  # 'diagnosis' or 'progression'\n",
        "    split: str  # 'train' or 'test'\n",
        "    segmentation_path: Optional[str] = None\n",
        "    file_size: Optional[int] = None\n",
        "    duration: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class DatasetInfo:\n",
        "    \"\"\"Data class for dataset information\"\"\"\n",
        "    total_files: int\n",
        "    categories: Dict[str, int]\n",
        "    dataset_types: Dict[str, int]\n",
        "    splits: Dict[str, int]\n",
        "    total_size_mb: float\n",
        "    audio_files: List[AudioFile]\n",
        "\n",
        "class DataManagerService:\n",
        "    \"\"\"Service for managing ADReSSo21 dataset files and metadata\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str, logger: Optional[logging.Logger] = None):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.logger = logger or setup_logging()\n",
        "\n",
        "        # Define dataset structure based on your paths\n",
        "        self.dataset_paths = {\n",
        "            'diagnosis_train': {\n",
        "                'audio': {\n",
        "                    'ad': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\",\n",
        "                    'cn': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\"\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'ad': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\ad\",\n",
        "                    'cn': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\cn\"\n",
        "                }\n",
        "            },\n",
        "            'progression_train': {\n",
        "                'audio': {\n",
        "                    'decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\",\n",
        "                    'no_decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\"\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\decline\",\n",
        "                    'no_decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\no_decline\"\n",
        "                }\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'audio': {\n",
        "                    'test': r\"ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\"\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'test': r\"ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\segmentation\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Create output directories\n",
        "        self.output_dir = self.base_path / \"output\"\n",
        "        self.create_output_directories()\n",
        "\n",
        "    def create_output_directories(self):\n",
        "        \"\"\"Create necessary output directories\"\"\"\n",
        "        directories = [\n",
        "            self.output_dir,\n",
        "            self.output_dir / \"features\",\n",
        "            self.output_dir / \"transcripts\",\n",
        "            self.output_dir / \"models\",\n",
        "            self.output_dir / \"results\",\n",
        "            self.output_dir / \"logs\",\n",
        "            self.output_dir / \"cache\"\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.logger.info(f\"✓ Output directories created at {self.output_dir}\")\n",
        "\n",
        "    def scan_audio_files(self) -> List[AudioFile]:\n",
        "        \"\"\"Scan and catalog all audio files in the dataset\"\"\"\n",
        "        self.logger.info(\"Scanning audio files in dataset...\")\n",
        "        audio_files = []\n",
        "\n",
        "        with ProcessingTimer() as timer:\n",
        "            for dataset_name, paths in self.dataset_paths.items():\n",
        "                self.logger.info(f\"Scanning {dataset_name}...\")\n",
        "\n",
        "                # Extract dataset info from name\n",
        "                if 'diagnosis' in dataset_name:\n",
        "                    dataset_type = 'diagnosis'\n",
        "                    split = 'train'\n",
        "                else:  # progression\n",
        "                    dataset_type = 'progression'\n",
        "                    split = 'test' if 'test' in dataset_name else 'train'\n",
        "\n",
        "                # Scan audio directories\n",
        "                for label, audio_path in paths['audio'].items():\n",
        "                    full_audio_path = self.base_path / audio_path\n",
        "\n",
        "                    if not full_audio_path.exists():\n",
        "                        self.logger.warning(f\"Audio path not found: {full_audio_path}\")\n",
        "                        continue\n",
        "\n",
        "                    # Find segmentation path\n",
        "                    seg_path = None\n",
        "                    if 'segmentation' in paths:\n",
        "                        seg_key = label if label in paths['segmentation'] else 'test'\n",
        "                        if seg_key in paths['segmentation']:\n",
        "                            seg_path = self.base_path / paths['segmentation'][seg_key]\n",
        "\n",
        "                    # Get all WAV files\n",
        "                    wav_files = list(full_audio_path.glob(\"*.wav\"))\n",
        "\n",
        "                    for wav_file in wav_files:\n",
        "                        # Find corresponding segmentation file\n",
        "                        seg_file = None\n",
        "                        if seg_path and seg_path.exists():\n",
        "                            seg_file_path = seg_path / f\"{wav_file.stem}.csv\"\n",
        "                            if seg_file_path.exists():\n",
        "                                seg_file = str(seg_file_path)\n",
        "\n",
        "                        audio_file = AudioFile(\n",
        "                            file_path=str(wav_file),\n",
        "                            filename=wav_file.name,\n",
        "                            category=f\"{dataset_type}_{label}\",\n",
        "                            label=label,\n",
        "                            dataset_type=dataset_type,\n",
        "                            split=split,\n",
        "                            segmentation_path=seg_file,\n",
        "                            file_size=wav_file.stat().st_size if wav_file.exists() else None\n",
        "                        )\n",
        "\n",
        "                        audio_files.append(audio_file)\n",
        "\n",
        "        self.logger.info(f\"✓ Found {len(audio_files)} audio files in {timer.elapsed:.2f}s\")\n",
        "        return audio_files\n",
        "\n",
        "    def get_dataset_info(self, audio_files: List[AudioFile]) -> DatasetInfo:\n",
        "        \"\"\"Generate comprehensive dataset information\"\"\"\n",
        "        if not audio_files:\n",
        "            return DatasetInfo(0, {}, {}, {}, 0.0, [])\n",
        "\n",
        "        # Count by categories\n",
        "        categories = {}\n",
        "        dataset_types = {}\n",
        "        splits = {}\n",
        "        total_size = 0\n",
        "\n",
        "        for af in audio_files:\n",
        "            # Count categories\n",
        "            categories[af.category] = categories.get(af.category, 0) + 1\n",
        "            dataset_types[af.dataset_type] = dataset_types.get(af.dataset_type, 0) + 1\n",
        "            splits[af.split] = splits.get(af.split, 0) + 1\n",
        "\n",
        "            # Sum file sizes\n",
        "            if af.file_size:\n",
        "                total_size += af.file_size\n",
        "\n",
        "        return DatasetInfo(\n",
        "            total_files=len(audio_files),\n",
        "            categories=categories,\n",
        "            dataset_types=dataset_types,\n",
        "            splits=splits,\n",
        "            total_size_mb=total_size / (1024 * 1024),\n",
        "            audio_files=audio_files\n",
        "        )\n",
        "\n",
        "    def create_file_manifest(self, audio_files: List[AudioFile]) -> pd.DataFrame:\n",
        "        \"\"\"Create a detailed file manifest\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for af in audio_files:\n",
        "            data.append({\n",
        "                'filename': af.filename,\n",
        "                'file_path': af.file_path,\n",
        "                'category': af.category,\n",
        "                'label': af.label,\n",
        "                'dataset_type': af.dataset_type,\n",
        "                'split': af.split,\n",
        "                'segmentation_path': af.segmentation_path,\n",
        "                'has_segmentation': af.segmentation_path is not None,\n",
        "                'file_size_mb': af.file_size / (1024 * 1024) if af.file_size else None,\n",
        "                'file_exists': os.path.exists(af.file_path)\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Save manifest\n",
        "        manifest_path = self.output_dir / \"file_manifest.csv\"\n",
        "        df.to_csv(manifest_path, index=False)\n",
        "        self.logger.info(f\"✓ File manifest saved to {manifest_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def validate_dataset(self, audio_files: List[AudioFile]) -> Dict[str, Any]:\n",
        "        \"\"\"Validate dataset integrity\"\"\"\n",
        "        self.logger.info(\"Validating dataset integrity...\")\n",
        "\n",
        "        validation_results = {\n",
        "            'total_files': len(audio_files),\n",
        "            'valid_files': 0,\n",
        "            'missing_files': 0,\n",
        "            'files_with_segmentation': 0,\n",
        "            'missing_segmentation': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "        for af in audio_files:\n",
        "            # Check if audio file exists\n",
        "            if not os.path.exists(af.file_path):\n",
        "                validation_results['missing_files'] += 1\n",
        "                validation_results['errors'].append(f\"Missing audio: {af.file_path}\")\n",
        "                continue\n",
        "\n",
        "            validation_results['valid_files'] += 1\n",
        "\n",
        "            # Check segmentation file\n",
        "            if af.segmentation_path:\n",
        "                if os.path.exists(af.segmentation_path):\n",
        "                    validation_results['files_with_segmentation'] += 1\n",
        "                else:\n",
        "                    validation_results['missing_segmentation'] += 1\n",
        "                    validation_results['errors'].append(f\"Missing segmentation: {af.segmentation_path}\")\n",
        "\n",
        "        # Save validation report\n",
        "        report_path = self.output_dir / \"validation_report.json\"\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(validation_results, f, indent=2)\n",
        "\n",
        "        self.logger.info(f\"✓ Dataset validation complete. Report saved to {report_path}\")\n",
        "        return validation_results\n",
        "\n",
        "    def get_files_by_category(self, audio_files: List[AudioFile],\n",
        "                            category: Optional[str] = None,\n",
        "                            dataset_type: Optional[str] = None,\n",
        "                            split: Optional[str] = None) -> List[AudioFile]:\n",
        "        \"\"\"Filter audio files by category, dataset type, or split\"\"\"\n",
        "        filtered_files = audio_files\n",
        "\n",
        "        if category:\n",
        "            filtered_files = [af for af in filtered_files if af.category == category]\n",
        "\n",
        "        if dataset_type:\n",
        "            filtered_files = [af for af in filtered_files if af.dataset_type == dataset_type]\n",
        "\n",
        "        if split:\n",
        "            filtered_files = [af for af in filtered_files if af.split == split]\n",
        "\n",
        "        return filtered_files\n",
        "\n",
        "    def create_train_test_splits(self, audio_files: List[AudioFile],\n",
        "                                test_size: float = 0.2) -> Tuple[List[AudioFile], List[AudioFile]]:\n",
        "        \"\"\"Create train/test splits for datasets that don't have predefined splits\"\"\"\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        # Group by category to ensure balanced splits\n",
        "        category_files = {}\n",
        "        for af in audio_files:\n",
        "            if af.category not in category_files:\n",
        "                category_files[af.category] = []\n",
        "            category_files[af.category].append(af)\n",
        "\n",
        "        train_files = []\n",
        "        test_files = []\n",
        "\n",
        "        for category, files in category_files.items():\n",
        "            if len(files) < 2:\n",
        "                # If too few files, put all in training\n",
        "                train_files.extend(files)\n",
        "                continue\n",
        "\n",
        "            cat_train, cat_test = train_test_split(\n",
        "                files,\n",
        "                test_size=test_size,\n",
        "                random_state=42,\n",
        "                stratify=None  # Can't stratify on single category\n",
        "            )\n",
        "\n",
        "            train_files.extend(cat_train)\n",
        "            test_files.extend(cat_test)\n",
        "\n",
        "        self.logger.info(f\"✓ Created splits: {len(train_files)} train, {len(test_files)} test\")\n",
        "        return train_files, test_files\n",
        "\n",
        "    def batch_load_files(self, audio_files: List[AudioFile],\n",
        "                        batch_size: int = 32) -> List[List[AudioFile]]:\n",
        "        \"\"\"Create batches of files for parallel processing\"\"\"\n",
        "        batches = []\n",
        "        for i in range(0, len(audio_files), batch_size):\n",
        "            batch = audio_files[i:i + batch_size]\n",
        "            batches.append(batch)\n",
        "\n",
        "        self.logger.info(f\"✓ Created {len(batches)} batches of size {batch_size}\")\n",
        "        return batches\n",
        "\n",
        "    def save_dataset_cache(self, audio_files: List[AudioFile],\n",
        "                          dataset_info: DatasetInfo) -> bool:\n",
        "        \"\"\"Save dataset information to cache for faster loading\"\"\"\n",
        "        try:\n",
        "            cache_data = {\n",
        "                'audio_files': audio_files,\n",
        "                'dataset_info': dataset_info,\n",
        "                'scan_timestamp': pd.Timestamp.now().isoformat(),\n",
        "                'base_path': str(self.base_path)\n",
        "            }\n",
        "\n",
        "            cache_path = self.output_dir / \"cache\" / \"dataset_cache.pkl\"\n",
        "            success = safe_save_pickle(cache_data, cache_path)\n",
        "\n",
        "            if success:\n",
        "                self.logger.info(f\"✓ Dataset cache saved to {cache_path}\")\n",
        "\n",
        "            return success\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving dataset cache: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_dataset_cache(self) -> Optional[Tuple[List[AudioFile], DatasetInfo]]:\n",
        "        \"\"\"Load dataset information from cache\"\"\"\n",
        "        try:\n",
        "            cache_path = self.output_dir / \"cache\" / \"dataset_cache.pkl\"\n",
        "\n",
        "            if not cache_path.exists():\n",
        "                return None\n",
        "\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                cache_data = pickle.load(f)\n",
        "\n",
        "            # Verify cache is for same base path\n",
        "            if cache_data.get('base_path') != str(self.base_path):\n",
        "                self.logger.warning(\"Cache base path mismatch, ignoring cache\")\n",
        "                return None\n",
        "\n",
        "            self.logger.info(\"✓ Loaded dataset from cache\")\n",
        "            return cache_data['audio_files'], cache_data['dataset_info']\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading dataset cache: {e}\")\n",
        "            return None\n",
        "\n",
        "    def print_dataset_summary(self, dataset_info: DatasetInfo):\n",
        "        \"\"\"Print a comprehensive dataset summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ADRESSO21 DATASET SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"Total Files: {dataset_info.total_files}\")\n",
        "        print(f\"Total Size: {dataset_info.total_size_mb:.2f} MB\")\n",
        "        print()\n",
        "\n",
        "        print(\"By Dataset Type:\")\n",
        "        for dtype, count in dataset_info.dataset_types.items():\n",
        "            print(f\"  {dtype}: {count} files\")\n",
        "        print()\n",
        "\n",
        "        print(\"By Split:\")\n",
        "        for split, count in dataset_info.splits.items():\n",
        "            print(f\"  {split}: {count} files\")\n",
        "        print()\n",
        "\n",
        "        print(\"By Category:\")\n",
        "        for category, count in dataset_info.categories.items():\n",
        "            print(f\"  {category}: {count} files\")\n",
        "        print()\n",
        "\n",
        "    def cleanup_output_directory(self, keep_cache: bool = True):\n",
        "        \"\"\"Clean up output directory\"\"\"\n",
        "        try:\n",
        "            for item in self.output_dir.iterdir():\n",
        "                if item.is_dir():\n",
        "                    if item.name == 'cache' and keep_cache:\n",
        "                        continue\n",
        "                    shutil.rmtree(item)\n",
        "                else:\n",
        "                    item.unlink()\n",
        "\n",
        "            # Recreate directories\n",
        "            self.create_output_directories()\n",
        "            self.logger.info(\"✓ Output directory cleaned\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error cleaning output directory: {e}\")\n",
        "\n",
        "# Factory function\n",
        "def create_data_manager(base_path: str, logger: Optional[logging.Logger] = None) -> DataManagerService:\n",
        "    \"\"\"Factory function to create DataManagerService\"\"\"\n",
        "    return DataManagerService(base_path, logger)"
      ],
      "metadata": {
        "id": "6_uUyumLfo2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline_service.py - Main Pipeline Orchestrator\n"
      ],
      "metadata": {
        "id": "qq6Ee_A3iK9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pipeline Service - Main orchestrator for ADReSSo21 speech analysis pipeline\n",
        "Coordinates all microservices for complete analysis workflow\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import multiprocessing as mp\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Import your microservices\n",
        "from config import Config\n",
        "from utils import setup_logging, ensure_directory\n",
        "from data_manager_service import DataManagerService\n",
        "from acoustic_features_service import AcousticFeaturesService\n",
        "from transcription_service import TranscriptionService\n",
        "from linguistic_features_service import LinguisticFeaturesService\n",
        "\n",
        "\n",
        "class PipelineService:\n",
        "    \"\"\"\n",
        "    Main pipeline service that orchestrates all analysis components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize pipeline with configuration and services\n",
        "\n",
        "        Args:\n",
        "            config_path: Path to configuration file\n",
        "        \"\"\"\n",
        "        # Load configuration\n",
        "        self.config = Config(config_path)\n",
        "\n",
        "        # Setup logging\n",
        "        self.logger = setup_logging(\n",
        "            log_level=self.config.get('logging.level', 'INFO'),\n",
        "            log_file=self.config.get('logging.file')\n",
        "        )\n",
        "\n",
        "        # Initialize services\n",
        "        self.data_manager = DataManagerService(self.config)\n",
        "        self.acoustic_service = AcousticFeaturesService(self.config)\n",
        "        self.transcription_service = TranscriptionService(self.config)\n",
        "        self.linguistic_service = LinguisticFeaturesService(self.config)\n",
        "\n",
        "        # Pipeline state\n",
        "        self.results = {}\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "\n",
        "        self.logger.info(\"Pipeline initialized successfully\")\n",
        "\n",
        "    def run_complete_pipeline(self, parallel: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run the complete analysis pipeline\n",
        "\n",
        "        Args:\n",
        "            parallel: Whether to use parallel processing where possible\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all pipeline results\n",
        "        \"\"\"\n",
        "        self.start_time = datetime.now()\n",
        "        self.logger.info(\"=== Starting ADReSSo21 Speech Analysis Pipeline ===\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load dataset and get audio files\n",
        "            self.logger.info(\"Step 1: Loading dataset...\")\n",
        "            audio_files = self._load_dataset()\n",
        "\n",
        "            # Step 2: Extract acoustic features\n",
        "            self.logger.info(\"Step 2: Extracting acoustic features...\")\n",
        "            acoustic_features = self._extract_acoustic_features(audio_files, parallel)\n",
        "\n",
        "            # Step 3: Extract transcripts\n",
        "            self.logger.info(\"Step 3: Extracting transcripts...\")\n",
        "            transcripts = self._extract_transcripts(audio_files, parallel)\n",
        "\n",
        "            # Step 4: Extract linguistic features\n",
        "            self.logger.info(\"Step 4: Extracting linguistic features...\")\n",
        "            linguistic_features = self._extract_linguistic_features(transcripts)\n",
        "\n",
        "            # Step 5: Combine and save results\n",
        "            self.logger.info(\"Step 5: Combining and saving results...\")\n",
        "            final_results = self._combine_and_save_results(\n",
        "                audio_files, acoustic_features, transcripts, linguistic_features\n",
        "            )\n",
        "\n",
        "            self.end_time = datetime.now()\n",
        "            duration = self.end_time - self.start_time\n",
        "\n",
        "            self.logger.info(f\"Pipeline completed successfully in {duration}\")\n",
        "            self.logger.info(f\"Results saved to: {self.config.output_path}\")\n",
        "\n",
        "            return final_results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_dataset(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Load dataset and get audio file paths\"\"\"\n",
        "        audio_files = self.data_manager.get_audio_files()\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        self.logger.info(f\"Found {total_files} audio files across all categories\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            self.logger.info(f\"  {category}: {len(files)} files\")\n",
        "\n",
        "        if total_files == 0:\n",
        "            raise ValueError(\"No audio files found. Please check the dataset path.\")\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def _extract_acoustic_features(self, audio_files: Dict[str, List[str]],\n",
        "                                 parallel: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features from all audio files\"\"\"\n",
        "        all_features = {}\n",
        "\n",
        "        if parallel:\n",
        "            all_features = self._extract_acoustic_features_parallel(audio_files)\n",
        "        else:\n",
        "            all_features = self._extract_acoustic_features_sequential(audio_files)\n",
        "\n",
        "        # Save acoustic features\n",
        "        features_path = os.path.join(self.config.output_path, \"acoustic_features.pkl\")\n",
        "        with open(features_path, 'wb') as f:\n",
        "            pickle.dump(all_features, f)\n",
        "\n",
        "        self.logger.info(f\"Acoustic features saved to {features_path}\")\n",
        "        return all_features\n",
        "\n",
        "    def _extract_acoustic_features_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features using parallel processing\"\"\"\n",
        "        all_features = {}\n",
        "        max_workers = min(self.config.get('processing.max_workers', mp.cpu_count()), mp.cpu_count())\n",
        "\n",
        "        # Flatten all files with their categories\n",
        "        file_tasks = []\n",
        "        for category, files in audio_files.items():\n",
        "            for file_path in files:\n",
        "                file_tasks.append((file_path, category))\n",
        "\n",
        "        self.logger.info(f\"Processing {len(file_tasks)} files with {max_workers} workers\")\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_file = {\n",
        "                executor.submit(self.acoustic_service.extract_features, file_path): (file_path, category)\n",
        "                for file_path, category in file_tasks\n",
        "            }\n",
        "\n",
        "            # Collect results\n",
        "            completed = 0\n",
        "            for future in as_completed(future_to_file):\n",
        "                file_path, category = future_to_file[future]\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    features = future.result()\n",
        "                    if features is not None:\n",
        "                        all_features[f\"{category}_{filename}\"] = {\n",
        "                            'file_path': file_path,\n",
        "                            'category': category,\n",
        "                            'filename': filename,\n",
        "                            'features': features\n",
        "                        }\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Failed to extract features from {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "                completed += 1\n",
        "                if completed % 10 == 0:\n",
        "                    self.logger.info(f\"Completed acoustic feature extraction for {completed}/{len(file_tasks)} files\")\n",
        "\n",
        "        return all_features\n",
        "\n",
        "    def _extract_acoustic_features_sequential(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features sequentially\"\"\"\n",
        "        all_features = {}\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed = 0\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            self.logger.info(f\"Processing acoustic features for {category}...\")\n",
        "\n",
        "            for file_path in files:\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    features = self.acoustic_service.extract_features(file_path)\n",
        "                    if features is not None:\n",
        "                        all_features[f\"{category}_{filename}\"] = {\n",
        "                            'file_path': file_path,\n",
        "                            'category': category,\n",
        "                            'filename': filename,\n",
        "                            'features': features\n",
        "                        }\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Failed to extract features from {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "                processed += 1\n",
        "                if processed % 10 == 0:\n",
        "                    self.logger.info(f\"Completed {processed}/{total_files} files\")\n",
        "\n",
        "        return all_features\n",
        "\n",
        "    def _extract_transcripts(self, audio_files: Dict[str, List[str]],\n",
        "                           parallel: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Extract transcripts from all audio files\"\"\"\n",
        "        if parallel:\n",
        "            transcripts = self._extract_transcripts_parallel(audio_files)\n",
        "        else:\n",
        "            transcripts = self._extract_transcripts_sequential(audio_files)\n",
        "\n",
        "        # Save transcripts\n",
        "        self._save_transcripts(transcripts)\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_transcripts_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract transcripts using parallel processing\"\"\"\n",
        "        transcripts = {}\n",
        "        max_workers = min(self.config.get('processing.transcription_workers', 2), 4)  # Limit for memory\n",
        "\n",
        "        # Flatten all files with their categories\n",
        "        file_tasks = []\n",
        "        for category, files in audio_files.items():\n",
        "            for file_path in files:\n",
        "                file_tasks.append((file_path, category))\n",
        "\n",
        "        self.logger.info(f\"Transcribing {len(file_tasks)} files with {max_workers} workers\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_file = {\n",
        "                executor.submit(self.transcription_service.transcribe_audio, file_path): (file_path, category)\n",
        "                for file_path, category in file_tasks\n",
        "            }\n",
        "\n",
        "            # Collect results\n",
        "            completed = 0\n",
        "            for future in as_completed(future_to_file):\n",
        "                file_path, category = future_to_file[future]\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    transcript_data = future.result()\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        **transcript_data\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': '',\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "                completed += 1\n",
        "                if completed % 5 == 0:\n",
        "                    self.logger.info(f\"Completed transcription for {completed}/{len(file_tasks)} files\")\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_transcripts_sequential(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract transcripts sequentially\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            self.logger.info(f\"Transcribing {category}...\")\n",
        "\n",
        "            for file_path in files:\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    transcript_data = self.transcription_service.transcribe_audio(file_path)\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        **transcript_data\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': '',\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_linguistic_features(self, transcripts: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract linguistic features from transcripts\"\"\"\n",
        "        linguistic_features = self.linguistic_service.extract_features(transcripts)\n",
        "\n",
        "        # Save linguistic features\n",
        "        features_path = os.path.join(self.config.output_path, \"linguistic_features.pkl\")\n",
        "        with open(features_path, 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "\n",
        "        self.logger.info(f\"Linguistic features saved to {features_path}\")\n",
        "        return linguistic_features\n",
        "\n",
        "    def _save_transcripts(self, transcripts: Dict[str, Any]):\n",
        "        \"\"\"Save transcripts to various formats\"\"\"\n",
        "        transcripts_dir = os.path.join(self.config.output_path, \"transcripts\")\n",
        "        ensure_directory(transcripts_dir)\n",
        "\n",
        "        # Save individual transcript files\n",
        "        for key, data in transcripts.items():\n",
        "            if 'transcript' in data and data['transcript']:\n",
        "                filename = f\"{key}_transcript.txt\"\n",
        "                filepath = os.path.join(transcripts_dir, filename)\n",
        "\n",
        "                with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                    f.write(data['transcript'])\n",
        "\n",
        "        # Save consolidated JSON\n",
        "        json_path = os.path.join(transcripts_dir, \"all_transcripts.json\")\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save as pickle\n",
        "        pkl_path = os.path.join(transcripts_dir, \"transcripts.pkl\")\n",
        "        with open(pkl_path, 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "\n",
        "        self.logger.info(f\"Transcripts saved to {transcripts_dir}\")\n",
        "\n",
        "    def _combine_and_save_results(self, audio_files: Dict[str, List[str]],\n",
        "                                acoustic_features: Dict[str, Any],\n",
        "                                transcripts: Dict[str, Any],\n",
        "                                linguistic_features: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Combine all results and save comprehensive dataset\"\"\"\n",
        "\n",
        "        # Create comprehensive results dictionary\n",
        "        final_results = {\n",
        "            'pipeline_info': {\n",
        "                'start_time': self.start_time.isoformat(),\n",
        "                'end_time': self.end_time.isoformat() if self.end_time else None,\n",
        "                'total_files': sum(len(files) for files in audio_files.values()),\n",
        "                'categories': list(audio_files.keys()),\n",
        "                'config': self.config.to_dict()\n",
        "            },\n",
        "            'audio_files': audio_files,\n",
        "            'acoustic_features': acoustic_features,\n",
        "            'transcripts': transcripts,\n",
        "            'linguistic_features': linguistic_features\n",
        "        }\n",
        "\n",
        "        # Create summary DataFrame\n",
        "        summary_data = []\n",
        "        for key in set(acoustic_features.keys()) | set(transcripts.keys()):\n",
        "            row = {'file_id': key}\n",
        "\n",
        "            # Add acoustic info\n",
        "            if key in acoustic_features:\n",
        "                row.update({\n",
        "                    'category': acoustic_features[key]['category'],\n",
        "                    'filename': acoustic_features[key]['filename'],\n",
        "                    'has_acoustic_features': True\n",
        "                })\n",
        "\n",
        "            # Add transcript info\n",
        "            if key in transcripts:\n",
        "                row.update({\n",
        "                    'has_transcript': True,\n",
        "                    'transcript_length': len(transcripts[key].get('transcript', '')),\n",
        "                    'word_count': len(transcripts[key].get('transcript', '').split()),\n",
        "                    'language': transcripts[key].get('language', 'unknown'),\n",
        "                    'has_transcript_error': 'error' in transcripts[key]\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    'has_transcript': False,\n",
        "                    'transcript_length': 0,\n",
        "                    'word_count': 0\n",
        "                })\n",
        "\n",
        "            # Add linguistic info\n",
        "            if key in linguistic_features:\n",
        "                row.update({\n",
        "                    'has_linguistic_features': True,\n",
        "                    'unique_words': linguistic_features[key].get('unique_words', 0),\n",
        "                    'lexical_diversity': linguistic_features[key].get('lexical_diversity', 0)\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    'has_linguistic_features': False,\n",
        "                    'unique_words': 0,\n",
        "                    'lexical_diversity': 0\n",
        "                })\n",
        "\n",
        "            summary_data.append(row)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "        # Save summary\n",
        "        summary_path = os.path.join(self.config.output_path, \"pipeline_summary.csv\")\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "        # Save complete results\n",
        "        results_path = os.path.join(self.config.output_path, \"complete_results.pkl\")\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(final_results, f)\n",
        "\n",
        "        self.logger.info(\"=\"*50)\n",
        "        self.logger.info(\"PIPELINE SUMMARY\")\n",
        "        self.logger.info(\"=\"*50)\n",
        "        self.logger.info(f\"Total files processed: {len(summary_data)}\")\n",
        "        self.logger.info(f\"Files with acoustic features: {summary_df['has_acoustic_features'].sum()}\")\n",
        "        self.logger.info(f\"Files with transcripts: {summary_df['has_transcript'].sum()}\")\n",
        "        self.logger.info(f\"Files with linguistic features: {summary_df['has_linguistic_features'].sum()}\")\n",
        "        self.logger.info(f\"Average words per transcript: {summary_df['word_count'].mean():.1f}\")\n",
        "        self.logger.info(\"=\"*50)\n",
        "        self.logger.info(\"Output files:\")\n",
        "        self.logger.info(f\"  - Complete results: {results_path}\")\n",
        "        self.logger.info(f\"  - Pipeline summary: {summary_path}\")\n",
        "        self.logger.info(f\"  - Acoustic features: {os.path.join(self.config.output_path, 'acoustic_features.pkl')}\")\n",
        "        self.logger.info(f\"  - Transcripts: {os.path.join(self.config.output_path, 'transcripts/')}\")\n",
        "        self.logger.info(f\"  - Linguistic features: {os.path.join(self.config.output_path, 'linguistic_features.pkl')}\")\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def run_sample_analysis(self, max_files_per_category: int = 2):\n",
        "        \"\"\"Run pipeline on a small sample for testing\"\"\"\n",
        "        self.logger.info(f\"Running sample analysis with max {max_files_per_category} files per category\")\n",
        "\n",
        "        # Get limited audio files\n",
        "        all_audio_files = self.data_manager.get_audio_files()\n",
        "        sample_audio_files = {}\n",
        "\n",
        "        for category, files in all_audio_files.items():\n",
        "            sample_audio_files[category] = files[:max_files_per_category]\n",
        "\n",
        "        # Run pipeline on sample\n",
        "        return self.run_complete_pipeline(parallel=False)\n",
        "\n",
        "    def get_pipeline_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current pipeline status\"\"\"\n",
        "        return {\n",
        "            'start_time': self.start_time.isoformat() if self.start_time else None,\n",
        "            'end_time': self.end_time.isoformat() if self.end_time else None,\n",
        "            'is_running': self.start_time is not None and self.end_time is None,\n",
        "            'output_path': self.config.output_path,\n",
        "            'results_available': bool(self.results)\n",
        "        }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    pipeline = PipelineService()\n",
        "\n",
        "    # Run sample analysis first\n",
        "    print(\"Running sample analysis...\")\n",
        "    sample_results = pipeline.run_sample_analysis(max_files_per_category=1)\n",
        "\n",
        "    # Then run full pipeline\n",
        "    print(\"\\nRunning full pipeline...\")\n",
        "    results = pipeline.run_complete_pipeline(parallel=True)"
      ],
      "metadata": {
        "id": "L0fbaUYvivUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py - Main Application Entry Point"
      ],
      "metadata": {
        "id": "WtVwcDn6iZA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Main Application Entry Point for ADReSSo21 Speech Analysis\n",
        "Command-line interface for running the complete analysis pipeline\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "\n",
        "# Add current directory to Python path\n",
        "current_dir = Path(__file__).parent\n",
        "sys.path.append(str(current_dir))\n",
        "\n",
        "from pipeline_service import PipelineService\n",
        "from config import Config\n",
        "from utils import setup_logging\n",
        "\n",
        "\n",
        "def create_sample_config():\n",
        "    \"\"\"Create a sample configuration file for first-time setup\"\"\"\n",
        "    sample_config = {\n",
        "        \"dataset\": {\n",
        "            \"base_path\": \"C:/Users/Administrator/Desktop/Speech/ADReSSo21\",\n",
        "            \"diagnosis_train_path\": \"ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train\",\n",
        "            \"progression_train_path\": \"ADReSSo21-progression-train/ADReSSo21/progression/train\",\n",
        "            \"progression_test_path\": \"ADReSSo21-progression-test/ADReSSo21/progression/test-dist\"\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"base_path\": \"C:/Users/Administrator/Desktop/Speech/output\",\n",
        "            \"create_timestamped_folders\": True\n",
        "        },\n",
        "        \"processing\": {\n",
        "            \"max_workers\": 8,\n",
        "            \"transcription_workers\": 2,\n",
        "            \"batch_size\": 10,\n",
        "            \"enable_parallel\": True\n",
        "        },\n",
        "        \"models\": {\n",
        "            \"whisper_model\": \"base\",\n",
        "            \"wav2vec_model\": \"facebook/wav2vec2-base-960h\",\n",
        "            \"bert_model\": \"bert-base-uncased\"\n",
        "        },\n",
        "        \"features\": {\n",
        "            \"acoustic\": {\n",
        "                \"sample_rate\": 16000,\n",
        "                \"n_mfcc\": 13,\n",
        "                \"n_mels\": 80,\n",
        "                \"extract_egemaps\": True,\n",
        "                \"extract_prosodic\": True\n",
        "            },\n",
        "            \"linguistic\": {\n",
        "                \"max_sequence_length\": 512,\n",
        "                \"extract_basic_stats\": True,\n",
        "                \"extract_bert_features\": True\n",
        "            }\n",
        "        },\n",
        "        \"logging\": {\n",
        "            \"level\": \"INFO\",\n",
        "            \"file\": \"adresso_pipeline.log\",\n",
        "            \"console\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config_path = \"config.json\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(sample_config, f, indent=2)\n",
        "\n",
        "    print(f\"Sample configuration created: {config_path}\")\n",
        "    print(\"Please edit the paths in config.json to match your setup before running the pipeline.\")\n",
        "    return config_path\n",
        "\n",
        "\n",
        "def validate_paths(config: Config) -> bool:\n",
        "    \"\"\"Validate that required paths exist\"\"\"\n",
        "    base_path = config.get('dataset.base_path')\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Error: Dataset base path does not exist: {base_path}\")\n",
        "        return False\n",
        "\n",
        "    # Check for at least one of the dataset directories\n",
        "    required_subdirs = [\n",
        "        config.get('dataset.diagnosis_train_path'),\n",
        "        config.get('dataset.progression_train_path'),\n",
        "        config.get('dataset.progression_test_path')\n",
        "    ]\n",
        "\n",
        "    found_dirs = []\n",
        "    for subdir in required_subdirs:\n",
        "        full_path = os.path.join(base_path, subdir)\n",
        "        if os.path.exists(full_path):\n",
        "            found_dirs.append(subdir)\n",
        "\n",
        "    if not found_dirs:\n",
        "        print(\"Error: No valid dataset directories found!\")\n",
        "        print(f\"Checked paths under {base_path}:\")\n",
        "        for subdir in required_subdirs:\n",
        "            print(f\"  - {subdir}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Found dataset directories: {found_dirs}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def run_pipeline_command(args):\n",
        "    \"\"\"Run the complete pipeline\"\"\"\n",
        "    try:\n",
        "        # Initialize pipeline\n",
        "        pipeline = PipelineService(args.config)\n",
        "\n",
        "        # Validate configuration\n",
        "        if not validate_paths(pipeline.config):\n",
        "            return 1\n",
        "\n",
        "        # Run pipeline\n",
        "        if args.sample:\n",
        "            print(\"Running sample analysis...\")\n",
        "            results = pipeline.run_sample_analysis(max_files_per_category=args.sample_size)\n",
        "        else:\n",
        "            print(\"Running complete pipeline...\")\n",
        "            results = pipeline.run_complete_pipeline(parallel=args.parallel)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Print summary\n",
        "        total_files = results['pipeline_info']['total_files']\n",
        "        print(f\"Total files processed: {total_files}\")\n",
        "        print(f\"Output directory: {pipeline.config.output_path}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nPipeline failed with error: {str(e)}\")\n",
        "        if args.debug:\n",
        "            print(\"\\nFull traceback:\")\n",
        "            traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "\n",
        "def run_status_command(args):\n",
        "    \"\"\"Check pipeline status\"\"\"\n",
        "    try:\n",
        "        pipeline = PipelineService(args.config)\n",
        "        status = pipeline.get_pipeline_status()\n",
        "\n",
        "        print(\"Pipeline Status:\")\n",
        "        print(f\"  Output Path: {status['output_path']}\")\n",
        "        print(f\"  Is Running: {status['is_running']}\")\n",
        "        print(f\"  Results Available: {status['results_available']}\")\n",
        "\n",
        "        if status['start_time']:\n",
        "            print(f\"  Last Start Time: {status['start_time']}\")\n",
        "        if status['end_time']:\n",
        "            print(f\"  Last End Time: {status['end_time']}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking status: {str(e)}\")\n",
        "        return 1\n",
        "\n",
        "\n",
        "def run_demo_command(args):\n",
        "    \"\"\"Run demo with single file from each category\"\"\"\n",
        "    try:\n",
        "        pipeline = PipelineService(args.config)\n",
        "\n",
        "        print(\"Running demo analysis...\")\n",
        "        print(\"This will process 1 file from each available category\")\n",
        "\n",
        "        # Run with minimal files\n",
        "        results = pipeline.run_sample_analysis(max_files_per_category=1)\n",
        "\n",
        "        print(\"\\nDemo completed successfully!\")\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Demo failed: {str(e)}\")\n",
        "        if args.debug:\n",
        "            traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main application entry point\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"ADReSSo21 Speech Analysis Pipeline\",\n",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "        epilog=\"\"\"\n",
        "Examples:\n",
        "  # Create sample configuration\n",
        "  python main.py init\n",
        "\n",
        "  # Run demo analysis\n",
        "  python main.py demo\n",
        "\n",
        "  # Run complete pipeline\n",
        "  python main.py run\n",
        "\n",
        "  # Run with custom config\n",
        "  python main.py run --config my_config.json\n",
        "\n",
        "  # Run sample analysis only\n",
        "  python main.py run --sample --sample-size 2\n",
        "\n",
        "  # Run without parallel processing\n",
        "  python main.py run --no-parallel\n",
        "\n",
        "  # Check status\n",
        "  python main.py status\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Global arguments\n",
        "    parser.add_argument('--config', '-c', default='config.json',\n",
        "                       help='Configuration file path (default: config.json)')\n",
        "    parser.add_argument('--debug', action='store_true',\n",
        "                       help='Enable debug mode with full error traces')\n",
        "\n",
        "    # Subcommands\n",
        "    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n",
        "\n",
        "    # Init command\n",
        "    init_parser = subparsers.add_parser('init', help='Create sample configuration file')\n",
        "\n",
        "    # Run command\n",
        "    run_parser = subparsers.add_parser('run', help='Run the analysis pipeline')\n",
        "    run_parser.add_argument('--sample', action='store_true',\n",
        "                           help='Run on sample data only')\n",
        "    run_parser.add_argument('--sample-size', type=int, default=2,\n",
        "                           help='Number of files per category for sample run (default: 2)')\n",
        "    run_parser.add_argument('--no-parallel', dest='parallel', action='store_false',\n",
        "                           help='Disable parallel processing')\n",
        "\n",
        "    # Demo command\n",
        "    demo_parser = subparsers.add_parser('demo', help='Run demo analysis')\n",
        "\n",
        "    # Status command\n",
        "    status_parser = subparsers.add_parser('status', help='Check pipeline status')\n",
        "\n",
        "    # Parse arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Handle commands\n",
        "    if args.command == 'init':\n",
        "        create_sample_config()\n",
        "        return 0\n",
        "\n",
        "    elif args.command == 'run':\n",
        "        return run_pipeline_command(args)\n",
        "\n",
        "    elif args.command == 'demo':\n",
        "        return run_demo_command(args)\n",
        "\n",
        "    elif args.command == 'status':\n",
        "        return run_status_command(args)\n",
        "\n",
        "    else:\n",
        "        # No command specified, show help\n",
        "        parser.print_help()\n",
        "        return 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    exit_code = main()\n",
        "    sys.exit(exit_code)"
      ],
      "metadata": {
        "id": "eZi9N2LTi0HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# requirements.txt - Project Dependencies\n"
      ],
      "metadata": {
        "id": "oCpE1WnZidLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core dependencies for ADReSSo21 Speech Analysis Pipeline\n",
        "\n",
        "# Audio processing\n",
        "librosa>=0.10.0\n",
        "soundfile>=0.12.1\n",
        "opensmile>=2.4.2\n",
        "\n",
        "# Speech recognition and transcription\n",
        "openai-whisper>=20231117\n",
        "transformers>=4.35.0\n",
        "torch>=2.0.0\n",
        "torchaudio>=2.0.0\n",
        "\n",
        "# NLP and language models\n",
        "tokenizers>=0.14.0\n",
        "numpy>=1.24.0\n",
        "scipy>=1.10.0\n",
        "\n",
        "# Data handling and processing\n",
        "pandas>=2.0.0\n",
        "scikit-learn>=1.3.0\n",
        "\n",
        "# Parallel processing\n",
        "joblib>=1.3.0\n",
        "\n",
        "# Configuration and utilities\n",
        "pyyaml>=6.0\n",
        "python-dotenv>=1.0.0\n",
        "\n",
        "# Optional GPU support (uncomment if using CUDA)\n",
        "# torch>=2.0.0+cu118\n",
        "# torchaudio>=2.0.0+cu118\n",
        "\n",
        "# Development and testing (optional)\n",
        "pytest>=7.4.0\n",
        "jupyter>=1.0.0\n",
        "matplotlib>=3.7.0\n",
        "seaborn>=0.12.0\n",
        "\n"
      ],
      "metadata": {
        "id": "2bS87LZAi2YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# setup.py - Project Setup Script\n",
        "\n"
      ],
      "metadata": {
        "id": "LIlu8w1Lin9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Setup script for ADReSSo21 Speech Analysis Pipeline\n",
        "Handles installation, environment setup, and model downloads\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import json\n",
        "\n",
        "\n",
        "class PipelineSetup:\n",
        "    \"\"\"Setup and installation handler for the pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.project_root = Path(__file__).parent\n",
        "        self.system_info = {\n",
        "            'os': platform.system(),\n",
        "            'python_version': sys.version,\n",
        "            'architecture': platform.architecture()[0]\n",
        "        }\n",
        "\n",
        "    def check_system_requirements(self):\n",
        "        \"\"\"Check if system meets minimum requirements\"\"\"\n",
        "        print(\"Checking system requirements...\")\n",
        "\n",
        "        # Check Python version\n",
        "        if sys.version_info < (3, 8):\n",
        "            print(\"❌ Python 3.8+ required. Current version:\", sys.version)\n",
        "            return False\n",
        "        print(\"✅ Python version:\", sys.version.split()[0])\n",
        "\n",
        "        # Check available memory (approximate)\n",
        "        try:\n",
        "            import psutil\n",
        "            memory_gb = psutil.virtual_memory().total / (1024**3)\n",
        "            if memory_gb < 8:\n",
        "                print(f\"⚠️  Warning: Low memory detected ({memory_gb:.1f}GB). 16GB+ recommended.\")\n",
        "            else:\n",
        "                print(f\"✅ Memory: {memory_gb:.1f}GB\")\n",
        "        except ImportError:\n",
        "            print(\"⚠️  Cannot check memory (psutil not available)\")\n",
        "\n",
        "        # Check disk space\n",
        "        try:\n",
        "            disk_space = psutil.disk_usage('.').free / (1024**3)\n",
        "            if disk_space < 10:\n",
        "                print(f\"⚠️  Warning: Low disk space ({disk_space:.1f}GB). 20GB+ recommended.\")\n",
        "            else:\n",
        "                print(f\"✅ Disk space: {disk_space:.1f}GB available\")\n",
        "        except:\n",
        "            print(\"⚠️  Cannot check disk space\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def install_dependencies(self):\n",
        "        \"\"\"Install Python dependencies\"\"\"\n",
        "        print(\"\\nInstalling Python dependencies...\")\n",
        "\n",
        "        requirements_file = self.project_root / \"requirements.txt\"\n",
        "\n",
        "        if not requirements_file.exists():\n",
        "            print(\"❌ requirements.txt not found!\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Upgrade pip first\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n",
        "                         check=True)\n",
        "\n",
        "            # Install requirements\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)],\n",
        "                         check=True)\n",
        "\n",
        "            print(\"✅ Dependencies installed successfully\")\n",
        "            return True\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"❌ Failed to install dependencies: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"Create necessary directories\"\"\"\n",
        "        print(\"\\nSetting up directories...\")\n",
        "\n",
        "        directories = [\n",
        "            \"output\",\n",
        "            \"logs\",\n",
        "            \"models\",\n",
        "            \"temp\",\n",
        "            \"data\"\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            dir_path = self.project_root / directory\n",
        "            dir_path.mkdir(exist_ok=True)\n",
        "            print(f\"✅ Created/verified: {directory}/\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def download_sample_data(self):\n",
        "        \"\"\"Download sample data for testing (if available)\"\"\"\n",
        "        print(\"\\nSetting up sample data...\")\n",
        "\n",
        "        # Create a minimal sample structure for testing\n",
        "        sample_dir = self.project_root / \"data\" / \"sample\"\n",
        "        sample_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create sample directory structure\n",
        "        sample_structure = [\n",
        "            \"diagnosis/train/audio/ad\",\n",
        "            \"diagnosis/train/audio/cn\",\n",
        "            \"diagnosis/train/segmentation/ad\",\n",
        "            \"diagnosis/train/segmentation/cn\",\n",
        "            \"progression/train/audio/decline\",\n",
        "            \"progression/train/audio/no_decline\",\n",
        "            \"progression/train/segmentation/decline\",\n",
        "            \"progression/train/segmentation/no_decline\",\n",
        "            \"progression/test-dist/audio\",\n",
        "            \"progression/test-dist/segmentation\"\n",
        "        ]\n",
        "\n",
        "        for structure in sample_structure:\n",
        "            (sample_dir / structure).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create a sample README\n",
        "        readme_content = \"\"\"\n",
        "# Sample Data Directory Structure\n",
        "\n",
        "This directory contains the expected structure for ADReSSo21 dataset.\n",
        "\n",
        "## Directory Structure:\n",
        "- diagnosis/train/audio/ad/          - Alzheimer's audio files\n",
        "- diagnosis/train/audio/cn/          - Control audio files\n",
        "- diagnosis/train/segmentation/      - Segmentation files\n",
        "- progression/train/audio/           - Progression training audio\n",
        "- progression/test-dist/audio/       - Progression test audio\n",
        "\n",
        "## Usage:\n",
        "Place your actual ADReSSo21 dataset files in this structure, or update\n",
        "the paths in config.json to point to your dataset location.\n",
        "\"\"\"\n",
        "\n",
        "        with open(sample_dir / \"README.md\", \"w\") as f:\n",
        "            f.write(readme_content)\n",
        "\n",
        "        print(\"✅ Sample directory structure created\")\n",
        "        return True\n",
        "\n",
        "    def create_default_config(self):\n",
        "        \"\"\"Create default configuration file\"\"\"\n",
        "        print(\"\\nCreating default configuration...\")\n",
        "\n",
        "        config = {\n",
        "            \"dataset\": {\n",
        "                \"base_path\": str(self.project_root / \"data\" / \"sample\"),\n",
        "                \"diagnosis_train_path\": \"diagnosis/train\",\n",
        "                \"progression_train_path\": \"progression/train\",\n",
        "                \"progression_test_path\": \"progression/test-dist\"\n",
        "            },\n",
        "            \"output\": {\n",
        "                \"base_path\": str(self.project_root / \"output\"),\n",
        "                \"create_timestamped_folders\": True\n",
        "            },\n",
        "            \"processing\": {\n",
        "                \"max_workers\": min(os.cpu_count(), 8),\n",
        "                \"transcription_workers\": 2,\n",
        "                \"batch_size\": 10,\n",
        "                \"enable_parallel\": True\n",
        "            },\n",
        "            \"models\": {\n",
        "                \"whisper_model\": \"base\",\n",
        "                \"wav2vec_model\": \"facebook/wav2vec2-base-960h\",\n",
        "                \"bert_model\": \"bert-base-uncased\"\n",
        "            },\n",
        "            \"features\": {\n",
        "                \"acoustic\": {\n",
        "                    \"sample_rate\": 16000,\n",
        "                    \"n_mfcc\": 13,\n",
        "                    \"n_mels\": 80,\n",
        "                    \"extract_egemaps\": True,\n",
        "                    \"extract_prosodic\": True\n",
        "                },\n",
        "                \"linguistic\": {\n",
        "                    \"max_sequence_length\": 512,\n",
        "                    \"extract_basic_stats\": True,\n",
        "                    \"extract_bert_features\": True\n",
        "                }\n",
        "            },\n",
        "            \"logging\": {\n",
        "                \"level\": \"INFO\",\n",
        "                \"file\": \"adresso_pipeline.log\",\n",
        "                \"console\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = self.project_root / \"config.json\"\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "\n",
        "        print(f\"✅ Default configuration created: {config_path}\")\n",
        "        return True\n",
        "\n",
        "    def verify_installation(self):\n",
        "        \"\"\"Verify that installation was successful\"\"\"\n",
        "        print(\"\\nVerifying installation...\")\n",
        "\n",
        "        # Test imports\n",
        "        test_imports = [\n",
        "            'librosa',\n",
        "            'whisper',\n",
        "            'transformers',\n",
        "            'torch',\n",
        "            'opensmile',\n",
        "            'pandas',\n",
        "            'numpy'\n",
        "        ]\n",
        "\n",
        "        failed_imports = []\n",
        "        for module in test_imports:\n",
        "            try:\n",
        "                __import__(module)\n",
        "                print(f\"✅ {module}\")\n",
        "            except ImportError as e:\n",
        "                print(f\"❌ {module}: {e}\")\n",
        "                failed_imports.append(module)\n",
        "\n",
        "        if failed_imports:\n",
        "            print(f\"\\n❌ Failed to import: {failed_imports}\")\n",
        "            print(\"Please check the installation and try running:\")\n",
        "            print(\"pip install -r requirements.txt\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n✅ All modules imported successfully!\")\n",
        "        return True\n",
        "\n",
        "    def run_setup(self):\n",
        "        \"\"\"Run complete setup process\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ADReSSo21 Speech Analysis Pipeline Setup\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        steps = [\n",
        "            (\"System Requirements\", self.check_system_requirements),\n",
        "            (\"Dependencies\", self.install_dependencies),\n",
        "            (\"Directories\", self.setup_directories),\n",
        "            (\"Sample Data\", self.download_sample_data),\n",
        "            (\"Configuration\", self.create_default_config),\n",
        "            (\"Verification\", self.verify_installation)\n",
        "        ]\n",
        "\n",
        "        for step_name, step_func in steps:\n",
        "            print(f\"\\n{'='*20} {step_name} {'='*20}\")\n",
        "            if not step_func():\n",
        "                print(f\"\\n❌ Setup failed at step: {step_name}\")\n",
        "                return False\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🎉 SETUP COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Update config.json with your dataset paths\")\n",
        "        print(\"2. Run: python main.py demo\")\n",
        "        print(\"3. Run: python main.py run\")\n",
        "        print(\"\\nFor help: python main.py --help\")\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main setup function\"\"\"\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"--help\":\n",
        "        print(\"\"\"\n",
        "ADReSSo21 Pipeline Setup\n",
        "\n",
        "Usage:\n",
        "    python setup.py                 - Run complete setup\n",
        "    python setup.py --help          - Show this help\n",
        "    python setup.py --verify-only   - Only verify installation\n",
        "    python setup.py --deps-only     - Only install dependencies\n",
        "        \"\"\")\n",
        "        return\n",
        "\n",
        "    setup = PipelineSetup()\n",
        "\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"--verify-only\":\n",
        "        setup.verify_installation()\n",
        "    elif len(sys.argv) > 1 and sys.argv[1] == \"--deps-only\":\n",
        "        setup.install_dependencies()\n",
        "    else:\n",
        "        setup.run_setup()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bObYFUbxi8kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md - Project Documentation"
      ],
      "metadata": {
        "id": "Unh_01eXirx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADReSSo21 Speech Analysis Pipeline\n",
        "\n",
        "A modular, high-performance pipeline for analyzing speech data from the ADReSSo21 dataset (Alzheimer's Dementia Recognition through Spontaneous Speech). This pipeline extracts comprehensive acoustic, linguistic, and semantic features for dementia detection and progression analysis.\n",
        "\n",
        "## Features\n",
        "\n",
        "🎯 **Comprehensive Analysis**\n",
        "- Acoustic feature extraction (eGeMAPS, MFCCs, Mel-spectrograms, Wav2Vec2)\n",
        "- Speech-to-text transcription (Whisper)\n",
        "- Linguistic feature analysis (BERT embeddings, lexical diversity)\n",
        "- Prosodic analysis (F0, energy, spectral features)\n",
        "\n",
        "⚡ **High Performance**\n",
        "- Multi-core parallel processing\n",
        "- Optimized for Windows 10 with 35GB RAM, 10 cores\n",
        "- Memory-efficient batch processing\n",
        "- Modular microservice architecture\n",
        "\n",
        "🔧 **Easy to Use**\n",
        "- Command-line interface\n",
        "- Configurable via JSON\n",
        "- Sample data support\n",
        "- Comprehensive logging\n",
        "\n",
        "## System Requirements\n",
        "\n",
        "- **OS**: Windows 10/11, Linux, macOS\n",
        "- **Python**: 3.8+\n",
        "- **RAM**: 16GB+ recommended (35GB optimal)\n",
        "- **CPU**: Multi-core processor (10 cores optimal)\n",
        "- **Storage**: 20GB+ free space\n",
        "- **GPU**: Optional (CUDA-compatible for faster processing)\n",
        "\n",
        "## Installation\n",
        "\n",
        "### Quick Setup\n",
        "\n",
        "```bash\n",
        "# Clone or download the project\n",
        "git clone <repository-url>\n",
        "cd adresso21-pipeline\n",
        "\n",
        "# Run setup script\n",
        "python setup.py\n",
        "```\n",
        "\n",
        "### Manual Installation\n",
        "\n",
        "```bash\n",
        "# Install dependencies\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Create configuration\n",
        "python main.py init\n",
        "\n",
        "# Setup directories\n",
        "mkdir output logs models temp data\n",
        "```\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Edit `config.json` to match your setup:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"dataset\": {\n",
        "    \"base_path\": \"C:/Users/Administrator/Desktop/Speech/ADReSSo21\",\n",
        "    \"diagnosis_train_path\": \"ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train\",\n",
        "    \"progression_train_path\": \"ADReSSo21-progression-train/ADReSSo21/progression/train\",\n",
        "    \"progression_test_path\": \"ADReSSo21-progression-test/ADReSSo21/progression/test-dist\"\n",
        "  },\n",
        "  \"output\": {\n",
        "    \"base_path\": \"C:/Users/Administrator/Desktop/Speech/output\",\n",
        "    \"create_timestamped_folders\": true\n",
        "  },\n",
        "  \"processing\": {\n",
        "    \"max_workers\": 8,\n",
        "    \"transcription_workers\": 2,\n",
        "    \"enable_parallel\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "Ensure your ADReSSo21 dataset follows this structure:\n",
        "\n",
        "```\n",
        "ADReSSo21/\n",
        "├── diagnosis/train/\n",
        "│   ├── audio/\n",
        "│   │   ├── ad/*.wav          # Alzheimer's audio files\n",
        "│   │   └── cn/*.wav          # Control audio files\n",
        "│   └── segmentation/\n",
        "│       ├── ad/*.csv          # Alzheimer's segmentation\n",
        "│       └── cn/*.csv          # Control segmentation\n",
        "├── progression/train/\n",
        "│   ├── audio/\n",
        "│   │   ├── decline/*.wav     # Decline audio files\n",
        "│   │   └── no_decline/*.wav  # No decline audio files\n",
        "│   └── segmentation/\n",
        "│       ├── decline/*.csv     # Decline segmentation\n",
        "│       └── no_decline/*.csv  # No decline segmentation\n",
        "└── progression/test-dist/\n",
        "    ├── audio/*.wav           # Test audio files\n",
        "    └── segmentation/*.csv    # Test segmentation\n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Command Line Interface\n",
        "\n",
        "```bash\n",
        "# Initialize configuration\n",
        "python main.py init\n",
        "\n",
        "# Run demo analysis (1 file per category)\n",
        "python main.py demo\n",
        "\n",
        "# Run sample analysis (2 files per category)  \n",
        "python main.py run --sample --sample-size 2\n",
        "\n",
        "# Run complete pipeline\n",
        "python main.py run\n",
        "\n",
        "# Run without parallel processing\n",
        "python main.py run --no-parallel\n",
        "\n",
        "# Check pipeline status\n",
        "python main.py status\n",
        "\n",
        "# Custom configuration\n",
        "python main.py run --config my_config.json\n",
        "```\n",
        "\n",
        "### Python API\n",
        "\n",
        "```python\n",
        "from pipeline_service import PipelineService\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = PipelineService('config.json')\n",
        "\n",
        "# Run complete analysis\n",
        "results = pipeline.run_complete_pipeline(parallel=True)\n",
        "\n",
        "# Run sample analysis  \n",
        "results = pipeline.run_sample_analysis(max_files_per_category=2)\n",
        "\n",
        "# Check status\n",
        "status = pipeline.get_pipeline_status()\n",
        "```\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The pipeline follows a modular microservice architecture:\n",
        "\n",
        "```\n",
        "main.py                     # Entry point and CLI\n",
        "├── pipeline_service.py     # Main orchestrator\n",
        "├── config.py              # Configuration management\n",
        "├── utils.py               # Utilities and helpers\n",
        "├── data_manager_service.py          # Dataset loading\n",
        "├── acoustic_features_service.py     # Audio feature extraction\n",
        "├── transcription_service.py         # Speech-to-text\n",
        "└── linguistic_features_service.py   # Text analysis\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **PipelineService**: Main orchestrator that coordinates all services\n",
        "2. **DataManagerService**: Handles dataset loading and file management\n",
        "3. **AcousticFeaturesService**: Extracts audio features (eGeMAPS, MFCCs, etc.)\n",
        "4. **TranscriptionService**: Converts speech to text using Whisper\n",
        "5. **LinguisticFeaturesService**: Analyzes text features and BERT embeddings\n",
        "\n",
        "## Output\n",
        "\n",
        "The pipeline generates comprehensive outputs:\n",
        "\n",
        "```\n",
        "output/\n",
        "├── acoustic_features.pkl        # All acoustic features\n",
        "├── transcripts/\n",
        "│   ├── all_transcripts.json    # All transcriptions\n",
        "│   ├── transcripts.pkl         # Pickle format\n",
        "│   └── *_transcript.txt        # Individual transcripts\n",
        "├── linguistic_features.pkl     # Text analysis results\n",
        "├── pipeline_summary.csv        # Processing summary\n",
        "├── complete_results.pkl        # Combined results\n",
        "└── adresso_pipeline.log        # Processing logs\n",
        "```\n",
        "\n",
        "### Feature Types\n",
        "\n",
        "**Acoustic Features:**\n",
        "- eGeMAPS (88 features)\n",
        "- MFCCs (13 coefficients + deltas)\n",
        "- Mel-spectrograms (80 bands)\n",
        "- Wav2Vec2 embeddings (768 dimensions)\n",
        "- Prosodic features (F0, energy, spectral)\n",
        "\n",
        "**Linguistic Features:**\n",
        "- Basic statistics (word count, sentence count)\n",
        "- Lexical diversity measures\n",
        "- BERT embeddings (768 dimensions)\n",
        "- Language detection\n",
        "- Segmentation analysis\n",
        "\n",
        "## Performance\n",
        "\n",
        "Typical processing times on recommended hardware:\n",
        "\n",
        "- **Demo** (5 files): ~2-3 minutes\n",
        "- **Sample** (20 files): ~5-10 minutes  \n",
        "- **Complete dataset** (500+ files): ~2-4 hours\n",
        "\n",
        "Memory usage:\n",
        "- Base: ~2-4 GB\n",
        "- With parallel processing: ~8-12 GB\n",
        "- Peak (large files): ~16-20 GB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "**1. Import Errors**\n",
        "```bash\n",
        "# Reinstall dependencies\n",
        "pip install -r requirements.txt --force-reinstall\n",
        "```\n",
        "\n",
        "**2. Memory Issues**\n",
        "- Reduce `max_workers` in config\n",
        "- Disable parallel processing: `--no-parallel`\n",
        "- Process in smaller batches\n",
        "\n",
        "**3. Model Download Issues**\n",
        "```bash\n",
        "# Pre-download models\n",
        "python -c \"import whisper; whisper.load_model('base')\"\n",
        "python -c \"from transformers import AutoModel; AutoModel.from_pretrained('facebook/wav2vec2-base-960h')\"\n",
        "```\n",
        "\n",
        "**4. Path Issues**\n",
        "- Use absolute paths in config.json\n",
        "- Check file permissions\n",
        "- Verify dataset structure\n",
        "\n",
        "### Performance Optimization\n",
        "\n",
        "**For Limited RAM:**\n",
        "```json\n",
        "{\n",
        "  \"processing\": {\n",
        "    \"max_workers\": 4,\n",
        "    \"transcription_workers\": 1,\n",
        "    \"enable_parallel\": false\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**For High Performance:**\n",
        "```json\n",
        "{\n",
        "  \"processing\": {\n",
        "    \"max_workers\": 10,\n",
        "    \"transcription_workers\": 4,\n",
        "    \"batch_size\": 20,\n",
        "    \"enable_parallel\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## Development\n",
        "\n",
        "### Adding New Features\n",
        "\n",
        "1. Create new service in `services/`\n",
        "2. Add configuration options\n",
        "3. Update `pipeline_service.py`\n",
        "4. Add tests and documentation\n",
        "\n",
        "### Testing\n",
        "\n",
        "```bash\n",
        "# Run demo for testing\n",
        "python main.py demo\n",
        "\n",
        "# Run with debug output\n",
        "python main.py run --debug\n",
        "\n",
        "# Test specific components\n",
        "python -c \"from acoustic_features_service import AcousticFeaturesService; service = AcousticFeaturesService()\"\n",
        "```\n",
        "\n",
        "## Contributing\n",
        "\n",
        "1. Fork the repository\n",
        "2. Create a feature branch\n",
        "3. Make changes with tests\n",
        "4. Submit a pull request\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License - see the LICENSE file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this pipeline in your research, please cite:\n",
        "\n",
        "```bibtex\n",
        "@software{adresso21_pipeline,\n",
        "  title={ADReSSo21 Speech Analysis Pipeline},\n",
        "  author={Your Name},\n",
        "  year={2024},\n",
        "  url={https://github.com/your-repo/adresso21-pipeline}\n",
        "}\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "- ADReSSo21 Dataset creators\n",
        "- OpenAI Whisper team\n",
        "- Hugging Face Transformers\n",
        "- OpenSMILE developers\n",
        "\n",
        "## Support\n",
        "\n",
        "For support and questions:\n",
        "- Check the troubleshooting section\n",
        "- Review logs in `output/adresso_pipeline.log`\n",
        "- Open an issue on GitHub\n",
        "- Contact: your.email@domain.com"
      ],
      "metadata": {
        "id": "Wzy6IAhAi-oN"
      }
    }
  ]
}