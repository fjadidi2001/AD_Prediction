{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWnip2JB0oI7Agw637g7EI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Workflow for Detecting Dementia from Speech and Transcripts Using Transformers\n",
        "\n",
        "## 1. Data Preparation\n",
        "\n",
        "* **Dataset**: Utilize the **ADReSS Challenge Dataset**, consisting of 78 AD and 78 non-AD patients. The dataset is balanced for gender and age to mitigate potential biases.\n",
        "\n",
        "* **Speech Data**:\n",
        "  Convert audio files into images with three channels:\n",
        "\n",
        "  * Log-Mel spectrograms (or Mel-frequency cepstral coefficients - **MFCCs**)\n",
        "  * **Delta** features\n",
        "  * **Delta-delta** features\n",
        "    These dynamic features incorporate temporal information into the static cepstral features.\n",
        "\n",
        "* **Transcript Data**:\n",
        "  Obtain corresponding **textual transcripts** of the audio files for linguistic analysis.\n",
        "\n",
        "* **Data Splitting**:\n",
        "  Divide the dataset as follows:\n",
        "\n",
        "  * **Training Set**: 65%\n",
        "  * **Validation Set**: 35%\n",
        "    A separate **test set** is provided by the ADReSS Challenge.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Feature Extraction\n",
        "\n",
        "### Acoustic Features\n",
        "\n",
        "* Convert audio files into log-Mel spectrograms or MFCCs with delta and delta-delta components.\n",
        "* Stack these into **three-channel image representations**.\n",
        "* Feed the images into a **Vision Transformer (ViT)**.\n",
        "  ViT is selected as the best-performing model after evaluating alternatives like AlexNet, VGG16, DenseNet, and EfficientNet.\n",
        "\n",
        "### Textual Features\n",
        "\n",
        "* Process transcripts using **BERT (base, uncased)** to extract **contextualized text embeddings**.\n",
        "\n",
        "> **Rationale**:\n",
        ">\n",
        "> * Delta and delta-delta features enrich speech dynamics.\n",
        "> * Transformer architectures (ViT and BERT) provide robust feature representations for images and text, respectively.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Model Architecture\n",
        "\n",
        "### Unimodal Models\n",
        "\n",
        "* **Speech-Only**:\n",
        "  Evaluate pretrained models (AlexNet, VGG16, DenseNet, EfficientNet, ViT) using the three-channel images.\n",
        "  **ViT** is the top performer.\n",
        "\n",
        "* **Text-Only**:\n",
        "  Train a **BERT model** on transcripts to establish a baseline for AD detection from text.\n",
        "\n",
        "### Multimodal Models\n",
        "\n",
        "* **BERT + ViT**:\n",
        "  Concatenate the features from BERT and ViT for a baseline multimodal model.\n",
        "\n",
        "* **BERT + ViT + Gated Multimodal Unit (GMU)**:\n",
        "  Introduce a **GMU** to assign dynamic weights to each modality, suppressing irrelevant information.\n",
        "  Inspired by gating mechanisms in GRU/LSTM.\n",
        "\n",
        "* **BERT + ViT + Crossmodal Attention**:\n",
        "  Use **crossmodal attention mechanisms** to model interactions between speech and text:\n",
        "\n",
        "  * **Text-to-Image Attention**\n",
        "  * **Image-to-Text Attention**\n",
        "\n",
        "  Concatenate attention outputs, apply global average pooling, and feed into a final dense layer for **binary classification** (AD vs. non-AD).\n",
        "\n",
        "> **Rationale**:\n",
        ">\n",
        "> * GMU enables adaptive modality fusion.\n",
        "> * Crossmodal attention outperforms both early and late fusion by capturing fine-grained intermodal relationships.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model Training\n",
        "\n",
        "* **Environment**:\n",
        "  Tesla P100-PCIE-16GB GPU, using **PyTorch**.\n",
        "\n",
        "* **Optimization**:\n",
        "\n",
        "  * **Optimizer**: Adam (learning rate: `1e-5`)\n",
        "  * **Learning Rate Scheduler**: ReduceLROnPlateau (factor: `0.1`, patience: `3 epochs`)\n",
        "  * **EarlyStopping**: Triggered if validation loss doesn't improve for `6 epochs`\n",
        "  * **Loss Function**: Cross-entropy loss\n",
        "\n",
        "* **Training Protocol**:\n",
        "\n",
        "  * Each model is trained **five times**.\n",
        "  * Average the results to reduce variability.\n",
        "  * Use the validation set for monitoring and the ADReSS **test set** for final evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Evaluation\n",
        "\n",
        "* **Metrics**:\n",
        "\n",
        "  * Accuracy\n",
        "  * Precision\n",
        "  * Recall\n",
        "  * F1-Score\n",
        "  * Specificity\n",
        "    *(With dementia class as the **positive class**)*\n",
        "\n",
        "* **Comparison**:\n",
        "\n",
        "  * **Unimodal**: Compare ViT and BERT with other SOTA approaches.\n",
        "  * **Multimodal**: Compare baseline (BERT+ViT), GMU, and Crossmodal Attention models with traditional fusion strategies.\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "* **ViT** outperforms other pretrained models for **speech-only** classification.\n",
        "* **BERT + ViT + Crossmodal Attention** achieves the **highest performance**:\n",
        "\n",
        "  * **Accuracy**:\n",
        "\n",
        "    * 88.33% with log-Mel spectrograms\n",
        "    * 87.92% with MFCCs\n",
        "  * **F1-Score**:\n",
        "\n",
        "    * 88.69% with log-Mel spectrograms\n",
        "    * 87.99% with MFCCs\n",
        "* Crossmodal attention exceeds GMU and concatenation by:\n",
        "\n",
        "  * **Accuracy** improvement: +3.13% to +15.41%\n",
        "  * **F1-Score** improvement: +3.29% to +18.93%\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Analysis and Discussion\n",
        "\n",
        "### Limitations\n",
        "\n",
        "* The ADReSS dataset is **relatively small** (156 samples).\n",
        "* **Concatenation-based fusion** treats modalities equally, which is suboptimal.\n",
        "* GMU controls information flow but lacks the capability to model **crossmodal interactions** effectively.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "* **ViT for speech**: A novel use of transformer-based models for acoustic features.\n",
        "* **Crossmodal attention**: Dynamically models inter-modal interactions, overcoming early/late fusion drawbacks.\n",
        "\n",
        "### Future Work\n",
        "\n",
        "* Explore **optimal transport** methods for modality fusion.\n",
        "* Investigate **wav2vec 2.0** for creating speech image representations.\n",
        "* Expand dataset size for greater **model robustness**.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Conclusion\n",
        "\n",
        "This workflow demonstrates an effective approach for **Alzheimer’s Disease (AD)** detection using **transformer-based multimodal architectures**. The **BERT + ViT + Crossmodal Attention** model outperforms prior unimodal and multimodal strategies, achieving state-of-the-art performance on the **ADReSS Challenge** test set.\n",
        "\n",
        "> This work advances AD detection by improving feature extraction, modality fusion, and intermodal interaction modeling—key steps toward more accurate and interpretable dementia prediction systems.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uGrY4i1XgmGT"
      }
    }
  ]
}