{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFSVjlbOGEjZG4u32CyWJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Set Up Google Colab Environment"
      ],
      "metadata": {
        "id": "uM-qAsB1Ddqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyzHclW0CDnB",
        "outputId": "ae64fa1c-054c-4b00-c6bf-8765bdc7daf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Extracted ADReSSo21-diagnosis-train.tgz\n",
            "Extracted ADReSSo21-progression-test.tgz\n",
            "Extracted ADReSSo21-progression-train.tgz\n",
            "GPU Available: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import tarfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install librosa\n",
        "!pip install numpy pandas scikit-learn\n",
        "!pip install matplotlib\n",
        "\n",
        "# Extract datasets\n",
        "data_dir = '/content/drive/MyDrive/Voice/'\n",
        "extract_dir = '/content/ADReSSo21/'\n",
        "\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "datasets = [\n",
        "    'ADReSSo21-diagnosis-train.tgz',\n",
        "    'ADReSSo21-progression-test.tgz',\n",
        "    'ADReSSo21-progression-train.tgz'\n",
        "]\n",
        "\n",
        "for dataset in datasets:\n",
        "    tar_path = os.path.join(data_dir, dataset)\n",
        "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "        tar.extractall(extract_dir)\n",
        "    print(f\"Extracted {dataset}\")\n",
        "\n",
        "# Verify GPU availability\n",
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prepare the Dataset"
      ],
      "metadata": {
        "id": "Y0EkpbDAEF_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "\n",
        "# Define base path\n",
        "base_dir = '/content/ADReSSo21/ADReSSo21/'\n",
        "train_base_dir = os.path.join(base_dir, 'diagnosis/train')\n",
        "test_base_dir = os.path.join(base_dir, 'progression/test-dist')\n",
        "output_dir = '/content/ADReSSo21/'  # Directory to save pickle files\n",
        "\n",
        "# Function to extract log-Mel spectrogram and MFCCs with delta and delta-delta\n",
        "def extract_audio_features(audio_path, sr=16000, n_mels=128, n_mfcc=13):\n",
        "    # Load audio\n",
        "    y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # Log-Mel spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    # MFCCs\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "    # Delta and delta-delta\n",
        "    delta_mfcc = librosa.feature.delta(mfcc)\n",
        "    delta_delta_mfcc = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    # Stack features as 3-channel image\n",
        "    log_mel_image = np.stack([log_mel_spec, librosa.feature.delta(log_mel_spec), librosa.feature.delta(log_mel_spec, order=2)], axis=-1)\n",
        "    mfcc_image = np.stack([mfcc, delta_mfcc, delta_delta_mfcc], axis=-1)\n",
        "\n",
        "    return log_mel_image, mfcc_image\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(train_base_dir, test_base_dir=None, audio_only=False):\n",
        "    data = []\n",
        "    train_audio_dir = os.path.join(train_base_dir, 'audio')\n",
        "    train_transcript_dir = os.path.join(train_base_dir, 'segmentation')\n",
        "\n",
        "    # Debug: List directory contents\n",
        "    print(\"Checking train base directory:\", train_base_dir)\n",
        "    if os.path.exists(train_base_dir):\n",
        "        print(\"Files in train base:\", os.listdir(train_base_dir))\n",
        "        print(\"Audio subfolders:\", os.listdir(train_audio_dir))\n",
        "        print(\"Segmentation subfolders:\", os.listdir(train_transcript_dir))\n",
        "    else:\n",
        "        print(\"Train base directory does not exist:\", train_base_dir)\n",
        "\n",
        "    # List potential transcript files\n",
        "    transcript_candidates = []\n",
        "    for ext in ['*.cha', '*.txt', '*.transcript', '*.par', '*.TextGrid']:\n",
        "        transcript_candidates.extend(glob.glob(os.path.join(train_base_dir, '**', ext), recursive=True))\n",
        "    print(\"Potential transcript files:\", transcript_candidates[:10])\n",
        "\n",
        "    # Check for metadata file\n",
        "    metadata_file = None\n",
        "    for fname in ['diagnosis.csv', 'metadata.csv', 'labels.csv', 'adresso-train-mmse-scores.csv']:\n",
        "        if os.path.exists(os.path.join(train_base_dir, fname)):\n",
        "            metadata_file = os.path.join(train_base_dir, fname)\n",
        "            break\n",
        "\n",
        "    if metadata_file:\n",
        "        print(\"Found metadata file:\", metadata_file)\n",
        "        metadata = pd.read_csv(metadata_file)\n",
        "        print(\"Metadata columns:\", metadata.columns.tolist())\n",
        "        print(\"Unique dx values:\", metadata['dx'].unique())\n",
        "        for _, row in metadata.iterrows():\n",
        "            audio_id = str(row.get('adressfname', ''))\n",
        "            if not audio_id:\n",
        "                continue\n",
        "            dx = str(row.get('dx', '')).lower()\n",
        "            subfolder = 'ad' if dx == 'ad' else 'cn'\n",
        "            audio_file = os.path.join(train_audio_dir, subfolder, f\"{audio_id}.wav\")\n",
        "            # Try multiple transcript naming conventions\n",
        "            transcript_files = [\n",
        "                os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.cha\"),\n",
        "                os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.txt\"),\n",
        "                os.path.join(train_transcript_dir, subfolder, f\"S{audio_id[-3:]}.cha\"),\n",
        "                os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.transcript\"),\n",
        "                os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.par\"),\n",
        "                os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.TextGrid\"),\n",
        "                os.path.join(train_audio_dir, subfolder, f\"{audio_id}.cha\"),\n",
        "                os.path.join(train_audio_dir, subfolder, f\"{audio_id}.txt\"),\n",
        "                os.path.join(train_transcript_dir, f\"{audio_id}.cha\"),\n",
        "                os.path.join(train_audio_dir, f\"{audio_id}.cha\")\n",
        "            ]\n",
        "            transcript_file = None\n",
        "            for tf in transcript_files:\n",
        "                if os.path.exists(tf):\n",
        "                    transcript_file = tf\n",
        "                    break\n",
        "            if os.path.exists(audio_file) and (transcript_file or audio_only):\n",
        "                label = 1 if dx == 'ad' else 0\n",
        "                data.append({\n",
        "                    'audio_path': audio_file,\n",
        "                    'transcript_path': transcript_file if transcript_file else None,\n",
        "                    'label': label\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Missing pair for ID {audio_id} (dx={dx}): Audio exists={os.path.exists(audio_file)}, Transcript exists={transcript_file is not None}\")\n",
        "        print(f\"Loaded {len(data)} samples from metadata\")\n",
        "\n",
        "    # Fallback: Pair audio files without metadata\n",
        "    else:\n",
        "        print(\"No metadata file found, pairing audio files\")\n",
        "        for subfolder in ['ad', 'cn']:\n",
        "            audio_files = glob.glob(os.path.join(train_audio_dir, subfolder, '*.wav'))\n",
        "            print(f\"Found {len(audio_files)} audio files in {train_audio_dir}/{subfolder}\")\n",
        "            print(\"Sample audio files:\", audio_files[:5])\n",
        "            for audio_file in audio_files:\n",
        "                audio_id = os.path.basename(audio_file).replace('.wav', '')\n",
        "                transcript_files = [\n",
        "                    os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.cha\"),\n",
        "                    os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.txt\"),\n",
        "                    os.path.join(train_transcript_dir, subfolder, f\"S{audio_id[-3:]}.cha\"),\n",
        "                    os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.transcript\"),\n",
        "                    os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.par\"),\n",
        "                    os.path.join(train_transcript_dir, subfolder, f\"{audio_id}.TextGrid\"),\n",
        "                    os.path.join(train_audio_dir, subfolder, f\"{audio_id}.cha\"),\n",
        "                    os.path.join(train_audio_dir, subfolder, f\"{audio_id}.txt\")\n",
        "                ]\n",
        "                transcript_file = None\n",
        "                for tf in transcript_files:\n",
        "                    if os.path.exists(tf):\n",
        "                        transcript_file = tf\n",
        "                        break\n",
        "                if transcript_file or audio_only:\n",
        "                    label = 1 if subfolder == 'ad' else 0\n",
        "                    data.append({\n",
        "                        'audio_path': audio_file,\n",
        "                        'transcript_path': transcript_file if transcript_file else None,\n",
        "                        'label': label\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"No transcript found for {audio_id} in {subfolder}. Checked: {transcript_files}\")\n",
        "        print(f\"Loaded {len(data)} samples from file pairing\")\n",
        "\n",
        "    train_df = pd.DataFrame(data)\n",
        "    print(f\"Total samples loaded: {len(train_df)}\")\n",
        "\n",
        "    # Debug: Show sample data\n",
        "    if not train_df.empty:\n",
        "        print(\"Sample data:\", train_df.head().to_dict())\n",
        "\n",
        "    # Check if train_df is empty\n",
        "    if train_df.empty:\n",
        "        audio_samples = glob.glob(os.path.join(train_audio_dir, '**/*.wav'), recursive=True)\n",
        "        raise ValueError(f\"No valid audio-transcript pairs found. Check directories:\\n- Audio: {train_audio_dir}\\n- Transcripts: {train_transcript_dir}\\nRun '!ls -R /content/ADReSSo21/ADReSSo21/diagnosis/train/' to inspect.\\nSample audio files: {audio_samples[:5]}\\nPotential transcripts: {transcript_candidates[:5]}\")\n",
        "\n",
        "    # Split train and validation (65%-35%)\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.35, random_state=42)\n",
        "\n",
        "    # Load test data\n",
        "    test_df = pd.DataFrame()\n",
        "    if test_base_dir and os.path.exists(test_base_dir):\n",
        "        data = []\n",
        "        test_audio_dir = os.path.join(test_base_dir, 'audio')\n",
        "        test_transcript_dir = os.path.join(test_base_dir, 'segmentation')\n",
        "        audio_files = glob.glob(os.path.join(test_audio_dir, '**/*.wav'), recursive=True)\n",
        "        print(f\"Found {len(audio_files)} test audio files in {test_audio_dir}\")\n",
        "        for audio_file in audio_files:\n",
        "            audio_id = os.path.basename(audio_file).replace('.wav', '')\n",
        "            transcript_files = [\n",
        "                os.path.join(test_transcript_dir, f\"{audio_id}.cha\"),\n",
        "                os.path.join(test_transcript_dir, f\"{audio_id}.txt\"),\n",
        "                os.path.join(test_audio_dir, f\"{audio_id}.cha\"),\n",
        "                os.path.join(test_audio_dir, f\"{audio_id}.txt\")\n",
        "            ]\n",
        "            transcript_file = None\n",
        "            for tf in transcript_files:\n",
        "                if os.path.exists(tf):\n",
        "                    transcript_file = tf\n",
        "                    break\n",
        "            if transcript_file or audio_only:\n",
        "                label = 0  # Placeholder\n",
        "                data.append({\n",
        "                    'audio_path': audio_file,\n",
        "                    'transcript_path': transcript_file if transcript_file else None,\n",
        "                    'label': label\n",
        "                })\n",
        "        test_df = pd.DataFrame(data)\n",
        "        print(f\"Test samples loaded: {len(test_df)}\")\n",
        "\n",
        "    # Save dataframes as pickle files\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    train_df.to_pickle(os.path.join(output_dir, 'train_df.pkl'))\n",
        "    val_df.to_pickle(os.path.join(output_dir, 'val_df.pkl'))\n",
        "    test_df.to_pickle(os.path.join(output_dir, 'test_df.pkl'))\n",
        "    print(f\"Saved dataframes to {output_dir}: train_df.pkl, val_df.pkl, test_df.pkl\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Preprocess dataset\n",
        "try:\n",
        "    # Keep audio_only=True since transcripts are missing\n",
        "    train_df, val_df, test_df = load_dataset(train_base_dir, test_base_dir, audio_only=True)\n",
        "    print(\"Training samples:\", len(train_df))\n",
        "    print(\"Validation samples:\", len(val_df))\n",
        "    print(\"Test samples:\", len(test_df))\n",
        "except ValueError as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76MG5_B3DO6M",
        "outputId": "1f6b223a-7be7-4dbb-a2de-e3d3466ab355"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking train base directory: /content/ADReSSo21/ADReSSo21/diagnosis/train\n",
            "Files in train base: ['segmentation', 'audio', 'adresso-train-mmse-scores.csv']\n",
            "Audio subfolders: ['ad', 'cn']\n",
            "Segmentation subfolders: ['ad', 'cn']\n",
            "Potential transcript files: []\n",
            "Found metadata file: /content/ADReSSo21/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv\n",
            "Metadata columns: ['Unnamed: 0', 'adressfname', 'mmse', 'dx']\n",
            "Unique dx values: ['ad' 'cn']\n",
            "Loaded 166 samples from metadata\n",
            "Total samples loaded: 166\n",
            "Sample data: {'audio_path': {0: '/content/ADReSSo21/ADReSSo21/diagnosis/train/audio/ad/adrso024.wav', 1: '/content/ADReSSo21/ADReSSo21/diagnosis/train/audio/ad/adrso025.wav', 2: '/content/ADReSSo21/ADReSSo21/diagnosis/train/audio/ad/adrso027.wav', 3: '/content/ADReSSo21/ADReSSo21/diagnosis/train/audio/ad/adrso028.wav', 4: '/content/ADReSSo21/ADReSSo21/diagnosis/train/audio/ad/adrso031.wav'}, 'transcript_path': {0: None, 1: None, 2: None, 3: None, 4: None}, 'label': {0: 1, 1: 1, 2: 1, 3: 1, 4: 1}}\n",
            "Found 32 test audio files in /content/ADReSSo21/ADReSSo21/progression/test-dist/audio\n",
            "Test samples loaded: 32\n",
            "Saved dataframes to /content/ADReSSo21/: train_df.pkl, val_df.pkl, test_df.pkl\n",
            "Training samples: 107\n",
            "Validation samples: 59\n",
            "Test samples: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, BertModel, BertTokenizer\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/ADReSSo21/'\n",
        "\n",
        "class ADReSSoDataset(Dataset):\n",
        "    def __init__(self, dataframe, vit_model, bert_model, tokenizer):\n",
        "        self.dataframe = dataframe\n",
        "        self.vit_model = vit_model\n",
        "        self.bert_model = bert_model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataframe.iloc[idx]\n",
        "        try:\n",
        "            log_mel_image, mfcc_image = extract_audio_features(item['audio_path'])\n",
        "            text_features = torch.zeros(768)\n",
        "            if item['transcript_path'] is not None:\n",
        "                text = clean_cha_file(item['transcript_path'])\n",
        "                encoding = self.tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.bert_model(**encoding)\n",
        "                    text_features = outputs.last_hidden_state[:, 0, :]\n",
        "            return {\n",
        "                'log_mel_image': torch.tensor(log_mel_image).permute(2, 0, 1),  # [C, H, W]\n",
        "                'mfcc_image': torch.tensor(mfcc_image).permute(2, 0, 1),\n",
        "                'text_features': text_features,\n",
        "                'label': torch.tensor(item['label'], dtype=torch.long)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {item['audio_path']}: {e}\")\n",
        "            return None\n",
        "\n",
        "# Custom collate function to filter None items\n",
        "def custom_collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if not batch:\n",
        "        raise ValueError(\"Empty batch after filtering\")\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# Placeholder for clean_cha_file\n",
        "def clean_cha_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Updated extract_audio_features with fixed length\n",
        "def extract_audio_features(audio_path, sr=16000, n_mels=128, n_mfcc=13, max_frames=1000):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=sr)\n",
        "        # Log-Mel spectrogram\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        # MFCCs\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        delta_mfcc = librosa.feature.delta(mfcc)\n",
        "        delta_delta_mfcc = librosa.feature.delta(mfcc, order=2)\n",
        "        # Pad or truncate\n",
        "        def pad_or_truncate(feature, max_frames):\n",
        "            h, w = feature.shape\n",
        "            if w < max_frames:\n",
        "                padded = np.zeros((h, max_frames))\n",
        "                padded[:, :w] = feature\n",
        "                return padded\n",
        "            return feature[:, :max_frames]\n",
        "        log_mel_spec = pad_or_truncate(log_mel_spec, max_frames)\n",
        "        delta_mel = pad_or_truncate(librosa.feature.delta(log_mel_spec), max_frames)\n",
        "        delta_delta_mel = pad_or_truncate(librosa.feature.delta(log_mel_spec, order=2), max_frames)\n",
        "        mfcc = pad_or_truncate(mfcc, max_frames)\n",
        "        delta_mfcc = pad_or_truncate(delta_mfcc, max_frames)\n",
        "        delta_delta_mfcc = pad_or_truncate(delta_delta_mfcc, max_frames)\n",
        "        # Stack features\n",
        "        log_mel_image = np.stack([log_mel_spec, delta_mel, delta_delta_mel], axis=-1)\n",
        "        mfcc_image = np.stack([mfcc, delta_mfcc, delta_delta_mfcc], axis=-1)\n",
        "        return log_mel_image, mfcc_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio {audio_path}: {e}\")\n",
        "        raise\n",
        "\n",
        "class ADReSSoModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ADReSSoModel, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.fc = nn.Linear(768 + 768, 2)\n",
        "\n",
        "    def forward(self, log_mel_image, mfcc_image, text_features):\n",
        "        vit_outputs = self.vit(pixel_values=log_mel_image)\n",
        "        vit_features = vit_outputs.last_hidden_state[:, 0, :]\n",
        "        combined_features = torch.cat([vit_features, text_features], dim=-1)\n",
        "        output = self.fc(combined_features)\n",
        "        return output\n",
        "\n",
        "def train_model(train_df, val_df):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224').to(device)\n",
        "    bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    train_dataset = ADReSSoDataset(train_df, vit_model, bert_model, tokenizer)\n",
        "    val_dataset = ADReSSoDataset(val_df, vit_model, bert_model, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=custom_collate_fn)\n",
        "\n",
        "    model = ADReSSoModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            log_mel_image = batch['log_mel_image'].to(device)\n",
        "            mfcc_image = batch['mfcc_image'].to(device)\n",
        "            text_features = batch['text_features'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(log_mel_image, mfcc_image, text_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            log_mel_image = batch['log_mel_image'].to(device)\n",
        "            mfcc_image = batch['mfcc_image'].to(device)\n",
        "            text_features = batch['text_features'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(log_mel_image, mfcc_image, text_features)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Validation Accuracy: {accuracy}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_pickle(os.path.join(data_dir, 'train_df.pkl'))\n",
        "    val_df = pd.read_pickle(os.path.join(data_dir, 'val_df.pkl'))\n",
        "    test_df = pd.read_pickle(os.path.join(data_dir, 'test_df.pkl'))\n",
        "    print(\"Loaded dataframes:\", len(train_df), len(val_df), len(test_df))\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading pickle files: {e}\")\n",
        "    os.system(f\"ls -l {data_dir}\")\n",
        "    raise\n",
        "\n",
        "model = train_model(train_df, val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "dJMOUBrmO52_",
        "outputId": "0c0c56e1-220c-4b19-ae5f-02270b91f1db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataframes: 107 59 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input image size (128*1000) doesn't match model (224*224).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3ca9cf04cafc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-3ca9cf04cafc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_df, val_df)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_mel_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmfcc_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-3ca9cf04cafc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_mel_image, mfcc_image, text_features)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_mel_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmfcc_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mvit_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_mel_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mvit_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mcombined_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvit_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool_masked_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    122\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m    123\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbool_masked_pos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    180\u001b[0m                     \u001b[0;34mf\"Input image size ({height}*{width}) doesn't match model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0;34mf\" ({self.image_size[0]}*{self.image_size[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input image size (128*1000) doesn't match model (224*224)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfvo_ybKPP1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}