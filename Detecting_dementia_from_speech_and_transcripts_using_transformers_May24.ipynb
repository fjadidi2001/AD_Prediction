{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Environment Setup and Dependencies\n",
        "# Run this cell first to install all required packages\n",
        "\n",
        "!pip install transformers torch torchvision torchaudio\n",
        "!pip install librosa pandas numpy scikit-learn\n",
        "!pip install datasets accelerate\n",
        "\n",
        "# Import all necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel,\n",
        "    ViTFeatureExtractor, ViTModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive (uncomment if using Google Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Q_nW-B5vcu",
        "outputId": "15b4d094-10f7-4032-f7f9-f8650f44cf04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Using device: cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Extraction and Organization\n",
        "\n",
        "def extract_tgz_archive(archive_path, extract_to):\n",
        "    \"\"\"\n",
        "    Extract .tgz archive to specified directory\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "            tar.extractall(path=extract_to)\n",
        "        print(f\"Successfully extracted {archive_path} to {extract_to}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {archive_path}: {str(e)}\")\n",
        "\n",
        "def collect_adresso_data(base_path, task_type, split_type):\n",
        "    \"\"\"\n",
        "    Collect ADReSSo data and create DataFrame with file paths and labels\n",
        "\n",
        "    Args:\n",
        "        base_path: Path to extracted ADReSSo data\n",
        "        task_type: 'diagnosis' or 'progression'\n",
        "        split_type: 'train' or 'test'\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with columns: audio_path, transcript_path, label, participant_id\n",
        "    \"\"\"\n",
        "    data_list = []\n",
        "\n",
        "    # Define paths - try multiple possible directory structures\n",
        "    possible_paths = [\n",
        "        os.path.join(base_path, f\"ADReSSo21-{task_type}-{split_type}\"),\n",
        "        os.path.join(base_path, f\"ADReSSo21/{task_type}/{split_type}\"),\n",
        "        os.path.join(base_path, f\"{task_type}-{split_type}\"),\n",
        "        os.path.join(base_path, task_type, split_type)\n",
        "    ]\n",
        "\n",
        "    data_root = None\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            data_root = path\n",
        "            break\n",
        "\n",
        "    if data_root is None:\n",
        "        print(f\"Could not find data directory for {task_type}-{split_type}\")\n",
        "        print(f\"Searched in: {possible_paths}\")\n",
        "        print(f\"Available directories in {base_path}:\")\n",
        "        if os.path.exists(base_path):\n",
        "            for item in os.listdir(base_path):\n",
        "                item_path = os.path.join(base_path, item)\n",
        "                if os.path.isdir(item_path):\n",
        "                    print(f\"  - {item}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"Found data root: {data_root}\")\n",
        "\n",
        "    # Look for audio and transcript directories\n",
        "    audio_base = os.path.join(data_root, \"audio\")\n",
        "    transcript_base = os.path.join(data_root, \"segmentation\")\n",
        "\n",
        "    # If not found, try alternative names\n",
        "    if not os.path.exists(audio_base):\n",
        "        for alt_name in [\"Audio\", \"wav\", \"sound\"]:\n",
        "            alt_path = os.path.join(data_root, alt_name)\n",
        "            if os.path.exists(alt_path):\n",
        "                audio_base = alt_path\n",
        "                break\n",
        "\n",
        "    if not os.path.exists(transcript_base):\n",
        "        for alt_name in [\"transcripts\", \"text\", \"csv\", \"Transcripts\"]:\n",
        "            alt_path = os.path.join(data_root, alt_name)\n",
        "            if os.path.exists(alt_path):\n",
        "                transcript_base = alt_path\n",
        "                break\n",
        "\n",
        "    print(f\"Audio directory: {audio_base} (exists: {os.path.exists(audio_base)})\")\n",
        "    print(f\"Transcript directory: {transcript_base} (exists: {os.path.exists(transcript_base)})\")\n",
        "\n",
        "    if not os.path.exists(audio_base) or not os.path.exists(transcript_base):\n",
        "        print(\"Could not find both audio and transcript directories\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Define label mapping based on task type\n",
        "    if task_type == 'diagnosis':\n",
        "        label_dirs = ['ad', 'cn']  # Alzheimer's Disease, Cognitively Normal\n",
        "    else:  # progression\n",
        "        label_dirs = ['decline', 'no_decline']\n",
        "\n",
        "    # Check if label subdirectories exist, if not, process all files in the base directory\n",
        "    has_label_subdirs = any(os.path.exists(os.path.join(audio_base, label_dir)) for label_dir in label_dirs)\n",
        "\n",
        "    if has_label_subdirs:\n",
        "        print(\"Found label subdirectories\")\n",
        "        for label_dir in label_dirs:\n",
        "            audio_dir = os.path.join(audio_base, label_dir)\n",
        "            transcript_dir = os.path.join(transcript_base, label_dir)\n",
        "\n",
        "            if os.path.exists(audio_dir) and os.path.exists(transcript_dir):\n",
        "                # Get all audio files\n",
        "                audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
        "\n",
        "                for audio_file in audio_files:\n",
        "                    # Extract participant ID (assuming format like 'S001.wav')\n",
        "                    participant_id = audio_file.split('.')[0]\n",
        "\n",
        "                    # Find corresponding transcript file\n",
        "                    transcript_file = f\"{participant_id}.csv\"\n",
        "\n",
        "                    audio_path = os.path.join(audio_dir, audio_file)\n",
        "                    transcript_path = os.path.join(transcript_dir, transcript_file)\n",
        "\n",
        "                    # Check if both files exist\n",
        "                    if os.path.exists(audio_path) and os.path.exists(transcript_path):\n",
        "                        data_list.append({\n",
        "                            'audio_path': audio_path,\n",
        "                            'transcript_path': transcript_path,\n",
        "                            'label': label_dir,\n",
        "                            'participant_id': participant_id\n",
        "                        })\n",
        "    else:\n",
        "        print(\"No label subdirectories found, processing all files\")\n",
        "        # Process all files in the base directories\n",
        "        if os.path.exists(audio_base):\n",
        "            audio_files = [f for f in os.listdir(audio_base) if f.endswith('.wav')]\n",
        "\n",
        "            for audio_file in audio_files:\n",
        "                participant_id = audio_file.split('.')[0]\n",
        "\n",
        "                # Look for corresponding transcript\n",
        "                transcript_file = f\"{participant_id}.csv\"\n",
        "                transcript_path = os.path.join(transcript_base, transcript_file)\n",
        "\n",
        "                if not os.path.exists(transcript_path):\n",
        "                    # Try alternative extensions\n",
        "                    for ext in ['.txt', '.tsv']:\n",
        "                        alt_transcript = f\"{participant_id}{ext}\"\n",
        "                        alt_path = os.path.join(transcript_base, alt_transcript)\n",
        "                        if os.path.exists(alt_path):\n",
        "                            transcript_path = alt_path\n",
        "                            break\n",
        "\n",
        "                audio_path = os.path.join(audio_base, audio_file)\n",
        "\n",
        "                if os.path.exists(transcript_path):\n",
        "                    # Default label assignment - you may need to modify this based on your data\n",
        "                    label = 'unknown'  # You'll need to determine labels from metadata or filenames\n",
        "\n",
        "                    data_list.append({\n",
        "                        'audio_path': audio_path,\n",
        "                        'transcript_path': transcript_path,\n",
        "                        'label': label,\n",
        "                        'participant_id': participant_id\n",
        "                    })\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    print(f\"Collected {len(df)} samples for {task_type}-{split_type}\")\n",
        "\n",
        "    if len(df) > 0:\n",
        "        print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
        "    else:\n",
        "        print(\"No data collected. Please check your directory structure.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage - update paths according to your Drive structure\n",
        "BASE_PATH = \"/content/drive/MyDrive/ADReSSo_extracted\"  # Update this path\n",
        "\n",
        "# Extract archives if needed (uncomment and update paths)\n",
        "# extract_tgz_archive(\"/path/to/ADReSSo21-diagnosis-train.tgz\", BASE_PATH)\n",
        "# extract_tgz_archive(\"/path/to/ADReSSo21-diagnosis-test.tgz\", BASE_PATH)\n",
        "\n",
        "# Collect training data\n",
        "train_df = collect_adresso_data(BASE_PATH, 'diagnosis', 'train')\n",
        "\n",
        "# Only proceed if we have data\n",
        "if len(train_df) > 0:\n",
        "    test_df = collect_adresso_data(BASE_PATH, 'diagnosis', 'test')\n",
        "\n",
        "    # Split training data into train and validation only if we have labels\n",
        "    if train_df['label'].nunique() > 1:\n",
        "        train_split, val_split = train_test_split(\n",
        "            train_df,\n",
        "            test_size=0.35,\n",
        "            stratify=train_df['label'],\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        # If only one label, do simple split\n",
        "        train_split, val_split = train_test_split(\n",
        "            train_df,\n",
        "            test_size=0.35,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    print(f\"\\nDataset splits:\")\n",
        "    print(f\"Training: {len(train_split)} samples\")\n",
        "    print(f\"Validation: {len(val_split)} samples\")\n",
        "    print(f\"Test: {len(test_df)} samples\")\n",
        "else:\n",
        "    print(\"\\nNo training data found. Please check your dataset path and structure.\")\n",
        "    print(\"Make sure you have:\")\n",
        "    print(\"1. Extracted the ADReSSo dataset archives\")\n",
        "    print(\"2. Updated the BASE_PATH variable to point to the correct directory\")\n",
        "    print(\"3. The directory structure matches the expected format\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg5_i9Nv_a2n",
        "outputId": "659375cd-94d4-462f-9881-18b5dc69219c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find data directory for diagnosis-train\n",
            "Searched in: ['/content/drive/MyDrive/ADReSSo_extracted/ADReSSo21-diagnosis-train', '/content/drive/MyDrive/ADReSSo_extracted/ADReSSo21/diagnosis/train', '/content/drive/MyDrive/ADReSSo_extracted/diagnosis-train', '/content/drive/MyDrive/ADReSSo_extracted/diagnosis/train']\n",
            "Available directories in /content/drive/MyDrive/ADReSSo_extracted:\n",
            "\n",
            "No training data found. Please check your dataset path and structure.\n",
            "Make sure you have:\n",
            "1. Extracted the ADReSSo dataset archives\n",
            "2. Updated the BASE_PATH variable to point to the correct directory\n",
            "3. The directory structure matches the expected format\n"
          ]
        }
      ]
    }
  ]
}