{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTRZ8xiO4Gqa",
        "outputId": "b195fc7a-9b73-46f4-e5eb-9cd72b4dd441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All necessary libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, ViTFeatureExtractor, ViTModel\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"All necessary libraries imported successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "# IMPORTANT: Before running this code, you must manually extract your .tgz archives.\n",
        "# For example, extract ADReSSo21-progression-train.tgz to a directory like 'ADReSSo21_extracted/progression/train/'\n",
        "# And ADReSSo21-diagnosis-train.tgz to 'ADReSSo21_extracted/diagnosis/train/' etc.\n",
        "\n",
        "# Define the base directory where you extracted your ADReSSo dataset\n",
        "BASE_EXTRACTED_DIR = \"ADReSSo21_extracted\" # Adjust this path as per your extraction location\n",
        "\n",
        "# --- Data Collection Function ---\n",
        "def collect_adress_data(base_path, task_type, split_type):\n",
        "    \"\"\"\n",
        "    Collects audio and transcript file paths and their corresponding labels\n",
        "    based on the ADReSSo dataset structure.\n",
        "\n",
        "    Args:\n",
        "        base_path (str): The root directory of the extracted ADReSSo dataset.\n",
        "        task_type (str): 'progression' or 'diagnosis'.\n",
        "        split_type (str): 'train' or 'test'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with 'audio_path', 'transcript_path', and 'label'.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    # Define the base path for the current task and split\n",
        "    current_base_path = os.path.join(base_path, task_type, split_type)\n",
        "\n",
        "    if task_type == 'progression':\n",
        "        if split_type == 'train':\n",
        "            audio_subdirs = ['no_decline', 'decline']\n",
        "            segmentation_subdirs = ['no_decline', 'decline']\n",
        "        elif split_type == 'test':\n",
        "            # For test-dist, the structure is slightly different as per your description\n",
        "            audio_subdirs = [''] # Audio files directly under audio/\n",
        "            segmentation_subdirs = [''] # CSV files directly under segmentation/\n",
        "            current_base_path = os.path.join(base_path, task_type, 'test-dist') # Adjust path for test-dist\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid split_type '{split_type}' for 'progression' task.\")\n",
        "\n",
        "        label_mapping = {'no_decline': 0, 'decline': 1} # Example mapping\n",
        "\n",
        "    elif task_type == 'diagnosis':\n",
        "        audio_subdirs = ['cn', 'ad']\n",
        "        segmentation_subdirs = ['cn', 'ad']\n",
        "        label_mapping = {'cn': 0, 'ad': 1} # CN: Control, AD: Alzheimer's Disease\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid task_type: {task_type}. Must be 'progression' or 'diagnosis'.\")\n",
        "\n",
        "    for i, audio_subdir in enumerate(audio_subdirs):\n",
        "        audio_dir = os.path.join(current_base_path, 'audio', audio_subdir)\n",
        "        segmentation_dir = os.path.join(current_base_path, 'segmentation', segmentation_subdirs[i])\n",
        "\n",
        "        # Get all .wav files in the current audio directory\n",
        "        audio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
        "\n",
        "        for audio_filename in audio_files:\n",
        "            patient_id = os.path.splitext(audio_filename)[0] # e.g., 'patient_001'\n",
        "            audio_path = os.path.join(audio_dir, audio_filename)\n",
        "            transcript_path = os.path.join(segmentation_dir, patient_id + '.csv') # Assuming .csv for transcripts\n",
        "\n",
        "            # Assign label based on subdirectory\n",
        "            if split_type == 'test' and task_type == 'progression':\n",
        "                # For test-dist, labels might not be explicitly in subfolders.\n",
        "                # You might need a manifest file or infer from filenames if available.\n",
        "                # For now, we'll assume a dummy label if no specific subfolder indicates it.\n",
        "                # In a real scenario, you'd load a manifest or infer from file names.\n",
        "                # The ADReSSo test set usually has a separate manifest for labels.\n",
        "                # For this example, we'll assign a placeholder label for test.\n",
        "                current_label = -1 # Placeholder for test set, will be ignored during evaluation\n",
        "            else:\n",
        "                current_label = label_mapping[audio_subdir]\n",
        "\n",
        "            # Check if transcript file exists (important for real data)\n",
        "            if not os.path.exists(transcript_path):\n",
        "                print(f\"Warning: Transcript file not found for {audio_path} at {transcript_path}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            data.append({\n",
        "                \"patient_id\": patient_id,\n",
        "                \"audio_path\": audio_path,\n",
        "                \"transcript_path\": transcript_path,\n",
        "                \"label\": current_label\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# --- Collect data for a specific task (e.g., 'diagnosis') ---\n",
        "# You can change 'diagnosis' to 'progression' if you want to train on that task.\n",
        "# The paper focuses on diagnosis, so we'll use that as the primary example.\n",
        "TASK_TO_USE = 'diagnosis'\n",
        "\n",
        "# Collect training data\n",
        "train_df = collect_adress_data(BASE_EXTRACTED_DIR, TASK_TO_USE, 'train')\n",
        "# Collect test data\n",
        "test_df = collect_adress_data(BASE_EXTRACTED_DIR, TASK_TO_USE, 'test')\n",
        "\n",
        "# For the ADReSSo dataset, the paper splits the *provided* train set into\n",
        "# train and validation. The test set is separate.\n",
        "# So, we split the collected 'train_df' further.\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.35, stratify=train_df['label'], random_state=42)\n",
        "\n",
        "print(f\"--- Data Collection Summary for Task: {TASK_TO_USE} ---\")\n",
        "print(f\"Train samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "print(\"Labels in training set:\", train_df['label'].value_counts())\n",
        "print(\"Labels in validation set:\", val_df['label'].value_counts())\n",
        "print(\"Labels in test set:\", test_df['label'].value_counts())\n",
        "\n",
        "\n",
        "class ADRESSODataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, feature_extractor, max_seq_len=512, sr=16000):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): DataFrame containing 'audio_path', 'transcript_path', and 'label'.\n",
        "            tokenizer (transformers.PreTrainedTokenizer): Tokenizer for text (e.g., BertTokenizer).\n",
        "            feature_extractor (transformers.ViTFeatureExtractor): Feature extractor for Vision Transformer.\n",
        "            max_seq_len (int): Maximum sequence length for BERT tokenizer.\n",
        "            sr (int): Sampling rate for audio processing.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.sr = sr\n",
        "        self.n_mels = 224 # As per paper for log-Mel spectrograms\n",
        "        self.n_mfcc = 40 # As per paper for MFCCs\n",
        "        self.n_fft = 2048\n",
        "        self.hop_length_mel = 1024\n",
        "        self.hop_length_mfcc = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        audio_path = row['audio_path']\n",
        "        transcript_path = row['transcript_path']\n",
        "        label = row['label']\n",
        "\n",
        "        # --- Audio Feature Extraction (Log-Mel Spectrograms and MFCCs) ---\n",
        "        try:\n",
        "            # Load the actual audio file\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sr)\n",
        "\n",
        "            # Log-Mel Spectrogram\n",
        "            mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=self.n_fft,\n",
        "                                                              hop_length=self.hop_length_mel, n_mels=self.n_mels)\n",
        "            log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "            # MFCCs (not used for the ViT input in this example, but extracted as per paper)\n",
        "            # mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_fft=self.n_fft,\n",
        "            #                              hop_length=self.hop_length_mfcc, n_mfcc=self.n_mfcc)\n",
        "\n",
        "            # Delta and Delta-Delta features for Log-Mel Spectrogram\n",
        "            delta_log_mel = librosa.feature.delta(log_mel_spectrogram)\n",
        "            delta_delta_log_mel = librosa.feature.delta(log_mel_spectrogram, order=2)\n",
        "\n",
        "            # Pad or truncate to a fixed size for consistent image dimensions (224x224 for ViT)\n",
        "            target_width = 224 # For ViT input width (frames)\n",
        "\n",
        "            # Ensure all features have the same number of frames\n",
        "            min_frames = min(log_mel_spectrogram.shape[1], delta_log_mel.shape[1], delta_delta_log_mel.shape[1])\n",
        "\n",
        "            log_mel_spectrogram = log_mel_spectrogram[:, :min_frames]\n",
        "            delta_log_mel = delta_log_mel[:, :min_frames]\n",
        "            delta_delta_log_mel = delta_delta_log_mel[:, :min_frames]\n",
        "\n",
        "            # Pad or truncate to target_width (224 frames)\n",
        "            if log_mel_spectrogram.shape[1] < target_width:\n",
        "                pad_width = target_width - log_mel_spectrogram.shape[1]\n",
        "                log_mel_spectrogram = np.pad(log_mel_spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
        "                delta_log_mel = np.pad(delta_log_mel, ((0, 0), (0, pad_width)), mode='constant')\n",
        "                delta_delta_log_mel = np.pad(delta_delta_log_mel, ((0, 0), (0, pad_width)), mode='constant')\n",
        "            elif log_mel_spectrogram.shape[1] > target_width:\n",
        "                log_mel_spectrogram = log_mel_spectrogram[:, :target_width]\n",
        "                delta_log_mel = delta_log_mel[:, :target_width]\n",
        "                delta_delta_log_mel = delta_delta_log_mel[:, :target_width]\n",
        "\n",
        "            # Ensure the height matches ViT's expected input height (224)\n",
        "            # This is crucial if n_mels is not 224. The paper uses n_mels=224, so this might not be strictly needed.\n",
        "            # If your n_mels is different, you'd need resizing here.\n",
        "            if log_mel_spectrogram.shape[0] != self.feature_extractor.size:\n",
        "                # Simple resizing by repeating rows or interpolation.\n",
        "                # For real implementation, consider `torchvision.transforms.Resize` or `cv2.resize`\n",
        "                log_mel_spectrogram = np.resize(log_mel_spectrogram, (self.feature_extractor.size, target_width))\n",
        "                delta_log_mel = np.resize(delta_log_mel, (self.feature_extractor.size, target_width))\n",
        "                delta_delta_log_mel = np.resize(delta_delta_log_mel, (self.feature_extractor.size, target_width))\n",
        "\n",
        "            # Stack the 3 channels (C, H, W) for ViT input\n",
        "            speech_image = np.stack([log_mel_spectrogram, delta_log_mel, delta_delta_log_mel], axis=0)\n",
        "            speech_image = torch.tensor(speech_image, dtype=torch.float32)\n",
        "\n",
        "            # Prepare image for ViT using the feature extractor (handles normalization and final resizing)\n",
        "            pixel_values = self.feature_extractor(images=speech_image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio file {audio_path}: {e}\")\n",
        "            # Return dummy tensor if audio processing fails\n",
        "            pixel_values = torch.zeros(3, self.feature_extractor.size, self.feature_extractor.size)\n",
        "\n",
        "\n",
        "        # --- Text Feature Extraction (BERT) ---\n",
        "        transcript_text = \"\"\n",
        "        try:\n",
        "            # The .csv files in 'segmentation' directories might contain speaker turns and timestamps.\n",
        "            # You will need to parse this CSV to extract the actual spoken text.\n",
        "            # Example: If the CSV has a column named 'text' with the transcript.\n",
        "            # For demonstration, we'll assume the CSV is simple and just read its content.\n",
        "            # You might need to use pandas: pd.read_csv(transcript_path) and extract the relevant column.\n",
        "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
        "                # Read the CSV content. This is a placeholder.\n",
        "                # You'll likely need more sophisticated CSV parsing here.\n",
        "                # E.g., df_transcript = pd.read_csv(transcript_path)\n",
        "                # transcript_text = \" \".join(df_transcript['transcript_column'].tolist())\n",
        "                transcript_text = f.read() # Simple read, assuming raw text or single line\n",
        "\n",
        "            if not transcript_text.strip(): # Check if transcript is empty after stripping whitespace\n",
        "                print(f\"Warning: Empty transcript for {transcript_path}. Using placeholder.\")\n",
        "                transcript_text = \"[CLS] [SEP]\" # Placeholder for empty transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading transcript file {transcript_path}: {e}\")\n",
        "            transcript_text = \"[CLS] [SEP]\" # Fallback for corrupted/missing transcript\n",
        "\n",
        "        text_inputs = self.tokenizer(transcript_text, return_tensors=\"pt\",\n",
        "                                     max_length=self.max_seq_len, truncation=True, padding=\"max_length\")\n",
        "\n",
        "        # Squeeze the batch dimension added by return_tensors=\"pt\"\n",
        "        input_ids = text_inputs['input_ids'].squeeze(0)\n",
        "        attention_mask = text_inputs['attention_mask'].squeeze(0)\n",
        "        token_type_ids = text_inputs['token_type_ids'].squeeze(0) if 'token_type_ids' in text_inputs else None\n",
        "\n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize tokenizer and feature extractor\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = ADRESSODataset(train_df, tokenizer, feature_extractor)\n",
        "val_dataset = ADRESSODataset(val_df, tokenizer, feature_extractor)\n",
        "test_dataset = ADRESSODataset(test_df, tokenizer, feature_extractor)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 8 # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"\\nDataset and DataLoaders created.\")\n"
      ],
      "metadata": {
        "id": "-v7ofeZk4PWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "9338c317-3423-47c6-9ef5-90e1d9955fcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'ADReSSo21_extracted/diagnosis/train/audio/cn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-edcbc7697804>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Collect training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_adress_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_EXTRACTED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK_TO_USE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;31m# Collect test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_adress_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_EXTRACTED_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK_TO_USE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-edcbc7697804>\u001b[0m in \u001b[0;36mcollect_adress_data\u001b[0;34m(base_path, task_type, split_type)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Get all .wav files in the current audio directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0maudio_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maudio_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ADReSSo21_extracted/diagnosis/train/audio/cn'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J8Q_nW-B5vcu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}