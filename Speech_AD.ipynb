{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP5k8QoVKHRKeWxnWzeKWq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Speech_AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Set Up Google Colab Environment"
      ],
      "metadata": {
        "id": "fFDKiuIGnXZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXOA4-ZsnSni",
        "outputId": "0c53d2d8-49b7-4299-f7d7-d74e6d29f16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting pyAudioAnalysis\n",
            "  Downloading pyAudioAnalysis-0.3.14.tar.gz (41.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting audobject>=0.6.1 (from opensmile)\n",
            "  Downloading audobject-0.7.11-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting audinterface>=0.7.0 (from opensmile)\n",
            "  Downloading audinterface-1.2.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting audeer>=2.1.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audeer-2.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audformat-1.3.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audiofile-1.5.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audmath-1.4.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audresample-1.3.3-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (8.7.0)\n",
            "Collecting oyaml (from audobject>=0.6.1->opensmile)\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (24.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from audeer>=2.1.1->audinterface>=0.7.0->opensmile) (4.67.1)\n",
            "Collecting iso639-lang (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso639_lang-2.6.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting iso3166 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile)\n",
            "  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (6.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (0.13.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->audiofile>=1.3.0->audinterface>=0.7.0->opensmile) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile) (1.17.0)\n",
            "Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.0/996.0 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audinterface-1.2.3-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audobject-0.7.11-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audeer-2.2.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audformat-1.3.1-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audiofile-1.5.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audmath-1.4.1-py3-none-any.whl (23 kB)\n",
            "Downloading audresample-1.3.3-py3-none-manylinux_2_17_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
            "Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading iso639_lang-2.6.0-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyAudioAnalysis\n",
            "  Building wheel for pyAudioAnalysis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyAudioAnalysis: filename=pyAudioAnalysis-0.3.14-py3-none-any.whl size=41264371 sha256=423dff03cc1e77ebbfcacea5734b820dc7c5e7e28b8c176b33a2d7f4904d4fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/fe/e4/85f7492f82ccfe48d0f096507b271c0661f51d53c612020655\n",
            "Successfully built pyAudioAnalysis\n",
            "Installing collected packages: pyAudioAnalysis, oyaml, iso639-lang, iso3166, audresample, audmath, audeer, audobject, audiofile, audformat, audinterface, opensmile\n",
            "Successfully installed audeer-2.2.1 audformat-1.3.1 audinterface-1.2.3 audiofile-1.5.1 audmath-1.4.1 audobject-0.7.11 audresample-1.3.3 iso3166-2.1.1 iso639-lang-2.6.0 opensmile-2.5.1 oyaml-1.0 pyAudioAnalysis-0.3.14\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required libraries\n",
        "!pip install opensmile pyAudioAnalysis\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import opensmile\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mount Google Drive to access the .tgz files and CSV files.\n",
        "- Install opensmile for eGeMAPS acoustic feature extraction, librosa for audio processing, and scikit-learn for machine learning models.\n",
        "- Import libraries for data handling, feature extraction, and visualization."
      ],
      "metadata": {
        "id": "DAnGiz4PnzwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load and Organize Datasets"
      ],
      "metadata": {
        "id": "1AeAup5KoFQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define paths to datasets in Google Drive\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "diagnosis_train = data_path + 'ADReSSo21-diagnosis-train.tgz'\n",
        "progression_train = data_path + 'ADReSSo21-progression-train.tgz'\n",
        "progression_test = data_path + 'ADReSSo21-progression-test.tgz'\n",
        "\n",
        "# Create directories for extraction\n",
        "os.makedirs('/content/diagnosis_train', exist_ok=True)\n",
        "os.makedirs('/content/progression_train', exist_ok=True)\n",
        "os.makedirs('/content/progression_test', exist_ok=True)\n",
        "\n",
        "# Unzip datasets\n",
        "!tar -xvzf \"{diagnosis_train}\" -C \"/content/diagnosis_train\"\n",
        "!tar -xvzf \"{progression_train}\" -C \"/content/progression_train\"\n",
        "!tar -xvzf \"{progression_test}\" -C \"/content/progression_test\"\n",
        "\n",
        "# Verify extracted files\n",
        "print(\"Diagnosis Train Files:\", os.listdir('/content/diagnosis_train'))\n",
        "print(\"Progression Train Files:\", os.listdir('/content/progression_train'))\n",
        "print(\"Progression Test Files:\", os.listdir('/content/progression_test'))\n",
        "\n",
        "# Load CSV files\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')  # AD vs Control labels\n",
        "task2 = pd.read_csv(data_path + 'task2.csv')  # MMSE scores\n",
        "task3 = pd.read_csv(data_path + 'task3.csv')  # Cognitive decline labels\n",
        "\n",
        "# Display dataset info\n",
        "print(\"\\nTask 1 (AD Classification):\")\n",
        "print(task1.head())\n",
        "print(\"\\nTask 2 (MMSE Regression):\")\n",
        "print(task2.head())\n",
        "print(\"\\nTask 3 (Cognitive Decline):\")\n",
        "print(task3.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0pK0PBMoCZJ",
        "outputId": "2c081b7b-4509-4ec8-92b1-bab0874aa296"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADReSSo21/diagnosis/\n",
            "ADReSSo21/diagnosis/README.md\n",
            "ADReSSo21/diagnosis/train/\n",
            "ADReSSo21/diagnosis/train/segmentation/\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso281.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso308.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso270.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso022.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso298.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso300.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso265.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso186.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso148.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso152.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso182.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso268.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso259.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso276.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso261.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso262.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso018.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso170.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso263.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso172.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso277.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso280.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso168.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso267.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso007.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso158.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso309.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso292.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso021.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso157.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso286.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso291.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso260.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso156.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso315.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso167.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso302.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso178.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso289.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso161.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso162.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso173.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso154.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso003.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso019.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso023.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso017.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso274.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso299.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso264.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso180.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso014.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso183.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso266.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso159.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso153.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso283.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso316.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso312.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso310.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso164.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso016.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso002.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso008.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso285.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso307.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso177.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso160.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso015.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso278.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso273.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso257.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso012.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso169.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso010.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso165.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso296.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso005.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/cn/adrso151.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso229.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso106.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso144.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso049.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso078.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso209.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso247.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso092.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso077.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso198.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso031.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso090.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso116.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso036.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso206.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso128.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso110.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso032.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso224.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso142.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso126.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso054.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso187.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso028.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso222.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso212.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso053.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso189.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso244.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso200.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso098.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso192.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso033.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso218.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso027.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso246.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso130.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso056.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso190.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso245.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso070.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso215.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso220.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso232.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso075.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso035.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso055.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso188.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso109.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso047.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso063.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso089.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso248.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso138.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso072.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso228.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso093.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso134.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso216.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso046.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso249.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso068.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso237.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso060.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso202.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso223.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso233.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso059.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso234.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso123.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso025.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso253.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso045.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso043.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso250.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso071.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso074.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso122.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso197.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso141.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso236.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso125.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso211.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso024.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso112.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso205.csv\n",
            "ADReSSo21/diagnosis/train/segmentation/ad/adrso039.csv\n",
            "ADReSSo21/diagnosis/train/audio/\n",
            "ADReSSo21/diagnosis/train/audio/cn/\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso173.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso015.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso307.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso283.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso167.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso168.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso172.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso292.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso316.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso162.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso296.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso278.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso300.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso291.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso169.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso178.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso165.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso262.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso177.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso265.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso014.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso261.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso268.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso021.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso156.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso310.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso016.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso148.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso302.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso308.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso018.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso309.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso180.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso298.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso154.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso273.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso259.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso151.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso159.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso267.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso274.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso019.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso153.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso023.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso012.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso280.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso002.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso266.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso022.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso152.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso276.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso260.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso005.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso017.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso299.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso157.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso182.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso008.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso161.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso263.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso257.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso164.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso270.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso289.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso264.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso277.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso160.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso286.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso003.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso186.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso285.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso170.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso183.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso281.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso315.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso010.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso312.wav\n",
            "ADReSSo21/diagnosis/train/audio/cn/adrso158.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso047.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso128.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso045.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso110.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso036.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso189.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso093.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso112.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso205.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso089.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso060.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso232.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso075.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso063.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso106.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso202.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso043.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso206.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso039.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso109.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso126.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso071.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso209.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso244.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso228.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso122.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso116.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso141.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso130.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso248.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso070.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso055.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso222.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso190.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso215.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso223.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso192.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso236.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso234.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso059.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso098.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso090.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso250.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso025.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso031.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso197.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso224.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso074.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso049.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso211.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso229.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso138.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso123.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso027.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso072.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso056.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso068.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso054.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso187.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso078.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso053.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso200.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso249.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso028.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso245.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso216.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso092.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso220.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso134.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso142.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso198.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso077.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso024.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso212.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso046.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso035.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso233.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso247.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso033.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso125.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso188.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso237.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso032.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso253.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso218.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso144.wav\n",
            "ADReSSo21/diagnosis/train/audio/ad/adrso246.wav\n",
            "ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv\n",
            "ADReSSo21/progression/\n",
            "ADReSSo21/progression/README.md\n",
            "ADReSSo21/progression/train/\n",
            "ADReSSo21/progression/train/segmentation/\n",
            "ADReSSo21/progression/train/segmentation/no_decline/\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp195.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp041.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp030.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp052.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp349.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp197.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp207.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp043.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp198.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp148.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp056.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp042.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp161.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp137.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp157.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp321.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp251.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp196.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp306.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp028.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp039.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp122.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp193.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp319.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp177.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp130.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp136.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp350.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp253.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp001.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp124.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp007.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp310.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp024.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp192.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp200.csv\n",
            "ADReSSo21/progression/train/segmentation/no_decline/adrsp031.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp051.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp313.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp101.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp055.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp179.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp300.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp209.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp127.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp266.csv\n",
            "ADReSSo21/progression/train/segmentation/decline/adrsp003.csv\n",
            "ADReSSo21/progression/train/audio/\n",
            "ADReSSo21/progression/train/audio/no_decline/\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp196.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp137.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp130.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp349.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp198.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp321.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp136.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp024.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp007.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp382.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp043.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp019.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp333.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp056.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp042.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp310.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp377.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp363.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp028.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp350.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp096.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp052.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp204.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp380.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp109.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp255.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp157.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp306.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp197.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp031.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp368.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp032.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp091.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp344.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp124.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp195.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp253.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp251.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp039.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp001.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp041.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp384.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp207.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp379.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp324.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp177.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp148.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp023.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp359.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp122.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp200.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp030.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp319.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp378.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp193.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp128.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp161.wav\n",
            "ADReSSo21/progression/train/audio/no_decline/adrsp192.wav\n",
            "ADReSSo21/progression/train/audio/decline/\n",
            "ADReSSo21/progression/train/audio/decline/adrsp055.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp003.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp266.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp300.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp320.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp313.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp179.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp357.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp051.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp101.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp326.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp127.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp276.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp209.wav\n",
            "ADReSSo21/progression/train/audio/decline/adrsp318.wav\n",
            "ADReSSo21/progression/test-dist/\n",
            "ADReSSo21/progression/test-dist/segmentation/\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt24.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt15.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt12.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt2.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt9.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt10.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt29.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt11.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt17.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt18.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt1.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt13.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt26.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt21.csv\n",
            "ADReSSo21/progression/test-dist/segmentation/adrspt23.csv\n",
            "ADReSSo21/progression/test-dist/audio/\n",
            "ADReSSo21/progression/test-dist/audio/adrspt20.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt15.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt4.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt28.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt16.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt27.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt9.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt10.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt13.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt26.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt23.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt31.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt14.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt6.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt12.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt32.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt21.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt1.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt29.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt30.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt3.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt8.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt19.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt18.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt25.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt2.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt24.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt11.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt17.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt22.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt7.wav\n",
            "ADReSSo21/progression/test-dist/audio/adrspt5.wav\n",
            "ADReSSo21/progression/test-dist/test_results_task3.csv\n",
            "ADReSSo21/progression/test-dist/README\n",
            "Diagnosis Train Files: ['ADReSSo21']\n",
            "Progression Train Files: ['ADReSSo21']\n",
            "Progression Test Files: ['ADReSSo21']\n",
            "\n",
            "Task 1 (AD Classification):\n",
            "         ID       Dx\n",
            "0  adrsdt15  Control\n",
            "1  adrsdt40  Control\n",
            "2  adrsdt26  Control\n",
            "3  adrsdt67  Control\n",
            "4  adrsdt58  Control\n",
            "\n",
            "Task 2 (MMSE Regression):\n",
            "         ID  MMSE\n",
            "0  adrsdt15    30\n",
            "1  adrsdt40    28\n",
            "2  adrsdt26    29\n",
            "3  adrsdt67    30\n",
            "4  adrsdt58    29\n",
            "\n",
            "Task 3 (Cognitive Decline):\n",
            "         ID  Decline\n",
            "0   adrspt2    False\n",
            "1  adrspt18    False\n",
            "2   adrspt9     True\n",
            "3  adrspt21    False\n",
            "4  adrspt29    False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Acoustic Feature Extraction (eGeMAPS)"
      ],
      "metadata": {
        "id": "Q4NRJ_ayspat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Initialize opensmile for eGeMAPS feature extraction\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "# Function to extract eGeMAPS features from an audio file\n",
        "def extract_egemaps(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
        "        features = smile.process_signal(y, sr)  # Extract eGeMAPS\n",
        "        return features.values.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to audio files (diagnosis train: cn and ad subdirectories)\n",
        "diagnosis_audio_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/'\n",
        "cn_audio_path = os.path.join(diagnosis_audio_base, 'cn/')\n",
        "ad_audio_path = os.path.join(diagnosis_audio_base, 'ad/')\n",
        "\n",
        "# Collect all .wav files from both cn/ and ad/ directories\n",
        "audio_files = []\n",
        "for path in [cn_audio_path, ad_audio_path]:\n",
        "    if os.path.exists(path):\n",
        "        audio_files.extend([os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')])\n",
        "    else:\n",
        "        print(f\"Directory not found: {path}\")\n",
        "\n",
        "# Extract features for all audio files\n",
        "audio_features = []  # Initialize as empty list\n",
        "audio_ids = []\n",
        "for audio_path in audio_files:\n",
        "    audio_file = os.path.basename(audio_path)\n",
        "    audio_id = audio_file.split('.')[0]  # Extract ID from filename (e.g., adrso123)\n",
        "    features = extract_egemaps(audio_path)\n",
        "    if features is not None:\n",
        "        audio_features.append(features)\n",
        "        audio_ids.append(audio_id)\n",
        "    else:\n",
        "        print(f\"Skipping {audio_id} due to feature extraction failure\")\n",
        "\n",
        "# Check if any features were extracted\n",
        "if not audio_features:\n",
        "    raise ValueError(\"No audio features extracted. Check audio files or extraction process.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "audio_features_df = pd.DataFrame(audio_features)\n",
        "audio_features_df['ID'] = audio_ids\n",
        "\n",
        "# Load task1.csv for labels\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')\n",
        "\n",
        "# Normalize IDs in task1.csv to match audio file IDs\n",
        "task1['ID'] = task1['ID'].apply(lambda x: 'adrso' + x.replace('adrsdt', '').zfill(3))\n",
        "\n",
        "# Merge with task1 labels\n",
        "task1_data = pd.merge(audio_features_df, task1, on='ID', how='inner')\n",
        "print(\"Merged Acoustic Features with Labels:\")\n",
        "print(task1_data.head())\n",
        "print(f\"Number of matched records: {len(task1_data)}\")\n",
        "\n",
        "# Save the merged DataFrame for debugging\n",
        "task1_data.to_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv', index=False)\n",
        "print(\"Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCjXc9lNtD4Q",
        "outputId": "2cb24f1e-5ee9-4375-a1ee-695006c47d85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Acoustic Features with Labels:\n",
            "           0         1          2          3          4          5  \\\n",
            "0  34.314342  0.172523  31.954039  34.558792  38.469227   6.515188   \n",
            "1  34.439098  0.178912  29.944578  33.201965  39.123035   9.178457   \n",
            "2  34.765678  0.144698  31.995552  35.011833  37.706375   5.710823   \n",
            "3  30.145615  0.129570  28.376390  29.582561  32.609303   4.232912   \n",
            "4  31.052141  0.345289  22.244028  25.008968  40.717941  18.473913   \n",
            "\n",
            "            6           7           8           9  ...        80        81  \\\n",
            "0  332.870453  461.649567  120.299301   81.307747  ...  0.544044  1.976285   \n",
            "1  169.268906  333.093689  160.969574  267.036377  ...  0.434866  2.533154   \n",
            "2  231.058731  375.145050  145.108765  262.729279  ...  0.130108  1.557071   \n",
            "3  292.438629  535.989441   76.417542   96.036621  ...  0.086000  1.878543   \n",
            "4  546.450195  783.379028  370.438660  510.268463  ...  0.291039  1.648093   \n",
            "\n",
            "         82        83        84        85        86         87        ID  \\\n",
            "0  2.508803  0.112807  0.123327  0.279630  0.317927 -12.181213  adrso010   \n",
            "1  2.251715  0.204901  0.223378  0.228958  0.342663 -11.879815  adrso014   \n",
            "2  2.098893  0.159448  0.190493  0.289759  0.382377 -23.670115  adrso015   \n",
            "3  2.674664  0.190667  0.199302  0.170000  0.260768 -26.691757  adrso005   \n",
            "4  3.502985  0.085695  0.098524  0.192019  0.217589 -19.202145  adrso018   \n",
            "\n",
            "           Dx  \n",
            "0     Control  \n",
            "1  ProbableAD  \n",
            "2     Control  \n",
            "3     Control  \n",
            "4  ProbableAD  \n",
            "\n",
            "[5 rows x 90 columns]\n",
            "Number of matched records: 41\n",
            "Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## low number of matches"
      ],
      "metadata": {
        "id": "hEcHlTEwxJDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Audio IDs:\", audio_ids[:5])\n",
        "print(\"Task1 IDs:\", task1['ID'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZ2-MMgwjtn",
        "outputId": "f5d5c8c5-0109-4e16-9d21-08e0084bfca6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio IDs: ['adrso010', 'adrso014', 'adrso015', 'adrso005', 'adrso312']\n",
            "Task1 IDs: 0    adrso015\n",
            "1    adrso040\n",
            "2    adrso026\n",
            "3    adrso067\n",
            "4    adrso058\n",
            "Name: ID, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = extract_egemaps('/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso173.wav')\n",
        "print(\"Test features shape:\", test_features.shape if test_features is not None else \"Failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5AP1pQuwnM8",
        "outputId": "ea0e6bc4-abe8-4ba4-92c4-67698d4ef632"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test features shape: (88,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of audio files:\", len(audio_files))\n",
        "print(\"Sample audio files:\", audio_files[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ufgr-kLwXQ7",
        "outputId": "eaf72444-c212-4bf6-aab2-84c0baa33211"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of audio files: 166\n",
            "Sample audio files: ['/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso010.wav', '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso014.wav', '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso015.wav', '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso005.wav', '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso312.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Initialize opensmile for eGeMAPS feature extraction\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "# Function to extract eGeMAPS features from an audio file\n",
        "def extract_egemaps(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
        "        features = smile.process_signal(y, sr)  # Extract eGeMAPS\n",
        "        return features.values.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to audio files (diagnosis train: cn and ad subdirectories)\n",
        "diagnosis_audio_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/'\n",
        "cn_audio_path = os.path.join(diagnosis_audio_base, 'cn/')\n",
        "ad_audio_path = os.path.join(diagnosis_audio_base, 'ad/')\n",
        "\n",
        "# Collect all .wav files from both cn/ and ad/ directories\n",
        "audio_files = []\n",
        "for path in [cn_audio_path, ad_audio_path]:\n",
        "    if os.path.exists(path):\n",
        "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        audio_files.extend(files)\n",
        "        print(f\"Found {len(files)} audio files in {path}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {path}\")\n",
        "\n",
        "print(f\"Total audio files found: {len(audio_files)}\")\n",
        "\n",
        "# Extract features for all audio files\n",
        "audio_features = []\n",
        "audio_ids = []\n",
        "skipped_files = []\n",
        "for audio_path in audio_files:\n",
        "    audio_file = os.path.basename(audio_path)\n",
        "    audio_id = audio_file.split('.')[0]  # Extract ID (e.g., adrso123)\n",
        "    features = extract_egemaps(audio_path)\n",
        "    if features is not None:\n",
        "        audio_features.append(features)\n",
        "        audio_ids.append(audio_id)\n",
        "    else:\n",
        "        print(f\"Skipping {audio_id} due to feature extraction failure\")\n",
        "        skipped_files.append(audio_id)\n",
        "\n",
        "print(f\"Extracted features for {len(audio_features)} audio files\")\n",
        "print(f\"Skipped {len(skipped_files)} audio files: {skipped_files}\")\n",
        "\n",
        "# Check if any features were extracted\n",
        "if not audio_features:\n",
        "    raise ValueError(\"No audio features extracted. Check audio files or extraction process.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "audio_features_df = pd.DataFrame(audio_features)\n",
        "audio_features_df['ID'] = audio_ids\n",
        "\n",
        "# Load task1.csv for labels\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')\n",
        "\n",
        "# Normalize IDs in task1.csv to match audio file IDs\n",
        "task1['ID'] = task1['ID'].apply(lambda x: 'adrso' + x.replace('adrsdt', '').zfill(3))\n",
        "\n",
        "# Check ID overlap\n",
        "audio_id_set = set(audio_ids)\n",
        "task1_id_set = set(task1['ID'])\n",
        "print(f\"Audio IDs in audio_files: {len(audio_id_set)}\")\n",
        "print(f\"Task1 IDs: {len(task1_id_set)}\")\n",
        "print(f\"Common IDs: {len(audio_id_set & task1_id_set)}\")\n",
        "print(f\"Audio IDs not in task1: {audio_id_set - task1_id_set}\")\n",
        "print(f\"Task1 IDs not in audio: {task1_id_set - audio_id_set}\")\n",
        "\n",
        "# Merge with task1 labels\n",
        "task1_data = pd.merge(audio_features_df, task1, on='ID', how='inner')\n",
        "print(\"Merged Acoustic Features with Labels:\")\n",
        "print(task1_data.head())\n",
        "print(f\"Number of matched records: {len(task1_data)}\")\n",
        "\n",
        "# Save the merged DataFrame\n",
        "task1_data.to_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv', index=False)\n",
        "print(\"Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\")\n",
        "\n",
        "# Save unmatched IDs for debugging\n",
        "unmatched_audio_ids = list(audio_id_set - task1_id_set)\n",
        "unmatched_task1_ids = list(task1_id_set - audio_id_set)\n",
        "pd.DataFrame({'unmatched_audio_ids': unmatched_audio_ids}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/unmatched_audio_ids.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'unmatched_task1_ids': unmatched_task1_ids}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/unmatched_task1_ids.csv', index=False\n",
        ")\n",
        "print(\"Saved unmatched IDs to /content/drive/MyDrive/Voice/unmatched_{audio,task1}_ids.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo93GMscw53m",
        "outputId": "cc60717c-4df4-48a7-c7d8-25eabed63d0a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 79 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/\n",
            "Found 87 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/\n",
            "Total audio files found: 166\n",
            "Extracted features for 166 audio files\n",
            "Skipped 0 audio files: []\n",
            "Audio IDs in audio_files: 166\n",
            "Task1 IDs: 71\n",
            "Common IDs: 41\n",
            "Audio IDs not in task1: {'adrso228', 'adrso211', 'adrso291', 'adrso189', 'adrso128', 'adrso177', 'adrso172', 'adrso167', 'adrso157', 'adrso188', 'adrso206', 'adrso156', 'adrso308', 'adrso154', 'adrso178', 'adrso216', 'adrso169', 'adrso202', 'adrso276', 'adrso274', 'adrso285', 'adrso200', 'adrso186', 'adrso197', 'adrso266', 'adrso265', 'adrso160', 'adrso090', 'adrso223', 'adrso161', 'adrso259', 'adrso198', 'adrso170', 'adrso141', 'adrso165', 'adrso122', 'adrso093', 'adrso299', 'adrso074', 'adrso257', 'adrso234', 'adrso283', 'adrso281', 'adrso077', 'adrso209', 'adrso247', 'adrso277', 'adrso309', 'adrso151', 'adrso153', 'adrso307', 'adrso280', 'adrso268', 'adrso289', 'adrso229', 'adrso236', 'adrso310', 'adrso222', 'adrso245', 'adrso159', 'adrso112', 'adrso158', 'adrso180', 'adrso261', 'adrso298', 'adrso215', 'adrso296', 'adrso116', 'adrso123', 'adrso232', 'adrso106', 'adrso125', 'adrso072', 'adrso302', 'adrso300', 'adrso134', 'adrso270', 'adrso316', 'adrso262', 'adrso244', 'adrso278', 'adrso183', 'adrso192', 'adrso164', 'adrso152', 'adrso273', 'adrso205', 'adrso162', 'adrso264', 'adrso142', 'adrso148', 'adrso089', 'adrso249', 'adrso246', 'adrso098', 'adrso173', 'adrso075', 'adrso237', 'adrso168', 'adrso233', 'adrso292', 'adrso144', 'adrso267', 'adrso263', 'adrso190', 'adrso218', 'adrso212', 'adrso220', 'adrso315', 'adrso130', 'adrso110', 'adrso224', 'adrso182', 'adrso092', 'adrso078', 'adrso138', 'adrso253', 'adrso260', 'adrso286', 'adrso126', 'adrso109', 'adrso248', 'adrso187', 'adrso250', 'adrso312'}\n",
            "Task1 IDs not in audio: {'adrso050', 'adrso051', 'adrso067', 'adrso004', 'adrso011', 'adrso062', 'adrso058', 'adrso009', 'adrso044', 'adrso064', 'adrso061', 'adrso052', 'adrso069', 'adrso040', 'adrso006', 'adrso034', 'adrso038', 'adrso065', 'adrso041', 'adrso020', 'adrso048', 'adrso001', 'adrso026', 'adrso066', 'adrso042', 'adrso037', 'adrso029', 'adrso057', 'adrso030', 'adrso013'}\n",
            "Merged Acoustic Features with Labels:\n",
            "           0         1          2          3          4          5  \\\n",
            "0  34.314342  0.172523  31.954039  34.558792  38.469227   6.515188   \n",
            "1  34.439098  0.178912  29.944578  33.201965  39.123035   9.178457   \n",
            "2  34.765678  0.144698  31.995552  35.011833  37.706375   5.710823   \n",
            "3  30.145615  0.129570  28.376390  29.582561  32.609303   4.232912   \n",
            "4  31.052141  0.345289  22.244028  25.008968  40.717941  18.473913   \n",
            "\n",
            "            6           7           8           9  ...        80        81  \\\n",
            "0  332.870453  461.649567  120.299301   81.307747  ...  0.544044  1.976285   \n",
            "1  169.268906  333.093689  160.969574  267.036377  ...  0.434866  2.533154   \n",
            "2  231.058731  375.145050  145.108765  262.729279  ...  0.130108  1.557071   \n",
            "3  292.438629  535.989441   76.417542   96.036621  ...  0.086000  1.878543   \n",
            "4  546.450195  783.379028  370.438660  510.268463  ...  0.291039  1.648093   \n",
            "\n",
            "         82        83        84        85        86         87        ID  \\\n",
            "0  2.508803  0.112807  0.123327  0.279630  0.317927 -12.181213  adrso010   \n",
            "1  2.251715  0.204901  0.223378  0.228958  0.342663 -11.879815  adrso014   \n",
            "2  2.098893  0.159448  0.190493  0.289759  0.382377 -23.670115  adrso015   \n",
            "3  2.674664  0.190667  0.199302  0.170000  0.260768 -26.691757  adrso005   \n",
            "4  3.502985  0.085695  0.098524  0.192019  0.217589 -19.202145  adrso018   \n",
            "\n",
            "           Dx  \n",
            "0     Control  \n",
            "1  ProbableAD  \n",
            "2     Control  \n",
            "3     Control  \n",
            "4  ProbableAD  \n",
            "\n",
            "[5 rows x 90 columns]\n",
            "Number of matched records: 41\n",
            "Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\n",
            "Saved unmatched IDs to /content/drive/MyDrive/Voice/unmatched_{audio,task1}_ids.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Changes\n",
        "1. **Debugging Audio Files**:\n",
        "   - Print the number of `.wav` files in `cn/` and `ad/` directories and the total count.\n",
        "   - Expect ~108 files (based on ADReSSo train split). If fewer, some audio files are missing.\n",
        "\n",
        "2. **Tracking Skipped Files**:\n",
        "   - Maintain a `skipped_files` list to log audio IDs where feature extraction failed.\n",
        "   - Print the number of skipped files and their IDs.\n",
        "\n",
        "3. **ID Overlap Analysis**:\n",
        "   - Compare `audio_ids` (from audio files) with `task1['ID']` (after normalization) using set operations.\n",
        "   - Print:\n",
        "     - Number of unique audio IDs.\n",
        "     - Number of unique `task1` IDs.\n",
        "     - Number of common IDs (should be close to 108).\n",
        "     - Audio IDs not in `task1.csv`.\n",
        "     - `task1.csv` IDs not in audio files.\n",
        "   - Save unmatched IDs to CSV files for inspection.\n",
        "\n",
        "4. **Preserved Core Logic**:\n",
        "   - Kept eGeMAPS extraction, ID normalization (`adrso` + zero-padded ID), and merging logic unchanged.\n",
        "   - Ensured 88 features are extracted per audio file.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MLm_FhFByMTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Initialize opensmile for eGeMAPS feature extraction\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "# Function to extract eGeMAPS features from an audio file\n",
        "def extract_egemaps(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
        "        features = smile.process_signal(y, sr)  # Extract eGeMAPS\n",
        "        return features.values.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to audio files (diagnosis train: cn and ad subdirectories)\n",
        "diagnosis_audio_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/'\n",
        "cn_audio_path = os.path.join(diagnosis_audio_base, 'cn/')\n",
        "ad_audio_path = os.path.join(diagnosis_audio_base, 'ad/')\n",
        "\n",
        "# Load task1.csv for labels\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')\n",
        "\n",
        "# Normalize IDs in task1.csv to match audio file IDs\n",
        "task1['ID'] = task1['ID'].apply(lambda x: 'adrso' + x.replace('adrsdt', '').zfill(3))\n",
        "task1_ids = set(task1['ID'])\n",
        "print(f\"Task1.csv contains {len(task1)} IDs\")\n",
        "\n",
        "# Collect .wav files from cn/ and ad/, filtering by task1 IDs\n",
        "audio_files = []\n",
        "audio_id_to_path = {}\n",
        "for path in [cn_audio_path, ad_audio_path]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        for f in files:\n",
        "            audio_id = f.split('.')[0]\n",
        "            if audio_id in task1_ids:\n",
        "                audio_files.append(os.path.join(path, f))\n",
        "                audio_id_to_path[audio_id] = os.path.join(path, f)\n",
        "        print(f\"Found {len(files)} audio files in {path}, {sum(1 for f in files if f.split('.')[0] in task1_ids)} match task1 IDs\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {path}\")\n",
        "\n",
        "print(f\"Total audio files matching task1 IDs: {len(audio_files)}\")\n",
        "\n",
        "# Identify task1 IDs without audio files\n",
        "missing_audio_ids = task1_ids - set(audio_id_to_path.keys())\n",
        "print(f\"Task1 IDs missing audio files ({len(missing_audio_ids)}): {missing_audio_ids}\")\n",
        "\n",
        "# Extract features for matching audio files\n",
        "audio_features = []\n",
        "audio_ids = []\n",
        "skipped_files = []\n",
        "for audio_path in audio_files:\n",
        "    audio_file = os.path.basename(audio_path)\n",
        "    audio_id = audio_file.split('.')[0]\n",
        "    features = extract_egemaps(audio_path)\n",
        "    if features is not None:\n",
        "        audio_features.append(features)\n",
        "        audio_ids.append(audio_id)\n",
        "    else:\n",
        "        print(f\"Skipping {audio_id} due to feature extraction failure\")\n",
        "        skipped_files.append(audio_id)\n",
        "\n",
        "print(f\"Extracted features for {len(audio_features)} audio files\")\n",
        "print(f\"Skipped {len(skipped_files)} audio files: {skipped_files}\")\n",
        "\n",
        "# Check if any features were extracted\n",
        "if not audio_features:\n",
        "    raise ValueError(\"No audio features extracted. Check audio files or extraction process.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "audio_features_df = pd.DataFrame(audio_features)\n",
        "audio_features_df['ID'] = audio_ids\n",
        "\n",
        "# Merge with task1 labels\n",
        "task1_data = pd.merge(audio_features_df, task1, on='ID', how='inner')\n",
        "print(\"Merged Acoustic Features with Labels:\")\n",
        "print(task1_data.head())\n",
        "print(f\"Number of matched records: {len(task1_data)}\")\n",
        "\n",
        "# Save the merged DataFrame\n",
        "task1_data.to_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv', index=False)\n",
        "print(\"Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\")\n",
        "\n",
        "# Save debugging info\n",
        "pd.DataFrame({'missing_audio_ids': list(missing_audio_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/missing_audio_ids.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'skipped_files': skipped_files}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/skipped_files.csv', index=False\n",
        ")\n",
        "print(\"Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kqQcA3ay8rR",
        "outputId": "0a5d8773-be7e-4b1b-a587-3ec68282985e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task1.csv contains 71 IDs\n",
            "Found 79 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/, 16 match task1 IDs\n",
            "Found 87 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/, 25 match task1 IDs\n",
            "Total audio files matching task1 IDs: 41\n",
            "Task1 IDs missing audio files (30): {'adrso050', 'adrso051', 'adrso067', 'adrso004', 'adrso011', 'adrso062', 'adrso058', 'adrso009', 'adrso044', 'adrso064', 'adrso061', 'adrso052', 'adrso069', 'adrso040', 'adrso006', 'adrso034', 'adrso038', 'adrso065', 'adrso041', 'adrso020', 'adrso048', 'adrso001', 'adrso026', 'adrso066', 'adrso042', 'adrso037', 'adrso029', 'adrso057', 'adrso030', 'adrso013'}\n",
            "Extracted features for 41 audio files\n",
            "Skipped 0 audio files: []\n",
            "Merged Acoustic Features with Labels:\n",
            "           0         1          2          3          4          5  \\\n",
            "0  34.314342  0.172523  31.954039  34.558792  38.469227   6.515188   \n",
            "1  34.439098  0.178912  29.944578  33.201965  39.123035   9.178457   \n",
            "2  34.765678  0.144698  31.995552  35.011833  37.706375   5.710823   \n",
            "3  30.145615  0.129570  28.376390  29.582561  32.609303   4.232912   \n",
            "4  31.052141  0.345289  22.244028  25.008968  40.717941  18.473913   \n",
            "\n",
            "            6           7           8           9  ...        80        81  \\\n",
            "0  332.870453  461.649567  120.299301   81.307747  ...  0.544044  1.976285   \n",
            "1  169.268906  333.093689  160.969574  267.036377  ...  0.434866  2.533154   \n",
            "2  231.058731  375.145050  145.108765  262.729279  ...  0.130108  1.557071   \n",
            "3  292.438629  535.989441   76.417542   96.036621  ...  0.086000  1.878543   \n",
            "4  546.450195  783.379028  370.438660  510.268463  ...  0.291039  1.648093   \n",
            "\n",
            "         82        83        84        85        86         87        ID  \\\n",
            "0  2.508803  0.112807  0.123327  0.279630  0.317927 -12.181213  adrso010   \n",
            "1  2.251715  0.204901  0.223378  0.228958  0.342663 -11.879815  adrso014   \n",
            "2  2.098893  0.159448  0.190493  0.289759  0.382377 -23.670115  adrso015   \n",
            "3  2.674664  0.190667  0.199302  0.170000  0.260768 -26.691757  adrso005   \n",
            "4  3.502985  0.085695  0.098524  0.192019  0.217589 -19.202145  adrso018   \n",
            "\n",
            "           Dx  \n",
            "0     Control  \n",
            "1  ProbableAD  \n",
            "2     Control  \n",
            "3     Control  \n",
            "4  ProbableAD  \n",
            "\n",
            "[5 rows x 90 columns]\n",
            "Number of matched records: 41\n",
            "Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\n",
            "Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files}.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Initialize opensmile for eGeMAPS feature extraction\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "# Function to extract eGeMAPS features from an audio file\n",
        "def extract_egemaps(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
        "        features = smile.process_signal(y, sr)  # Extract eGeMAPS\n",
        "        return features.values.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to data\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "diagnosis_audio_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/'\n",
        "cn_audio_path = os.path.join(diagnosis_audio_base, 'cn/')\n",
        "ad_audio_path = os.path.join(diagnosis_audio_base, 'ad/')\n",
        "segmentation_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/'\n",
        "mmse_scores_file = '/content/diagnosis_train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv'\n",
        "\n",
        "# Load task1.csv\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')\n",
        "task1['ID'] = task1['ID'].apply(lambda x: 'adrso' + x.replace('adrsdt', '').zfill(3))\n",
        "task1_ids = set(task1['ID'])\n",
        "print(f\"Task1.csv contains {len(task1)} IDs\")\n",
        "\n",
        "# Load MMSE scores file\n",
        "try:\n",
        "    mmse_scores = pd.read_csv(mmse_scores_file)\n",
        "    mmse_ids = set(mmse_scores.get('ID', []))  # Adjust column name if needed\n",
        "    print(f\"MMSE scores file contains {len(mmse_ids)} IDs\")\n",
        "    print(f\"Task1 IDs in MMSE scores: {len(task1_ids & mmse_ids)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"MMSE scores file not found: {mmse_scores_file}\")\n",
        "    mmse_ids = set()\n",
        "\n",
        "# Collect segmentation IDs\n",
        "segmentation_ids = set()\n",
        "for path in [os.path.join(segmentation_base, 'cn/'), os.path.join(segmentation_base, 'ad/')]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f.split('.')[0] for f in os.listdir(path) if f.endswith('.csv')]\n",
        "        segmentation_ids.update(files)\n",
        "        print(f\"Found {len(files)} segmentation files in {path}\")\n",
        "    else:\n",
        "        print(f\"Segmentation directory not found: {path}\")\n",
        "print(f\"Total segmentation IDs: {len(segmentation_ids)}\")\n",
        "print(f\"Task1 IDs in segmentation: {len(task1_ids & segmentation_ids)}\")\n",
        "\n",
        "# Collect .wav files from cn/ and ad/, filtering by task1 IDs\n",
        "audio_files = []\n",
        "audio_id_to_path = {}\n",
        "for path in [cn_audio_path, ad_audio_path]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        for f in files:\n",
        "            audio_id = f.split('.')[0]\n",
        "            if audio_id in task1_ids:\n",
        "                audio_files.append(os.path.join(path, f))\n",
        "                audio_id_to_path[audio_id] = os.path.join(path, f)\n",
        "        print(f\"Found {len(files)} audio files in {path}, {sum(1 for f in files if f.split('.')[0] in task1_ids)} match task1 IDs\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {path}\")\n",
        "\n",
        "print(f\"Total audio files matching task1 IDs: {len(audio_files)}\")\n",
        "\n",
        "# Search for missing task1 IDs in other directories\n",
        "missing_audio_ids = task1_ids - set(audio_id_to_path.keys())\n",
        "print(f\"Task1 IDs missing audio files ({len(missing_audio_ids)}): {missing_audio_ids}\")\n",
        "\n",
        "# Check other directories for missing audio files\n",
        "other_dirs = [\n",
        "    '/content/progression_train/ADReSSo21/progression/train/audio/no_decline/',\n",
        "    '/content/progression_train/ADReSSo21/progression/train/audio/decline/',\n",
        "    '/content/progression_test/ADReSSo21/progression/test-dist/audio/'\n",
        "]\n",
        "found_missing = {}\n",
        "for missing_id in missing_audio_ids:\n",
        "    for d in other_dirs:\n",
        "        if os.path.exists(d):\n",
        "            if f\"{missing_id}.wav\" in os.listdir(d):\n",
        "                found_missing[missing_id] = os.path.join(d, f\"{missing_id}.wav\")\n",
        "if found_missing:\n",
        "    print(\"Found missing audio files in other directories:\")\n",
        "    for id_, path in found_missing.items():\n",
        "        print(f\"{id_}: {path}\")\n",
        "else:\n",
        "    print(\"No missing audio files found in other directories\")\n",
        "\n",
        "# Extract features for matching audio files\n",
        "audio_features = []\n",
        "audio_ids = []\n",
        "skipped_files = []\n",
        "for audio_path in audio_files:\n",
        "    audio_file = os.path.basename(audio_path)\n",
        "    audio_id = audio_file.split('.')[0]\n",
        "    features = extract_egemaps(audio_path)\n",
        "    if features is not None:\n",
        "        audio_features.append(features)\n",
        "        audio_ids.append(audio_id)\n",
        "    else:\n",
        "        print(f\"Skipping {audio_id} due to feature extraction failure\")\n",
        "        skipped_files.append(audio_id)\n",
        "\n",
        "print(f\"Extracted features for {len(audio_features)} audio files\")\n",
        "print(f\"Skipped {len(skipped_files)} audio files: {skipped_files}\")\n",
        "\n",
        "# Check if any features were extracted\n",
        "if not audio_features:\n",
        "    raise ValueError(\"No audio features extracted. Check audio files or extraction process.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "audio_features_df = pd.DataFrame(audio_features)\n",
        "audio_features_df['ID'] = audio_ids\n",
        "\n",
        "# Merge with task1 labels\n",
        "task1_data = pd.merge(audio_features_df, task1, on='ID', how='inner')\n",
        "print(\"Merged Acoustic Features with Labels:\")\n",
        "print(task1_data.head())\n",
        "print(f\"Number of matched records: {len(task1_data)}\")\n",
        "\n",
        "# Save the merged DataFrame\n",
        "task1_data.to_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv', index=False)\n",
        "print(\"Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\")\n",
        "\n",
        "# Save debugging info\n",
        "pd.DataFrame({'missing_audio_ids': list(missing_audio_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/missing_audio_ids.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'skipped_files': skipped_files}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/skipped_files.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'segmentation_ids': list(segmentation_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/segmentation_ids.csv', index=False\n",
        ")\n",
        "print(\"Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files,segmentation_ids}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9K991AY3TdH",
        "outputId": "8fccd85a-320d-4ff2-d9ad-4c9b5ab18837"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task1.csv contains 71 IDs\n",
            "MMSE scores file contains 0 IDs\n",
            "Task1 IDs in MMSE scores: 0\n",
            "Found 79 segmentation files in /content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/\n",
            "Found 87 segmentation files in /content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/\n",
            "Total segmentation IDs: 166\n",
            "Task1 IDs in segmentation: 41\n",
            "Found 79 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/, 16 match task1 IDs\n",
            "Found 87 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/, 25 match task1 IDs\n",
            "Total audio files matching task1 IDs: 41\n",
            "Task1 IDs missing audio files (30): {'adrso050', 'adrso051', 'adrso067', 'adrso004', 'adrso011', 'adrso062', 'adrso058', 'adrso009', 'adrso044', 'adrso064', 'adrso061', 'adrso052', 'adrso069', 'adrso040', 'adrso006', 'adrso034', 'adrso038', 'adrso065', 'adrso041', 'adrso020', 'adrso048', 'adrso001', 'adrso026', 'adrso066', 'adrso042', 'adrso037', 'adrso029', 'adrso057', 'adrso030', 'adrso013'}\n",
            "No missing audio files found in other directories\n",
            "Extracted features for 41 audio files\n",
            "Skipped 0 audio files: []\n",
            "Merged Acoustic Features with Labels:\n",
            "           0         1          2          3          4          5  \\\n",
            "0  34.314342  0.172523  31.954039  34.558792  38.469227   6.515188   \n",
            "1  34.439098  0.178912  29.944578  33.201965  39.123035   9.178457   \n",
            "2  34.765678  0.144698  31.995552  35.011833  37.706375   5.710823   \n",
            "3  30.145615  0.129570  28.376390  29.582561  32.609303   4.232912   \n",
            "4  31.052141  0.345289  22.244028  25.008968  40.717941  18.473913   \n",
            "\n",
            "            6           7           8           9  ...        80        81  \\\n",
            "0  332.870453  461.649567  120.299301   81.307747  ...  0.544044  1.976285   \n",
            "1  169.268906  333.093689  160.969574  267.036377  ...  0.434866  2.533154   \n",
            "2  231.058731  375.145050  145.108765  262.729279  ...  0.130108  1.557071   \n",
            "3  292.438629  535.989441   76.417542   96.036621  ...  0.086000  1.878543   \n",
            "4  546.450195  783.379028  370.438660  510.268463  ...  0.291039  1.648093   \n",
            "\n",
            "         82        83        84        85        86         87        ID  \\\n",
            "0  2.508803  0.112807  0.123327  0.279630  0.317927 -12.181213  adrso010   \n",
            "1  2.251715  0.204901  0.223378  0.228958  0.342663 -11.879815  adrso014   \n",
            "2  2.098893  0.159448  0.190493  0.289759  0.382377 -23.670115  adrso015   \n",
            "3  2.674664  0.190667  0.199302  0.170000  0.260768 -26.691757  adrso005   \n",
            "4  3.502985  0.085695  0.098524  0.192019  0.217589 -19.202145  adrso018   \n",
            "\n",
            "           Dx  \n",
            "0     Control  \n",
            "1  ProbableAD  \n",
            "2     Control  \n",
            "3     Control  \n",
            "4  ProbableAD  \n",
            "\n",
            "[5 rows x 90 columns]\n",
            "Number of matched records: 41\n",
            "Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\n",
            "Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files,segmentation_ids}.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "# Initialize opensmile for eGeMAPS feature extraction\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "# Function to extract eGeMAPS features from an audio file\n",
        "def extract_egemaps(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
        "        features = smile.process_signal(y, sr)  # Extract eGeMAPS\n",
        "        return features.values.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to data\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "diagnosis_audio_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/'\n",
        "cn_audio_path = os.path.join(diagnosis_audio_base, 'cn/')\n",
        "ad_audio_path = os.path.join(diagnosis_audio_base, 'ad/')\n",
        "segmentation_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/'\n",
        "mmse_scores_file = '/content/diagnosis_train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv'\n",
        "\n",
        "# Load task1.csv\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')\n",
        "task1['ID'] = task1['ID'].apply(lambda x: 'adrso' + x.replace('adrsdt', '').zfill(3))\n",
        "task1_ids = set(task1['ID'])\n",
        "print(f\"Task1.csv contains {len(task1)} IDs\")\n",
        "\n",
        "# Inspect MMSE scores file\n",
        "try:\n",
        "    mmse_scores = pd.read_csv(mmse_scores_file)\n",
        "    print(\"MMSE scores file columns:\", mmse_scores.columns.tolist())\n",
        "    print(\"MMSE scores sample:\")\n",
        "    print(mmse_scores.head())\n",
        "    # Try common ID column names\n",
        "    id_columns = ['ID', 'id', 'participant_id', 'Participant_ID']\n",
        "    mmse_ids = set()\n",
        "    for col in id_columns:\n",
        "        if col in mmse_scores.columns:\n",
        "            mmse_ids = set(mmse_scores[col].apply(lambda x: f\"adrso{str(x).zfill(3)}\" if str(x).isdigit() else x))\n",
        "            break\n",
        "    print(f\"MMSE scores file contains {len(mmse_ids)} IDs\")\n",
        "    print(f\"Task1 IDs in MMSE scores: {len(task1_ids & mmse_ids)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"MMSE scores file not found: {mmse_scores_file}\")\n",
        "    mmse_ids = set()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading MMSE scores file: {e}\")\n",
        "    mmse_ids = set()\n",
        "\n",
        "# Collect segmentation IDs\n",
        "segmentation_ids = set()\n",
        "for path in [os.path.join(segmentation_base, 'cn/'), os.path.join(segmentation_base, 'ad/')]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f.split('.')[0] for f in os.listdir(path) if f.endswith('.csv')]\n",
        "        segmentation_ids.update(files)\n",
        "        print(f\"Found {len(files)} segmentation files in {path}\")\n",
        "    else:\n",
        "        print(f\"Segmentation directory not found: {path}\")\n",
        "print(f\"Total segmentation IDs: {len(segmentation_ids)}\")\n",
        "print(f\"Task1 IDs in segmentation: {len(task1_ids & segmentation_ids)}\")\n",
        "\n",
        "# Collect .wav files from cn/ and ad/, filtering by task1 IDs\n",
        "audio_files = []\n",
        "audio_id_to_path = {}\n",
        "for path in [cn_audio_path, ad_audio_path]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        for f in files:\n",
        "            audio_id = f.split('.')[0]\n",
        "            if audio_id in task1_ids:\n",
        "                audio_files.append(os.path.join(path, f))\n",
        "                audio_id_to_path[audio_id] = os.path.join(path, f)\n",
        "        print(f\"Found {len(files)} audio files in {path}, {sum(1 for f in files if f.split('.')[0] in task1_ids)} match task1 IDs\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {path}\")\n",
        "\n",
        "print(f\"Total audio files matching task1 IDs: {len(audio_files)}\")\n",
        "\n",
        "# Search for missing task1 IDs across all directories\n",
        "missing_audio_ids = task1_ids - set(audio_id_to_path.keys())\n",
        "print(f\"Task1 IDs missing audio files ({len(missing_audio_ids)}): {missing_audio_ids}\")\n",
        "\n",
        "# Recursive search for missing audio files\n",
        "found_missing = {}\n",
        "for missing_id in missing_audio_ids:\n",
        "    matches = glob.glob(f\"/content/**/{missing_id}.wav\", recursive=True)\n",
        "    if matches:\n",
        "        found_missing[missing_id] = matches\n",
        "if found_missing:\n",
        "    print(\"Found missing audio files:\")\n",
        "    for id_, paths in found_missing.items():\n",
        "        print(f\"{id_}: {paths}\")\n",
        "else:\n",
        "    print(\"No missing audio files found in /content/\")\n",
        "\n",
        "# Extract features for matching audio files\n",
        "audio_features = []\n",
        "audio_ids = []\n",
        "skipped_files = []\n",
        "for audio_path in audio_files:\n",
        "    audio_file = os.path.basename(audio_path)\n",
        "    audio_id = audio_file.split('.')[0]\n",
        "    features = extract_egemaps(audio_path)\n",
        "    if features is not None:\n",
        "        audio_features.append(features)\n",
        "        audio_ids.append(audio_id)\n",
        "    else:\n",
        "        print(f\"Skipping {audio_id} due to feature extraction failure\")\n",
        "        skipped_files.append(audio_id)\n",
        "\n",
        "print(f\"Extracted features for {len(audio_features)} audio files\")\n",
        "print(f\"Skipped {len(skipped_files)} audio files: {skipped_files}\")\n",
        "\n",
        "# Check if any features were extracted\n",
        "if not audio_features:\n",
        "    raise ValueError(\"No audio features extracted. Check audio files or extraction process.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "audio_features_df = pd.DataFrame(audio_features)\n",
        "audio_features_df['ID'] = audio_ids\n",
        "\n",
        "# Merge with task1 labels\n",
        "task1_data = pd.merge(audio_features_df, task1, on='ID', how='inner')\n",
        "print(\"Merged Acoustic Features with Labels:\")\n",
        "print(task1_data.head())\n",
        "print(f\"Number of matched records: {len(task1_data)}\")\n",
        "\n",
        "# Save the merged DataFrame\n",
        "task1_data.to_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv', index=False)\n",
        "print(\"Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\")\n",
        "\n",
        "# Save debugging info\n",
        "pd.DataFrame({'missing_audio_ids': list(missing_audio_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/missing_audio_ids.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'skipped_files': skipped_files}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/skipped_files.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'segmentation_ids': list(segmentation_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/segmentation_ids.csv', index=False\n",
        ")\n",
        "print(\"Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files,segmentation_ids}.csv\")\n",
        "\n",
        "# Note dataset limitation\n",
        "if len(missing_audio_ids) > 0:\n",
        "    print(f\"Warning: {len(missing_audio_ids)} task1.csv IDs lack audio files. Verify dataset completeness or check for additional .tgz files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1LFUBxj4JuZ",
        "outputId": "8c7e659b-3dc5-403d-da6f-c4f7848e10d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task1.csv contains 71 IDs\n",
            "MMSE scores file columns: ['Unnamed: 0', 'adressfname', 'mmse', 'dx']\n",
            "MMSE scores sample:\n",
            "   Unnamed: 0 adressfname  mmse  dx\n",
            "0          23    adrso024    20  ad\n",
            "1          24    adrso025    11  ad\n",
            "2          25    adrso027    18  ad\n",
            "3          26    adrso028    18  ad\n",
            "4          28    adrso031    26  ad\n",
            "MMSE scores file contains 0 IDs\n",
            "Task1 IDs in MMSE scores: 0\n",
            "Found 79 segmentation files in /content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/\n",
            "Found 87 segmentation files in /content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/\n",
            "Total segmentation IDs: 166\n",
            "Task1 IDs in segmentation: 41\n",
            "Found 79 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/, 16 match task1 IDs\n",
            "Found 87 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/, 25 match task1 IDs\n",
            "Total audio files matching task1 IDs: 41\n",
            "Task1 IDs missing audio files (30): {'adrso050', 'adrso051', 'adrso067', 'adrso004', 'adrso011', 'adrso062', 'adrso058', 'adrso009', 'adrso044', 'adrso064', 'adrso061', 'adrso052', 'adrso069', 'adrso040', 'adrso006', 'adrso034', 'adrso038', 'adrso065', 'adrso041', 'adrso020', 'adrso048', 'adrso001', 'adrso026', 'adrso066', 'adrso042', 'adrso037', 'adrso029', 'adrso057', 'adrso030', 'adrso013'}\n",
            "No missing audio files found in /content/\n",
            "Extracted features for 41 audio files\n",
            "Skipped 0 audio files: []\n",
            "Merged Acoustic Features with Labels:\n",
            "           0         1          2          3          4          5  \\\n",
            "0  34.314342  0.172523  31.954039  34.558792  38.469227   6.515188   \n",
            "1  34.439098  0.178912  29.944578  33.201965  39.123035   9.178457   \n",
            "2  34.765678  0.144698  31.995552  35.011833  37.706375   5.710823   \n",
            "3  30.145615  0.129570  28.376390  29.582561  32.609303   4.232912   \n",
            "4  31.052141  0.345289  22.244028  25.008968  40.717941  18.473913   \n",
            "\n",
            "            6           7           8           9  ...        80        81  \\\n",
            "0  332.870453  461.649567  120.299301   81.307747  ...  0.544044  1.976285   \n",
            "1  169.268906  333.093689  160.969574  267.036377  ...  0.434866  2.533154   \n",
            "2  231.058731  375.145050  145.108765  262.729279  ...  0.130108  1.557071   \n",
            "3  292.438629  535.989441   76.417542   96.036621  ...  0.086000  1.878543   \n",
            "4  546.450195  783.379028  370.438660  510.268463  ...  0.291039  1.648093   \n",
            "\n",
            "         82        83        84        85        86         87        ID  \\\n",
            "0  2.508803  0.112807  0.123327  0.279630  0.317927 -12.181213  adrso010   \n",
            "1  2.251715  0.204901  0.223378  0.228958  0.342663 -11.879815  adrso014   \n",
            "2  2.098893  0.159448  0.190493  0.289759  0.382377 -23.670115  adrso015   \n",
            "3  2.674664  0.190667  0.199302  0.170000  0.260768 -26.691757  adrso005   \n",
            "4  3.502985  0.085695  0.098524  0.192019  0.217589 -19.202145  adrso018   \n",
            "\n",
            "           Dx  \n",
            "0     Control  \n",
            "1  ProbableAD  \n",
            "2     Control  \n",
            "3     Control  \n",
            "4  ProbableAD  \n",
            "\n",
            "[5 rows x 90 columns]\n",
            "Number of matched records: 41\n",
            "Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\n",
            "Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files,segmentation_ids}.csv\n",
            "Warning: 30 task1.csv IDs lack audio files. Verify dataset completeness or check for additional .tgz files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opensmile\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "# Initialize opensmile for eGeMAPS feature extraction\n",
        "smile = opensmile.Smile(\n",
        "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "    feature_level=opensmile.FeatureLevel.Functionals\n",
        ")\n",
        "\n",
        "# Function to extract eGeMAPS features from an audio file\n",
        "def extract_egemaps(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
        "        features = smile.process_signal(y, sr)  # Extract eGeMAPS\n",
        "        return features.values.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Paths to data\n",
        "data_path = '/content/drive/MyDrive/Voice/'\n",
        "diagnosis_audio_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/audio/'\n",
        "cn_audio_path = os.path.join(diagnosis_audio_base, 'cn/')\n",
        "ad_audio_path = os.path.join(diagnosis_audio_base, 'ad/')\n",
        "segmentation_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/'\n",
        "mmse_scores_file = '/content/diagnosis_train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv'\n",
        "\n",
        "# Load task1.csv\n",
        "task1 = pd.read_csv(data_path + 'task1.csv')\n",
        "task1['ID'] = task1['ID'].apply(lambda x: 'adrso' + x.replace('adrsdt', '').zfill(3))\n",
        "task1_ids = set(task1['ID'])\n",
        "print(f\"Task1.csv contains {len(task1)} IDs\")\n",
        "\n",
        "# Inspect MMSE scores file\n",
        "try:\n",
        "    mmse_scores = pd.read_csv(mmse_scores_file)\n",
        "    print(\"MMSE scores file columns:\", mmse_scores.columns.tolist())\n",
        "    print(\"MMSE scores sample:\")\n",
        "    print(mmse_scores.head())\n",
        "    if 'adressfname' in mmse_scores.columns:\n",
        "        mmse_ids = set(mmse_scores['adressfname'])\n",
        "    else:\n",
        "        mmse_ids = set()\n",
        "        print(\"No 'adressfname' column in MMSE scores file\")\n",
        "    print(f\"MMSE scores file contains {len(mmse_ids)} IDs\")\n",
        "    print(f\"Task1 IDs in MMSE scores: {len(task1_ids & mmse_ids)}\")\n",
        "    print(f\"Missing task1 IDs in MMSE scores: {len(missing_audio_ids & mmse_ids)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"MMSE scores file not found: {mmse_scores_file}\")\n",
        "    mmse_ids = set()\n",
        "except Exception as e:\n",
        "    print(f\"Error reading MMSE scores file: {e}\")\n",
        "    mmse_ids = set()\n",
        "\n",
        "# Collect segmentation IDs\n",
        "segmentation_ids = set()\n",
        "for path in [os.path.join(segmentation_base, 'cn/'), os.path.join(segmentation_base, 'ad/')]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f.split('.')[0] for f in os.listdir(path) if f.endswith('.csv')]\n",
        "        segmentation_ids.update(files)\n",
        "        print(f\"Found {len(files)} segmentation files in {path}\")\n",
        "    else:\n",
        "        print(f\"Segmentation directory not found: {path}\")\n",
        "print(f\"Total segmentation IDs: {len(segmentation_ids)}\")\n",
        "print(f\"Task1 IDs in segmentation: {len(task1_ids & segmentation_ids)}\")\n",
        "\n",
        "# Collect .wav files from cn/ and ad/, filtering by task1 IDs\n",
        "audio_files = []\n",
        "audio_id_to_path = {}\n",
        "for path in [cn_audio_path, ad_audio_path]:\n",
        "    if os.path.exists(path):\n",
        "        files = [f for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        for f in files:\n",
        "            audio_id = f.split('.')[0]\n",
        "            if audio_id in task1_ids:\n",
        "                audio_files.append(os.path.join(path, f))\n",
        "                audio_id_to_path[audio_id] = os.path.join(path, f)\n",
        "        print(f\"Found {len(files)} audio files in {path}, {sum(1 for f in files if f.split('.')[0] in task1_ids)} match task1 IDs\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {path}\")\n",
        "\n",
        "print(f\"Total audio files matching task1 IDs: {len(audio_files)}\")\n",
        "\n",
        "# Search for missing task1 IDs across all directories\n",
        "missing_audio_ids = task1_ids - set(audio_id_to_path.keys())\n",
        "print(f\"Task1 IDs missing audio files ({len(missing_audio_ids)}): {missing_audio_ids}\")\n",
        "\n",
        "# Recursive search for missing audio files\n",
        "found_missing = {}\n",
        "for missing_id in missing_audio_ids:\n",
        "    matches = glob.glob(f\"/content/**/{missing_id}.wav\", recursive=True)\n",
        "    if matches:\n",
        "        found_missing[missing_id] = matches\n",
        "if found_missing:\n",
        "    print(\"Found missing audio files:\")\n",
        "    for id_, paths in found_missing.items():\n",
        "        print(f\"{id_}: {paths}\")\n",
        "else:\n",
        "    print(\"No missing audio files found in /content/\")\n",
        "\n",
        "# Extract features for matching audio files\n",
        "audio_features = []\n",
        "audio_ids = []\n",
        "skipped_files = []\n",
        "for audio_path in audio_files:\n",
        "    audio_file = os.path.basename(audio_path)\n",
        "    audio_id = audio_file.split('.')[0]\n",
        "    features = extract_egemaps(audio_path)\n",
        "    if features is not None:\n",
        "        audio_features.append(features)\n",
        "        audio_ids.append(audio_id)\n",
        "    else:\n",
        "        print(f\"Skipping {audio_id} due to feature extraction failure\")\n",
        "        skipped_files.append(audio_id)\n",
        "\n",
        "print(f\"Extracted features for {len(audio_features)} audio files\")\n",
        "print(f\"Skipped {len(skipped_files)} audio files: {skipped_files}\")\n",
        "\n",
        "# Check if any features were extracted\n",
        "if not audio_features:\n",
        "    raise ValueError(\"No audio features extracted. Check audio files or extraction process.\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "audio_features_df = pd.DataFrame(audio_features)\n",
        "audio_features_df['ID'] = audio_ids\n",
        "\n",
        "# Merge with task1 labels\n",
        "task1_data = pd.merge(audio_features_df, task1, on='ID', how='inner')\n",
        "print(\"Merged Acoustic Features with Labels:\")\n",
        "print(task1_data.head())\n",
        "print(f\"Number of matched records: {len(task1_data)}\")\n",
        "\n",
        "# Save the merged DataFrame\n",
        "task1_data.to_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv', index=False)\n",
        "print(\"Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\")\n",
        "\n",
        "# Save debugging info\n",
        "pd.DataFrame({'missing_audio_ids': list(missing_audio_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/missing_audio_ids.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'skipped_files': skipped_files}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/skipped_files.csv', index=False\n",
        ")\n",
        "pd.DataFrame({'segmentation_ids': list(segmentation_ids)}).to_csv(\n",
        "    '/content/drive/MyDrive/Voice/segmentation_ids.csv', index=False\n",
        ")\n",
        "print(\"Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files,segmentation_ids}.csv\")\n",
        "\n",
        "# Note dataset limitation\n",
        "if len(missing_audio_ids) > 0:\n",
        "    print(f\"Warning: {len(missing_audio_ids)} task1.csv IDs lack audio files. Verify dataset completeness or check for additional .tgz files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kjHd5PX5BGx",
        "outputId": "d337ca29-3baf-41d3-ad1c-4a358754f8ae"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task1.csv contains 71 IDs\n",
            "MMSE scores file columns: ['Unnamed: 0', 'adressfname', 'mmse', 'dx']\n",
            "MMSE scores sample:\n",
            "   Unnamed: 0 adressfname  mmse  dx\n",
            "0          23    adrso024    20  ad\n",
            "1          24    adrso025    11  ad\n",
            "2          25    adrso027    18  ad\n",
            "3          26    adrso028    18  ad\n",
            "4          28    adrso031    26  ad\n",
            "MMSE scores file contains 166 IDs\n",
            "Task1 IDs in MMSE scores: 41\n",
            "Missing task1 IDs in MMSE scores: 0\n",
            "Found 79 segmentation files in /content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/\n",
            "Found 87 segmentation files in /content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/\n",
            "Total segmentation IDs: 166\n",
            "Task1 IDs in segmentation: 41\n",
            "Found 79 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/, 16 match task1 IDs\n",
            "Found 87 audio files in /content/diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/, 25 match task1 IDs\n",
            "Total audio files matching task1 IDs: 41\n",
            "Task1 IDs missing audio files (30): {'adrso050', 'adrso051', 'adrso067', 'adrso004', 'adrso011', 'adrso062', 'adrso058', 'adrso009', 'adrso044', 'adrso064', 'adrso061', 'adrso052', 'adrso069', 'adrso040', 'adrso006', 'adrso034', 'adrso038', 'adrso065', 'adrso041', 'adrso020', 'adrso048', 'adrso001', 'adrso026', 'adrso066', 'adrso042', 'adrso037', 'adrso029', 'adrso057', 'adrso030', 'adrso013'}\n",
            "No missing audio files found in /content/\n",
            "Extracted features for 41 audio files\n",
            "Skipped 0 audio files: []\n",
            "Merged Acoustic Features with Labels:\n",
            "           0         1          2          3          4          5  \\\n",
            "0  34.314342  0.172523  31.954039  34.558792  38.469227   6.515188   \n",
            "1  34.439098  0.178912  29.944578  33.201965  39.123035   9.178457   \n",
            "2  34.765678  0.144698  31.995552  35.011833  37.706375   5.710823   \n",
            "3  30.145615  0.129570  28.376390  29.582561  32.609303   4.232912   \n",
            "4  31.052141  0.345289  22.244028  25.008968  40.717941  18.473913   \n",
            "\n",
            "            6           7           8           9  ...        80        81  \\\n",
            "0  332.870453  461.649567  120.299301   81.307747  ...  0.544044  1.976285   \n",
            "1  169.268906  333.093689  160.969574  267.036377  ...  0.434866  2.533154   \n",
            "2  231.058731  375.145050  145.108765  262.729279  ...  0.130108  1.557071   \n",
            "3  292.438629  535.989441   76.417542   96.036621  ...  0.086000  1.878543   \n",
            "4  546.450195  783.379028  370.438660  510.268463  ...  0.291039  1.648093   \n",
            "\n",
            "         82        83        84        85        86         87        ID  \\\n",
            "0  2.508803  0.112807  0.123327  0.279630  0.317927 -12.181213  adrso010   \n",
            "1  2.251715  0.204901  0.223378  0.228958  0.342663 -11.879815  adrso014   \n",
            "2  2.098893  0.159448  0.190493  0.289759  0.382377 -23.670115  adrso015   \n",
            "3  2.674664  0.190667  0.199302  0.170000  0.260768 -26.691757  adrso005   \n",
            "4  3.502985  0.085695  0.098524  0.192019  0.217589 -19.202145  adrso018   \n",
            "\n",
            "           Dx  \n",
            "0     Control  \n",
            "1  ProbableAD  \n",
            "2     Control  \n",
            "3     Control  \n",
            "4  ProbableAD  \n",
            "\n",
            "[5 rows x 90 columns]\n",
            "Number of matched records: 41\n",
            "Saved acoustic features to /content/drive/MyDrive/Voice/task1_acoustic_features.csv\n",
            "Saved debugging info to /content/drive/MyDrive/Voice/{missing_audio_ids,skipped_files,segmentation_ids}.csv\n",
            "Warning: 30 task1.csv IDs lack audio files. Verify dataset completeness or check for additional .tgz files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Linguistic Feature Extraction (Simulated)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load task1_data from Step 3\n",
        "task1_data = pd.read_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv')\n",
        "\n",
        "# Simulate linguistic features from eval-summary.docx (DIAGNOSIS-TRAIN-AD/CN)\n",
        "linguistic_features = {\n",
        "    'ID': task1_data['ID'],\n",
        "    'MLU': np.random.normal(12.625, 11.835, len(task1_data)),  # Mean of AD (8.68) and CN (17.57)\n",
        "    'TTR': np.random.normal(0.665, 0.13, len(task1_data)),     # Mean of AD (0.66) and CN (0.67)\n",
        "    '%_nouns': np.random.normal(22.745, 7.3, len(task1_data)), # Mean of AD (20.62) and CN (24.87)\n",
        "    '%_verbs': np.random.normal(19.015, 4.55, len(task1_data)) # Mean of AD (19.12) and CN (18.91)\n",
        "}\n",
        "\n",
        "linguistic_features_df = pd.DataFrame(linguistic_features)\n",
        "\n",
        "# Merge with task1_data labels\n",
        "task1_linguistic_data = pd.merge(linguistic_features_df, task1_data[['ID', 'Dx']], on='ID')\n",
        "print(\"Merged Linguistic Features with Labels:\")\n",
        "print(task1_linguistic_data.head())\n",
        "\n",
        "# Save for next steps\n",
        "task1_linguistic_data.to_csv('/content/drive/MyDrive/Voice/task1_linguistic_features.csv', index=False)\n",
        "print(\"Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvHBQfvw5PuM",
        "outputId": "23a397b2-35ae-4aa0-e931-02c91250b89e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Linguistic Features with Labels:\n",
            "         ID        MLU       TTR    %_nouns    %_verbs          Dx\n",
            "0  adrso010   6.189669  0.826973  23.874254  19.282911     Control\n",
            "1  adrso014   8.670293  0.560556  28.436979  22.503496  ProbableAD\n",
            "2  adrso015  18.837356  0.811275  36.690959  23.657292     Control\n",
            "3  adrso005   4.086280  0.636485  26.283843  21.318973     Control\n",
            "4  adrso018  15.007849  0.469754  10.600154  19.260291  ProbableAD\n",
            "Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pylangacq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEEDuMbm70X8",
        "outputId": "d460cd0f-3e06-4a62-99e2-8e65044b6fb3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylangacq\n",
            "  Downloading pylangacq-0.19.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pylangacq) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from pylangacq) (2.32.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from tabulate[widechars]>=0.8.9->pylangacq) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.0.0->pylangacq) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.0->pylangacq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.0->pylangacq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.0->pylangacq) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.0->pylangacq) (2025.4.26)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from tabulate[widechars]>=0.8.9->pylangacq) (0.2.13)\n",
            "Downloading pylangacq-0.19.1-py3-none-any.whl (85 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pylangacq\n",
            "Successfully installed pylangacq-0.19.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "from pylangacq import Reader\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Paths\n",
        "cha_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/'\n",
        "task1_data = pd.read_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv')\n",
        "task1_ids = set(task1_data['ID'])\n",
        "\n",
        "# Collect .cha files\n",
        "cha_files = []\n",
        "for root, _, files in os.walk(cha_base):\n",
        "    cha_files.extend([os.path.join(root, f) for f in files if f.endswith('.cha')])\n",
        "print(f\"Found {len(cha_files)} .cha files\")\n",
        "\n",
        "# Extract linguistic features\n",
        "linguistic_features = {'ID': [], 'MLU': [], 'TTR': [], '%_nouns': [], '%_verbs': []}\n",
        "for cha_file in cha_files:\n",
        "    cha_id = os.path.basename(cha_file).split('.')[0]\n",
        "    if cha_id in task1_ids:\n",
        "        try:\n",
        "            # Read .cha file\n",
        "            chat = Reader.from_files([cha_file])\n",
        "\n",
        "            # Get utterances (participant 'PAR' for participant speech)\n",
        "            utterances = chat.utterances(participant='PAR')\n",
        "            words = []\n",
        "            for utt in utterances:\n",
        "                words.extend(word_tokenize(utt.tier.lower()))\n",
        "\n",
        "            # POS tagging\n",
        "            pos_tags = pos_tag(words)\n",
        "\n",
        "            # MLU: Mean length of utterances\n",
        "            mlu = len(words) / len(utterances) if utterances else 0\n",
        "\n",
        "            # TTR: Type-token ratio\n",
        "            types = len(set(words))\n",
        "            tokens = len(words)\n",
        "            ttr = types / tokens if tokens > 0 else 0\n",
        "\n",
        "            # % Nouns and Verbs\n",
        "            noun_count = sum(1 for _, tag in pos_tags if tag.startswith('NN'))\n",
        "            verb_count = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
        "            total_pos = len(pos_tags)\n",
        "            percent_nouns = (noun_count / total_pos * 100) if total_pos > 0 else 0\n",
        "            percent_verbs = (verb_count / total_pos * 100) if total_pos > 0 else 0\n",
        "\n",
        "            linguistic_features['ID'].append(cha_id)\n",
        "            linguistic_features['MLU'].append(mlu)\n",
        "            linguistic_features['TTR'].append(ttr)\n",
        "            linguistic_features['%_nouns'].append(percent_nouns)\n",
        "            linguistic_features['%_verbs'].append(percent_verbs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {cha_file}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "linguistic_features_df = pd.DataFrame(linguistic_features)\n",
        "\n",
        "# Merge with task1_data labels\n",
        "task1_linguistic_data = pd.merge(linguistic_features_df, task1_data[['ID', 'Dx']], on='ID')\n",
        "print(\"Merged Linguistic Features with Labels:\")\n",
        "print(task1_linguistic_data.head())\n",
        "\n",
        "# Save\n",
        "task1_linguistic_data.to_csv('/content/drive/MyDrive/Voice/task1_linguistic_features.csv', index=False)\n",
        "print(\"Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqXE3bRS6VZh",
        "outputId": "0ce25a1b-9538-4f7d-a842-099dd80c78e8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 .cha files\n",
            "Merged Linguistic Features with Labels:\n",
            "Empty DataFrame\n",
            "Columns: [ID, MLU, TTR, %_nouns, %_verbs, Dx]\n",
            "Index: []\n",
            "Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_csv = '/content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso010.csv'\n",
        "print(pd.read_csv(sample_csv).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXSMEHt_73Zl",
        "outputId": "3e9891ea-f383-464c-cfeb-52dc949f8ad5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0 speaker  begin    end\n",
            "0           1     PAR      0    503\n",
            "1           2     PAR    503   3613\n",
            "2           3     PAR   3613   6131\n",
            "3           4     PAR   6131   9415\n",
            "4           5     PAR   9415  17404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Paths\n",
        "segmentation_base = '/content/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/'\n",
        "task1_data = pd.read_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv')\n",
        "task1_ids = set(task1_data['ID'])\n",
        "\n",
        "# Inspect a segmentation CSV\n",
        "sample_csv = os.path.join(segmentation_base, 'cn/adrso010.csv')\n",
        "print(\"Sample segmentation CSV:\")\n",
        "print(pd.read_csv(sample_csv).head())\n",
        "\n",
        "# Collect segmentation CSVs\n",
        "linguistic_features = {'ID': [], 'MLU': [], 'TTR': [], '%_nouns': [], '%_verbs': []}\n",
        "for path in [os.path.join(segmentation_base, 'cn/'), os.path.join(segmentation_base, 'ad/')]:\n",
        "    if os.path.exists(path):\n",
        "        for f in os.listdir(path):\n",
        "            if f.endswith('.csv'):\n",
        "                seg_id = f.split('.')[0]\n",
        "                if seg_id in task1_ids:\n",
        "                    try:\n",
        "                        seg_df = pd.read_csv(os.path.join(path, f))\n",
        "                        # Adjust 'text' to the actual column name (e.g., 'utterance', 'transcription')\n",
        "                        if 'text' not in seg_df.columns:\n",
        "                            print(f\"No text column in {f}, skipping\")\n",
        "                            continue\n",
        "                        utterances = seg_df['text'].dropna().tolist()\n",
        "                        words = []\n",
        "                        for utt in utterances:\n",
        "                            words.extend(word_tokenize(str(utt).lower()))\n",
        "                        pos_tags = pos_tag(words)\n",
        "\n",
        "                        # MLU\n",
        "                        mlu = len(words) / len(utterances) if utterances else 0\n",
        "\n",
        "                        # TTR\n",
        "                        types = len(set(words))\n",
        "                        tokens = len(words)\n",
        "                        ttr = types / tokens if tokens > 0 else 0\n",
        "\n",
        "                        # % Nouns and Verbs\n",
        "                        noun_count = sum(1 for _, tag in pos_tags if tag.startswith('NN'))\n",
        "                        verb_count = sum(1 for _, tag in pos_tags if tag.startswith('VB'))\n",
        "                        total_pos = len(pos_tags)\n",
        "                        percent_nouns = (noun_count / total_pos * 100) if total_pos > 0 else 0\n",
        "                        percent_verbs = (verb_count / total_pos * 100) if total_pos > 0 else 0\n",
        "\n",
        "                        linguistic_features['ID'].append(seg_id)\n",
        "                        linguistic_features['MLU'].append(mlu)\n",
        "                        linguistic_features['TTR'].append(ttr)\n",
        "                        linguistic_features['%_nouns'].append(percent_nouns)\n",
        "                        linguistic_features['%_verbs'].append(percent_verbs)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing {f}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "linguistic_features_df = pd.DataFrame(linguistic_features)\n",
        "\n",
        "# Merge with task1_data labels\n",
        "task1_linguistic_data = pd.merge(linguistic_features_df, task1_data[['ID', 'Dx']], on='ID')\n",
        "print(\"Merged Linguistic Features with Labels:\")\n",
        "print(task1_linguistic_data.head())\n",
        "\n",
        "# Save\n",
        "task1_linguistic_data.to_csv('/content/drive/MyDrive/Voice/task1_linguistic_features.csv', index=False)\n",
        "print(\"Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX6F9jbQ8J-m",
        "outputId": "5fd24a0e-bb2c-4e6d-dd69-7cb5589959d5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample segmentation CSV:\n",
            "   Unnamed: 0 speaker  begin    end\n",
            "0           1     PAR      0    503\n",
            "1           2     PAR    503   3613\n",
            "2           3     PAR   3613   6131\n",
            "3           4     PAR   6131   9415\n",
            "4           5     PAR   9415  17404\n",
            "No text column in adrso018.csv, skipping\n",
            "No text column in adrso017.csv, skipping\n",
            "No text column in adrso012.csv, skipping\n",
            "No text column in adrso023.csv, skipping\n",
            "No text column in adrso007.csv, skipping\n",
            "No text column in adrso014.csv, skipping\n",
            "No text column in adrso022.csv, skipping\n",
            "No text column in adrso019.csv, skipping\n",
            "No text column in adrso021.csv, skipping\n",
            "No text column in adrso003.csv, skipping\n",
            "No text column in adrso005.csv, skipping\n",
            "No text column in adrso002.csv, skipping\n",
            "No text column in adrso015.csv, skipping\n",
            "No text column in adrso016.csv, skipping\n",
            "No text column in adrso010.csv, skipping\n",
            "No text column in adrso008.csv, skipping\n",
            "No text column in adrso039.csv, skipping\n",
            "No text column in adrso032.csv, skipping\n",
            "No text column in adrso060.csv, skipping\n",
            "No text column in adrso045.csv, skipping\n",
            "No text column in adrso054.csv, skipping\n",
            "No text column in adrso027.csv, skipping\n",
            "No text column in adrso049.csv, skipping\n",
            "No text column in adrso036.csv, skipping\n",
            "No text column in adrso025.csv, skipping\n",
            "No text column in adrso024.csv, skipping\n",
            "No text column in adrso059.csv, skipping\n",
            "No text column in adrso035.csv, skipping\n",
            "No text column in adrso043.csv, skipping\n",
            "No text column in adrso071.csv, skipping\n",
            "No text column in adrso055.csv, skipping\n",
            "No text column in adrso063.csv, skipping\n",
            "No text column in adrso053.csv, skipping\n",
            "No text column in adrso031.csv, skipping\n",
            "No text column in adrso028.csv, skipping\n",
            "No text column in adrso056.csv, skipping\n",
            "No text column in adrso046.csv, skipping\n",
            "No text column in adrso047.csv, skipping\n",
            "No text column in adrso068.csv, skipping\n",
            "No text column in adrso033.csv, skipping\n",
            "No text column in adrso070.csv, skipping\n",
            "Merged Linguistic Features with Labels:\n",
            "Empty DataFrame\n",
            "Columns: [ID, MLU, TTR, %_nouns, %_verbs, Dx]\n",
            "Index: []\n",
            "Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restored Step 4: Simulated Linguistic Features"
      ],
      "metadata": {
        "id": "BvbogH_T8tv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load task1_data from Step 3\n",
        "task1_data = pd.read_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv')\n",
        "\n",
        "# Simulate linguistic features from eval-summary.docx (DIAGNOSIS-TRAIN-AD/CN)\n",
        "linguistic_features = {\n",
        "    'ID': task1_data['ID'],\n",
        "    'MLU': np.random.normal(12.625, 11.835, len(task1_data)),  # Mean of AD (8.68) and CN (17.57)\n",
        "    'TTR': np.random.normal(0.665, 0.13, len(task1_data)),     # Mean of AD (0.66) and CN (0.67)\n",
        "    '%_nouns': np.random.normal(22.745, 7.3, len(task1_data)), # Mean of AD (20.62) and CN (24.87)\n",
        "    '%_verbs': np.random.normal(19.015, 4.55, len(task1_data)) # Mean of AD (19.12) and CN (18.91)\n",
        "}\n",
        "\n",
        "linguistic_features_df = pd.DataFrame(linguistic_features)\n",
        "\n",
        "# Merge with task1_data labels\n",
        "task1_linguistic_data = pd.merge(linguistic_features_df, task1_data[['ID', 'Dx']], on='ID')\n",
        "print(\"Merged Linguistic Features with Labels:\")\n",
        "print(task1_linguistic_data.head())\n",
        "\n",
        "# Save for next steps\n",
        "task1_linguistic_data.to_csv('/content/drive/MyDrive/Voice/task1_linguistic_features.csv', index=False)\n",
        "print(\"Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsB04PtM8p0t",
        "outputId": "706dc9e3-329e-4ed4-861e-fbbfb418786e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Linguistic Features with Labels:\n",
            "         ID        MLU       TTR    %_nouns    %_verbs          Dx\n",
            "0  adrso010  27.396049  0.441229  29.927782  20.649705     Control\n",
            "1  adrso014   8.033289  0.882510  10.582943  25.184818  ProbableAD\n",
            "2  adrso015  23.863713  0.854305  28.208900  20.508639     Control\n",
            "3  adrso005   9.619413  0.529333  28.138312  18.727957     Control\n",
            "4  adrso018  34.630692  0.533147  22.200092  25.231971  ProbableAD\n",
            "Saved linguistic features to /content/drive/MyDrive/Voice/task1_linguistic_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load data\n",
        "acoustic_df = pd.read_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv')\n",
        "linguistic_df = pd.read_csv('/content/drive/MyDrive/Voice/task1_linguistic_features.csv')\n",
        "\n",
        "# Merge features\n",
        "merged_df = pd.merge(acoustic_df.drop(columns=['Dx']), linguistic_df, on='ID')\n",
        "print(\"Merged features shape:\", merged_df.shape)\n",
        "print(\"Dx distribution:\", merged_df['Dx'].value_counts())\n",
        "\n",
        "# Prepare features and labels\n",
        "X = merged_df.drop(columns=['ID', 'Dx'])\n",
        "y = merged_df['Dx'].map({'Control': 0, 'ProbableAD': 1})\n",
        "\n",
        "# Split data (20% test set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear', random_state=42, class_weight='balanced')\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate SVM\n",
        "y_pred_svm = svm.predict(X_test_scaled)\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=['Control', 'ProbableAD']))\n",
        "\n",
        "# Cross-validation\n",
        "svm_cv_scores = cross_val_score(svm, scaler.transform(X), y, cv=5, scoring='accuracy')\n",
        "print(\"SVM Cross-Validation Accuracy: Mean =\", svm_cv_scores.mean(), \"Std =\", svm_cv_scores.std())\n",
        "\n",
        "# Try Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf.predict(X_test_scaled)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Control', 'ProbableAD']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TVRH67E8y3h",
        "outputId": "7e3f6f11-23b1-4412-9d63-85fd96dde5e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged features shape: (41, 94)\n",
            "Dx distribution: Dx\n",
            "ProbableAD    21\n",
            "Control       20\n",
            "Name: count, dtype: int64\n",
            "SVM Accuracy: 0.4444444444444444\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.40      0.50      0.44         4\n",
            "  ProbableAD       0.50      0.40      0.44         5\n",
            "\n",
            "    accuracy                           0.44         9\n",
            "   macro avg       0.45      0.45      0.44         9\n",
            "weighted avg       0.46      0.44      0.44         9\n",
            "\n",
            "SVM Cross-Validation Accuracy: Mean = 0.538888888888889 Std = 0.13362237189784104\n",
            "Random Forest Accuracy: 0.4444444444444444\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.40      0.50      0.44         4\n",
            "  ProbableAD       0.50      0.40      0.44         5\n",
            "\n",
            "    accuracy                           0.44         9\n",
            "   macro avg       0.45      0.45      0.44         9\n",
            "weighted avg       0.46      0.44      0.44         9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Load data\n",
        "acoustic_df = pd.read_csv('/content/drive/MyDrive/Voice/task1_acoustic_features.csv')\n",
        "linguistic_df = pd.read_csv('/content/drive/MyDrive/Voice/task1_linguistic_features.csv')\n",
        "\n",
        "# Merge features\n",
        "merged_df = pd.merge(acoustic_df.drop(columns=['Dx']), linguistic_df, on='ID')\n",
        "print(\"Merged features shape:\", merged_df.shape)\n",
        "print(\"Dx distribution:\", merged_df['Dx'].value_counts())\n",
        "\n",
        "# Prepare features and labels\n",
        "X = merged_df.drop(columns=['ID', 'Dx'])\n",
        "y = merged_df['Dx'].map({'Control': 0, 'ProbableAD': 1})\n",
        "\n",
        "# Split data (20% test set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature selection with Random Forest\n",
        "scaler = StandardScaler()\n",
        "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "selector = SelectFromModel(rf_selector, max_features=20)  # Select top 20 features\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', scaler),\n",
        "    ('selector', selector)\n",
        "])\n",
        "X_train_selected = pipeline.fit_transform(X_train, y_train)\n",
        "X_test_selected = pipeline.transform(X_test)\n",
        "print(\"Selected features shape:\", X_train_selected.shape)\n",
        "\n",
        "# SVM with Grid Search\n",
        "svm = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "svm_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "}\n",
        "svm_grid = GridSearchCV(svm, svm_param_grid, cv=5, scoring='accuracy')\n",
        "svm_grid.fit(X_train_selected, y_train)\n",
        "print(\"Best SVM Parameters:\", svm_grid.best_params_)\n",
        "print(\"Best SVM CV Score:\", svm_grid.best_score_)\n",
        "\n",
        "# Evaluate SVM\n",
        "y_pred_svm = svm_grid.predict(X_test_selected)\n",
        "print(\"SVM Test Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm, target_names=['Control', 'ProbableAD']))\n",
        "\n",
        "# Cross-validation for SVM\n",
        "svm_best = svm_grid.best_estimator_\n",
        "svm_cv_scores = cross_val_score(svm_best, pipeline.transform(X), y, cv=5, scoring='accuracy')\n",
        "print(\"SVM Cross-Validation Accuracy: Mean =\", svm_cv_scores.mean(), \"Std =\", svm_cv_scores.std())\n",
        "\n",
        "# Random Forest with Grid Search\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='accuracy')\n",
        "rf_grid.fit(X_train_selected, y_train)\n",
        "print(\"Best RF Parameters:\", rf_grid.best_params_)\n",
        "print(\"Best RF CV Score:\", rf_grid.best_score_)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "y_pred_rf = rf_grid.predict(X_test_selected)\n",
        "print(\"Random Forest Test Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Control', 'ProbableAD']))\n",
        "\n",
        "# Try Logistic Regression\n",
        "lr = LogisticRegression(class_weight='balanced', random_state=42)\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "}\n",
        "lr_grid = GridSearchCV(lr, lr_param_grid, cv=5, scoring='accuracy')\n",
        "lr_grid.fit(X_train_selected, y_train)\n",
        "print(\"Best LR Parameters:\", lr_grid.best_params_)\n",
        "print(\"Best LR CV Score:\", lr_grid.best_score_)\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "y_pred_lr = lr_grid.predict(X_test_selected)\n",
        "print(\"Logistic Regression Test Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Control', 'ProbableAD']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-GLHCpe9H2l",
        "outputId": "0e2df8ed-bdef-4985-a957-a121770f5965"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged features shape: (41, 94)\n",
            "Dx distribution: Dx\n",
            "ProbableAD    21\n",
            "Control       20\n",
            "Name: count, dtype: int64\n",
            "Selected features shape: (32, 20)\n",
            "Best SVM Parameters: {'C': 0.1}\n",
            "Best SVM CV Score: 0.8428571428571429\n",
            "SVM Test Accuracy: 0.2222222222222222\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.29      0.50      0.36         4\n",
            "  ProbableAD       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.22         9\n",
            "   macro avg       0.14      0.25      0.18         9\n",
            "weighted avg       0.13      0.22      0.16         9\n",
            "\n",
            "SVM Cross-Validation Accuracy: Mean = 0.6361111111111111 Std = 0.12832010513834552\n",
            "Best RF Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best RF CV Score: 0.7523809523809524\n",
            "Random Forest Test Accuracy: 0.4444444444444444\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.40      0.50      0.44         4\n",
            "  ProbableAD       0.50      0.40      0.44         5\n",
            "\n",
            "    accuracy                           0.44         9\n",
            "   macro avg       0.45      0.45      0.44         9\n",
            "weighted avg       0.46      0.44      0.44         9\n",
            "\n",
            "Best LR Parameters: {'C': 1}\n",
            "Best LR CV Score: 0.8761904761904763\n",
            "Logistic Regression Test Accuracy: 0.3333333333333333\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.33      0.50      0.40         4\n",
            "  ProbableAD       0.33      0.20      0.25         5\n",
            "\n",
            "    accuracy                           0.33         9\n",
            "   macro avg       0.33      0.35      0.33         9\n",
            "weighted avg       0.33      0.33      0.32         9\n",
            "\n"
          ]
        }
      ]
    }
  ]
}