{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6hYQVwNF/yYkfPf4Fl6Sq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Final_prediction_From_Addresso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28iz8aJYvEx",
        "outputId": "ce4811b6-4a96-4552-be3a-7f0d906f1268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Collecting audobject>=0.6.1 (from opensmile)\n",
            "  Downloading audobject-0.7.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting audinterface>=0.7.0 (from opensmile)\n",
            "  Downloading audinterface-1.3.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Collecting audeer>=2.1.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audeer-2.2.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audformat-1.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audiofile-1.5.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audmath-1.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting asttokens>=2.0.0 (from audobject>=0.6.1->opensmile)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (8.7.0)\n",
            "Collecting oyaml (from audobject>=0.6.1->opensmile)\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install librosa soundfile opensmile speechbrain transformers torch openai-whisper\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import opensmile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "import whisper\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4zI9rX_2ZNGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ADReSSoAnalyzer:\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content\"\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "\n",
        "        # Initialize feature extractors\n",
        "        self.smile = opensmile.Smile(\n",
        "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "            feature_level=opensmile.FeatureLevel.Functionals,\n",
        "        )\n",
        "\n",
        "        # Initialize Whisper for transcription\n",
        "        self.whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "        # Initialize Wav2Vec2\n",
        "        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "        # Initialize BERT\n",
        "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Get all audio files from the dataset\"\"\"\n",
        "        audio_files = {\n",
        "            'diagnosis_ad': [],\n",
        "            'diagnosis_cn': [],\n",
        "            'progression_decline': [],\n",
        "            'progression_no_decline': [],\n",
        "            'progression_test': []\n",
        "        }\n",
        "\n",
        "        # Diagnosis files\n",
        "        diag_ad_path = f\"{self.base_path}/diagnosis/train/audio/ad\"\n",
        "        diag_cn_path = f\"{self.base_path}/diagnosis/train/audio/cn\"\n",
        "\n",
        "        if os.path.exists(diag_ad_path):\n",
        "            audio_files['diagnosis_ad'] = [f\"{diag_ad_path}/{f}\" for f in os.listdir(diag_ad_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(diag_cn_path):\n",
        "            audio_files['diagnosis_cn'] = [f\"{diag_cn_path}/{f}\" for f in os.listdir(diag_cn_path) if f.endswith('.wav')]\n",
        "\n",
        "        # Progression files\n",
        "        prog_decline_path = f\"{self.base_path}/progression/train/audio/decline\"\n",
        "        prog_no_decline_path = f\"{self.base_path}/progression/train/audio/no_decline\"\n",
        "        prog_test_path = f\"{self.base_path}/progression/test-dist/audio\"\n",
        "\n",
        "        if os.path.exists(prog_decline_path):\n",
        "            audio_files['progression_decline'] = [f\"{prog_decline_path}/{f}\" for f in os.listdir(prog_decline_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(prog_no_decline_path):\n",
        "            audio_files['progression_no_decline'] = [f\"{prog_no_decline_path}/{f}\" for f in os.listdir(prog_no_decline_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(prog_test_path):\n",
        "            audio_files['progression_test'] = [f\"{prog_test_path}/{f}\" for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def extract_acoustic_features(self, audio_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract all acoustic features from audio file\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Load audio - resample to 16kHz for Wav2Vec2 compatibility\n",
        "            y, sr = librosa.load(audio_path, sr=16000)  # Force 16kHz sampling rate\n",
        "\n",
        "            # 1. eGeMAPS features using openSMILE\n",
        "            try:\n",
        "                features['egemaps'] = self.smile.process_file(audio_path).values.flatten()\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: eGeMAPS extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['egemaps'] = np.zeros(88)  # Default eGeMAPS feature size\n",
        "\n",
        "            # 2. MFCC features\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.mean(mfccs, axis=1),\n",
        "                    'std': np.std(mfccs, axis=1),\n",
        "                    'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                    'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: MFCC extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.zeros(13),\n",
        "                    'std': np.zeros(13),\n",
        "                    'delta': np.zeros(13),\n",
        "                    'delta2': np.zeros(13)\n",
        "                }\n",
        "\n",
        "            # 3. Log-mel spectrogram\n",
        "            try:\n",
        "                mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "                log_mel = librosa.power_to_db(mel_spec)\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.mean(log_mel, axis=1),\n",
        "                    'std': np.std(log_mel, axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Log-mel extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.zeros(80),\n",
        "                    'std': np.zeros(80)\n",
        "                }\n",
        "\n",
        "            # 4. Wav2Vec2 features - with proper sampling rate handling\n",
        "            try:\n",
        "                # Ensure sampling rate is exactly 16000 Hz for Wav2Vec2\n",
        "                if len(y) == 0:\n",
        "                    raise ValueError(\"Empty audio signal\")\n",
        "\n",
        "                input_values = self.wav2vec_processor(\n",
        "                    y,\n",
        "                    sampling_rate=16000,  # Explicitly set to 16000\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Wav2Vec2 extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['wav2vec2'] = np.zeros(768)  # Default Wav2Vec2 feature size\n",
        "\n",
        "            # 5. Additional prosodic features\n",
        "            try:\n",
        "                # Handle potential issues with F0 extraction\n",
        "                f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "                f0_clean = f0[f0 > 0]  # Remove unvoiced frames\n",
        "\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                    'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                    'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                    'duration': len(y) / sr\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Prosodic feature extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                    'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                    'duration': 0.0\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
        "            features = None\n",
        "\n",
        "        return features\n",
        "\n",
        "    def show_acoustic_features(self, sample_file: str):\n",
        "        \"\"\"Display acoustic features for a sample file\"\"\"\n",
        "        features = self.extract_acoustic_features(sample_file)\n",
        "\n",
        "        if features is None:\n",
        "            print(f\"Could not extract features from {sample_file}\")\n",
        "            return\n",
        "\n",
        "        print(f\"=== Acoustic Features for {os.path.basename(sample_file)} ===\\n\")\n",
        "\n",
        "        # eGeMAPS\n",
        "        print(f\"1. eGeMAPS Features: {len(features['egemaps'])} features\")\n",
        "        print(f\"   Shape: {features['egemaps'].shape}\")\n",
        "        print(f\"   Sample values: {features['egemaps'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # MFCCs\n",
        "        print(\"2. MFCC Features:\")\n",
        "        print(f\"   Mean: {features['mfccs']['mean'].shape} - {features['mfccs']['mean'][:5]}\")\n",
        "        print(f\"   Std: {features['mfccs']['std'].shape} - {features['mfccs']['std'][:5]}\")\n",
        "        print(f\"   Delta: {features['mfccs']['delta'].shape} - {features['mfccs']['delta'][:5]}\")\n",
        "        print(f\"   Delta-Delta: {features['mfccs']['delta2'].shape} - {features['mfccs']['delta2'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Log-mel\n",
        "        print(\"3. Log-Mel Spectrogram Features:\")\n",
        "        print(f\"   Mean: {features['log_mel']['mean'].shape} - {features['log_mel']['mean'][:5]}\")\n",
        "        print(f\"   Std: {features['log_mel']['std'].shape} - {features['log_mel']['std'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Wav2Vec2\n",
        "        print(f\"4. Wav2Vec2 Features: {features['wav2vec2'].shape}\")\n",
        "        print(f\"   Sample values: {features['wav2vec2'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Prosodic\n",
        "        print(\"5. Prosodic Features:\")\n",
        "        for key, value in features['prosodic'].items():\n",
        "            print(f\"   {key}: {value:.4f}\")\n",
        "        print()\n",
        "\n",
        "    def extract_transcripts(self, audio_files: Dict[str, List[str]]) -> Dict[str, str]:\n",
        "        \"\"\"Extract transcripts using Whisper\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(\"Extracting transcripts...\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "            for file_path in files:\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    print(f\"  Transcribing {filename}...\")\n",
        "\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "                    transcript_text = result[\"text\"].strip()\n",
        "\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': transcript_text,\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def save_transcripts(self, transcripts: Dict[str, str]):\n",
        "        \"\"\"Save transcripts to files\"\"\"\n",
        "        os.makedirs(f\"{self.output_path}/transcripts\", exist_ok=True)\n",
        "\n",
        "        # Save individual transcript files\n",
        "        for key, data in transcripts.items():\n",
        "            filename = f\"{key}_transcript.txt\"\n",
        "            filepath = f\"{self.output_path}/transcripts/{filename}\"\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(data['transcript'])\n",
        "\n",
        "        # Save consolidated JSON\n",
        "        with open(f\"{self.output_path}/transcripts/all_transcripts.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save as pickle for easy loading\n",
        "        with open(f\"{self.output_path}/transcripts/transcripts.pkl\", 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "\n",
        "        print(f\"Transcripts saved to {self.output_path}/transcripts/\")\n",
        "\n",
        "    def create_transcript_table(self, transcripts: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"Create a DataFrame with transcript information\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for key, info in transcripts.items():\n",
        "            data.append({\n",
        "                'File_ID': key,\n",
        "                'Category': info['category'],\n",
        "                'Filename': info['filename'],\n",
        "                'Transcript_Length': len(info['transcript']),\n",
        "                'Word_Count': len(info['transcript'].split()) if info['transcript'] else 0,\n",
        "                'Language': info.get('language', 'N/A'),\n",
        "                'Segments': info.get('segments', 'N/A'),\n",
        "                'Has_Error': 'error' in info,\n",
        "                'Transcript_Preview': info['transcript'][:100] + \"...\" if len(info['transcript']) > 100 else info['transcript']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Save the table\n",
        "        df.to_csv(f\"{self.output_path}/transcript_summary.csv\", index=False)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def extract_linguistic_features(self, transcripts: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract linguistic features for BERT preparation\"\"\"\n",
        "        linguistic_features = {}\n",
        "\n",
        "        print(\"Extracting linguistic features...\")\n",
        "\n",
        "        for key, data in transcripts.items():\n",
        "            transcript = data['transcript']\n",
        "\n",
        "            if not transcript:\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': '',\n",
        "                    'word_count': 0,\n",
        "                    'sentence_count': 0,\n",
        "                    'avg_word_length': 0,\n",
        "                    'bert_tokens': [],\n",
        "                    'bert_input_ids': [],\n",
        "                    'bert_attention_mask': []\n",
        "                }\n",
        "                continue\n",
        "\n",
        "            # Basic linguistic features\n",
        "            words = transcript.split()\n",
        "            sentences = transcript.split('.')\n",
        "\n",
        "            # BERT tokenization\n",
        "            bert_encoding = self.bert_tokenizer(\n",
        "                transcript,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            linguistic_features[key] = {\n",
        "                'raw_text': transcript,\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'bert_tokens': self.bert_tokenizer.tokenize(transcript),\n",
        "                'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                'bert_encoding': bert_encoding\n",
        "            }\n",
        "\n",
        "        # Save linguistic features\n",
        "        with open(f\"{self.output_path}/linguistic_features.pkl\", 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "\n",
        "        return linguistic_features\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"=== ADReSSo21 Speech Analysis Pipeline ===\\n\")\n",
        "\n",
        "        # Step 0: Get audio files\n",
        "        print(\"Step 0: Getting audio files...\")\n",
        "        audio_files = self.get_audio_files()\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        print(f\"Found {total_files} audio files across all categories\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"  {category}: {len(files)} files\")\n",
        "\n",
        "        if total_files == 0:\n",
        "            print(\"No audio files found. Please check the dataset path.\")\n",
        "            return\n",
        "\n",
        "        # Step 1: Show acoustic features for sample files\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 1: Demonstrating acoustic features...\")\n",
        "\n",
        "        # Show features for one file from each category that has files\n",
        "        for category, files in audio_files.items():\n",
        "            if files:\n",
        "                print(f\"\\nShowing features for {category}:\")\n",
        "                self.show_acoustic_features(files[0])\n",
        "                break  # Just show one example to avoid too much output\n",
        "\n",
        "        # Step 2: Extract transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 2: Extracting transcripts...\")\n",
        "        transcripts = self.extract_transcripts(audio_files)\n",
        "\n",
        "        # Step 3: Save transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 3: Saving transcripts...\")\n",
        "        self.save_transcripts(transcripts)\n",
        "\n",
        "        # Step 4: Create transcript table\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 4: Creating transcript table...\")\n",
        "        transcript_df = self.create_transcript_table(transcripts)\n",
        "\n",
        "        print(\"Transcript Summary Table:\")\n",
        "        print(transcript_df.to_string(index=False))\n",
        "\n",
        "        # Step 5: Extract linguistic features for BERT\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 5: Extracting linguistic features for BERT...\")\n",
        "        linguistic_features = self.extract_linguistic_features(transcripts)\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "        print(f\"Results saved to: {self.output_path}\")\n",
        "        print(\"\\nOutput files:\")\n",
        "        print(f\"  - Transcripts: {self.output_path}/transcripts/\")\n",
        "        print(f\"  - Transcript summary: {self.output_path}/transcript_summary.csv\")\n",
        "        print(f\"  - Linguistic features: {self.output_path}/linguistic_features.pkl\")\n",
        "\n",
        "        return {\n",
        "            'audio_files': audio_files,\n",
        "            'transcripts': transcripts,\n",
        "            'transcript_df': transcript_df,\n",
        "            'linguistic_features': linguistic_features\n",
        "        }"
      ],
      "metadata": {
        "id": "hZ-MzlNqZTMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionModule(nn.Module):\n",
        "    \"\"\"Graph-based attention module for semantic relationships\"\"\"\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, num_heads=8, num_layers=3):\n",
        "        super(GraphAttentionModule, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Graph attention layers\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            GATConv(input_dim if i == 0 else hidden_dim,\n",
        "                   hidden_dim, heads=num_heads, dropout=0.2)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final projection\n",
        "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # Apply GAT layers\n",
        "        for i, gat_layer in enumerate(self.gat_layers):\n",
        "            x = gat_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global pooling if batch is provided\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "        else:\n",
        "            x = torch.mean(x, dim=0, keepdim=True)\n",
        "\n",
        "        return self.projection(x)\n",
        "\n",
        "class VisionTransformerModule(nn.Module):\n",
        "    \"\"\"Vision Transformer for processing spectrograms\"\"\"\n",
        "    def __init__(self, input_dim=80, patch_size=8, embed_dim=768, num_heads=12, num_layers=6):\n",
        "        super(VisionTransformerModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))  # Max patches\n",
        "\n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n",
        "            dropout=0.1, activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embed_dim, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, channels, height, width)\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Create patches\n",
        "        x = self.patch_embed(x)  # (B, embed_dim, H', W')\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        num_patches = x.shape[1]\n",
        "        x = x + self.pos_embed[:, :num_patches, :]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = x.transpose(0, 1)  # (num_patches, B, embed_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.transpose(0, 1)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = torch.mean(x, dim=1)  # (B, embed_dim)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "class UNetModule(nn.Module):\n",
        "    \"\"\"U-Net for audio feature processing\"\"\"\n",
        "    def __init__(self, in_channels=1, out_channels=128):\n",
        "        super(UNetModule, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = self.upconv_block(1024, 512)\n",
        "        self.dec3 = self.upconv_block(512, 256)\n",
        "        self.dec2 = self.upconv_block(256, 128)\n",
        "        self.dec1 = self.upconv_block(128, 64)\n",
        "\n",
        "        # Final layer\n",
        "        self.final = nn.Conv1d(64, out_channels, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool1d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool1d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool1d(e3, 2))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool1d(e4, 2))\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.dec4(b)\n",
        "        d3 = self.dec3(d4)\n",
        "        d2 = self.dec2(d3)\n",
        "        d1 = self.dec1(d2)\n",
        "\n",
        "        # Final\n",
        "        out = self.final(d1)\n",
        "        out = self.pool(out).squeeze(-1)  # Global average pooling\n",
        "\n",
        "        return out\n",
        "\n",
        "class AlexNetModule(nn.Module):\n",
        "    \"\"\"Modified AlexNet for feature extraction\"\"\"\n",
        "    def __init__(self, input_dim=768, num_classes=256):\n",
        "        super(AlexNetModule, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class MultiModalADReSSoModel(nn.Module):\n",
        "    \"\"\"Complete multi-modal architecture\"\"\"\n",
        "    def __init__(self,\n",
        "                 audio_feature_dim=768,\n",
        "                 text_feature_dim=768,\n",
        "                 spectrogram_height=80,\n",
        "                 num_classes=2):\n",
        "        super(MultiModalADReSSoModel, self).__init__()\n",
        "\n",
        "        # Initialize modules\n",
        "        self.graph_attention = GraphAttentionModule(input_dim=text_feature_dim)\n",
        "        self.vision_transformer = VisionTransformerModule(input_dim=spectrogram_height)\n",
        "        self.unet = UNetModule()\n",
        "        self.alexnet = AlexNetModule(input_dim=audio_feature_dim)\n",
        "\n",
        "        # BERT for text processing\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(256 + 256 + 128 + 256, 512),  # Graph + ViT + UNet + AlexNet\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def create_semantic_graph(self, text_features, audio_features):\n",
        "        \"\"\"Create semantic relationship graph between audio and text\"\"\"\n",
        "        batch_size = text_features.shape[0]\n",
        "        graphs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Compute similarity matrix\n",
        "            text_feat = text_features[i].unsqueeze(0)  # (1, dim)\n",
        "            audio_feat = audio_features[i].unsqueeze(0)  # (1, dim)\n",
        "\n",
        "            # Create nodes (text + audio features)\n",
        "            node_features = torch.cat([text_feat, audio_feat], dim=0)  # (2, dim)\n",
        "\n",
        "            # Create edges based on similarity\n",
        "            similarity = F.cosine_similarity(text_feat, audio_feat, dim=1)\n",
        "\n",
        "            # Create bidirectional edges if similarity > threshold\n",
        "            if similarity.item() > 0.1:\n",
        "                edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long).t()\n",
        "            else:\n",
        "                # Self-loops only\n",
        "                edge_index = torch.tensor([[0, 1], [0, 1]], dtype=torch.long).t()\n",
        "\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            graphs.append(graph)\n",
        "\n",
        "        return Batch.from_data_list(graphs)\n",
        "\n",
        "    def forward(self, audio_features, text_input_ids, text_attention_mask, spectrograms):\n",
        "        batch_size = audio_features.shape[0]\n",
        "\n",
        "        # Process text with BERT\n",
        "        bert_outputs = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_features = bert_outputs.last_hidden_state.mean(dim=1)  # (batch_size, 768)\n",
        "\n",
        "        # Create semantic graph\n",
        "        graph_batch = self.create_semantic_graph(text_features, audio_features)\n",
        "\n",
        "        # Process through different modules\n",
        "        graph_out = self.graph_attention(graph_batch.x, graph_batch.edge_index, graph_batch.batch)\n",
        "        vit_out = self.vision_transformer(spectrograms)\n",
        "\n",
        "        # Prepare audio for U-Net (add channel dimension)\n",
        "        audio_1d = audio_features.unsqueeze(1)  # (batch_size, 1, features)\n",
        "        unet_out = self.unet(audio_1d)\n",
        "\n",
        "        alexnet_out = self.alexnet(audio_features)\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = torch.cat([graph_out, vit_out, unet_out, alexnet_out], dim=1)\n",
        "        fused_features = self.fusion_layer(fused_features)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(fused_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "class ADReSSoDataset(Dataset):\n",
        "    \"\"\"Dataset class for ADReSSo data\"\"\"\n",
        "    def __init__(self, features_dict, linguistic_features, labels, transform=None):\n",
        "        self.features_dict = features_dict\n",
        "        self.linguistic_features = linguistic_features\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.file_ids = list(features_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_ids[idx]\n",
        "\n",
        "        # Get features\n",
        "        features = self.features_dict[file_id]\n",
        "        linguistic = self.linguistic_features[file_id]\n",
        "\n",
        "        # Prepare audio features (Wav2Vec2)\n",
        "        audio_features = torch.FloatTensor(features['wav2vec2'])\n",
        "\n",
        "        # Prepare text features\n",
        "        text_input_ids = torch.LongTensor(linguistic['bert_input_ids'])\n",
        "        text_attention_mask = torch.LongTensor(linguistic['bert_attention_mask'])\n",
        "\n",
        "        # Prepare spectrogram (create from log-mel features)\n",
        "        log_mel_mean = features['log_mel']['mean']\n",
        "        log_mel_std = features['log_mel']['std']\n",
        "        spectrogram = np.stack([log_mel_mean, log_mel_std])  # (2, 80)\n",
        "        spectrogram = np.expand_dims(spectrogram.mean(axis=0), axis=0)  # (1, 80)\n",
        "        spectrogram = np.tile(spectrogram, (1, 1, 80))  # (1, 80, 80) - square image\n",
        "        spectrogram = torch.FloatTensor(spectrogram)\n",
        "\n",
        "        label = torch.LongTensor([self.labels[file_id]])\n",
        "\n",
        "        return {\n",
        "            'audio_features': audio_features,\n",
        "            'text_input_ids': text_input_ids,\n",
        "            'text_attention_mask': text_attention_mask,\n",
        "            'spectrogram': spectrogram,\n",
        "            'label': label,\n",
        "            'file_id': file_id\n",
        "        }\n",
        "\n",
        "class ADReSSoTrainer:\n",
        "    \"\"\"Training and evaluation class\"\"\"\n",
        "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            # Move to device\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "            text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "            spectrograms = batch['spectrogram'].to(self.device)\n",
        "            labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(audio_features, text_input_ids, text_attention_mask, spectrograms)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                # Move to device\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "                text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "                spectrograms = batch['spectrogram'].to(self.device)\n",
        "                labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(audio_features, text_input_ids, text_attention_mask, spectrograms)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # Store for detailed metrics\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        return avg_loss, accuracy, all_preds, all_labels, all_probs\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=5):\n",
        "        print(f\"Training on {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "            print('-' * 50)\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc, val_preds, val_labels, val_probs = self.validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping and model saving\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_adresso_model.pth')\n",
        "                patience_counter = 0\n",
        "                print(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 10:\n",
        "                    print('Early stopping triggered')\n",
        "                    break\n",
        "\n",
        "        print(f'\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%')\n",
        "\n",
        "    def evaluate_detailed(self, test_loader, class_names=['CN', 'AD']):\n",
        "        \"\"\"Detailed evaluation with metrics and visualizations\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "        all_file_ids = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                # Move to device\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "                text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "                spectrograms = batch['spectrogram'].to(self.device)\n",
        "                labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(audio_features, text_input_ids, text_attention_mask, spectrograms)\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                # Store results\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_file_ids.extend(batch['file_id'])\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        # ROC AUC (for binary classification)\n",
        "        if len(class_names) == 2:\n",
        "            probs_positive = [prob[1] for prob in all_probs]\n",
        "            auc = roc_auc_score(all_labels, probs_positive)\n",
        "        else:\n",
        "            auc = None\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        # Print results\n",
        "        print(\"=\"*60)\n",
        "        print(\"DETAILED EVALUATION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        if auc:\n",
        "            print(f\"ROC AUC: {auc:.4f}\")\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot training curves\n",
        "        self.plot_training_curves()\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame({\n",
        "            'file_id': all_file_ids,\n",
        "            'true_label': all_labels,\n",
        "            'predicted_label': all_preds,\n",
        "            'confidence': [max(prob) for prob in all_probs],\n",
        "            'prob_CN': [prob[0] for prob in all_probs],\n",
        "            'prob_AD': [prob[1] for prob in all_probs] if len(all_probs[0]) > 1 else [0] * len(all_probs)\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'auc': auc,\n",
        "            'confusion_matrix': cm,\n",
        "            'results_df': results_df\n",
        "        }\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"Plot training and validation curves\"\"\"\n",
        "        fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        ax1.plot(self.train_losses, label='Training Loss', color='blue')\n",
        "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Accuracy curves\n",
        "        ax2.plot(self.train_accuracies, label='Training Accuracy', color='blue')\n",
        "        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='red')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy (%)')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def visualize_semantic_graph(text_features, audio_features, file_id, save_path=None):\n",
        "    \"\"\"Visualize semantic relationships between audio and text\"\"\"\n",
        "    # Compute similarity\n",
        "    similarity = F.cosine_similarity(text_features, audio_features, dim=0).item()\n",
        "\n",
        "    # Create networkx graph\n",
        "    G = nx.Graph()\n",
        "    G.add_node(\"Text\", type=\"text\", features=text_features[:5].tolist())\n",
        "    G.add_node(\"Audio\", type=\"audio\", features=audio_features[:5].tolist())\n",
        "\n",
        "    # Add edge if similarity is significant\n",
        "    if similarity > 0.1:\n",
        "        G.add_edge(\"Text\", \"Audio\", weight=similarity, similarity=similarity)\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
        "\n",
        "    # Draw nodes\n",
        "    node_colors = ['lightblue' if G.nodes[node]['type'] == 'text' else 'lightcoral'\n",
        "                   for node in G.nodes()]\n",
        "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=3000, alpha=0.7)\n",
        "\n",
        "    # Draw edges\n",
        "    if G.edges():\n",
        "        edge_widths = [G[u][v]['weight'] * 10 for u, v in G.edges()]\n",
        "        nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.6, edge_color='gray')\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')\n",
        "\n",
        "    # Add edge labels\n",
        "    if G.edges():\n",
        "        edge_labels = {(u, v): f\"Sim: {G[u][v]['similarity']:.3f}\"\n",
        "                      for u, v in G.edges()}\n",
        "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)\n",
        "\n",
        "    plt.title(f'Semantic Relationship Graph - {file_id}\\nSimilarity: {similarity:.3f}')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return G, similarity"
      ],
      "metadata": {
        "id": "_BZbXSFrZlDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_analyzer_with_model():\n",
        "    \"\"\"Extend the ADReSSoAnalyzer class with the new model architecture\"\"\"\n",
        "\n",
        "    class ADReSSoAnalyzerExtended(ADReSSoAnalyzer):\n",
        "        def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "            super().__init__(base_path)\n",
        "            self.model = None\n",
        "            self.trainer = None\n",
        "            self.scaler = StandardScaler()\n",
        "\n",
        "        def step_6_define_model_architecture(self):\n",
        "            \"\"\"Step 6: Define the multi-modal model architecture\"\"\"\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"STEP 6: DEFINING MODEL ARCHITECTURE\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            print(\"Initializing Multi-Modal Architecture:\")\n",
        "            print(\"- Graph-based Attention Module\")\n",
        "            print(\"- Vision Transformer Module\")\n",
        "            print(\"- U-Net Module\")\n",
        "            print(\"- AlexNet Module\")\n",
        "            print(\"- BERT for text processing\")\n",
        "            print(\"- Fusion and Classification layers\")\n",
        "\n",
        "            # Initialize model\n",
        "            self.model = MultiModalADReSSoModel(\n",
        "                audio_feature_dim=768,  # Wav2Vec2 dimension\n",
        "                text_feature_dim=768,   # BERT dimension\n",
        "                spectrogram_height=80,  # Mel spectrogram bins\n",
        "                num_classes=2           # AD vs CN\n",
        "            )\n",
        "\n",
        "            # Initialize trainer\n",
        "            self.trainer = ADReSSoTrainer(self.model)\n",
        "\n",
        "            print(f\"\\nModel initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters\")\n",
        "            print(\"Architecture components ready for training!\")\n",
        "\n",
        "            return self.model\n",
        "\n",
        "        def step_7_train_model(self, features_dict, linguistic_features, batch_size=8, num_epochs=30):\n",
        "            \"\"\"Step 7: Train the multi-modal model\"\"\"\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"STEP 7: TRAINING MODEL\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            if self.model is None:\n",
        "                print(\"Model not initialized. Running Step 6 first...\")\n",
        "                self.step_6_define_model_architecture()\n",
        "\n",
        "            # Prepare labels based on file categories\n",
        "            labels = {}\n",
        "            for file_id in features_dict.keys():\n",
        "                if 'diagnosis_ad' in file_id or 'progression_decline' in file_id:\n",
        "                    labels[file_id] = 1  # AD/Decline\n",
        "                else:\n",
        "                    labels[file_id] = 0  # CN/No decline\n",
        "\n",
        "            print(f\"Dataset summary:\")\n",
        "            print(f\"- Total files: {len(features_dict)}\")\n",
        "            print(f\"- AD/Decline cases: {sum(labels.values())}\")\n",
        "            print(f\"- CN/No decline cases: {len(labels) - sum(labels.values())}\")\n",
        "\n",
        "            # Split data\n",
        "            file_ids = list(features_dict.keys())\n",
        "            train_ids, test_ids = train_test_split(file_ids, test_size=0.2,\n",
        "                                                 stratify=[labels[f] for f in file_ids],\n",
        "                                                 random_state=42)\n",
        "            train_ids, val_ids = train_test_split(train_ids, test_size=0.2,\n",
        "                                                stratify=[labels[f] for f in train_ids],\n",
        "                                                random_state=42)\n",
        "\n",
        "            print(f\"\\nData split:\")\n",
        "            print(f\"- Training: {len(train_ids)} files\")\n",
        "            print(f\"- Validation: {len(val_ids)} files\")\n",
        "            print(f\"- Testing: {len(test_ids)} files\")\n",
        "\n",
        "            # Create datasets\n",
        "            train_features = {fid: features_dict[fid] for fid in train_ids}\n",
        "            val_features = {fid: features_dict[fid] for fid in val_ids}\n",
        "            test_features = {fid: features_dict[fid] for fid in test_ids}\n",
        "\n",
        "            train_linguistic = {fid: linguistic_features[fid] for fid in train_ids}\n",
        "            val_linguistic = {fid: linguistic_features[fid] for fid in val_ids}\n",
        "            test_linguistic = {fid: linguistic_features[fid] for fid in test_ids}\n",
        "\n",
        "            train_labels = {fid: labels[fid] for fid in train_ids}\n",
        "            val_labels = {fid: labels[fid] for fid in val_ids}\n",
        "            test_labels = {fid: labels[fid] for fid in test_ids}\n",
        "\n",
        "            # Create data loaders\n",
        "            train_dataset = ADReSSoDataset(train_features, train_linguistic, train_labels)\n",
        "            val_dataset = ADReSSoDataset(val_features, val_linguistic, val_labels)\n",
        "            test_dataset = ADReSSoDataset(test_features, test_linguistic, test_labels)\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
        "\n",
        "            # Train the model\n",
        "            self.trainer.train(train_loader, val_loader, num_epochs=num_epochs)\n",
        "\n",
        "            # Store test loader for evaluation\n",
        "            self.test_loader = test_loader\n",
        "\n",
        "            print(\"Training completed!\")\n",
        "            return self.trainer\n",
        "\n",
        "        def step_8_evaluate_model(self, visualize_graphs=True, num_graph_samples=5):\n",
        "            \"\"\"Step 8: Evaluate model and visualize semantic relationships\"\"\"\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"STEP 8: MODEL EVALUATION AND SEMANTIC ANALYSIS\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            if self.trainer is None:\n",
        "                print(\"Model not trained. Please run Step 7 first.\")\n",
        "                return None\n",
        "\n",
        "            # Load best model\n",
        "            self.model.load_state_dict(torch.load('best_adresso_model.pth'))\n",
        "\n",
        "            # Detailed evaluation\n",
        "            print(\"Performing detailed evaluation...\")\n",
        "            evaluation_results = self.trainer.evaluate_detailed(\n",
        "                self.test_loader, class_names=['CN', 'AD']\n",
        "            )\n",
        "\n",
        "            # Visualize semantic relationships\n",
        "            if visualize_graphs:\n",
        "                print(f\"\\nVisualizing semantic relationships for {num_graph_samples} samples...\")\n",
        "                self.visualize_semantic_relationships(num_samples=num_graph_samples)\n",
        "\n",
        "            # Additional analysis\n",
        "            self.analyze_feature_importance()\n",
        "            self.generate_evaluation_report(evaluation_results)\n",
        "\n",
        "            return evaluation_results\n",
        "\n",
        "        def visualize_semantic_relationships(self, num_samples=5):\n",
        "            \"\"\"Visualize semantic graphs for sample files\"\"\"\n",
        "            self.model.eval()\n",
        "            sample_count = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.test_loader:\n",
        "                    if sample_count >= num_samples:\n",
        "                        break\n",
        "\n",
        "                    # Process batch\n",
        "                    audio_features = batch['audio_features'].to(self.trainer.device)\n",
        "                    text_input_ids = batch['text_input_ids'].to(self.trainer.device)\n",
        "                    text_attention_mask = batch['text_attention_mask'].to(self.trainer.device)\n",
        "                    file_ids = batch['file_id']\n",
        "\n",
        "                    # Get BERT features\n",
        "                    bert_outputs = self.model.bert(\n",
        "                        input_ids=text_input_ids,\n",
        "                        attention_mask=text_attention_mask\n",
        "                    )\n",
        "                    text_features = bert_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "                    # Visualize each sample in batch\n",
        "                    for i in range(min(len(file_ids), num_samples - sample_count)):\n",
        "                        file_id = file_ids[i]\n",
        "                        text_feat = text_features[i]\n",
        "                        audio_feat = audio_features[i]\n",
        "\n",
        "                        print(f\"\\nVisualizing semantic relationships for: {file_id}\")\n",
        "\n",
        "                        # Create and visualize graph\n",
        "                        graph, similarity = visualize_semantic_graph(\n",
        "                            text_feat.cpu(),\n",
        "                            audio_feat.cpu(),\n",
        "                            file_id,\n",
        "                            save_path=f\"semantic_graph_{file_id}.png\"\n",
        "                        )\n",
        "\n",
        "                        # Print relationship analysis\n",
        "                        self.analyze_semantic_relationship(text_feat.cpu(), audio_feat.cpu(), file_id)\n",
        "\n",
        "                        sample_count += 1\n",
        "\n",
        "                        if sample_count >= num_samples:\n",
        "                            break\n",
        "\n",
        "        def analyze_semantic_relationship(self, text_features, audio_features, file_id):\n",
        "            \"\"\"Analyze the semantic relationship between audio and text\"\"\"\n",
        "            # Compute various similarity metrics\n",
        "            cosine_sim = F.cosine_similarity(text_features, audio_features, dim=0).item()\n",
        "\n",
        "            # L2 distance (normalized)\n",
        "            l2_distance = torch.norm(text_features - audio_features).item()\n",
        "            normalized_l2 = l2_distance / (torch.norm(text_features) + torch.norm(audio_features)).item()\n",
        "\n",
        "            # Dot product similarity\n",
        "            dot_product = torch.dot(text_features, audio_features).item()\n",
        "\n",
        "            print(f\"Semantic Relationship Analysis for {file_id}:\")\n",
        "            print(f\"  - Cosine Similarity: {cosine_sim:.4f}\")\n",
        "            print(f\"  - Normalized L2 Distance: {normalized_l2:.4f}\")\n",
        "            print(f\"  - Dot Product: {dot_product:.4f}\")\n",
        "\n",
        "            # Interpretation\n",
        "            if cosine_sim > 0.7:\n",
        "                relationship = \"Strong positive correlation\"\n",
        "            elif cosine_sim > 0.3:\n",
        "                relationship = \"Moderate positive correlation\"\n",
        "            elif cosine_sim > 0.1:\n",
        "                relationship = \"Weak positive correlation\"\n",
        "            elif cosine_sim > -0.1:\n",
        "                relationship = \"No significant correlation\"\n",
        "            else:\n",
        "                relationship = \"Negative correlation\"\n",
        "\n",
        "            print(f\"  - Relationship Interpretation: {relationship}\")\n",
        "\n",
        "            return {\n",
        "                'cosine_similarity': cosine_sim,\n",
        "                'l2_distance': normalized_l2,\n",
        "                'dot_product': dot_product,\n",
        "                'relationship': relationship\n",
        "            }\n",
        "\n",
        "        def analyze_feature_importance(self):\n",
        "            \"\"\"Analyze feature importance across different modalities\"\"\"\n",
        "            print(\"\\nAnalyzing feature importance across modalities...\")\n",
        "\n",
        "            self.model.eval()\n",
        "            modality_contributions = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in self.test_loader:\n",
        "                    # Get features\n",
        "                    audio_features = batch['audio_features'].to(self.trainer.device)\n",
        "                    text_input_ids = batch['text_input_ids'].to(self.trainer.device)\n",
        "                    text_attention_mask = batch['text_attention_mask'].to(self.trainer.device)\n",
        "                    spectrograms = batch['spectrogram'].to(self.trainer.device)\n",
        "\n",
        "                    # Process text with BERT\n",
        "                    bert_outputs = self.model.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "                    text_features = bert_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "                    # Get individual modality outputs\n",
        "                    graph_batch = self.model.create_semantic_graph(text_features, audio_features)\n",
        "                    graph_out = self.model.graph_attention(graph_batch.x, graph_batch.edge_index, graph_batch.batch)\n",
        "                    vit_out = self.model.vision_transformer(spectrograms)\n",
        "\n",
        "                    audio_1d = audio_features.unsqueeze(1)\n",
        "                    unet_out = self.model.unet(audio_1d)\n",
        "                    alexnet_out = self.model.alexnet(audio_features)\n",
        "\n",
        "                    # Calculate contribution magnitudes\n",
        "                    contributions = {\n",
        "                        'Graph Attention': torch.norm(graph_out, dim=1).mean().item(),\n",
        "                        'Vision Transformer': torch.norm(vit_out, dim=1).mean().item(),\n",
        "                        'U-Net': torch.norm(unet_out, dim=1).mean().item(),\n",
        "                        'AlexNet': torch.norm(alexnet_out, dim=1).mean().item()\n",
        "                    }\n",
        "\n",
        "                    modality_contributions.append(contributions)\n",
        "\n",
        "                    # Only analyze first few batches to save time\n",
        "                    if len(modality_contributions) >= 5:\n",
        "                        break\n",
        "\n",
        "            # Aggregate results\n",
        "            avg_contributions = {}\n",
        "            for modality in modality_contributions[0].keys():\n",
        "                avg_contributions[modality] = np.mean([contrib[modality] for contrib in modality_contributions])\n",
        "\n",
        "            # Visualize feature importance\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            modalities = list(avg_contributions.keys())\n",
        "            contributions = list(avg_contributions.values())\n",
        "\n",
        "            bars = plt.bar(modalities, contributions, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
        "            plt.title('Average Feature Contribution by Modality')\n",
        "            plt.ylabel('Average L2 Norm')\n",
        "            plt.xlabel('Modality')\n",
        "            plt.xticks(rotation=45)\n",
        "\n",
        "            # Add value labels on bars\n",
        "            for bar, value in zip(bars, contributions):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                        f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            print(\"Feature Importance Analysis:\")\n",
        "            for modality, contribution in avg_contributions.items():\n",
        "                print(f\"  - {modality}: {contribution:.4f}\")\n",
        "\n",
        "            return avg_contributions\n",
        "\n",
        "        def generate_evaluation_report(self, evaluation_results):\n",
        "            \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"COMPREHENSIVE EVALUATION REPORT\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            # Performance metrics\n",
        "            print(\"PERFORMANCE METRICS:\")\n",
        "            print(f\"  - Overall Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
        "            print(f\"  - Precision: {evaluation_results['precision']:.4f}\")\n",
        "            print(f\"  - Recall: {evaluation_results['recall']:.4f}\")\n",
        "            print(f\"  - F1-Score: {evaluation_results['f1']:.4f}\")\n",
        "            if evaluation_results['auc']:\n",
        "                print(f\"  - ROC AUC: {evaluation_results['auc']:.4f}\")\n",
        "\n",
        "            # Results by category\n",
        "            results_df = evaluation_results['results_df']\n",
        "\n",
        "            print(\"\\nRESULTS BY CATEGORY:\")\n",
        "            for category in ['diagnosis_ad', 'diagnosis_cn', 'progression_decline', 'progression_no_decline']:\n",
        "                category_results = results_df[results_df['file_id'].str.contains(category)]\n",
        "                if len(category_results) > 0:\n",
        "                    accuracy = (category_results['true_label'] == category_results['predicted_label']).mean()\n",
        "                    avg_confidence = category_results['confidence'].mean()\n",
        "                    print(f\"  - {category}:\")\n",
        "                    print(f\"    * Accuracy: {accuracy:.4f}\")\n",
        "                    print(f\"    * Average Confidence: {avg_confidence:.4f}\")\n",
        "                    print(f\"    * Sample Count: {len(category_results)}\")\n",
        "\n",
        "            # Misclassification analysis\n",
        "            print(\"\\nMISCLASSIFICATION ANALYSIS:\")\n",
        "            misclassified = results_df[results_df['true_label'] != results_df['predicted_label']]\n",
        "            print(f\"  - Total Misclassified: {len(misclassified)}\")\n",
        "            print(f\"  - Misclassification Rate: {len(misclassified)/len(results_df):.4f}\")\n",
        "\n",
        "            if len(misclassified) > 0:\n",
        "                print(\"  - Misclassified Samples:\")\n",
        "                for _, row in misclassified.head().iterrows():\n",
        "                    print(f\"    * {row['file_id']}: True={row['true_label']}, Pred={row['predicted_label']}, Conf={row['confidence']:.3f}\")\n",
        "\n",
        "            # High confidence predictions\n",
        "            high_conf = results_df[results_df['confidence'] > 0.9]\n",
        "            print(f\"\\nHIGH CONFIDENCE PREDICTIONS (>0.9):\")\n",
        "            print(f\"  - Count: {len(high_conf)}\")\n",
        "            print(f\"  - Accuracy: {(high_conf['true_label'] == high_conf['predicted_label']).mean():.4f}\")\n",
        "\n",
        "            # Save detailed results\n",
        "            results_df.to_csv(f\"{self.output_path}/detailed_evaluation_results.csv\", index=False)\n",
        "\n",
        "            # Save evaluation summary\n",
        "            summary_dict = {\n",
        "                'accuracy': evaluation_results['accuracy'],\n",
        "                'precision': evaluation_results['precision'],\n",
        "                'recall': evaluation_results['recall'],\n",
        "                'f1_score': evaluation_results['f1'],\n",
        "                'roc_auc': evaluation_results['auc'],\n",
        "                'total_samples': len(results_df),\n",
        "                'misclassified_count': len(misclassified),\n",
        "                'high_confidence_count': len(high_conf)\n",
        "            }\n",
        "\n",
        "            with open(f\"{self.output_path}/evaluation_summary.json\", 'w') as f:\n",
        "                json.dump(summary_dict, f, indent=2)\n",
        "\n",
        "            print(f\"\\nDetailed results saved to:\")\n",
        "            print(f\"  - {self.output_path}/detailed_evaluation_results.csv\")\n",
        "            print(f\"  - {self.output_path}/evaluation_summary.json\")\n",
        "\n",
        "            return summary_dict\n",
        "\n",
        "        def run_complete_pipeline_with_model(self, num_epochs=30, batch_size=8):\n",
        "            \"\"\"Run the complete pipeline including model training and evaluation\"\"\"\n",
        "            print(\"=\"*80)\n",
        "            print(\"COMPLETE ADReSSo MULTI-MODAL ANALYSIS PIPELINE\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            # Run original pipeline (Steps 0-5)\n",
        "            results = self.run_complete_pipeline()\n",
        "\n",
        "            if results is None:\n",
        "                print(\"Error in initial pipeline. Cannot proceed with model training.\")\n",
        "                return None\n",
        "\n",
        "            # Extract features and linguistic data\n",
        "            features_dict = {}\n",
        "            audio_files = results['audio_files']\n",
        "\n",
        "            # Extract acoustic features for all files\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"EXTRACTING ACOUSTIC FEATURES FOR MODEL TRAINING\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            for category, files in audio_files.items():\n",
        "                print(f\"\\nProcessing {category}...\")\n",
        "                for file_path in files:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    file_id = f\"{category}_{filename}\"\n",
        "\n",
        "                    print(f\"  Extracting features for {filename}...\")\n",
        "                    features = self.extract_acoustic_features(file_path)\n",
        "\n",
        "                    if features is not None:\n",
        "                        features_dict[file_id] = features\n",
        "                    else:\n",
        "                        print(f\"  Warning: Could not extract features for {filename}\")\n",
        "\n",
        "            print(f\"\\nSuccessfully extracted features for {len(features_dict)} files\")\n",
        "\n",
        "            # Step 6: Define model architecture\n",
        "            self.step_6_define_model_architecture()\n",
        "\n",
        "            # Step 7: Train model\n",
        "            self.step_7_train_model(\n",
        "                features_dict,\n",
        "                results['linguistic_features'],\n",
        "                batch_size=batch_size,\n",
        "                num_epochs=num_epochs\n",
        "            )\n",
        "\n",
        "            # Step 8: Evaluate model\n",
        "            evaluation_results = self.step_8_evaluate_model()\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "            print(\"=\"*80)\n",
        "            print(\"\\nAll results saved to output directory.\")\n",
        "            print(\"Model training and evaluation completed with semantic relationship analysis.\")\n",
        "\n",
        "            return {\n",
        "                'original_results': results,\n",
        "                'features_dict': features_dict,\n",
        "                'model': self.model,\n",
        "                'trainer': self.trainer,\n",
        "                'evaluation_results': evaluation_results\n",
        "            }\n",
        "\n",
        "    return ADReSSoAnalyzerExtended"
      ],
      "metadata": {
        "id": "p9_wG6oYZvMb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractionCheckpointer:\n",
        "    def __init__(self, analyzer, output_dir=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21/checkpoints\"):\n",
        "        \"\"\"\n",
        "        Initialize checkpointer for feature extraction\n",
        "\n",
        "        Args:\n",
        "            analyzer: Instance of ADReSSoAnalyzer or ADReSSoAnalyzerExtended\n",
        "            output_dir: Directory to store checkpoints and features\n",
        "        \"\"\"\n",
        "        self.analyzer = analyzer\n",
        "        self.output_dir = output_dir\n",
        "        self.checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n",
        "        self.feature_dir = os.path.join(output_dir, \"features\")\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "        os.makedirs(self.feature_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize checkpoint tracking\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, \"checkpoint.pkl\")\n",
        "        self.processed_files = set()\n",
        "        self.features_dict = {}\n",
        "\n",
        "        # Load existing checkpoint if available\n",
        "        self.load_checkpoint()\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load existing checkpoint if available\"\"\"\n",
        "        if os.path.exists(self.checkpoint_file):\n",
        "            try:\n",
        "                with open(self.checkpoint_file, 'rb') as f:\n",
        "                    checkpoint = pickle.load(f)\n",
        "                    self.processed_files = checkpoint.get('processed_files', set())\n",
        "                    self.features_dict = checkpoint.get('features_dict', {})\n",
        "                    print(f\"Loaded checkpoint with {len(self.processed_files)} processed files\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading checkpoint: {str(e)}\")\n",
        "                self.processed_files = set()\n",
        "                self.features_dict = {}\n",
        "        else:\n",
        "            print(\"No checkpoint found, starting fresh\")\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        \"\"\"Save current state as checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'processed_files': self.processed_files,\n",
        "            'features_dict': self.features_dict\n",
        "        }\n",
        "        try:\n",
        "            with open(self.checkpoint_file, 'wb') as f:\n",
        "                pickle.dump(checkpoint, f)\n",
        "            print(f\"Saved checkpoint to {self.checkpoint_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving checkpoint: {str(e)}\")\n",
        "\n",
        "    def save_individual_feature(self, file_id: str, features: Dict):\n",
        "        \"\"\"Save features for a single file\"\"\"\n",
        "        feature_file = os.path.join(self.feature_dir, f\"{file_id}_features.pkl\")\n",
        "        try:\n",
        "            with open(feature_file, 'wb') as f:\n",
        "                pickle.dump(features, f)\n",
        "            print(f\"Saved features for {file_id} to {feature_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving features for {file_id}: {str(e)}\")\n",
        "\n",
        "    def load_individual_feature(self, file_id: str) -> Dict:\n",
        "        \"\"\"Load features for a single file\"\"\"\n",
        "        feature_file = os.path.join(self.feature_dir, f\"{file_id}_features.pkl\")\n",
        "        if os.path.exists(feature_file):\n",
        "            try:\n",
        "                with open(feature_file, 'rb') as f:\n",
        "                    return pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading features for {file_id}: {str(e)}\")\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    def extract_features_with_checkpoints(self, audio_files: Dict[str, List[str]]) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract features with checkpointing and incremental saving\n",
        "\n",
        "        Args:\n",
        "            audio_files: Dictionary of category-wise audio file paths\n",
        "        Returns:\n",
        "            Dictionary of extracted features\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EXTRACTING FEATURES WITH CHECKPOINTING\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed_count = len(self.processed_files)\n",
        "        print(f\"Total files to process: {total_files}\")\n",
        "        print(f\"Already processed: {processed_count}\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"\\nProcessing {category} ({len(files)} files)...\")\n",
        "\n",
        "            for file_path in files:\n",
        "                filename = os.path.basename(file_path)\n",
        "                file_id = f\"{category}_{filename}\"\n",
        "\n",
        "                # Skip if already processed\n",
        "                if file_id in self.processed_files:\n",
        "                    print(f\"  Skipping {filename} (already processed)\")\n",
        "                    # Load existing features\n",
        "                    features = self.load_individual_feature(file_id)\n",
        "                    if features is not None:\n",
        "                        self.features_dict[file_id] = features\n",
        "                    continue\n",
        "\n",
        "                print(f\"  Extracting features for {filename}...\")\n",
        "\n",
        "                # Extract features\n",
        "                try:\n",
        "                    features = self.analyzer.extract_acoustic_features(file_path)\n",
        "\n",
        "                    if features is not None:\n",
        "                        # Store in memory\n",
        "                        self.features_dict[file_id] = features\n",
        "                        # Save individual feature file\n",
        "                        self.save_individual_feature(file_id, features)\n",
        "                        # Add to processed files\n",
        "                        self.processed_files.add(file_id)\n",
        "                        # Save checkpoint\n",
        "                        self.save_checkpoint()\n",
        "                        processed_count += 1\n",
        "                        print(f\"  Successfully processed {filename} ({processed_count}/{total_files})\")\n",
        "                    else:\n",
        "                        print(f\"  Warning: Failed to extract features for {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error processing {filename}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                # Periodic status update\n",
        "                if processed_count % 10 == 0:\n",
        "                    print(f\"\\nProgress: {processed_count}/{total_files} files processed\")\n",
        "                    print(f\"Checkpoint saved at {processed_count} files\")\n",
        "\n",
        "        print(f\"\\nFeature extraction completed!\")\n",
        "        print(f\"Processed {processed_count}/{total_files} files\")\n",
        "        print(f\"Features saved to: {self.feature_dir}\")\n",
        "        print(f\"Checkpoints saved to: {self.checkpoint_dir}\")\n",
        "\n",
        "        return self.features_dict\n",
        "\n",
        "    def run_pipeline_with_checkpoints(self, num_epochs=30, batch_size=8):\n",
        "        \"\"\"\n",
        "        Run the complete pipeline with checkpointing\n",
        "        \"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"RUNNING ADReSSo PIPELINE WITH CHECKPOINTING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Run original pipeline up to feature extraction\n",
        "        results = self.analyzer.run_complete_pipeline()\n",
        "\n",
        "        if results is None:\n",
        "            print(\"Error in initial pipeline. Cannot proceed.\")\n",
        "            return None\n",
        "\n",
        "        # Extract features with checkpointing\n",
        "        features_dict = self.extract_features_with_checkpoints(results['audio_files'])\n",
        "\n",
        "        # Continue with model training and evaluation if extended analyzer\n",
        "        if hasattr(self.analyzer, 'run_complete_pipeline_with_model'):\n",
        "            print(\"\\nContinuing with model training...\")\n",
        "            results = self.analyzer.run_complete_pipeline_with_model(\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "        else:\n",
        "            print(\"\\nUsing base analyzer, skipping model training\")\n",
        "            results['features_dict'] = features_dict\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PIPELINE WITH CHECKPOINTING COMPLETED!\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "IusOz8xVZ3_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (required for Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    print(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "# Install required packages\n",
        "try:\n",
        "    import subprocess\n",
        "    packages = [\n",
        "        \"librosa\", \"soundfile\", \"opensmile\", \"speechbrain\",\n",
        "        \"transformers\", \"torch\", \"openai-whisper\",\n",
        "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"torch-geometric\"\n",
        "    ]\n",
        "    for pkg in packages:\n",
        "        subprocess.check_call([\"pip\", \"install\", pkg])\n",
        "    print(\"All required packages installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error installing packages: {str(e)}\")\n",
        "\n",
        "# Initialize the extended analyzer\n",
        "ExtendedAnalyzer = extend_analyzer_with_model()\n",
        "analyzer = ExtendedAnalyzer(base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\")\n",
        "\n",
        "# Create checkpointer with the initialized analyzer\n",
        "checkpointer = FeatureExtractionCheckpointer(analyzer)\n",
        "\n",
        "# Run pipeline with checkpointing\n",
        "results = checkpointer.run_pipeline_with_checkpoints(\n",
        "    num_epochs=5,\n",
        "    batch_size=4\n",
        ")\n",
        "\n",
        "if results:\n",
        "    print(\"\\nPipeline with checkpointing completed successfully!\")\n",
        "    print(f\"Check output directories for saved features and checkpoints:\")\n",
        "    print(f\"- Features: {checkpointer.feature_dir}\")\n",
        "    print(f\"- Checkpoints: {checkpointer.checkpoint_dir}\")\n",
        "else:\n",
        "    print(\"Pipeline encountered errors. Please check the logs.\")"
      ],
      "metadata": {
        "id": "hyYIXIo-Z9Tf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}