{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwZYctMnvYPgxK6Ocskw61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Final_prediction_From_Addresso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28iz8aJYvEx",
        "outputId": "ce4811b6-4a96-4552-be3a-7f0d906f1268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting opensmile\n",
            "  Downloading opensmile-2.5.1-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
            "Collecting speechbrain\n",
            "  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Collecting audobject>=0.6.1 (from opensmile)\n",
            "  Downloading audobject-0.7.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting audinterface>=0.7.0 (from opensmile)\n",
            "  Downloading audinterface-1.3.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting hyperpyyaml (from speechbrain)\n",
            "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Collecting audeer>=2.1.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audeer-2.2.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audformat-1.3.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audiofile-1.5.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting audmath>=1.4.1 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audmath-1.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile)\n",
            "  Downloading audresample-1.3.4-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting asttokens>=2.0.0 (from audobject>=0.6.1->opensmile)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from audobject>=0.6.1->opensmile) (8.7.0)\n",
            "Collecting oyaml (from audobject>=0.6.1->opensmile)\n",
            "  Downloading oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install librosa soundfile opensmile speechbrain transformers torch openai-whisper\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import opensmile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "import whisper\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4zI9rX_2ZNGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ADReSSoAnalyzer:\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content\"\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "\n",
        "        # Initialize feature extractors\n",
        "        self.smile = opensmile.Smile(\n",
        "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "            feature_level=opensmile.FeatureLevel.Functionals,\n",
        "        )\n",
        "\n",
        "        # Initialize Whisper for transcription\n",
        "        self.whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "        # Initialize Wav2Vec2\n",
        "        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "        # Initialize BERT\n",
        "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Get all audio files from the dataset\"\"\"\n",
        "        audio_files = {\n",
        "            'diagnosis_ad': [],\n",
        "            'diagnosis_cn': [],\n",
        "            'progression_decline': [],\n",
        "            'progression_no_decline': [],\n",
        "            'progression_test': []\n",
        "        }\n",
        "\n",
        "        # Diagnosis files\n",
        "        diag_ad_path = f\"{self.base_path}/diagnosis/train/audio/ad\"\n",
        "        diag_cn_path = f\"{self.base_path}/diagnosis/train/audio/cn\"\n",
        "\n",
        "        if os.path.exists(diag_ad_path):\n",
        "            audio_files['diagnosis_ad'] = [f\"{diag_ad_path}/{f}\" for f in os.listdir(diag_ad_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(diag_cn_path):\n",
        "            audio_files['diagnosis_cn'] = [f\"{diag_cn_path}/{f}\" for f in os.listdir(diag_cn_path) if f.endswith('.wav')]\n",
        "\n",
        "        # Progression files\n",
        "        prog_decline_path = f\"{self.base_path}/progression/train/audio/decline\"\n",
        "        prog_no_decline_path = f\"{self.base_path}/progression/train/audio/no_decline\"\n",
        "        prog_test_path = f\"{self.base_path}/progression/test-dist/audio\"\n",
        "\n",
        "        if os.path.exists(prog_decline_path):\n",
        "            audio_files['progression_decline'] = [f\"{prog_decline_path}/{f}\" for f in os.listdir(prog_decline_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(prog_no_decline_path):\n",
        "            audio_files['progression_no_decline'] = [f\"{prog_no_decline_path}/{f}\" for f in os.listdir(prog_no_decline_path) if f.endswith('.wav')]\n",
        "        if os.path.exists(prog_test_path):\n",
        "            audio_files['progression_test'] = [f\"{prog_test_path}/{f}\" for f in os.listdir(prog_test_path) if f.endswith('.wav')]\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def extract_acoustic_features(self, audio_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract all acoustic features from audio file\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Load audio - resample to 16kHz for Wav2Vec2 compatibility\n",
        "            y, sr = librosa.load(audio_path, sr=16000)  # Force 16kHz sampling rate\n",
        "\n",
        "            # 1. eGeMAPS features using openSMILE\n",
        "            try:\n",
        "                features['egemaps'] = self.smile.process_file(audio_path).values.flatten()\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: eGeMAPS extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['egemaps'] = np.zeros(88)  # Default eGeMAPS feature size\n",
        "\n",
        "            # 2. MFCC features\n",
        "            try:\n",
        "                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.mean(mfccs, axis=1),\n",
        "                    'std': np.std(mfccs, axis=1),\n",
        "                    'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                    'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: MFCC extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['mfccs'] = {\n",
        "                    'mean': np.zeros(13),\n",
        "                    'std': np.zeros(13),\n",
        "                    'delta': np.zeros(13),\n",
        "                    'delta2': np.zeros(13)\n",
        "                }\n",
        "\n",
        "            # 3. Log-mel spectrogram\n",
        "            try:\n",
        "                mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "                log_mel = librosa.power_to_db(mel_spec)\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.mean(log_mel, axis=1),\n",
        "                    'std': np.std(log_mel, axis=1)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Log-mel extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['log_mel'] = {\n",
        "                    'mean': np.zeros(80),\n",
        "                    'std': np.zeros(80)\n",
        "                }\n",
        "\n",
        "            # 4. Wav2Vec2 features - with proper sampling rate handling\n",
        "            try:\n",
        "                # Ensure sampling rate is exactly 16000 Hz for Wav2Vec2\n",
        "                if len(y) == 0:\n",
        "                    raise ValueError(\"Empty audio signal\")\n",
        "\n",
        "                input_values = self.wav2vec_processor(\n",
        "                    y,\n",
        "                    sampling_rate=16000,  # Explicitly set to 16000\n",
        "                    return_tensors=\"pt\"\n",
        "                ).input_values\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Wav2Vec2 extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['wav2vec2'] = np.zeros(768)  # Default Wav2Vec2 feature size\n",
        "\n",
        "            # 5. Additional prosodic features\n",
        "            try:\n",
        "                # Handle potential issues with F0 extraction\n",
        "                f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "                f0_clean = f0[f0 > 0]  # Remove unvoiced frames\n",
        "\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                    'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                    'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                    'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                    'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                    'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                    'duration': len(y) / sr\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Prosodic feature extraction failed for {os.path.basename(audio_path)}: {str(e)}\")\n",
        "                features['prosodic'] = {\n",
        "                    'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                    'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                    'duration': 0.0\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
        "            features = None\n",
        "\n",
        "        return features\n",
        "\n",
        "    def show_acoustic_features(self, sample_file: str):\n",
        "        \"\"\"Display acoustic features for a sample file\"\"\"\n",
        "        features = self.extract_acoustic_features(sample_file)\n",
        "\n",
        "        if features is None:\n",
        "            print(f\"Could not extract features from {sample_file}\")\n",
        "            return\n",
        "\n",
        "        print(f\"=== Acoustic Features for {os.path.basename(sample_file)} ===\\n\")\n",
        "\n",
        "        # eGeMAPS\n",
        "        print(f\"1. eGeMAPS Features: {len(features['egemaps'])} features\")\n",
        "        print(f\"   Shape: {features['egemaps'].shape}\")\n",
        "        print(f\"   Sample values: {features['egemaps'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # MFCCs\n",
        "        print(\"2. MFCC Features:\")\n",
        "        print(f\"   Mean: {features['mfccs']['mean'].shape} - {features['mfccs']['mean'][:5]}\")\n",
        "        print(f\"   Std: {features['mfccs']['std'].shape} - {features['mfccs']['std'][:5]}\")\n",
        "        print(f\"   Delta: {features['mfccs']['delta'].shape} - {features['mfccs']['delta'][:5]}\")\n",
        "        print(f\"   Delta-Delta: {features['mfccs']['delta2'].shape} - {features['mfccs']['delta2'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Log-mel\n",
        "        print(\"3. Log-Mel Spectrogram Features:\")\n",
        "        print(f\"   Mean: {features['log_mel']['mean'].shape} - {features['log_mel']['mean'][:5]}\")\n",
        "        print(f\"   Std: {features['log_mel']['std'].shape} - {features['log_mel']['std'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Wav2Vec2\n",
        "        print(f\"4. Wav2Vec2 Features: {features['wav2vec2'].shape}\")\n",
        "        print(f\"   Sample values: {features['wav2vec2'][:5]}\")\n",
        "        print()\n",
        "\n",
        "        # Prosodic\n",
        "        print(\"5. Prosodic Features:\")\n",
        "        for key, value in features['prosodic'].items():\n",
        "            print(f\"   {key}: {value:.4f}\")\n",
        "        print()\n",
        "\n",
        "    def extract_transcripts(self, audio_files: Dict[str, List[str]]) -> Dict[str, str]:\n",
        "        \"\"\"Extract transcripts using Whisper\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(\"Extracting transcripts...\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"\\nProcessing {category}...\")\n",
        "            for file_path in files:\n",
        "                try:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    print(f\"  Transcribing {filename}...\")\n",
        "\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "                    transcript_text = result[\"text\"].strip()\n",
        "\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': transcript_text,\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def save_transcripts(self, transcripts: Dict[str, str]):\n",
        "        \"\"\"Save transcripts to files\"\"\"\n",
        "        os.makedirs(f\"{self.output_path}/transcripts\", exist_ok=True)\n",
        "\n",
        "        # Save individual transcript files\n",
        "        for key, data in transcripts.items():\n",
        "            filename = f\"{key}_transcript.txt\"\n",
        "            filepath = f\"{self.output_path}/transcripts/{filename}\"\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(data['transcript'])\n",
        "\n",
        "        # Save consolidated JSON\n",
        "        with open(f\"{self.output_path}/transcripts/all_transcripts.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save as pickle for easy loading\n",
        "        with open(f\"{self.output_path}/transcripts/transcripts.pkl\", 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "\n",
        "        print(f\"Transcripts saved to {self.output_path}/transcripts/\")\n",
        "\n",
        "    def create_transcript_table(self, transcripts: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"Create a DataFrame with transcript information\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for key, info in transcripts.items():\n",
        "            data.append({\n",
        "                'File_ID': key,\n",
        "                'Category': info['category'],\n",
        "                'Filename': info['filename'],\n",
        "                'Transcript_Length': len(info['transcript']),\n",
        "                'Word_Count': len(info['transcript'].split()) if info['transcript'] else 0,\n",
        "                'Language': info.get('language', 'N/A'),\n",
        "                'Segments': info.get('segments', 'N/A'),\n",
        "                'Has_Error': 'error' in info,\n",
        "                'Transcript_Preview': info['transcript'][:100] + \"...\" if len(info['transcript']) > 100 else info['transcript']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Save the table\n",
        "        df.to_csv(f\"{self.output_path}/transcript_summary.csv\", index=False)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def extract_linguistic_features(self, transcripts: Dict[str, str]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract linguistic features for BERT preparation\"\"\"\n",
        "        linguistic_features = {}\n",
        "\n",
        "        print(\"Extracting linguistic features...\")\n",
        "\n",
        "        for key, data in transcripts.items():\n",
        "            transcript = data['transcript']\n",
        "\n",
        "            if not transcript:\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': '',\n",
        "                    'word_count': 0,\n",
        "                    'sentence_count': 0,\n",
        "                    'avg_word_length': 0,\n",
        "                    'bert_tokens': [],\n",
        "                    'bert_input_ids': [],\n",
        "                    'bert_attention_mask': []\n",
        "                }\n",
        "                continue\n",
        "\n",
        "            # Basic linguistic features\n",
        "            words = transcript.split()\n",
        "            sentences = transcript.split('.')\n",
        "\n",
        "            # BERT tokenization\n",
        "            bert_encoding = self.bert_tokenizer(\n",
        "                transcript,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=512,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            linguistic_features[key] = {\n",
        "                'raw_text': transcript,\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'bert_tokens': self.bert_tokenizer.tokenize(transcript),\n",
        "                'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                'bert_encoding': bert_encoding\n",
        "            }\n",
        "\n",
        "        # Save linguistic features\n",
        "        with open(f\"{self.output_path}/linguistic_features.pkl\", 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "\n",
        "        return linguistic_features\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete analysis pipeline\"\"\"\n",
        "        print(\"=== ADReSSo21 Speech Analysis Pipeline ===\\n\")\n",
        "\n",
        "        # Step 0: Get audio files\n",
        "        print(\"Step 0: Getting audio files...\")\n",
        "        audio_files = self.get_audio_files()\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        print(f\"Found {total_files} audio files across all categories\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            print(f\"  {category}: {len(files)} files\")\n",
        "\n",
        "        if total_files == 0:\n",
        "            print(\"No audio files found. Please check the dataset path.\")\n",
        "            return\n",
        "\n",
        "        # Step 1: Show acoustic features for sample files\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 1: Demonstrating acoustic features...\")\n",
        "\n",
        "        # Show features for one file from each category that has files\n",
        "        for category, files in audio_files.items():\n",
        "            if files:\n",
        "                print(f\"\\nShowing features for {category}:\")\n",
        "                self.show_acoustic_features(files[0])\n",
        "                break  # Just show one example to avoid too much output\n",
        "\n",
        "        # Step 2: Extract transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 2: Extracting transcripts...\")\n",
        "        transcripts = self.extract_transcripts(audio_files)\n",
        "\n",
        "        # Step 3: Save transcripts\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 3: Saving transcripts...\")\n",
        "        self.save_transcripts(transcripts)\n",
        "\n",
        "        # Step 4: Create transcript table\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 4: Creating transcript table...\")\n",
        "        transcript_df = self.create_transcript_table(transcripts)\n",
        "\n",
        "        print(\"Transcript Summary Table:\")\n",
        "        print(transcript_df.to_string(index=False))\n",
        "\n",
        "        # Step 5: Extract linguistic features for BERT\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Step 5: Extracting linguistic features for BERT...\")\n",
        "        linguistic_features = self.extract_linguistic_features(transcripts)\n",
        "\n",
        "        print(\"\\nPipeline completed successfully!\")\n",
        "        print(f\"Results saved to: {self.output_path}\")\n",
        "        print(\"\\nOutput files:\")\n",
        "        print(f\"  - Transcripts: {self.output_path}/transcripts/\")\n",
        "        print(f\"  - Transcript summary: {self.output_path}/transcript_summary.csv\")\n",
        "        print(f\"  - Linguistic features: {self.output_path}/linguistic_features.pkl\")\n",
        "\n",
        "        return {\n",
        "            'audio_files': audio_files,\n",
        "            'transcripts': transcripts,\n",
        "            'transcript_df': transcript_df,\n",
        "            'linguistic_features': linguistic_features\n",
        "        }"
      ],
      "metadata": {
        "id": "hZ-MzlNqZTMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionModule(nn.Module):\n",
        "    \"\"\"Graph-based attention module for semantic relationships\"\"\"\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, num_heads=8, num_layers=3):\n",
        "        super(GraphAttentionModule, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Graph attention layers\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            GATConv(input_dim if i == 0 else hidden_dim,\n",
        "                   hidden_dim, heads=num_heads, dropout=0.2)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final projection\n",
        "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # Apply GAT layers\n",
        "        for i, gat_layer in enumerate(self.gat_layers):\n",
        "            x = gat_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global pooling if batch is provided\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "        else:\n",
        "            x = torch.mean(x, dim=0, keepdim=True)\n",
        "\n",
        "        return self.projection(x)\n",
        "\n",
        "class VisionTransformerModule(nn.Module):\n",
        "    \"\"\"Vision Transformer for processing spectrograms\"\"\"\n",
        "    def __init__(self, input_dim=80, patch_size=8, embed_dim=768, num_heads=12, num_layers=6):\n",
        "        super(VisionTransformerModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))  # Max patches\n",
        "\n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n",
        "            dropout=0.1, activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embed_dim, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, channels, height, width)\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Create patches\n",
        "        x = self.patch_embed(x)  # (B, embed_dim, H', W')\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        num_patches = x.shape[1]\n",
        "        x = x + self.pos_embed[:, :num_patches, :]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = x.transpose(0, 1)  # (num_patches, B, embed_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.transpose(0, 1)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = torch.mean(x, dim=1)  # (B, embed_dim)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "class UNetModule(nn.Module):\n",
        "    \"\"\"U-Net for audio feature processing\"\"\"\n",
        "    def __init__(self, in_channels=1, out_channels=128):\n",
        "        super(UNetModule, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = self.upconv_block(1024, 512)\n",
        "        self.dec3 = self.upconv_block(512, 256)\n",
        "        self.dec2 = self.upconv_block(256, 128)\n",
        "        self.dec1 = self.upconv_block(128, 64)\n",
        "\n",
        "        # Final layer\n",
        "        self.final = nn.Conv1d(64, out_channels, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool1d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool1d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool1d(e3, 2))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool1d(e4, 2))\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.dec4(b)\n",
        "        d3 = self.dec3(d4)\n",
        "        d2 = self.dec2(d3)\n",
        "        d1 = self.dec1(d2)\n",
        "\n",
        "        # Final\n",
        "        out = self.final(d1)\n",
        "        out = self.pool(out).squeeze(-1)  # Global average pooling\n",
        "\n",
        "        return out\n",
        "\n",
        "class AlexNetModule(nn.Module):\n",
        "    \"\"\"Modified AlexNet for feature extraction\"\"\"\n",
        "    def __init__(self, input_dim=768, num_classes=256):\n",
        "        super(AlexNetModule, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class MultiModalADReSSoModel(nn.Module):\n",
        "    \"\"\"Complete multi-modal architecture\"\"\"\n",
        "    def __init__(self,\n",
        "                 audio_feature_dim=768,\n",
        "                 text_feature_dim=768,\n",
        "                 spectrogram_height=80,\n",
        "                 num_classes=2):\n",
        "        super(MultiModalADReSSoModel, self).__init__()\n",
        "\n",
        "        # Initialize modules\n",
        "        self.graph_attention = GraphAttentionModule(input_dim=text_feature_dim)\n",
        "        self.vision_transformer = VisionTransformerModule(input_dim=spectrogram_height)\n",
        "        self.unet = UNetModule()\n",
        "        self.alexnet = AlexNetModule(input_dim=audio_feature_dim)\n",
        "\n",
        "        # BERT for text processing\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(256 + 256 + 128 + 256, 512),  # Graph + ViT + UNet + AlexNet\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def create_semantic_graph(self, text_features, audio_features):\n",
        "        \"\"\"Create semantic relationship graph between audio and text\"\"\"\n",
        "        batch_size = text_features.shape[0]\n",
        "        graphs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Compute similarity matrix\n",
        "            text_feat = text_features[i].unsqueeze(0)  # (1, dim)\n",
        "            audio_feat = audio_features[i].unsqueeze(0)  # (1, dim)\n",
        "\n",
        "            # Create nodes (text + audio features)\n",
        "            node_features = torch.cat([text_feat, audio_feat], dim=0)  # (2, dim)\n",
        "\n",
        "            # Create edges based on similarity\n",
        "            similarity = F.cosine_similarity(text_feat, audio_feat, dim=1)\n",
        "\n",
        "            # Create bidirectional edges if similarity > threshold\n",
        "            if similarity.item() > 0.1:\n",
        "                edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long).t()\n",
        "            else:\n",
        "                # Self-loops only\n",
        "                edge_index = torch.tensor([[0, 1], [0, 1]], dtype=torch.long).t()\n",
        "\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            graphs.append(graph)\n",
        "\n",
        "        return Batch.from_data_list(graphs)\n",
        "\n",
        "    def forward(self, audio_features, text_input_ids, text_attention_mask, spectrograms):\n",
        "        batch_size = audio_features.shape[0]\n",
        "\n",
        "        # Process text with BERT\n",
        "        bert_outputs = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_features = bert_outputs.last_hidden_state.mean(dim=1)  # (batch_size, 768)\n",
        "\n",
        "        # Create semantic graph\n",
        "        graph_batch = self.create_semantic_graph(text_features, audio_features)\n",
        "\n",
        "        # Process through different modules\n",
        "        graph_out = self.graph_attention(graph_batch.x, graph_batch.edge_index, graph_batch.batch)\n",
        "        vit_out = self.vision_transformer(spectrograms)\n",
        "\n",
        "        # Prepare audio for U-Net (add channel dimension)\n",
        "        audio_1d = audio_features.unsqueeze(1)  # (batch_size, 1, features)\n",
        "        unet_out = self.unet(audio_1d)\n",
        "\n",
        "        alexnet_out = self.alexnet(audio_features)\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = torch.cat([graph_out, vit_out, unet_out, alexnet_out], dim=1)\n",
        "        fused_features = self.fusion_layer(fused_features)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(fused_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "class ADReSSoDataset(Dataset):\n",
        "    \"\"\"Dataset class for ADReSSo data\"\"\"\n",
        "    def __init__(self, features_dict, linguistic_features, labels, transform=None):\n",
        "        self.features_dict = features_dict\n",
        "        self.linguistic_features = linguistic_features\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.file_ids = list(features_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_ids[idx]\n",
        "\n",
        "        # Get features\n",
        "        features = self.features_dict[file_id]\n",
        "        linguistic = self.linguistic_features[file_id]\n",
        "\n",
        "        # Prepare audio features (Wav2Vec2)\n",
        "        audio_features = torch.FloatTensor(features['wav2vec2'])\n",
        "\n",
        "        # Prepare text features\n",
        "        text_input_ids = torch.LongTensor(linguistic['bert_input_ids'])\n",
        "        text_attention_mask = torch.LongTensor(linguistic['bert_attention_mask'])\n",
        "\n",
        "        # Prepare spectrogram (create from log-mel features)\n",
        "        log_mel_mean = features['log_mel']['mean']\n",
        "        log_mel_std = features['log_mel']['std']\n",
        "        spectrogram = np.stack([log_mel_mean, log_mel_std])  # (2, 80)\n",
        "        spectrogram = np.expand_dims(spectrogram.mean(axis=0), axis=0)  # (1, 80)\n",
        "        spectrogram = np.tile(spectrogram, (1, 1, 80))  # (1, 80, 80) - square image\n",
        "        spectrogram = torch.FloatTensor(spectrogram)\n",
        "\n",
        "        label = torch.LongTensor([self.labels[file_id]])\n",
        "\n",
        "        return {\n",
        "            'audio_features': audio_features,\n",
        "            'text_input_ids': text_input_ids,\n",
        "            'text_attention_mask': text_attention_mask,\n",
        "            'spectrogram': spectrogram,\n",
        "            'label': label,\n",
        "            'file_id': file_id\n",
        "        }\n",
        "\n",
        "class ADReSSoTrainer:\n",
        "    \"\"\"Training and evaluation class\"\"\"\n",
        "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            # Move to device\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "            text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "            spectrograms = batch['spectrogram'].to(self.device)\n",
        "            labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(audio_features, text_input_ids, text_attention_mask, spectrograms)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                # Move to device\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "                text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "                spectrograms = batch['spectrogram'].to(self.device)\n",
        "                labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(audio_features, text_input_ids, text_attention_mask, spectrograms)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # Store for detailed metrics\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = 100. * correct / total\n",
        "\n",
        "        return avg_loss, accuracy, all_preds, all_labels, all_probs\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=5):\n",
        "        print(f\"Training on {self.device}\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "            print('-' * 50)\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc, val_preds, val_labels, val_probs = self.validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping and model saving\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_adresso_model.pth')\n",
        "                patience_counter = 0\n",
        "                print(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= 10:\n",
        "                    print('Early stopping triggered')\n",
        "                    break\n",
        "\n",
        "        print(f'\\nTraining completed. Best validation accuracy: {best_val_acc:.2f}%')\n",
        "\n",
        "    def evaluate_detailed(self, test_loader, class_names=['CN', 'AD']):\n",
        "        \"\"\"Detailed evaluation with metrics and visualizations\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "        all_file_ids = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                # Move to device\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "                text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "                spectrograms = batch['spectrogram'].to(self.device)\n",
        "                labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(audio_features, text_input_ids, text_attention_mask, spectrograms)\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                _, predicted = outputs.max(1)\n",
        "\n",
        "                # Store results\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_file_ids.extend(batch['file_id'])\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        # ROC AUC (for binary classification)\n",
        "        if len(class_names) == 2:\n",
        "            probs_positive = [prob[1] for prob in all_probs]\n",
        "            auc = roc_auc_score(all_labels, probs_positive)\n",
        "        else:\n",
        "            auc = None\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        # Print results\n",
        "        print(\"=\"*60)\n",
        "        print(\"DETAILED EVALUATION RESULTS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        if auc:\n",
        "            print(f\"ROC AUC: {auc:.4f}\")\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot training curves\n",
        "        self.plot_training_curves()\n",
        "\n",
        "        # Create results DataFrame\n",
        "        results_df = pd.DataFrame({\n",
        "            'file_id': all_file_ids,\n",
        "            'true_label': all_labels,\n",
        "            'predicted_label': all_preds,\n",
        "            'confidence': [max(prob) for prob in all_probs],\n",
        "            'prob_CN': [prob[0] for prob in all_probs],\n",
        "            'prob_AD': [prob[1] for prob in all_probs] if len(all_probs[0]) > 1 else [0] * len(all_probs)\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'auc': auc,\n",
        "            'confusion_matrix': cm,\n",
        "            'results_df': results_df\n",
        "        }\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"Plot training and validation curves\"\"\"\n",
        "        fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        ax1.plot(self.train_losses, label='Training Loss', color='blue')\n",
        "        ax1.plot(self.val_losses, label='Validation Loss', color='red')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Accuracy curves\n",
        "        ax2.plot(self.train_accuracies, label='Training Accuracy', color='blue')\n",
        "        ax2.plot(self.val_accuracies, label='Validation Accuracy', color='red')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy (%)')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def visualize_semantic_graph(text_features, audio_features, file_id, save_path=None):\n",
        "    \"\"\"Visualize semantic relationships between audio and text\"\"\"\n",
        "    # Compute similarity\n",
        "    similarity = F.cosine_similarity(text_features, audio_features, dim=0).item()\n",
        "\n",
        "    # Create networkx graph\n",
        "    G = nx.Graph()\n",
        "    G.add_node(\"Text\", type=\"text\", features=text_features[:5].tolist())\n",
        "    G.add_node(\"Audio\", type=\"audio\", features=audio_features[:5].tolist())\n",
        "\n",
        "    # Add edge if similarity is significant\n",
        "    if similarity > 0.1:\n",
        "        G.add_edge(\"Text\", \"Audio\", weight=similarity, similarity=similarity)\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
        "\n",
        "    # Draw nodes\n",
        "    node_colors = ['lightblue' if G.nodes[node]['type'] == 'text' else 'lightcoral'\n",
        "                   for node in G.nodes()]\n",
        "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=3000, alpha=0.7)\n",
        "\n",
        "    # Draw edges\n",
        "    if G.edges():\n",
        "        edge_widths = [G[u][v]['weight'] * 10 for u, v in G.edges()]\n",
        "        nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.6, edge_color='gray')\n",
        "\n",
        "    # Draw labels\n",
        "    nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')\n",
        "\n",
        "    # Add edge labels\n",
        "    if G.edges():\n",
        "        edge_labels = {(u, v): f\"Sim: {G[u][v]['similarity']:.3f}\"\n",
        "                      for u, v in G.edges()}\n",
        "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)\n",
        "\n",
        "    plt.title(f'Semantic Relationship Graph - {file_id}\\nSimilarity: {similarity:.3f}')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return G, similarity"
      ],
      "metadata": {
        "id": "_BZbXSFrZlDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}