{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPOGv8oXJuHUtKXIoESOnNk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57b5cbab98584eb296c39adc9e1eed29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d85101a6b169484889339689363f21f5",
              "IPY_MODEL_f8920c3389bb4d6cbfe71169734da3b3",
              "IPY_MODEL_3136a350a6104b0eaa18613b98bc020c"
            ],
            "layout": "IPY_MODEL_22fc0381c8744556b97f157fbacd7c61"
          }
        },
        "d85101a6b169484889339689363f21f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fc8d7d8db1f46e4b26d4096840c4b61",
            "placeholder": "​",
            "style": "IPY_MODEL_f0ca80575c674693a35b2d7c48e8e51d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f8920c3389bb4d6cbfe71169734da3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4eb2414540408195415362074001b3",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06af4be60ddb4edab8570ddf739a7952",
            "value": 48
          }
        },
        "3136a350a6104b0eaa18613b98bc020c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31348ee2121047bab63b0263a4b97266",
            "placeholder": "​",
            "style": "IPY_MODEL_5a226f8587814f15aaf168cda773a7fb",
            "value": " 48.0/48.0 [00:00&lt;00:00, 991B/s]"
          }
        },
        "22fc0381c8744556b97f157fbacd7c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc8d7d8db1f46e4b26d4096840c4b61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ca80575c674693a35b2d7c48e8e51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb4eb2414540408195415362074001b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06af4be60ddb4edab8570ddf739a7952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31348ee2121047bab63b0263a4b97266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a226f8587814f15aaf168cda773a7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19897ba1ecce430e83340f51ac8a7b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad579952aaf94227879bf1d974f17d26",
              "IPY_MODEL_27dff0e6f9cb47c6ac1fe9e04b0574f5",
              "IPY_MODEL_2f28a9add5ce42bda93a589f018423e0"
            ],
            "layout": "IPY_MODEL_497bf48c35de48f7b08ef4ca6dfc4352"
          }
        },
        "ad579952aaf94227879bf1d974f17d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4972a13aacff4f159335e5da1a791037",
            "placeholder": "​",
            "style": "IPY_MODEL_70d9a4a0aa1d4ffc9d4a230ce971e8bf",
            "value": "vocab.txt: 100%"
          }
        },
        "27dff0e6f9cb47c6ac1fe9e04b0574f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0f31b24847f4b2cb4c6e960ba42a71e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dc1105349ac43bda0e8797cd194de42",
            "value": 231508
          }
        },
        "2f28a9add5ce42bda93a589f018423e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76143b64a0e745dda82e7c3632843e40",
            "placeholder": "​",
            "style": "IPY_MODEL_87df911fb9704615b1f002615d2e13f3",
            "value": " 232k/232k [00:00&lt;00:00, 1.85MB/s]"
          }
        },
        "497bf48c35de48f7b08ef4ca6dfc4352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4972a13aacff4f159335e5da1a791037": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d9a4a0aa1d4ffc9d4a230ce971e8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0f31b24847f4b2cb4c6e960ba42a71e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc1105349ac43bda0e8797cd194de42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76143b64a0e745dda82e7c3632843e40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87df911fb9704615b1f002615d2e13f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49a0c0f8e32341efa72b59cb8bf44a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5e1a39b5b084d0c92c60222800258d6",
              "IPY_MODEL_f41351d2c955425387d4e73d89a1342d",
              "IPY_MODEL_b19889a25d994bed8b39e7ea7c9b4193"
            ],
            "layout": "IPY_MODEL_507392ec476446d6b5620e41593b716e"
          }
        },
        "b5e1a39b5b084d0c92c60222800258d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844d77655ffd403baf3d1c7c32d5918f",
            "placeholder": "​",
            "style": "IPY_MODEL_1a1db62d36434592aae538a7004a0f3e",
            "value": "tokenizer.json: 100%"
          }
        },
        "f41351d2c955425387d4e73d89a1342d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb154f03eb9242b180dd24acfc0ed159",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11f57eeac14e440099934f5ff6543580",
            "value": 466062
          }
        },
        "b19889a25d994bed8b39e7ea7c9b4193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83a7ac9d8c674d2ab783a0df8df04992",
            "placeholder": "​",
            "style": "IPY_MODEL_ba73b004971a4edfa95d93bffde5a93c",
            "value": " 466k/466k [00:00&lt;00:00, 16.3MB/s]"
          }
        },
        "507392ec476446d6b5620e41593b716e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844d77655ffd403baf3d1c7c32d5918f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a1db62d36434592aae538a7004a0f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb154f03eb9242b180dd24acfc0ed159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f57eeac14e440099934f5ff6543580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83a7ac9d8c674d2ab783a0df8df04992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba73b004971a4edfa95d93bffde5a93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebcf8b5449a6431c9b14123207b42e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_605a9fdd3e0f4b3c9c8a479295ead1a3",
              "IPY_MODEL_eb26636ee5c4457bbb65621a1ca47185",
              "IPY_MODEL_ec0a2434f3c74f7f9eccb5e9b17d3cc6"
            ],
            "layout": "IPY_MODEL_09fd41c49e0346c4ab6062299d8e0926"
          }
        },
        "605a9fdd3e0f4b3c9c8a479295ead1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b6661a9218c4e039b3dca5508cd1ff0",
            "placeholder": "​",
            "style": "IPY_MODEL_26725ea16f8d4a728f290ca2fbf57d20",
            "value": "config.json: 100%"
          }
        },
        "eb26636ee5c4457bbb65621a1ca47185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e10b90bcd34695bc9c9476e52df202",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de2dcc0b61e94806864b1524e1ddb61c",
            "value": 570
          }
        },
        "ec0a2434f3c74f7f9eccb5e9b17d3cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ef0a246aade4576a7758103af49da7e",
            "placeholder": "​",
            "style": "IPY_MODEL_39577aa917e44accb8d61423c93c86c2",
            "value": " 570/570 [00:00&lt;00:00, 43.6kB/s]"
          }
        },
        "09fd41c49e0346c4ab6062299d8e0926": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6661a9218c4e039b3dca5508cd1ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26725ea16f8d4a728f290ca2fbf57d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e10b90bcd34695bc9c9476e52df202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2dcc0b61e94806864b1524e1ddb61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ef0a246aade4576a7758103af49da7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39577aa917e44accb8d61423c93c86c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51eef6650bd849d0a5f6993c2a0f8c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61fa0eb558bb4c129282734e4ba15f9a",
              "IPY_MODEL_344f515820c74585ad922f7006024ee9",
              "IPY_MODEL_af4c949cd1974344bf1a4e8cd18407ed"
            ],
            "layout": "IPY_MODEL_eb0117c581de44eb9dacb90e6d78a31c"
          }
        },
        "61fa0eb558bb4c129282734e4ba15f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18fb8751b1274becbd6280d754ef61f2",
            "placeholder": "​",
            "style": "IPY_MODEL_662f82b77c0246d6b30c3ba8af4f7682",
            "value": "model.safetensors: 100%"
          }
        },
        "344f515820c74585ad922f7006024ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0002e71ecd9146d390fb03302c72c9b2",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a5815a5f1c54a5db81a6926bc9ca9b7",
            "value": 440449768
          }
        },
        "af4c949cd1974344bf1a4e8cd18407ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f9089c91ec14169a155bea28aa5fbf0",
            "placeholder": "​",
            "style": "IPY_MODEL_b288b40349454b29a1ff2b05245ebc20",
            "value": " 440M/440M [00:05&lt;00:00, 101MB/s]"
          }
        },
        "eb0117c581de44eb9dacb90e6d78a31c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18fb8751b1274becbd6280d754ef61f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "662f82b77c0246d6b30c3ba8af4f7682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0002e71ecd9146d390fb03302c72c9b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5815a5f1c54a5db81a6926bc9ca9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f9089c91ec14169a155bea28aa5fbf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b288b40349454b29a1ff2b05245ebc20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63b1467759674244989689c5f7aabf1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f569ebf03114df0ab1718dd40847640",
              "IPY_MODEL_1470172f33c24cbdb6979491f7ea4848",
              "IPY_MODEL_3fece809e6bb4b5aa36b4524fcc660f7"
            ],
            "layout": "IPY_MODEL_43df1af5d5e14094aeba1ab19e52f0e7"
          }
        },
        "7f569ebf03114df0ab1718dd40847640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b382bc9c319c45e48092ebf34d61fcdc",
            "placeholder": "​",
            "style": "IPY_MODEL_44e3eb3e5c0e4fb38221064956e6af4f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1470172f33c24cbdb6979491f7ea4848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2315bab55bfa498aaef28a18b4d4a7b3",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d43ce162bc8b439c8fec4b567c757b12",
            "value": 48
          }
        },
        "3fece809e6bb4b5aa36b4524fcc660f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4e620c8b7246238a6a65c70c3c9729",
            "placeholder": "​",
            "style": "IPY_MODEL_0e43b10d80b6439aa33a3fa7955c986c",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.17kB/s]"
          }
        },
        "43df1af5d5e14094aeba1ab19e52f0e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b382bc9c319c45e48092ebf34d61fcdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e3eb3e5c0e4fb38221064956e6af4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2315bab55bfa498aaef28a18b4d4a7b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d43ce162bc8b439c8fec4b567c757b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db4e620c8b7246238a6a65c70c3c9729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e43b10d80b6439aa33a3fa7955c986c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ecd97d819364fe8bd43e8f64f1616f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b148787f9c814ade8e055318a153f7a8",
              "IPY_MODEL_a2a79814588b45079ed7547c2eefd939",
              "IPY_MODEL_2e9bdb3bc38b4c9babc20777dadc96a5"
            ],
            "layout": "IPY_MODEL_da0e9d98c68f4211847af7d51e7bcc6b"
          }
        },
        "b148787f9c814ade8e055318a153f7a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549172fe8c894251a0bf1147c0b51934",
            "placeholder": "​",
            "style": "IPY_MODEL_c3ed16642e87457ab4c75cd3fdc36bf8",
            "value": "vocab.txt: 100%"
          }
        },
        "a2a79814588b45079ed7547c2eefd939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24a05bf6d5634f479592dbf207578fac",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f0198fdb4204b0196b973aac6b7bcac",
            "value": 231508
          }
        },
        "2e9bdb3bc38b4c9babc20777dadc96a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7474ee0861af4d90aa97e8c1ee6f654a",
            "placeholder": "​",
            "style": "IPY_MODEL_3d33dfa2ee9e491eb81afcdddcd19586",
            "value": " 232k/232k [00:00&lt;00:00, 4.36MB/s]"
          }
        },
        "da0e9d98c68f4211847af7d51e7bcc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549172fe8c894251a0bf1147c0b51934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ed16642e87457ab4c75cd3fdc36bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24a05bf6d5634f479592dbf207578fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0198fdb4204b0196b973aac6b7bcac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7474ee0861af4d90aa97e8c1ee6f654a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d33dfa2ee9e491eb81afcdddcd19586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bc341f0bd47423c95f60ef4c0755b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_530d189a7ee24d01b21fe9bee075cdb6",
              "IPY_MODEL_ac4c6f9a7f70419788d910763f41168c",
              "IPY_MODEL_5f57c4113a444105900ecdff6476c907"
            ],
            "layout": "IPY_MODEL_90a46ca7b2a64b1d880c45aa32087486"
          }
        },
        "530d189a7ee24d01b21fe9bee075cdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1acd32fbfde4e87854416f4a7f06c58",
            "placeholder": "​",
            "style": "IPY_MODEL_1412636de53641d89f377302829189f2",
            "value": "tokenizer.json: 100%"
          }
        },
        "ac4c6f9a7f70419788d910763f41168c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e17addb026eb45018aa4c90207a185e7",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bfa6f21e6d046a58263037e744dbce5",
            "value": 466062
          }
        },
        "5f57c4113a444105900ecdff6476c907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67dd4d1f5a8643539f2a04fbb9d7c54c",
            "placeholder": "​",
            "style": "IPY_MODEL_be7f30f3da0748c08f29919456789731",
            "value": " 466k/466k [00:00&lt;00:00, 14.4MB/s]"
          }
        },
        "90a46ca7b2a64b1d880c45aa32087486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1acd32fbfde4e87854416f4a7f06c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1412636de53641d89f377302829189f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e17addb026eb45018aa4c90207a185e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bfa6f21e6d046a58263037e744dbce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67dd4d1f5a8643539f2a04fbb9d7c54c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7f30f3da0748c08f29919456789731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cf052ec4b5346828d3262a5073d7f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71c05aa265e147bc92ab357d78cb899a",
              "IPY_MODEL_08933bf101b94f1b9f607971c8c3c365",
              "IPY_MODEL_e5b607afd6aa46eab7eecc7f65c5ffc6"
            ],
            "layout": "IPY_MODEL_0abe2f618f654f3eb384be036433ba1b"
          }
        },
        "71c05aa265e147bc92ab357d78cb899a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98a3e6053aca4a34b5ab1dae6941edba",
            "placeholder": "​",
            "style": "IPY_MODEL_6b1c9fc842b0476ea7fd62514aab283e",
            "value": "config.json: 100%"
          }
        },
        "08933bf101b94f1b9f607971c8c3c365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6abe95a496eb4926a3bd0d6c38ee3052",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1caf47e1b5c41e49d5d440819eec58d",
            "value": 570
          }
        },
        "e5b607afd6aa46eab7eecc7f65c5ffc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac6f11f5390f4ea48d430c8b8d7d1c3e",
            "placeholder": "​",
            "style": "IPY_MODEL_02c5bd3559474eeab8fea3a34c75688c",
            "value": " 570/570 [00:00&lt;00:00, 55.7kB/s]"
          }
        },
        "0abe2f618f654f3eb384be036433ba1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a3e6053aca4a34b5ab1dae6941edba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b1c9fc842b0476ea7fd62514aab283e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6abe95a496eb4926a3bd0d6c38ee3052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1caf47e1b5c41e49d5d440819eec58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac6f11f5390f4ea48d430c8b8d7d1c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02c5bd3559474eeab8fea3a34c75688c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4133d8e056b549eabfa931cc50bba906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_137be4b430fa4ca58287df9a5b007164",
              "IPY_MODEL_524015b746234003b8bc80c28c2074fb",
              "IPY_MODEL_dff51636451748a7acde8e45b26f208c"
            ],
            "layout": "IPY_MODEL_6f6f20e28acd432caf289713886500d5"
          }
        },
        "137be4b430fa4ca58287df9a5b007164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db82d9725a0c45349f1bee5175f0ba06",
            "placeholder": "​",
            "style": "IPY_MODEL_29aebfd7d8924b199891f24d252e06fb",
            "value": "model.safetensors: 100%"
          }
        },
        "524015b746234003b8bc80c28c2074fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7611a04fbfb144bdbf8bec018d6a9652",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4edaee56644e4f19892ae6e5a351f9bc",
            "value": 440449768
          }
        },
        "dff51636451748a7acde8e45b26f208c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689071d0026b47dfa15be72ba94b938f",
            "placeholder": "​",
            "style": "IPY_MODEL_ac0c45959e104859b6ddb1e2e8ad038a",
            "value": " 440M/440M [00:18&lt;00:00, 24.3MB/s]"
          }
        },
        "6f6f20e28acd432caf289713886500d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db82d9725a0c45349f1bee5175f0ba06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29aebfd7d8924b199891f24d252e06fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7611a04fbfb144bdbf8bec018d6a9652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edaee56644e4f19892ae6e5a351f9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "689071d0026b47dfa15be72ba94b938f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0c45959e104859b6ddb1e2e8ad038a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/DetectAlzFromSpeechGBA_runable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI-ChIc4Zgx-",
        "outputId": "d3c61daf-219c-4574-961d-bccbb9c68e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directories:\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test\n",
            "  /content/drive/MyDrive/Speech/linguistic_features\n",
            "  /content/drive/MyDrive/Speech/lightweight_features\n",
            "  /content/drive/MyDrive/Speech/transcripts\n",
            "  /content/drive/MyDrive/Speech/processed_datasets\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_decline\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_ad\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_cn\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_test\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_no_decline\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts\n",
            "\n",
            "Files:\n",
            "  /content/drive/MyDrive/Speech/diagnosis_model.pth\n",
            "  /content/drive/MyDrive/Speech/progression_model.pth\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/README.md\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso152.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso014.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso005.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso276.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso003.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso259.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso022.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso156.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso278.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso274.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso002.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso160.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso019.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso168.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso172.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso177.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso023.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso268.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso266.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso007.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso186.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso157.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso257.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso263.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso161.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso018.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso280.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso178.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso265.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso153.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso010.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso151.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso169.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso267.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso159.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso012.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso016.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso165.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso170.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso183.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso015.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso270.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso173.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso260.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso264.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso277.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso180.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso273.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso008.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso162.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso167.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso182.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso017.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso154.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso261.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso021.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso164.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso158.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso148.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso262.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso285.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso312.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso315.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso298.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso310.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso291.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso316.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso308.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso283.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso300.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso289.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso302.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso299.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso281.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso292.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso296.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso307.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso309.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso286.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso025.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso033.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso027.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso046.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso032.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso049.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso047.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso053.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso054.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso036.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso035.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso043.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso024.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso039.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso028.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso031.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso045.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso200.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso116.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso071.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso212.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso209.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso232.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso247.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso222.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso236.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso078.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso106.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso070.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso141.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso138.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso110.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso218.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso144.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso206.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso205.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso244.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso056.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso077.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso090.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso134.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso249.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso098.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso123.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso072.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso126.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso253.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso245.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso089.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso192.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso092.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso068.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso223.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso122.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso224.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso188.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso130.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso202.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso229.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso216.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso198.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso093.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso063.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso112.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso142.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso187.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso059.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso237.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso215.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso189.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso055.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso125.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso250.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso109.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso233.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso190.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso060.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso246.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso234.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso197.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso228.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso128.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso075.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso220.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso211.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso074.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso248.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso003.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso014.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso012.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso017.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso010.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso008.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso018.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso005.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso019.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso002.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso022.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso021.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso016.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso015.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso153.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso165.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso152.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso164.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso156.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso157.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso161.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso160.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso154.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso172.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso148.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso173.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso170.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso023.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso168.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso159.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso167.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso169.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso260.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso183.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso264.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso276.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso180.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso266.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso177.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso178.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso273.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso274.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso262.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso182.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso186.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso286.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso298.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso310.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso308.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso312.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso281.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso291.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso285.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso315.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso283.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso277.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso289.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso296.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso299.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso309.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso307.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso316.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso302.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso278.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso162.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso300.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso292.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso261.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso265.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso151.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso268.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso259.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso267.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso257.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso263.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso280.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso270.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso158.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso036.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso043.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso024.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso045.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso077.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso072.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso055.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso060.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso047.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso074.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso049.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso075.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso068.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso053.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso071.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso070.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso059.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso109.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso089.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso141.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso188.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso092.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso093.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso134.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso106.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso138.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso110.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso144.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso130.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso123.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso116.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso142.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso125.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso187.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso128.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso112.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso098.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso126.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso209.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso218.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso216.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso206.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso205.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso198.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso211.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso245.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso237.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso229.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso246.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso236.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso222.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso234.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso249.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso228.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso223.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso233.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso247.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso232.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso250.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso253.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso248.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso224.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso189.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso202.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso063.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso039.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso244.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso122.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso190.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso192.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso215.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso090.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso025.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso031.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso027.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso197.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso056.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso054.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso078.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso028.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso200.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso220.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso212.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso046.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso035.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso033.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso032.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/README.md\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp056.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp024.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp157.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp042.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp349.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp148.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp200.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp137.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp136.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp321.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp039.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp306.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp007.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp030.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp001.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp052.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp197.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp310.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp196.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp192.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp177.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp041.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp207.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp253.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp161.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp043.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp319.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp193.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp130.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp195.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp028.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp124.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp350.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp198.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp251.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp031.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp122.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp003.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp055.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp313.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp051.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp179.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp209.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp266.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp127.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp300.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp101.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp196.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp137.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp130.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp349.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp198.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp321.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp136.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp024.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp007.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp382.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp043.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp019.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp333.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp056.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp310.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp042.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp377.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp363.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp028.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp350.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp096.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp052.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp204.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp380.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp109.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp255.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp157.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp306.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp197.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp031.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp368.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp032.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp091.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp344.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp124.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp195.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp253.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp251.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp039.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp001.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp041.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp384.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp207.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp379.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp324.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp177.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp148.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp023.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp359.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp122.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp200.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp030.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp319.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp378.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp193.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp128.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp161.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp192.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp055.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp003.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp300.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp266.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp320.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp313.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp179.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp357.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp101.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp051.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp326.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp127.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp276.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp209.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp318.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/README\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/test_results_task3.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt29.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt13.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt21.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt1.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt18.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt10.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt23.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt26.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt17.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt11.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt2.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt12.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt9.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt15.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt24.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt26.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt5.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt20.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt22.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt32.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt11.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt7.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt2.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt8.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt18.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt17.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt12.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt3.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt28.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt4.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt6.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt15.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt21.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt19.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt31.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt29.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt14.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt13.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt23.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt10.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt30.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt16.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt27.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt1.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt25.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt24.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt9.wav\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/linguistic_features.json\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/linguistic_features.pkl\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/feature_summary.txt\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/feature_analysis.csv\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/diagnosis_ad_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/diagnosis_cn_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_decline_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_no_decline_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_test_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/diagnosis_ad_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/diagnosis_cn_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_decline_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_no_decline_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_test_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/all_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/final_stats.json\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/openpkl.py\n",
            "  /content/drive/MyDrive/Speech/transcripts/error_analysis.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/all_categories_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_results.pkl\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_summary.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_decline/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_ad/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_cn/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_test/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_no_decline/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso047.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso298.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso296.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso157.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso245.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso273.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso316.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso059.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso148.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso308.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso016.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso110.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso190.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso307.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso187.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso162.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso257.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso054.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso075.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso056.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso268.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso197.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso060.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso156.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso142.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso039.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso237.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso202.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso189.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso224.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso182.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso024.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso218.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso259.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso260.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso165.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso285.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso209.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso200.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso215.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso173.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso299.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso151.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso233.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso017.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso055.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso228.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso312.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso280.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso109.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso229.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso267.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso022.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso068.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso002.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso032.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso134.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso053.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso072.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso164.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso247.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso008.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso192.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso212.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso036.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso141.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso090.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso152.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso077.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso130.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso158.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso281.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso249.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso014.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso205.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso112.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso125.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso236.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso074.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso300.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso106.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso274.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso070.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso012.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso216.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso098.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso180.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso126.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso266.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso261.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso232.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso035.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso283.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso021.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso248.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso168.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso291.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso116.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso172.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso045.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso071.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso183.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso310.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso234.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso093.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso005.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso289.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso263.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso007.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso265.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso028.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso253.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso177.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso170.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso188.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso027.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso078.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso154.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso033.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso211.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso092.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso246.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso206.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso046.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso161.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso043.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso128.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso250.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso292.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso223.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso159.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso153.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso178.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso003.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso023.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso286.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso302.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso169.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso144.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso315.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso270.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso276.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso222.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso278.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso244.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso186.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso122.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso198.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso015.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso019.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso220.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso089.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso049.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso025.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso018.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso123.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso063.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso031.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso262.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso138.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp266.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp032.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp310.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp043.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp161.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp197.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt13.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp003.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp137.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt12.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp357.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp101.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp350.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt11.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp198.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp056.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp023.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp300.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp253.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso167.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt24.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt8.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp031.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt19.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp148.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp055.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp377.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp382.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp028.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt29.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp319.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt6.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt18.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp179.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp384.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp096.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp321.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp007.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp306.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp177.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp128.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp195.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp324.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp136.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp251.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt5.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp192.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp209.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp039.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp193.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt17.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt14.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp200.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt20.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp344.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt26.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp041.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso264.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp320.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp204.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso277.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp042.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt30.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp368.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt23.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt22.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt15.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp207.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt9.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp130.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt1.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp052.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp051.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp019.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt25.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp122.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt3.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp363.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp349.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp109.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso160.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt16.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt4.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp196.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp276.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp380.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt10.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt21.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt2.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp255.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp030.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp359.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp001.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp124.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso010.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp157.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp024.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso309.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt7.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp326.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp318.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt28.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt27.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp091.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp378.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp127.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp313.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp379.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp333.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt32.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt31.wav.txt\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_train.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_train.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_train.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_train.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_test.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_test.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/all_data.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/all_data.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_label_encoder.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/labels_dict.json\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_label_encoder.pkl\n",
            "\n",
            "Summary:\n",
            "  Total directories: 37\n",
            "  Total files: 815\n",
            "  Total paths: 852\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory\n",
        "base_directory = '/content/drive/MyDrive/Speech'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(base_directory):\n",
        "    # Separate lists for files and directories\n",
        "    file_paths = []\n",
        "    directory_paths = []\n",
        "\n",
        "    for root, directories, files in os.walk(base_directory):\n",
        "        # Add directory paths\n",
        "        for directory in directories:\n",
        "            dir_path = os.path.join(root, directory)\n",
        "            directory_paths.append(dir_path)\n",
        "\n",
        "        # Add file paths\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_paths.append(file_path)\n",
        "\n",
        "    print(\"Directories:\")\n",
        "    for directory in directory_paths:\n",
        "        print(f\"  {directory}\")\n",
        "\n",
        "    print(f\"\\nFiles:\")\n",
        "    for file in file_paths:\n",
        "        print(f\"  {file}\")\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"  Total directories: {len(directory_paths)}\")\n",
        "    print(f\"  Total files: {len(file_paths)}\")\n",
        "    print(f\"  Total paths: {len(file_paths) + len(directory_paths)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Directory {base_directory} does not exist!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def extract_labels_from_directory_structure():\n",
        "    base_paths = {\n",
        "        'diagnosis_train': '/content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio',\n",
        "        'progression_train': '/content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio',\n",
        "        'progression_test': '/content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio'\n",
        "    }\n",
        "\n",
        "    labels = {}\n",
        "\n",
        "    for dataset_type, base_path in base_paths.items():\n",
        "        if dataset_type == 'progression_test':\n",
        "            files = os.listdir(base_path)\n",
        "            wav_files = [f for f in files if f.endswith('.wav')]\n",
        "            for wav_file in wav_files:\n",
        "                file_id = wav_file.replace('.wav', '')\n",
        "                labels[file_id] = {\n",
        "                    'dataset': 'progression_test',\n",
        "                    'label': 'unknown',\n",
        "                    'file_path': os.path.join(base_path, wav_file)\n",
        "                }\n",
        "        else:\n",
        "            subdirs = os.listdir(base_path)\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(base_path, subdir)\n",
        "                if os.path.isdir(subdir_path):\n",
        "                    wav_files = os.listdir(subdir_path)\n",
        "                    for wav_file in wav_files:\n",
        "                        if wav_file.endswith('.wav'):\n",
        "                            file_id = wav_file.replace('.wav', '')\n",
        "                            if dataset_type == 'diagnosis_train':\n",
        "                                label = 'ad' if subdir == 'ad' else 'cn'\n",
        "                            else:\n",
        "                                label = 'decline' if subdir == 'decline' else 'no_decline'\n",
        "\n",
        "                            labels[file_id] = {\n",
        "                                'dataset': dataset_type,\n",
        "                                'label': label,\n",
        "                                'file_path': os.path.join(subdir_path, wav_file)\n",
        "                            }\n",
        "\n",
        "    return labels\n",
        "\n",
        "def extract_mel_spectrogram(audio_path, n_mels=128, hop_length=512, n_fft=2048):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
        "                                                hop_length=hop_length, n_fft=n_fft)\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        return {\n",
        "            'mel_spectrogram': mel_spec,\n",
        "            'log_mel_spectrogram': log_mel_spec,\n",
        "            'mel_mean': np.mean(mel_spec, axis=1),\n",
        "            'mel_std': np.std(mel_spec, axis=1),\n",
        "            'log_mel_mean': np.mean(log_mel_spec, axis=1),\n",
        "            'log_mel_std': np.std(log_mel_spec, axis=1)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_wav2vec2_features(audio_path, model_name=\"facebook/wav2vec2-base-960h\"):\n",
        "    try:\n",
        "        processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "        model = Wav2Vec2Model.from_pretrained(model_name)\n",
        "\n",
        "        y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        inputs = processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        features = last_hidden_states.squeeze().numpy()\n",
        "\n",
        "        return {\n",
        "            'wav2vec2_features': features,\n",
        "            'wav2vec2_mean': np.mean(features, axis=0),\n",
        "            'wav2vec2_std': np.std(features, axis=0),\n",
        "            'wav2vec2_max': np.max(features, axis=0),\n",
        "            'wav2vec2_min': np.min(features, axis=0)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing wav2vec2 for {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_acoustic_features(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        # Fix: Use correct librosa function names\n",
        "        features['mfcc'] = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
        "        features['chroma'] = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)  # Fixed function name\n",
        "        features['spectral_contrast'] = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr), axis=1)\n",
        "        features['tonnetz'] = np.mean(librosa.feature.tonnetz(y=y, sr=sr), axis=1)\n",
        "        features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "        features['rms_energy'] = np.mean(librosa.feature.rms(y=y))\n",
        "\n",
        "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "        features['tempo'] = tempo\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting acoustic features for {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_linguistic_features():\n",
        "    ling_features = {}\n",
        "\n",
        "    ling_path = '/content/drive/MyDrive/Speech/linguistic_features'\n",
        "    if os.path.exists(ling_path):\n",
        "        try:\n",
        "            with open(os.path.join(ling_path, 'linguistic_features.pkl'), 'rb') as f:\n",
        "                ling_features = pickle.load(f)\n",
        "        except:\n",
        "            try:\n",
        "                with open(os.path.join(ling_path, 'linguistic_features.json'), 'r') as f:\n",
        "                    ling_features = json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading linguistic features: {e}\")\n",
        "\n",
        "    return ling_features\n",
        "def load_transcripts():\n",
        "    transcripts = {}\n",
        "\n",
        "    transcript_files = [\n",
        "        '/content/drive/MyDrive/Speech/transcripts/all_categories_results.json',\n",
        "        '/content/drive/MyDrive/Speech/transcripts/transcription_results.json'\n",
        "    ]\n",
        "\n",
        "    for transcript_file in transcript_files:\n",
        "        if os.path.exists(transcript_file):\n",
        "            try:\n",
        "                with open(transcript_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        transcripts.update(data)\n",
        "                    elif isinstance(data, list):\n",
        "                        # Handle list format if that's what the file contains\n",
        "                        for item in data:\n",
        "                            if isinstance(item, dict) and len(item) >= 2:\n",
        "                                # Extract key-value pairs from list items\n",
        "                                keys = list(item.keys())\n",
        "                                transcripts[keys[0]] = item[keys[1]]\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error in {transcript_file}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {transcript_file}: {e}\")\n",
        "\n",
        "    # Continue with individual transcript loading...\n",
        "\n",
        "    individual_transcript_path = '/content/drive/MyDrive/Speech/transcripts/individual_transcripts'\n",
        "    if os.path.exists(individual_transcript_path):\n",
        "        txt_files = [f for f in os.listdir(individual_transcript_path) if f.endswith('.txt')]\n",
        "        for txt_file in txt_files:\n",
        "            try:\n",
        "                file_id = txt_file.replace('.wav.txt', '')\n",
        "                with open(os.path.join(individual_transcript_path, txt_file), 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    transcripts[file_id] = content\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {txt_file}: {e}\")\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "# Define a placeholder function for load_existing_features\n",
        "def load_existing_features():\n",
        "    \"\"\"\n",
        "    Placeholder function to load existing features.\n",
        "    Replace with actual loading logic if needed.\n",
        "    \"\"\"\n",
        "    return {}\n",
        "\n",
        "\n",
        "def create_comprehensive_dataset():\n",
        "    print(\"Extracting labels from directory structure...\")\n",
        "    labels_dict = extract_labels_from_directory_structure()\n",
        "\n",
        "    print(\"Loading existing features...\")\n",
        "    existing_features = load_existing_features()\n",
        "\n",
        "    print(\"Loading linguistic features...\")\n",
        "    linguistic_features = load_linguistic_features()\n",
        "\n",
        "    print(\"Loading transcripts...\")\n",
        "    transcripts = load_transcripts()\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        row = {\n",
        "            'file_id': file_id,\n",
        "            'dataset': label_info['dataset'],\n",
        "            'label': label_info['label'],\n",
        "            'file_path': label_info['file_path']\n",
        "        }\n",
        "\n",
        "        if file_id in transcripts:\n",
        "            row['transcript'] = transcripts[file_id]\n",
        "\n",
        "        for category, features in existing_features.items():\n",
        "            if file_id in features:\n",
        "                feature_data = features[file_id]\n",
        "                if isinstance(feature_data, dict):\n",
        "                    for key, value in feature_data.items():\n",
        "                        row[f'{category}_{key}'] = value\n",
        "                else:\n",
        "                    row[f'{category}_features'] = feature_data\n",
        "\n",
        "        if file_id in linguistic_features:\n",
        "            ling_data = linguistic_features[file_id]\n",
        "            if isinstance(ling_data, dict):\n",
        "                for key, value in ling_data.items():\n",
        "                    row[f'linguistic_{key}'] = value\n",
        "\n",
        "        dataset.append(row)\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    print(f\"Dataset created with {len(df)} samples\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "    return df, labels_dict\n",
        "\n",
        "def extract_audio_features_batch(labels_dict, sample_limit=None):\n",
        "    print(\"Extracting mel-spectrograms and acoustic features...\")\n",
        "\n",
        "    audio_features = {}\n",
        "    processed_count = 0\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        if sample_limit and processed_count >= sample_limit:\n",
        "            break\n",
        "\n",
        "        audio_path = label_info['file_path']\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            print(f\"Processing {file_id}...\")\n",
        "\n",
        "            mel_features = extract_mel_spectrogram(audio_path)\n",
        "            acoustic_features = extract_acoustic_features(audio_path)\n",
        "\n",
        "            if mel_features and acoustic_features:\n",
        "                combined_features = {**mel_features, **acoustic_features}\n",
        "                audio_features[file_id] = combined_features\n",
        "                processed_count += 1\n",
        "        else:\n",
        "            print(f\"File not found: {audio_path}\")\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "def create_training_datasets():\n",
        "    df, labels_dict = create_comprehensive_dataset()\n",
        "\n",
        "    diagnosis_train = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "    progression_train = df[df['dataset'] == 'progression_train'].copy()\n",
        "    progression_test = df[df['dataset'] == 'progression_test'].copy()\n",
        "\n",
        "    diagnosis_le = LabelEncoder()\n",
        "    if len(diagnosis_train) > 0:\n",
        "        diagnosis_train['label_encoded'] = diagnosis_le.fit_transform(diagnosis_train['label'])\n",
        "\n",
        "    progression_le = LabelEncoder()\n",
        "    if len(progression_train) > 0:\n",
        "        progression_train['label_encoded'] = progression_le.fit_transform(progression_train['label'])\n",
        "\n",
        "    datasets = {\n",
        "        'diagnosis_train': diagnosis_train,\n",
        "        'progression_train': progression_train,\n",
        "        'progression_test': progression_test,\n",
        "        'all_data': df,\n",
        "        'labels_dict': labels_dict,\n",
        "        'diagnosis_label_encoder': diagnosis_le,\n",
        "        'progression_label_encoder': progression_le\n",
        "    }\n",
        "\n",
        "    print(\"\\nDataset Summary:\")\n",
        "    print(f\"Diagnosis Training: {len(diagnosis_train)} samples\")\n",
        "    if len(diagnosis_train) > 0:\n",
        "        print(f\"  - AD: {len(diagnosis_train[diagnosis_train['label'] == 'ad'])}\")\n",
        "        print(f\"  - CN: {len(diagnosis_train[diagnosis_train['label'] == 'cn'])}\")\n",
        "\n",
        "    print(f\"Progression Training: {len(progression_train)} samples\")\n",
        "    if len(progression_train) > 0:\n",
        "        print(f\"  - Decline: {len(progression_train[progression_train['label'] == 'decline'])}\")\n",
        "        print(f\"  - No Decline: {len(progression_train[progression_train['label'] == 'no_decline'])}\")\n",
        "\n",
        "    print(f\"Progression Test: {len(progression_test)} samples\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def save_datasets(datasets, output_path='/content/drive/MyDrive/Speech/processed_datasets'):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    for name, data in datasets.items():\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            data.to_csv(os.path.join(output_path, f'{name}.csv'), index=False)\n",
        "            data.to_pickle(os.path.join(output_path, f'{name}.pkl'))\n",
        "        elif name == 'labels_dict':\n",
        "            with open(os.path.join(output_path, 'labels_dict.json'), 'w') as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "        else:\n",
        "            with open(os.path.join(output_path, f'{name}.pkl'), 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "\n",
        "    print(f\"Datasets saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    datasets = create_training_datasets()\n",
        "    save_datasets(datasets)\n",
        "\n",
        "    print(\"\\nExtracting audio features for a sample...\")\n",
        "    sample_audio_features = extract_audio_features_batch(datasets['labels_dict'], sample_limit=5)\n",
        "\n",
        "    if sample_audio_features:\n",
        "        print(f\"Successfully extracted features for {len(sample_audio_features)} audio files\")\n",
        "        sample_id = list(sample_audio_features.keys())[0]\n",
        "        sample_features = sample_audio_features[sample_id]\n",
        "        print(f\"Feature keys for {sample_id}: {list(sample_features.keys())}\")\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(\"Available datasets:\")\n",
        "    for name in datasets.keys():\n",
        "        if isinstance(datasets[name], pd.DataFrame):\n",
        "            print(f\"  - {name}: {len(datasets[name])} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzAC94TIb5u7",
        "outputId": "1ac74ee3-ec5d-4301-beab-4f4620eb1a4e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting labels from directory structure...\n",
            "Loading existing features...\n",
            "Loading linguistic features...\n",
            "Loading transcripts...\n",
            "Dataset created with 271 samples\n",
            "Columns: ['file_id', 'dataset', 'label', 'file_path', 'transcript']\n",
            "Label distribution:\n",
            "label\n",
            "ad            87\n",
            "cn            79\n",
            "no_decline    58\n",
            "unknown       32\n",
            "decline       15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset Summary:\n",
            "Diagnosis Training: 166 samples\n",
            "  - AD: 87\n",
            "  - CN: 79\n",
            "Progression Training: 73 samples\n",
            "  - Decline: 15\n",
            "  - No Decline: 58\n",
            "Progression Test: 32 samples\n",
            "Datasets saved to /content/drive/MyDrive/Speech/processed_datasets\n",
            "\n",
            "Extracting audio features for a sample...\n",
            "Extracting mel-spectrograms and acoustic features...\n",
            "Processing adrso003...\n",
            "Processing adrso014...\n",
            "Processing adrso012...\n",
            "Processing adrso017...\n",
            "Processing adrso010...\n",
            "Successfully extracted features for 5 audio files\n",
            "Feature keys for adrso003: ['mel_spectrogram', 'log_mel_spectrogram', 'mel_mean', 'mel_std', 'log_mel_mean', 'log_mel_std', 'mfcc', 'chroma', 'spectral_contrast', 'tonnetz', 'zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']\n",
            "\n",
            "Processing complete!\n",
            "Available datasets:\n",
            "  - diagnosis_train: 166 samples\n",
            "  - progression_train: 73 samples\n",
            "  - progression_test: 32 samples\n",
            "  - all_data: 271 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import math\n",
        "\n",
        "class XceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(XceptionBlock, self).__init__()\n",
        "        self.separable_conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),\n",
        "            nn.Conv2d(in_channels, out_channels, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.separable_conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, groups=out_channels),\n",
        "            nn.Conv2d(out_channels, out_channels, 1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "        self.skip_connection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        ) if in_channels != out_channels or stride != 1 else nn.Identity()\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.skip_connection(x)\n",
        "        x = self.separable_conv1(x)\n",
        "        x = self.separable_conv2(x)\n",
        "        if self.stride > 1:\n",
        "            x = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n",
        "        return F.relu(x + residual)\n",
        "\n",
        "class XceptionNet(nn.Module):\n",
        "    def __init__(self, input_channels=1, num_classes=512):\n",
        "        super(XceptionNet, self).__init__()\n",
        "        self.entry_flow = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.middle_flow = nn.Sequential(\n",
        "            XceptionBlock(64, 128, stride=2),\n",
        "            XceptionBlock(128, 256, stride=2),\n",
        "            XceptionBlock(256, 728, stride=2),\n",
        "            XceptionBlock(728, 728),\n",
        "            XceptionBlock(728, 728),\n",
        "            XceptionBlock(728, 728),\n",
        "        )\n",
        "\n",
        "        self.exit_flow = nn.Sequential(\n",
        "            XceptionBlock(728, 1024, stride=2),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry_flow(x)\n",
        "        x = self.middle_flow(x)\n",
        "        x = self.exit_flow(x)\n",
        "        return x\n",
        "\n",
        "class ViTBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super(ViTBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, num_classes=512):\n",
        "        super(ViTEncoder, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            ViTBlock(embed_dim, num_heads) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed[:, :x.size(1)]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "class SpectrogramEncoder(nn.Module):\n",
        "    def __init__(self, use_xception=True, feature_dim=512):\n",
        "        super(SpectrogramEncoder, self).__init__()\n",
        "        self.use_xception = use_xception\n",
        "        if use_xception:\n",
        "            self.xception = XceptionNet(input_channels=1, num_classes=feature_dim)\n",
        "        else:\n",
        "            self.vit = ViTEncoder(img_size=224, patch_size=16, embed_dim=768, depth=6, num_heads=8, num_classes=feature_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_xception:\n",
        "            return self.xception(x)\n",
        "        else:\n",
        "            return self.vit(x)\n",
        "\n",
        "class AcousticFeatureEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, output_dim=512):\n",
        "        super(AcousticFeatureEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class LinguisticEncoder(nn.Module):\n",
        "    def __init__(self, bert_model_name='bert-base-uncased', output_dim=512, max_length=512):\n",
        "        super(LinguisticEncoder, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.max_length = max_length\n",
        "        self.projection = nn.Linear(self.bert.config.hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.projection(pooled_output)\n",
        "\n",
        "class GraphAttention(nn.Module):\n",
        "    def __init__(self, feature_dim=512, num_heads=8, dropout=0.1):\n",
        "        super(GraphAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = feature_dim // num_heads\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "        self.query = nn.Linear(feature_dim, feature_dim)\n",
        "        self.key = nn.Linear(feature_dim, feature_dim)\n",
        "        self.value = nn.Linear(feature_dim, feature_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(feature_dim, feature_dim)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        B, N, D = nodes.shape\n",
        "\n",
        "        q = self.query(nodes).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.key(nodes).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(nodes).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) / self.scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).contiguous().view(B, N, D)\n",
        "        return self.proj(out)\n",
        "\n",
        "class MultiModalFusionModule(nn.Module):\n",
        "    def __init__(self, feature_dim=512, num_layers=3):\n",
        "        super(MultiModalFusionModule, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            GraphAttention(feature_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm_layers = nn.ModuleList([\n",
        "            nn.LayerNorm(feature_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = torch.stack(features, dim=1)\n",
        "\n",
        "        for layer, norm in zip(self.layers, self.norm_layers):\n",
        "            residual = x\n",
        "            x = layer(x)\n",
        "            x = norm(x + residual)\n",
        "\n",
        "        return x.mean(dim=1)\n",
        "\n",
        "class MultiModalSpeechModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 acoustic_input_dim=50,\n",
        "                 bert_model_name='bert-base-uncased',\n",
        "                 feature_dim=512,\n",
        "                 num_classes_diagnosis=2,\n",
        "                 num_classes_progression=2,\n",
        "                 use_xception_for_spec=True):\n",
        "        super(MultiModalSpeechModel, self).__init__()\n",
        "\n",
        "        self.spectrogram_encoder = SpectrogramEncoder(use_xception=use_xception_for_spec, feature_dim=feature_dim)\n",
        "        self.acoustic_encoder = AcousticFeatureEncoder(acoustic_input_dim, output_dim=feature_dim)\n",
        "        self.linguistic_encoder = LinguisticEncoder(bert_model_name, output_dim=feature_dim)\n",
        "\n",
        "        self.fusion_module = MultiModalFusionModule(feature_dim)\n",
        "\n",
        "        self.diagnosis_classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes_diagnosis)\n",
        "        )\n",
        "\n",
        "        self.progression_classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes_progression)\n",
        "        )\n",
        "\n",
        "    def forward(self, spectrogram=None, acoustic_features=None, input_ids=None, attention_mask=None, task='diagnosis'):\n",
        "        features = []\n",
        "\n",
        "        if spectrogram is not None:\n",
        "            spec_features = self.spectrogram_encoder(spectrogram)\n",
        "            features.append(spec_features)\n",
        "\n",
        "        if acoustic_features is not None:\n",
        "            acoustic_feat = self.acoustic_encoder(acoustic_features)\n",
        "            features.append(acoustic_feat)\n",
        "\n",
        "        if input_ids is not None and attention_mask is not None:\n",
        "            linguistic_feat = self.linguistic_encoder(input_ids, attention_mask)\n",
        "            features.append(linguistic_feat)\n",
        "\n",
        "        if len(features) == 0:\n",
        "            raise ValueError(\"At least one modality must be provided\")\n",
        "\n",
        "        if len(features) == 1:\n",
        "            fused_features = features[0]\n",
        "        else:\n",
        "            fused_features = self.fusion_module(features)\n",
        "\n",
        "        if task == 'diagnosis':\n",
        "            return self.diagnosis_classifier(fused_features)\n",
        "        elif task == 'progression':\n",
        "            return self.progression_classifier(fused_features)\n",
        "        else:\n",
        "            return {\n",
        "                'diagnosis': self.diagnosis_classifier(fused_features),\n",
        "                'progression': self.progression_classifier(fused_features)\n",
        "            }\n",
        "\n",
        "class MultiModalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, spectrograms, acoustic_features, texts, labels, tokenizer, max_length=512):\n",
        "        self.spectrograms = spectrograms\n",
        "        self.acoustic_features = acoustic_features\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {}\n",
        "\n",
        "        if self.spectrograms is not None and idx < len(self.spectrograms):\n",
        "            spec = self.spectrograms[idx]\n",
        "            if isinstance(spec, np.ndarray):\n",
        "                spec = torch.FloatTensor(spec)\n",
        "            if len(spec.shape) == 2:\n",
        "                spec = spec.unsqueeze(0)\n",
        "            item['spectrogram'] = spec\n",
        "\n",
        "        if self.acoustic_features is not None and idx < len(self.acoustic_features):\n",
        "            acoustic = self.acoustic_features[idx]\n",
        "            if isinstance(acoustic, np.ndarray):\n",
        "                acoustic = torch.FloatTensor(acoustic)\n",
        "            item['acoustic_features'] = acoustic\n",
        "\n",
        "        if self.texts is not None and idx < len(self.texts):\n",
        "            text = str(self.texts[idx])\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            item['input_ids'] = encoding['input_ids'].squeeze()\n",
        "            item['attention_mask'] = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        item['label'] = torch.LongTensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "def create_model_and_dataloaders(train_data, val_data=None, batch_size=16):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    train_dataset = MultiModalDataset(\n",
        "        spectrograms=train_data.get('spectrograms'),\n",
        "        acoustic_features=train_data.get('acoustic_features'),\n",
        "        texts=train_data.get('texts'),\n",
        "        labels=train_data['labels'],\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = None\n",
        "    if val_data is not None:\n",
        "        val_dataset = MultiModalDataset(\n",
        "            spectrograms=val_data.get('spectrograms'),\n",
        "            acoustic_features=val_data.get('acoustic_features'),\n",
        "            texts=val_data.get('texts'),\n",
        "            labels=val_data['labels'],\n",
        "            tokenizer=tokenizer\n",
        "        )\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn\n",
        "        )\n",
        "\n",
        "    # Fix: Handle the array checking properly\n",
        "    acoustic_features = train_data.get('acoustic_features')\n",
        "    if acoustic_features is not None and len(acoustic_features) > 0:\n",
        "        acoustic_dim = len(acoustic_features[0])\n",
        "    else:\n",
        "        acoustic_dim = 50\n",
        "\n",
        "    model = MultiModalSpeechModel(acoustic_input_dim=acoustic_dim)\n",
        "\n",
        "    return model, train_loader, val_loader, tokenizer\n",
        "\n",
        "def train_model(model, train_loader, val_loader=None, epochs=10, learning_rate=1e-4, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = {}\n",
        "            for key in ['spectrogram', 'acoustic_features', 'input_ids', 'attention_mask']:\n",
        "                if key in batch:\n",
        "                    inputs[key] = batch[key].to(device)\n",
        "\n",
        "            labels = batch['label'].squeeze().to(device)\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    inputs = {}\n",
        "                    for key in ['spectrogram', 'acoustic_features', 'input_ids', 'attention_mask']:\n",
        "                        if key in batch:\n",
        "                            inputs[key] = batch[key].to(device)\n",
        "\n",
        "                    labels = batch['label'].squeeze().to(device)\n",
        "                    outputs = model(**inputs)\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_accuracy = 100 * val_correct / val_total\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Kld1ytTWj3UT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import BertTokenizer\n",
        "import os\n",
        "\n",
        "def collate_fn(batch):\n",
        "    collated = {}\n",
        "    for key in batch[0].keys():\n",
        "        if key in ['spectrogram', 'acoustic_features', 'input_ids', 'attention_mask']:\n",
        "            try:\n",
        "                collated[key] = torch.stack([item[key] for item in batch if key in item])\n",
        "            except:\n",
        "                tensors = []\n",
        "                for item in batch:\n",
        "                    if key in item:\n",
        "                        tensor = item[key]\n",
        "                        if len(tensor.shape) == 1:\n",
        "                            tensor = tensor.unsqueeze(0)\n",
        "                        tensors.append(tensor)\n",
        "                if tensors:\n",
        "                    collated[key] = torch.cat(tensors, dim=0)\n",
        "        elif key == 'label':\n",
        "            collated[key] = torch.cat([item[key] for item in batch])\n",
        "    return collated\n",
        "\n",
        "def load_processed_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "\n",
        "    diagnosis_train = pd.read_pickle(os.path.join(data_path, 'diagnosis_train.pkl'))\n",
        "    progression_train = pd.read_pickle(os.path.join(data_path, 'progression_train.pkl'))\n",
        "    progression_test = pd.read_pickle(os.path.join(data_path, 'progression_test.pkl'))\n",
        "\n",
        "    with open(os.path.join(data_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "        diagnosis_le = pickle.load(f)\n",
        "\n",
        "    with open(os.path.join(data_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "        progression_le = pickle.load(f)\n",
        "\n",
        "    return diagnosis_train, progression_train, progression_test, diagnosis_le, progression_le\n",
        "\n",
        "def load_lightweight_features():\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "\n",
        "    features = {}\n",
        "    feature_files = [\n",
        "        'diagnosis_ad_features.pkl',\n",
        "        'diagnosis_cn_features.pkl',\n",
        "        'progression_decline_features.pkl',\n",
        "        'progression_no_decline_features.pkl',\n",
        "        'progression_test_features.pkl'\n",
        "    ]\n",
        "\n",
        "    for file in feature_files:\n",
        "        file_path = os.path.join(features_path, file)\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                features.update(data)\n",
        "\n",
        "    return features\n",
        "\n",
        "def prepare_data_for_training(df, features_dict, tokenizer, task='diagnosis'):\n",
        "    valid_indices = []\n",
        "    spectrograms = []\n",
        "    acoustic_features_list = []\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            if 'mel_spectrogram' in feature_data:\n",
        "                mel_spec = feature_data['mel_spectrogram']\n",
        "                if mel_spec.shape[1] < 224:\n",
        "                    pad_width = 224 - mel_spec.shape[1]\n",
        "                    mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')\n",
        "                else:\n",
        "                    mel_spec = mel_spec[:, :224]\n",
        "\n",
        "                if mel_spec.shape[0] < 224:\n",
        "                    pad_height = 224 - mel_spec.shape[0]\n",
        "                    mel_spec = np.pad(mel_spec, ((0, pad_height), (0, 0)), mode='constant')\n",
        "                else:\n",
        "                    mel_spec = mel_spec[:224, :]\n",
        "\n",
        "                spectrograms.append(mel_spec)\n",
        "            else:\n",
        "                spectrograms.append(np.zeros((224, 224)))\n",
        "\n",
        "            acoustic_feat = []\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    acoustic_feat.extend(feature_data[key])\n",
        "\n",
        "            scalar_features = []\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        scalar_features.extend(val if len(val) > 0 else [0])\n",
        "                    else:\n",
        "                        scalar_features.append(val)\n",
        "\n",
        "            acoustic_feat.extend(scalar_features)\n",
        "\n",
        "            while len(acoustic_feat) < 50:\n",
        "                acoustic_feat.append(0)\n",
        "            acoustic_feat = acoustic_feat[:50]\n",
        "\n",
        "            acoustic_features_list.append(acoustic_feat)\n",
        "            texts.append(row.get('transcript', ''))\n",
        "            labels.append(row['label_encoded'])\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    return {\n",
        "        'spectrograms': np.array(spectrograms),\n",
        "        'acoustic_features': np.array(acoustic_features_list),\n",
        "        'texts': texts,\n",
        "        'labels': np.array(labels)\n",
        "    }\n",
        "\n",
        "def evaluate_model(model, test_loader, device='cuda', task='diagnosis'):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = {}\n",
        "            for key in ['spectrogram', 'acoustic_features', 'input_ids', 'attention_mask']:\n",
        "                if key in batch:\n",
        "                    inputs[key] = batch[key].to(device)\n",
        "\n",
        "            labels = batch['label'].squeeze().to(device)\n",
        "            outputs = model(**inputs, task=task)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    return accuracy, all_predictions, all_labels\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    diagnosis_train, progression_train, progression_test, diagnosis_le, progression_le = load_processed_data()\n",
        "\n",
        "    features_dict = load_lightweight_features()\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    diagnosis_data = prepare_data_for_training(diagnosis_train, features_dict, tokenizer, 'diagnosis')\n",
        "    progression_data = prepare_data_for_training(progression_train, features_dict, tokenizer, 'progression')\n",
        "\n",
        "    X_diag_train, X_diag_val, y_diag_train, y_diag_val = train_test_split(\n",
        "        range(len(diagnosis_data['labels'])), diagnosis_data['labels'],\n",
        "        test_size=0.2, random_state=42, stratify=diagnosis_data['labels']\n",
        "    )\n",
        "\n",
        "    diag_train_data = {\n",
        "        'spectrograms': diagnosis_data['spectrograms'][X_diag_train],\n",
        "        'acoustic_features': diagnosis_data['acoustic_features'][X_diag_train],\n",
        "        'texts': [diagnosis_data['texts'][i] for i in X_diag_train],\n",
        "        'labels': y_diag_train\n",
        "    }\n",
        "\n",
        "    diag_val_data = {\n",
        "        'spectrograms': diagnosis_data['spectrograms'][X_diag_val],\n",
        "        'acoustic_features': diagnosis_data['acoustic_features'][X_diag_val],\n",
        "        'texts': [diagnosis_data['texts'][i] for i in X_diag_val],\n",
        "        'labels': y_diag_val\n",
        "    }\n",
        "\n",
        "    model_diag, train_loader_diag, val_loader_diag, _ = create_model_and_dataloaders(diag_train_data, diag_val_data)\n",
        "\n",
        "    print(\"Training diagnosis model...\")\n",
        "    model_diag = train_model(model_diag, train_loader_diag, val_loader_diag, epochs=10, device=device)\n",
        "\n",
        "    print(\"Evaluating diagnosis model...\")\n",
        "    diag_accuracy, diag_preds, diag_labels = evaluate_model(model_diag, val_loader_diag, device, 'diagnosis')\n",
        "    print(f\"Diagnosis Accuracy: {diag_accuracy:.4f}\")\n",
        "    print(\"Diagnosis Classification Report:\")\n",
        "    print(classification_report(diag_labels, diag_preds, target_names=diagnosis_le.classes_))\n",
        "\n",
        "    X_prog_train, X_prog_val, y_prog_train, y_prog_val = train_test_split(\n",
        "        range(len(progression_data['labels'])), progression_data['labels'],\n",
        "        test_size=0.2, random_state=42, stratify=progression_data['labels']\n",
        "    )\n",
        "\n",
        "    prog_train_data = {\n",
        "        'spectrograms': progression_data['spectrograms'][X_prog_train],\n",
        "        'acoustic_features': progression_data['acoustic_features'][X_prog_train],\n",
        "        'texts': [progression_data['texts'][i] for i in X_prog_train],\n",
        "        'labels': y_prog_train\n",
        "    }\n",
        "\n",
        "    prog_val_data = {\n",
        "        'spectrograms': progression_data['spectrograms'][X_prog_val],\n",
        "        'acoustic_features': progression_data['acoustic_features'][X_prog_val],\n",
        "        'texts': [progression_data['texts'][i] for i in X_prog_val],\n",
        "        'labels': y_prog_val\n",
        "    }\n",
        "\n",
        "    model_prog, train_loader_prog, val_loader_prog, _ = create_model_and_dataloaders(prog_train_data, prog_val_data)\n",
        "\n",
        "    print(\"Training progression model...\")\n",
        "    model_prog = train_model(model_prog, train_loader_prog, val_loader_prog, epochs=10, device=device)\n",
        "\n",
        "    print(\"Evaluating progression model...\")\n",
        "    prog_accuracy, prog_preds, prog_labels = evaluate_model(model_prog, val_loader_prog, device, 'progression')\n",
        "    print(f\"Progression Accuracy: {prog_accuracy:.4f}\")\n",
        "    print(\"Progression Classification Report:\")\n",
        "    print(classification_report(prog_labels, prog_preds, target_names=progression_le.classes_))\n",
        "\n",
        "    torch.save(model_diag.state_dict(), '/content/drive/MyDrive/Speech/diagnosis_model.pth')\n",
        "    torch.save(model_prog.state_dict(), '/content/drive/MyDrive/Speech/progression_model.pth')\n",
        "\n",
        "    return model_diag, model_prog\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "57b5cbab98584eb296c39adc9e1eed29",
            "d85101a6b169484889339689363f21f5",
            "f8920c3389bb4d6cbfe71169734da3b3",
            "3136a350a6104b0eaa18613b98bc020c",
            "22fc0381c8744556b97f157fbacd7c61",
            "0fc8d7d8db1f46e4b26d4096840c4b61",
            "f0ca80575c674693a35b2d7c48e8e51d",
            "bb4eb2414540408195415362074001b3",
            "06af4be60ddb4edab8570ddf739a7952",
            "31348ee2121047bab63b0263a4b97266",
            "5a226f8587814f15aaf168cda773a7fb",
            "19897ba1ecce430e83340f51ac8a7b1a",
            "ad579952aaf94227879bf1d974f17d26",
            "27dff0e6f9cb47c6ac1fe9e04b0574f5",
            "2f28a9add5ce42bda93a589f018423e0",
            "497bf48c35de48f7b08ef4ca6dfc4352",
            "4972a13aacff4f159335e5da1a791037",
            "70d9a4a0aa1d4ffc9d4a230ce971e8bf",
            "e0f31b24847f4b2cb4c6e960ba42a71e",
            "1dc1105349ac43bda0e8797cd194de42",
            "76143b64a0e745dda82e7c3632843e40",
            "87df911fb9704615b1f002615d2e13f3",
            "49a0c0f8e32341efa72b59cb8bf44a60",
            "b5e1a39b5b084d0c92c60222800258d6",
            "f41351d2c955425387d4e73d89a1342d",
            "b19889a25d994bed8b39e7ea7c9b4193",
            "507392ec476446d6b5620e41593b716e",
            "844d77655ffd403baf3d1c7c32d5918f",
            "1a1db62d36434592aae538a7004a0f3e",
            "eb154f03eb9242b180dd24acfc0ed159",
            "11f57eeac14e440099934f5ff6543580",
            "83a7ac9d8c674d2ab783a0df8df04992",
            "ba73b004971a4edfa95d93bffde5a93c",
            "ebcf8b5449a6431c9b14123207b42e00",
            "605a9fdd3e0f4b3c9c8a479295ead1a3",
            "eb26636ee5c4457bbb65621a1ca47185",
            "ec0a2434f3c74f7f9eccb5e9b17d3cc6",
            "09fd41c49e0346c4ab6062299d8e0926",
            "6b6661a9218c4e039b3dca5508cd1ff0",
            "26725ea16f8d4a728f290ca2fbf57d20",
            "f2e10b90bcd34695bc9c9476e52df202",
            "de2dcc0b61e94806864b1524e1ddb61c",
            "0ef0a246aade4576a7758103af49da7e",
            "39577aa917e44accb8d61423c93c86c2",
            "51eef6650bd849d0a5f6993c2a0f8c17",
            "61fa0eb558bb4c129282734e4ba15f9a",
            "344f515820c74585ad922f7006024ee9",
            "af4c949cd1974344bf1a4e8cd18407ed",
            "eb0117c581de44eb9dacb90e6d78a31c",
            "18fb8751b1274becbd6280d754ef61f2",
            "662f82b77c0246d6b30c3ba8af4f7682",
            "0002e71ecd9146d390fb03302c72c9b2",
            "8a5815a5f1c54a5db81a6926bc9ca9b7",
            "3f9089c91ec14169a155bea28aa5fbf0",
            "b288b40349454b29a1ff2b05245ebc20"
          ]
        },
        "id": "Vxy3J9WHC9sK",
        "outputId": "b348fa88-0c5f-4cd3-be93-d53614dd1326"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57b5cbab98584eb296c39adc9e1eed29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19897ba1ecce430e83340f51ac8a7b1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49a0c0f8e32341efa72b59cb8bf44a60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebcf8b5449a6431c9b14123207b42e00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51eef6650bd849d0a5f6993c2a0f8c17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training diagnosis model...\n",
            "Epoch [1/10], Loss: 0.7065, Accuracy: 49.24%\n",
            "Validation Loss: 0.6364, Validation Accuracy: 52.94%\n",
            "Epoch [2/10], Loss: 0.7136, Accuracy: 53.03%\n",
            "Validation Loss: 0.6730, Validation Accuracy: 61.76%\n",
            "Epoch [3/10], Loss: 0.7252, Accuracy: 50.00%\n",
            "Validation Loss: 0.9617, Validation Accuracy: 52.94%\n",
            "Epoch [4/10], Loss: 0.7539, Accuracy: 62.12%\n",
            "Validation Loss: 0.6361, Validation Accuracy: 52.94%\n",
            "Epoch [5/10], Loss: 0.7288, Accuracy: 48.48%\n",
            "Validation Loss: 0.6411, Validation Accuracy: 52.94%\n",
            "Epoch [6/10], Loss: 0.7237, Accuracy: 50.76%\n",
            "Validation Loss: 0.6644, Validation Accuracy: 52.94%\n",
            "Epoch [7/10], Loss: 0.7138, Accuracy: 50.76%\n",
            "Validation Loss: 0.6643, Validation Accuracy: 52.94%\n",
            "Epoch [8/10], Loss: 0.6756, Accuracy: 57.58%\n",
            "Validation Loss: 0.6729, Validation Accuracy: 52.94%\n",
            "Epoch [9/10], Loss: 0.6911, Accuracy: 53.79%\n",
            "Validation Loss: 0.6802, Validation Accuracy: 52.94%\n",
            "Epoch [10/10], Loss: 0.7114, Accuracy: 53.79%\n",
            "Validation Loss: 0.6961, Validation Accuracy: 47.06%\n",
            "Evaluating diagnosis model...\n",
            "Diagnosis Accuracy: 0.4706\n",
            "Diagnosis Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ad       0.00      0.00      0.00        18\n",
            "          cn       0.47      1.00      0.64        16\n",
            "\n",
            "    accuracy                           0.47        34\n",
            "   macro avg       0.24      0.50      0.32        34\n",
            "weighted avg       0.22      0.47      0.30        34\n",
            "\n",
            "Training progression model...\n",
            "Epoch [1/10], Loss: 0.6207, Accuracy: 58.62%\n",
            "Validation Loss: 0.5030, Validation Accuracy: 80.00%\n",
            "Epoch [2/10], Loss: 0.5159, Accuracy: 79.31%\n",
            "Validation Loss: 0.7532, Validation Accuracy: 20.00%\n",
            "Epoch [3/10], Loss: 0.6010, Accuracy: 79.31%\n",
            "Validation Loss: 0.4998, Validation Accuracy: 80.00%\n",
            "Epoch [4/10], Loss: 0.5403, Accuracy: 79.31%\n",
            "Validation Loss: 0.5006, Validation Accuracy: 80.00%\n",
            "Epoch [5/10], Loss: 0.5518, Accuracy: 79.31%\n",
            "Validation Loss: 0.5016, Validation Accuracy: 80.00%\n",
            "Epoch [6/10], Loss: 0.5708, Accuracy: 79.31%\n",
            "Validation Loss: 0.5042, Validation Accuracy: 80.00%\n",
            "Epoch [7/10], Loss: 0.4701, Accuracy: 79.31%\n",
            "Validation Loss: 0.5046, Validation Accuracy: 80.00%\n",
            "Epoch [8/10], Loss: 0.5091, Accuracy: 79.31%\n",
            "Validation Loss: 0.5026, Validation Accuracy: 80.00%\n",
            "Epoch [9/10], Loss: 0.4853, Accuracy: 79.31%\n",
            "Validation Loss: 0.5009, Validation Accuracy: 80.00%\n",
            "Epoch [10/10], Loss: 0.5058, Accuracy: 79.31%\n",
            "Validation Loss: 0.5006, Validation Accuracy: 80.00%\n",
            "Evaluating progression model...\n",
            "Progression Accuracy: 0.2000\n",
            "Progression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     decline       0.20      1.00      0.33         3\n",
            "  no_decline       0.00      0.00      0.00        12\n",
            "\n",
            "    accuracy                           0.20        15\n",
            "   macro avg       0.10      0.50      0.17        15\n",
            "weighted avg       0.04      0.20      0.07        15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SimpleMultiModalModel(nn.Module):\n",
        "    def __init__(self, acoustic_dim=50, text_dim=768, hidden_dim=512, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.acoustic_encoder = nn.Sequential(\n",
        "            nn.Linear(acoustic_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, acoustic_features=None, text_features=None):\n",
        "        features = []\n",
        "\n",
        "        if acoustic_features is not None:\n",
        "            acoustic_out = self.acoustic_encoder(acoustic_features)\n",
        "            features.append(acoustic_out)\n",
        "\n",
        "        if text_features is not None:\n",
        "            text_out = self.text_encoder(text_features)\n",
        "            features.append(text_out)\n",
        "\n",
        "        if len(features) == 0:\n",
        "            raise ValueError(\"At least one input required\")\n",
        "        elif len(features) == 1:\n",
        "            combined = features[0]\n",
        "        else:\n",
        "            combined = torch.cat(features, dim=1)\n",
        "\n",
        "        return self.fusion(combined)\n",
        "\n",
        "def load_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "\n",
        "    diagnosis_train = pd.read_pickle(os.path.join(data_path, 'diagnosis_train.pkl'))\n",
        "    progression_train = pd.read_pickle(os.path.join(data_path, 'progression_train.pkl'))\n",
        "\n",
        "    with open(os.path.join(data_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "        diagnosis_le = pickle.load(f)\n",
        "    with open(os.path.join(data_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "        progression_le = pickle.load(f)\n",
        "\n",
        "    features = {}\n",
        "    for file in os.listdir(features_path):\n",
        "        if file.endswith('.pkl') and 'features' in file:\n",
        "            with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                features.update(data)\n",
        "\n",
        "    return diagnosis_train, progression_train, diagnosis_le, progression_le, features\n",
        "\n",
        "def extract_bert_features(texts, model_name='bert-base-uncased', max_length=128):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            if pd.isna(text) or text == '':\n",
        "                embeddings.append(np.zeros(768))\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(str(text), return_tensors='pt', padding=True,\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "            embeddings.append(pooled)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def prepare_features(df, features_dict):\n",
        "    acoustic_features = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            acoustic_feat = []\n",
        "\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        acoustic_feat.extend(feat.flatten())\n",
        "\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        if len(val) > 0:\n",
        "                            acoustic_feat.append(np.mean(val))\n",
        "                        else:\n",
        "                            acoustic_feat.append(0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val)\n",
        "\n",
        "            if len(acoustic_feat) > 100:\n",
        "                acoustic_feat = acoustic_feat[:100]\n",
        "            while len(acoustic_feat) < 100:\n",
        "                acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    return np.array(acoustic_features), valid_indices\n",
        "\n",
        "def train_ensemble_models(X_train, y_train, X_val, y_val):\n",
        "    models = {\n",
        "        'rf': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42,\n",
        "                                   class_weight='balanced', min_samples_split=5),\n",
        "        'gb': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42,\n",
        "                                       learning_rate=0.1),\n",
        "        'svm': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True, random_state=42),\n",
        "        'lr': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    trained_models = {}\n",
        "    val_scores = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        val_pred = model.predict(X_val)\n",
        "        val_acc = accuracy_score(y_val, val_pred)\n",
        "        trained_models[name] = model\n",
        "        val_scores[name] = val_acc\n",
        "        print(f\"{name.upper()} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    return trained_models, val_scores\n",
        "\n",
        "def ensemble_predict(models, X_test, weights=None):\n",
        "    if weights is None:\n",
        "        weights = [1] * len(models)\n",
        "\n",
        "    predictions = []\n",
        "    for model in models.values():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            pred_proba = model.predict_proba(X_test)\n",
        "            predictions.append(pred_proba)\n",
        "        else:\n",
        "            pred = model.predict(X_test)\n",
        "            pred_onehot = np.eye(len(np.unique(pred)))[pred]\n",
        "            predictions.append(pred_onehot)\n",
        "\n",
        "    weighted_avg = np.average(predictions, axis=0, weights=weights)\n",
        "    final_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    return final_predictions\n",
        "\n",
        "def enhanced_feature_engineering(acoustic_features, text_embeddings):\n",
        "    scaler = StandardScaler()\n",
        "    acoustic_scaled = scaler.fit_transform(acoustic_features)\n",
        "\n",
        "    text_scaled = StandardScaler().fit_transform(text_embeddings)\n",
        "\n",
        "    acoustic_stats = np.column_stack([\n",
        "        np.mean(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.std(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.max(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.min(acoustic_scaled, axis=1, keepdims=True)\n",
        "    ])\n",
        "\n",
        "    text_stats = np.column_stack([\n",
        "        np.mean(text_scaled, axis=1, keepdims=True),\n",
        "        np.std(text_scaled, axis=1, keepdims=True),\n",
        "        np.max(text_scaled, axis=1, keepdims=True),\n",
        "        np.min(text_scaled, axis=1, keepdims=True)\n",
        "    ])\n",
        "\n",
        "    text_stats_expanded = np.repeat(text_stats, acoustic_scaled.shape[1], axis=1)\n",
        "    acoustic_stats_expanded = np.repeat(acoustic_stats, text_scaled.shape[1], axis=1)\n",
        "\n",
        "    combined_features = np.hstack([\n",
        "        acoustic_scaled,\n",
        "        text_scaled,\n",
        "        acoustic_stats,\n",
        "        text_stats\n",
        "    ])\n",
        "\n",
        "    return combined_features, scaler\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "\n",
        "    print(\"Processing diagnosis task...\")\n",
        "    diag_acoustic, diag_indices = prepare_features(diagnosis_train, features_dict)\n",
        "    diag_df_filtered = diagnosis_train.iloc[diag_indices].reset_index(drop=True)\n",
        "    diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "    diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for diagnosis...\")\n",
        "    diag_text_features = extract_bert_features(diag_texts)\n",
        "\n",
        "    print(\"Engineering features for diagnosis...\")\n",
        "    diag_enhanced_features, diag_scaler = enhanced_feature_engineering(diag_acoustic, diag_text_features)\n",
        "\n",
        "    X_diag_train, X_diag_val, y_diag_train, y_diag_val = train_test_split(\n",
        "        diag_enhanced_features, diag_labels, test_size=0.2, random_state=42, stratify=diag_labels\n",
        "    )\n",
        "\n",
        "    print(\"Training diagnosis ensemble...\")\n",
        "    diag_models, diag_scores = train_ensemble_models(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "\n",
        "    best_diag_models = {k: v for k, v in diag_models.items() if diag_scores[k] > 0.6}\n",
        "    if not best_diag_models:\n",
        "        best_diag_models = diag_models\n",
        "\n",
        "    diag_ensemble_pred = ensemble_predict(best_diag_models, X_diag_val)\n",
        "    diag_ensemble_acc = accuracy_score(y_diag_val, diag_ensemble_pred)\n",
        "\n",
        "    print(f\"\\nDiagnosis Ensemble Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(\"Diagnosis Classification Report:\")\n",
        "    print(classification_report(y_diag_val, diag_ensemble_pred, target_names=diagnosis_le.classes_))\n",
        "\n",
        "    print(\"\\nProcessing progression task...\")\n",
        "    prog_acoustic, prog_indices = prepare_features(progression_train, features_dict)\n",
        "    prog_df_filtered = progression_train.iloc[prog_indices].reset_index(drop=True)\n",
        "    prog_texts = prog_df_filtered['transcript'].fillna('').tolist()\n",
        "    prog_labels = prog_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for progression...\")\n",
        "    prog_text_features = extract_bert_features(prog_texts)\n",
        "\n",
        "    print(\"Engineering features for progression...\")\n",
        "    prog_enhanced_features, prog_scaler = enhanced_feature_engineering(prog_acoustic, prog_text_features)\n",
        "\n",
        "    from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
        "    from imblearn.combine import SMOTEENN\n",
        "\n",
        "    try:\n",
        "        smote_enn = SMOTEENN(random_state=42)\n",
        "        prog_enhanced_balanced, prog_labels_balanced = smote_enn.fit_resample(prog_enhanced_features, prog_labels)\n",
        "    except:\n",
        "        try:\n",
        "            adasyn = ADASYN(random_state=42, n_neighbors=2)\n",
        "            prog_enhanced_balanced, prog_labels_balanced = adasyn.fit_resample(prog_enhanced_features, prog_labels)\n",
        "        except:\n",
        "            prog_enhanced_balanced, prog_labels_balanced = prog_enhanced_features, prog_labels\n",
        "\n",
        "    X_prog_train, X_prog_val, y_prog_train, y_prog_val = train_test_split(\n",
        "        prog_enhanced_balanced, prog_labels_balanced, test_size=0.2, random_state=42, stratify=prog_labels_balanced\n",
        "    )\n",
        "\n",
        "    print(\"Training progression ensemble...\")\n",
        "    prog_models, prog_scores = train_ensemble_models(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "\n",
        "    best_prog_models = {k: v for k, v in prog_models.items() if prog_scores[k] > 0.6}\n",
        "    if not best_prog_models:\n",
        "        best_prog_models = prog_models\n",
        "\n",
        "    prog_ensemble_pred = ensemble_predict(best_prog_models, X_prog_val)\n",
        "    prog_ensemble_acc = accuracy_score(y_prog_val, prog_ensemble_pred)\n",
        "\n",
        "    print(f\"\\nProgression Ensemble Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "    print(\"Progression Classification Report:\")\n",
        "    print(classification_report(y_prog_val, prog_ensemble_pred, target_names=progression_le.classes_))\n",
        "\n",
        "    if diag_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for diagnosis...\")\n",
        "        from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=min(200, X_diag_train.shape[1]//2))\n",
        "        X_diag_train_selected = selector.fit_transform(X_diag_train, y_diag_train)\n",
        "        X_diag_val_selected = selector.transform(X_diag_val)\n",
        "\n",
        "        diag_models_v2, diag_scores_v2 = train_ensemble_models(X_diag_train_selected, y_diag_train, X_diag_val_selected, y_diag_val)\n",
        "        diag_ensemble_pred_v2 = ensemble_predict(diag_models_v2, X_diag_val_selected)\n",
        "        diag_ensemble_acc_v2 = accuracy_score(y_diag_val, diag_ensemble_pred_v2)\n",
        "\n",
        "        print(f\"Diagnosis Improved Accuracy: {diag_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "        if diag_ensemble_acc_v2 > diag_ensemble_acc:\n",
        "            diag_ensemble_acc = diag_ensemble_acc_v2\n",
        "            diag_ensemble_pred = diag_ensemble_pred_v2\n",
        "\n",
        "    if prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for progression...\")\n",
        "        from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=min(150, X_prog_train.shape[1]//2))\n",
        "        X_prog_train_selected = selector.fit_transform(X_prog_train, y_prog_train)\n",
        "        X_prog_val_selected = selector.transform(X_prog_val)\n",
        "\n",
        "        prog_models_v2, prog_scores_v2 = train_ensemble_models(X_prog_train_selected, y_prog_train, X_prog_val_selected, y_prog_val)\n",
        "        prog_ensemble_pred_v2 = ensemble_predict(prog_models_v2, X_prog_val_selected)\n",
        "        prog_ensemble_acc_v2 = accuracy_score(y_prog_val, prog_ensemble_pred_v2)\n",
        "\n",
        "        print(f\"Progression Improved Accuracy: {prog_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "        if prog_ensemble_acc_v2 > prog_ensemble_acc:\n",
        "            prog_ensemble_acc = prog_ensemble_acc_v2\n",
        "            prog_ensemble_pred = prog_ensemble_pred_v2\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Diagnosis Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(f\"Progression Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying synthetic data generation...\")\n",
        "\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "        def generate_synthetic_samples(X, y, target_samples=200):\n",
        "            synthetic_X = []\n",
        "            synthetic_y = []\n",
        "\n",
        "            for class_label in np.unique(y):\n",
        "                class_samples = X[y == class_label]\n",
        "\n",
        "                if len(class_samples) < 5:\n",
        "                    continue\n",
        "\n",
        "                nn_model = NearestNeighbors(n_neighbors=min(5, len(class_samples)))\n",
        "                nn_model.fit(class_samples)\n",
        "\n",
        "                for _ in range(target_samples // len(np.unique(y))):\n",
        "                    base_idx = np.random.randint(0, len(class_samples))\n",
        "                    base_sample = class_samples[base_idx]\n",
        "\n",
        "                    distances, indices = nn_model.kneighbors([base_sample])\n",
        "                    neighbor_idx = np.random.choice(indices[0])\n",
        "                    neighbor_sample = class_samples[neighbor_idx]\n",
        "\n",
        "                    alpha = np.random.uniform(0.2, 0.8)\n",
        "                    synthetic_sample = alpha * base_sample + (1 - alpha) * neighbor_sample\n",
        "\n",
        "                    noise = np.random.normal(0, 0.01, synthetic_sample.shape)\n",
        "                    synthetic_sample += noise\n",
        "\n",
        "                    synthetic_X.append(synthetic_sample)\n",
        "                    synthetic_y.append(class_label)\n",
        "\n",
        "            return np.array(synthetic_X), np.array(synthetic_y)\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            synth_X_diag, synth_y_diag = generate_synthetic_samples(X_diag_train, y_diag_train)\n",
        "            X_diag_augmented = np.vstack([X_diag_train, synth_X_diag])\n",
        "            y_diag_augmented = np.hstack([y_diag_train, synth_y_diag])\n",
        "\n",
        "            diag_models_synth, _ = train_ensemble_models(X_diag_augmented, y_diag_augmented, X_diag_val, y_diag_val)\n",
        "            diag_synth_pred = ensemble_predict(diag_models_synth, X_diag_val)\n",
        "            diag_synth_acc = accuracy_score(y_diag_val, diag_synth_pred)\n",
        "\n",
        "            print(f\"Diagnosis with Synthetic Data: {diag_synth_acc:.4f}\")\n",
        "\n",
        "            if diag_synth_acc > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_synth_acc\n",
        "                diag_ensemble_pred = diag_synth_pred\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            synth_X_prog, synth_y_prog = generate_synthetic_samples(X_prog_train, y_prog_train)\n",
        "            X_prog_augmented = np.vstack([X_prog_train, synth_X_prog])\n",
        "            y_prog_augmented = np.hstack([y_prog_train, synth_y_prog])\n",
        "\n",
        "            prog_models_synth, _ = train_ensemble_models(X_prog_augmented, y_prog_augmented, X_prog_val, y_prog_val)\n",
        "            prog_synth_pred = ensemble_predict(prog_models_synth, X_prog_val)\n",
        "            prog_synth_acc = accuracy_score(y_prog_val, prog_synth_pred)\n",
        "\n",
        "            print(f\"Progression with Synthetic Data: {prog_synth_acc:.4f}\")\n",
        "\n",
        "            if prog_synth_acc > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_synth_acc\n",
        "                prog_ensemble_pred = prog_synth_pred\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying neural network with heavy augmentation...\")\n",
        "\n",
        "        class HeavyAugmentationModel(nn.Module):\n",
        "            def __init__(self, input_dim, num_classes=2):\n",
        "                super().__init__()\n",
        "                self.layers = nn.Sequential(\n",
        "                    nn.Linear(input_dim, 1024),\n",
        "                    nn.BatchNorm1d(1024),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.5),\n",
        "\n",
        "                    nn.Linear(1024, 512),\n",
        "                    nn.BatchNorm1d(512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.4),\n",
        "\n",
        "                    nn.Linear(512, 256),\n",
        "                    nn.BatchNorm1d(256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3),\n",
        "\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.BatchNorm1d(128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.2),\n",
        "\n",
        "                    nn.Linear(128, num_classes)\n",
        "                )\n",
        "\n",
        "            def forward(self, x):\n",
        "                return self.layers(x)\n",
        "\n",
        "        def train_heavy_model(X_train, y_train, X_val, y_val, epochs=50):\n",
        "            model = HeavyAugmentationModel(X_train.shape[1]).to(device)\n",
        "\n",
        "            class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "            criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "            X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "            y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "            X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "            y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "            best_acc = 0\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "\n",
        "                batch_size = 32\n",
        "                num_batches = len(X_train_tensor) // batch_size + 1\n",
        "\n",
        "                total_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for i in range(num_batches):\n",
        "                    start_idx = i * batch_size\n",
        "                    end_idx = min((i + 1) * batch_size, len(X_train_tensor))\n",
        "\n",
        "                    if start_idx >= end_idx:\n",
        "                        break\n",
        "\n",
        "                    batch_X = X_train_tensor[start_idx:end_idx]\n",
        "                    batch_y = y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "                    if len(batch_X) == 0:\n",
        "                        continue\n",
        "\n",
        "                    noise = torch.randn_like(batch_X) * 0.01\n",
        "                    batch_X = batch_X + noise\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += batch_y.size(0)\n",
        "                    correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "                train_acc = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_val_tensor)\n",
        "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                    val_acc = 100 * (val_predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "            return model, best_acc / 100\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            print(\"Training heavy diagnosis model...\")\n",
        "            diag_heavy_model, diag_heavy_acc = train_heavy_model(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "            print(f\"Heavy Diagnosis Model Accuracy: {diag_heavy_acc:.4f}\")\n",
        "\n",
        "            if diag_heavy_acc > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_heavy_acc\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            print(\"Training heavy progression model...\")\n",
        "            prog_heavy_model, prog_heavy_acc = train_heavy_model(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "            print(f\"Heavy Progression Model Accuracy: {prog_heavy_acc:.4f}\")\n",
        "\n",
        "            if prog_heavy_acc > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_heavy_acc\n",
        "\n",
        "    print(f\"\\n=== FINAL RESULTS ===\")\n",
        "    print(f\"Diagnosis Task Accuracy: {diag_ensemble_acc:.4f} ({diag_ensemble_acc*100:.1f}%)\")\n",
        "    print(f\"Progression Task Accuracy: {prog_ensemble_acc:.4f} ({prog_ensemble_acc*100:.1f}%)\")\n",
        "\n",
        "    if diag_ensemble_acc >= 0.85 and prog_ensemble_acc >= 0.85:\n",
        "        print(\"SUCCESS: Both models achieved 85%+ accuracy!\")\n",
        "    else:\n",
        "        print(\"Still optimizing...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967,
          "referenced_widgets": [
            "63b1467759674244989689c5f7aabf1d",
            "7f569ebf03114df0ab1718dd40847640",
            "1470172f33c24cbdb6979491f7ea4848",
            "3fece809e6bb4b5aa36b4524fcc660f7",
            "43df1af5d5e14094aeba1ab19e52f0e7",
            "b382bc9c319c45e48092ebf34d61fcdc",
            "44e3eb3e5c0e4fb38221064956e6af4f",
            "2315bab55bfa498aaef28a18b4d4a7b3",
            "d43ce162bc8b439c8fec4b567c757b12",
            "db4e620c8b7246238a6a65c70c3c9729",
            "0e43b10d80b6439aa33a3fa7955c986c",
            "0ecd97d819364fe8bd43e8f64f1616f6",
            "b148787f9c814ade8e055318a153f7a8",
            "a2a79814588b45079ed7547c2eefd939",
            "2e9bdb3bc38b4c9babc20777dadc96a5",
            "da0e9d98c68f4211847af7d51e7bcc6b",
            "549172fe8c894251a0bf1147c0b51934",
            "c3ed16642e87457ab4c75cd3fdc36bf8",
            "24a05bf6d5634f479592dbf207578fac",
            "1f0198fdb4204b0196b973aac6b7bcac",
            "7474ee0861af4d90aa97e8c1ee6f654a",
            "3d33dfa2ee9e491eb81afcdddcd19586",
            "1bc341f0bd47423c95f60ef4c0755b95",
            "530d189a7ee24d01b21fe9bee075cdb6",
            "ac4c6f9a7f70419788d910763f41168c",
            "5f57c4113a444105900ecdff6476c907",
            "90a46ca7b2a64b1d880c45aa32087486",
            "a1acd32fbfde4e87854416f4a7f06c58",
            "1412636de53641d89f377302829189f2",
            "e17addb026eb45018aa4c90207a185e7",
            "0bfa6f21e6d046a58263037e744dbce5",
            "67dd4d1f5a8643539f2a04fbb9d7c54c",
            "be7f30f3da0748c08f29919456789731",
            "4cf052ec4b5346828d3262a5073d7f6b",
            "71c05aa265e147bc92ab357d78cb899a",
            "08933bf101b94f1b9f607971c8c3c365",
            "e5b607afd6aa46eab7eecc7f65c5ffc6",
            "0abe2f618f654f3eb384be036433ba1b",
            "98a3e6053aca4a34b5ab1dae6941edba",
            "6b1c9fc842b0476ea7fd62514aab283e",
            "6abe95a496eb4926a3bd0d6c38ee3052",
            "c1caf47e1b5c41e49d5d440819eec58d",
            "ac6f11f5390f4ea48d430c8b8d7d1c3e",
            "02c5bd3559474eeab8fea3a34c75688c",
            "4133d8e056b549eabfa931cc50bba906",
            "137be4b430fa4ca58287df9a5b007164",
            "524015b746234003b8bc80c28c2074fb",
            "dff51636451748a7acde8e45b26f208c",
            "6f6f20e28acd432caf289713886500d5",
            "db82d9725a0c45349f1bee5175f0ba06",
            "29aebfd7d8924b199891f24d252e06fb",
            "7611a04fbfb144bdbf8bec018d6a9652",
            "4edaee56644e4f19892ae6e5a351f9bc",
            "689071d0026b47dfa15be72ba94b938f",
            "ac0c45959e104859b6ddb1e2e8ad038a"
          ]
        },
        "id": "7EtjLLR_HtyD",
        "outputId": "9d47ec10-0cd2-4b63-df00-b1645f8be991"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Processing diagnosis task...\n",
            "Extracting BERT features for diagnosis...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63b1467759674244989689c5f7aabf1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ecd97d819364fe8bd43e8f64f1616f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bc341f0bd47423c95f60ef4c0755b95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cf052ec4b5346828d3262a5073d7f6b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4133d8e056b549eabfa931cc50bba906"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engineering features for diagnosis...\n",
            "Training diagnosis ensemble...\n",
            "RF Validation Accuracy: 0.6765\n",
            "GB Validation Accuracy: 0.6765\n",
            "SVM Validation Accuracy: 0.9412\n",
            "LR Validation Accuracy: 0.9706\n",
            "\n",
            "Diagnosis Ensemble Accuracy: 0.9118\n",
            "Diagnosis Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ad       0.89      0.94      0.92        18\n",
            "          cn       0.93      0.88      0.90        16\n",
            "\n",
            "    accuracy                           0.91        34\n",
            "   macro avg       0.91      0.91      0.91        34\n",
            "weighted avg       0.91      0.91      0.91        34\n",
            "\n",
            "\n",
            "Processing progression task...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "positional indexers are out-of-bounds",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \"\"\"\n\u001b[0;32m-> 4153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexers/utils.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-571873301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-571873301.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProcessing progression task...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mprog_acoustic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogression_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mprog_df_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogression_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprog_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mprog_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprog_df_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'transcript'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mprog_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprog_df_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_encoded'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0;31m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SimpleMultiModalModel(nn.Module):\n",
        "    def __init__(self, acoustic_dim=50, text_dim=768, hidden_dim=512, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.acoustic_encoder = nn.Sequential(\n",
        "            nn.Linear(acoustic_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, acoustic_features=None, text_features=None):\n",
        "        features = []\n",
        "\n",
        "        if acoustic_features is not None:\n",
        "            acoustic_out = self.acoustic_encoder(acoustic_features)\n",
        "            features.append(acoustic_out)\n",
        "\n",
        "        if text_features is not None:\n",
        "            text_out = self.text_encoder(text_features)\n",
        "            features.append(text_out)\n",
        "\n",
        "        if len(features) == 0:\n",
        "            raise ValueError(\"At least one input required\")\n",
        "        elif len(features) == 1:\n",
        "            combined = features[0]\n",
        "        else:\n",
        "            combined = torch.cat(features, dim=1)\n",
        "\n",
        "        return self.fusion(combined)\n",
        "\n",
        "def load_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "\n",
        "    diagnosis_train = pd.read_pickle(os.path.join(data_path, 'diagnosis_train.pkl'))\n",
        "    progression_train = pd.read_pickle(os.path.join(data_path, 'progression_train.pkl'))\n",
        "\n",
        "    with open(os.path.join(data_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "        diagnosis_le = pickle.load(f)\n",
        "    with open(os.path.join(data_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "        progression_le = pickle.load(f)\n",
        "\n",
        "    features = {}\n",
        "    for file in os.listdir(features_path):\n",
        "        if file.endswith('.pkl') and 'features' in file:\n",
        "            with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                features.update(data)\n",
        "\n",
        "    return diagnosis_train, progression_train, diagnosis_le, progression_le, features\n",
        "\n",
        "def extract_bert_features(texts, model_name='bert-base-uncased', max_length=128):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            if pd.isna(text) or text == '':\n",
        "                embeddings.append(np.zeros(768))\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(str(text), return_tensors='pt', padding=True,\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "            embeddings.append(pooled)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def prepare_features(df, features_dict):\n",
        "    acoustic_features = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            acoustic_feat = []\n",
        "\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        acoustic_feat.extend(feat.flatten())\n",
        "\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        if len(val) > 0:\n",
        "                            acoustic_feat.append(np.mean(val))\n",
        "                        else:\n",
        "                            acoustic_feat.append(0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val)\n",
        "\n",
        "            if len(acoustic_feat) > 100:\n",
        "                acoustic_feat = acoustic_feat[:100]\n",
        "            while len(acoustic_feat) < 100:\n",
        "                acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    return np.array(acoustic_features), valid_indices\n",
        "\n",
        "def train_ensemble_models(X_train, y_train, X_val, y_val):\n",
        "    models = {\n",
        "        'rf': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42,\n",
        "                                   class_weight='balanced', min_samples_split=5),\n",
        "        'gb': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42,\n",
        "                                       learning_rate=0.1),\n",
        "        'svm': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True, random_state=42),\n",
        "        'lr': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    trained_models = {}\n",
        "    val_scores = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        val_pred = model.predict(X_val)\n",
        "        val_acc = accuracy_score(y_val, val_pred)\n",
        "        trained_models[name] = model\n",
        "        val_scores[name] = val_acc\n",
        "        print(f\"{name.upper()} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    return trained_models, val_scores\n",
        "\n",
        "def ensemble_predict(models, X_test, weights=None):\n",
        "    if weights is None:\n",
        "        weights = [1] * len(models)\n",
        "\n",
        "    predictions = []\n",
        "    for model in models.values():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            pred_proba = model.predict_proba(X_test)\n",
        "            predictions.append(pred_proba)\n",
        "        else:\n",
        "            pred = model.predict(X_test)\n",
        "            pred_onehot = np.eye(len(np.unique(pred)))[pred]\n",
        "            predictions.append(pred_onehot)\n",
        "\n",
        "    weighted_avg = np.average(predictions, axis=0, weights=weights)\n",
        "    final_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    return final_predictions\n",
        "\n",
        "def enhanced_feature_engineering(acoustic_features, text_embeddings):\n",
        "    scaler = StandardScaler()\n",
        "    acoustic_scaled = scaler.fit_transform(acoustic_features)\n",
        "\n",
        "    text_scaled = StandardScaler().fit_transform(text_embeddings)\n",
        "\n",
        "    acoustic_stats = np.column_stack([\n",
        "        np.mean(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.std(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.max(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.min(acoustic_scaled, axis=1, keepdims=True)\n",
        "    ])\n",
        "\n",
        "    text_stats = np.column_stack([\n",
        "        np.mean(text_scaled, axis=1, keepdims=True),\n",
        "        np.std(text_scaled, axis=1, keepdims=True),\n",
        "        np.max(text_scaled, axis=1, keepdims=True),\n",
        "        np.min(text_scaled, axis=1, keepdims=True)\n",
        "    ])\n",
        "\n",
        "    text_stats_expanded = np.repeat(text_stats, acoustic_scaled.shape[1], axis=1)\n",
        "    acoustic_stats_expanded = np.repeat(acoustic_stats, text_scaled.shape[1], axis=1)\n",
        "\n",
        "    combined_features = np.hstack([\n",
        "        acoustic_scaled,\n",
        "        text_scaled,\n",
        "        acoustic_stats,\n",
        "        text_stats\n",
        "    ])\n",
        "\n",
        "    return combined_features, scaler\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "\n",
        "    print(\"Processing diagnosis task...\")\n",
        "    diag_acoustic, diag_indices = prepare_features(diagnosis_train, features_dict)\n",
        "    diag_df_filtered = diagnosis_train.iloc[diag_indices].reset_index(drop=True)\n",
        "    diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "    diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for diagnosis...\")\n",
        "    diag_text_features = extract_bert_features(diag_texts)\n",
        "\n",
        "    print(\"Engineering features for diagnosis...\")\n",
        "    diag_enhanced_features, diag_scaler = enhanced_feature_engineering(diag_acoustic, diag_text_features)\n",
        "\n",
        "    X_diag_train, X_diag_val, y_diag_train, y_diag_val = train_test_split(\n",
        "        diag_enhanced_features, diag_labels, test_size=0.2, random_state=42, stratify=diag_labels\n",
        "    )\n",
        "\n",
        "    print(\"Training diagnosis ensemble...\")\n",
        "    diag_models, diag_scores = train_ensemble_models(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "\n",
        "    best_diag_models = {k: v for k, v in diag_models.items() if diag_scores[k] > 0.6}\n",
        "    if not best_diag_models:\n",
        "        best_diag_models = diag_models\n",
        "\n",
        "    diag_ensemble_pred = ensemble_predict(best_diag_models, X_diag_val)\n",
        "    diag_ensemble_acc = accuracy_score(y_diag_val, diag_ensemble_pred)\n",
        "\n",
        "    print(f\"\\nDiagnosis Ensemble Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(\"Diagnosis Classification Report:\")\n",
        "    print(classification_report(y_diag_val, diag_ensemble_pred, target_names=diagnosis_le.classes_))\n",
        "\n",
        "    print(\"\\nProcessing progression task...\")\n",
        "    prog_acoustic, prog_indices = prepare_features(progression_train, features_dict)\n",
        "\n",
        "    if len(prog_indices) == 0 or len(prog_acoustic) == 0:\n",
        "        print(\"No valid progression features found!\")\n",
        "        return\n",
        "\n",
        "    prog_indices = [i for i in prog_indices if i < len(progression_train)]\n",
        "    prog_df_filtered = progression_train.iloc[prog_indices].reset_index(drop=True)\n",
        "    prog_texts = prog_df_filtered['transcript'].fillna('').tolist()\n",
        "    prog_labels = prog_df_filtered['label_encoded'].values\n",
        "\n",
        "    if len(prog_labels) == 0:\n",
        "        print(\"No progression labels found!\")\n",
        "        return\n",
        "\n",
        "    print(\"Extracting BERT features for progression...\")\n",
        "    prog_text_features = extract_bert_features(prog_texts)\n",
        "\n",
        "    if prog_text_features.size == 0 or prog_acoustic.size == 0:\n",
        "        print(\"Empty features detected, using simple approach...\")\n",
        "\n",
        "        simple_features = []\n",
        "        for idx, row in prog_df_filtered.iterrows():\n",
        "            file_id = row['file_id']\n",
        "            if file_id in features_dict:\n",
        "                feat_data = features_dict[file_id]\n",
        "                simple_feat = []\n",
        "                for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                    if key in feat_data:\n",
        "                        val = feat_data[key]\n",
        "                        if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
        "                            simple_feat.append(np.mean(val))\n",
        "                        elif isinstance(val, (int, float)):\n",
        "                            simple_feat.append(val)\n",
        "                        else:\n",
        "                            simple_feat.append(0)\n",
        "                    else:\n",
        "                        simple_feat.append(0)\n",
        "\n",
        "                while len(simple_feat) < 20:\n",
        "                    simple_feat.append(0)\n",
        "                simple_features.append(simple_feat[:20])\n",
        "\n",
        "        prog_enhanced_features = StandardScaler().fit_transform(np.array(simple_features))\n",
        "    else:\n",
        "        print(\"Engineering features for progression...\")\n",
        "        prog_enhanced_features, prog_scaler = enhanced_feature_engineering(prog_acoustic, prog_text_features)\n",
        "\n",
        "    from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
        "    from imblearn.combine import SMOTEENN\n",
        "\n",
        "    try:\n",
        "        smote_enn = SMOTEENN(random_state=42)\n",
        "        prog_enhanced_balanced, prog_labels_balanced = smote_enn.fit_resample(prog_enhanced_features, prog_labels)\n",
        "    except:\n",
        "        try:\n",
        "            adasyn = ADASYN(random_state=42, n_neighbors=2)\n",
        "            prog_enhanced_balanced, prog_labels_balanced = adasyn.fit_resample(prog_enhanced_features, prog_labels)\n",
        "        except:\n",
        "            prog_enhanced_balanced, prog_labels_balanced = prog_enhanced_features, prog_labels\n",
        "\n",
        "    X_prog_train, X_prog_val, y_prog_train, y_prog_val = train_test_split(\n",
        "        prog_enhanced_balanced, prog_labels_balanced, test_size=0.2, random_state=42, stratify=prog_labels_balanced\n",
        "    )\n",
        "\n",
        "    print(\"Training progression ensemble...\")\n",
        "    prog_models, prog_scores = train_ensemble_models(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "\n",
        "    best_prog_models = {k: v for k, v in prog_models.items() if prog_scores[k] > 0.6}\n",
        "    if not best_prog_models:\n",
        "        best_prog_models = prog_models\n",
        "\n",
        "    prog_ensemble_pred = ensemble_predict(best_prog_models, X_prog_val)\n",
        "    prog_ensemble_acc = accuracy_score(y_prog_val, prog_ensemble_pred)\n",
        "\n",
        "    print(f\"\\nProgression Ensemble Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "    print(\"Progression Classification Report:\")\n",
        "    print(classification_report(y_prog_val, prog_ensemble_pred, target_names=progression_le.classes_))\n",
        "\n",
        "    if diag_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for diagnosis...\")\n",
        "        from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=min(200, X_diag_train.shape[1]//2))\n",
        "        X_diag_train_selected = selector.fit_transform(X_diag_train, y_diag_train)\n",
        "        X_diag_val_selected = selector.transform(X_diag_val)\n",
        "\n",
        "        diag_models_v2, diag_scores_v2 = train_ensemble_models(X_diag_train_selected, y_diag_train, X_diag_val_selected, y_diag_val)\n",
        "        diag_ensemble_pred_v2 = ensemble_predict(diag_models_v2, X_diag_val_selected)\n",
        "        diag_ensemble_acc_v2 = accuracy_score(y_diag_val, diag_ensemble_pred_v2)\n",
        "\n",
        "        print(f\"Diagnosis Improved Accuracy: {diag_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "        if diag_ensemble_acc_v2 > diag_ensemble_acc:\n",
        "            diag_ensemble_acc = diag_ensemble_acc_v2\n",
        "            diag_ensemble_pred = diag_ensemble_pred_v2\n",
        "\n",
        "    if prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for progression...\")\n",
        "        from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=min(150, X_prog_train.shape[1]//2))\n",
        "        X_prog_train_selected = selector.fit_transform(X_prog_train, y_prog_train)\n",
        "        X_prog_val_selected = selector.transform(X_prog_val)\n",
        "\n",
        "        prog_models_v2, prog_scores_v2 = train_ensemble_models(X_prog_train_selected, y_prog_train, X_prog_val_selected, y_prog_val)\n",
        "        prog_ensemble_pred_v2 = ensemble_predict(prog_models_v2, X_prog_val_selected)\n",
        "        prog_ensemble_acc_v2 = accuracy_score(y_prog_val, prog_ensemble_pred_v2)\n",
        "\n",
        "        print(f\"Progression Improved Accuracy: {prog_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "        if prog_ensemble_acc_v2 > prog_ensemble_acc:\n",
        "            prog_ensemble_acc = prog_ensemble_acc_v2\n",
        "            prog_ensemble_pred = prog_ensemble_pred_v2\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Diagnosis Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(f\"Progression Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying synthetic data generation...\")\n",
        "\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "        def generate_synthetic_samples(X, y, target_samples=200):\n",
        "            synthetic_X = []\n",
        "            synthetic_y = []\n",
        "\n",
        "            for class_label in np.unique(y):\n",
        "                class_samples = X[y == class_label]\n",
        "\n",
        "                if len(class_samples) < 5:\n",
        "                    continue\n",
        "\n",
        "                nn_model = NearestNeighbors(n_neighbors=min(5, len(class_samples)))\n",
        "                nn_model.fit(class_samples)\n",
        "\n",
        "                for _ in range(target_samples // len(np.unique(y))):\n",
        "                    base_idx = np.random.randint(0, len(class_samples))\n",
        "                    base_sample = class_samples[base_idx]\n",
        "\n",
        "                    distances, indices = nn_model.kneighbors([base_sample])\n",
        "                    neighbor_idx = np.random.choice(indices[0])\n",
        "                    neighbor_sample = class_samples[neighbor_idx]\n",
        "\n",
        "                    alpha = np.random.uniform(0.2, 0.8)\n",
        "                    synthetic_sample = alpha * base_sample + (1 - alpha) * neighbor_sample\n",
        "\n",
        "                    noise = np.random.normal(0, 0.01, synthetic_sample.shape)\n",
        "                    synthetic_sample += noise\n",
        "\n",
        "                    synthetic_X.append(synthetic_sample)\n",
        "                    synthetic_y.append(class_label)\n",
        "\n",
        "            return np.array(synthetic_X), np.array(synthetic_y)\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            synth_X_diag, synth_y_diag = generate_synthetic_samples(X_diag_train, y_diag_train)\n",
        "            X_diag_augmented = np.vstack([X_diag_train, synth_X_diag])\n",
        "            y_diag_augmented = np.hstack([y_diag_train, synth_y_diag])\n",
        "\n",
        "            diag_models_synth, _ = train_ensemble_models(X_diag_augmented, y_diag_augmented, X_diag_val, y_diag_val)\n",
        "            diag_synth_pred = ensemble_predict(diag_models_synth, X_diag_val)\n",
        "            diag_synth_acc = accuracy_score(y_diag_val, diag_synth_pred)\n",
        "\n",
        "            print(f\"Diagnosis with Synthetic Data: {diag_synth_acc:.4f}\")\n",
        "\n",
        "            if diag_synth_acc > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_synth_acc\n",
        "                diag_ensemble_pred = diag_synth_pred\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            synth_X_prog, synth_y_prog = generate_synthetic_samples(X_prog_train, y_prog_train)\n",
        "            X_prog_augmented = np.vstack([X_prog_train, synth_X_prog])\n",
        "            y_prog_augmented = np.hstack([y_prog_train, synth_y_prog])\n",
        "\n",
        "            prog_models_synth, _ = train_ensemble_models(X_prog_augmented, y_prog_augmented, X_prog_val, y_prog_val)\n",
        "            prog_synth_pred = ensemble_predict(prog_models_synth, X_prog_val)\n",
        "            prog_synth_acc = accuracy_score(y_prog_val, prog_synth_pred)\n",
        "\n",
        "            print(f\"Progression with Synthetic Data: {prog_synth_acc:.4f}\")\n",
        "\n",
        "            if prog_synth_acc > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_synth_acc\n",
        "                prog_ensemble_pred = prog_synth_pred\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying neural network with heavy augmentation...\")\n",
        "\n",
        "        class HeavyAugmentationModel(nn.Module):\n",
        "            def __init__(self, input_dim, num_classes=2):\n",
        "                super().__init__()\n",
        "                self.layers = nn.Sequential(\n",
        "                    nn.Linear(input_dim, 1024),\n",
        "                    nn.BatchNorm1d(1024),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.5),\n",
        "\n",
        "                    nn.Linear(1024, 512),\n",
        "                    nn.BatchNorm1d(512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.4),\n",
        "\n",
        "                    nn.Linear(512, 256),\n",
        "                    nn.BatchNorm1d(256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3),\n",
        "\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.BatchNorm1d(128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.2),\n",
        "\n",
        "                    nn.Linear(128, num_classes)\n",
        "                )\n",
        "\n",
        "            def forward(self, x):\n",
        "                return self.layers(x)\n",
        "\n",
        "        def train_heavy_model(X_train, y_train, X_val, y_val, epochs=50):\n",
        "            model = HeavyAugmentationModel(X_train.shape[1]).to(device)\n",
        "\n",
        "            class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "            criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "            X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "            y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "            X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "            y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "            best_acc = 0\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "\n",
        "                batch_size = 32\n",
        "                num_batches = len(X_train_tensor) // batch_size + 1\n",
        "\n",
        "                total_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for i in range(num_batches):\n",
        "                    start_idx = i * batch_size\n",
        "                    end_idx = min((i + 1) * batch_size, len(X_train_tensor))\n",
        "\n",
        "                    if start_idx >= end_idx:\n",
        "                        break\n",
        "\n",
        "                    batch_X = X_train_tensor[start_idx:end_idx]\n",
        "                    batch_y = y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "                    if len(batch_X) == 0:\n",
        "                        continue\n",
        "\n",
        "                    noise = torch.randn_like(batch_X) * 0.01\n",
        "                    batch_X = batch_X + noise\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += batch_y.size(0)\n",
        "                    correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "                train_acc = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_val_tensor)\n",
        "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                    val_acc = 100 * (val_predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "            return model, best_acc / 100\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            print(\"Training heavy diagnosis model...\")\n",
        "            diag_heavy_model, diag_heavy_acc = train_heavy_model(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "            print(f\"Heavy Diagnosis Model Accuracy: {diag_heavy_acc:.4f}\")\n",
        "\n",
        "            if diag_heavy_acc > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_heavy_acc\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            print(\"Training heavy progression model...\")\n",
        "            prog_heavy_model, prog_heavy_acc = train_heavy_model(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "            print(f\"Heavy Progression Model Accuracy: {prog_heavy_acc:.4f}\")\n",
        "\n",
        "            if prog_heavy_acc > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_heavy_acc\n",
        "\n",
        "    print(f\"\\n=== FINAL RESULTS ===\")\n",
        "    print(f\"Diagnosis Task Accuracy: {diag_ensemble_acc:.4f} ({diag_ensemble_acc*100:.1f}%)\")\n",
        "    print(f\"Progression Task Accuracy: {prog_ensemble_acc:.4f} ({prog_ensemble_acc*100:.1f}%)\")\n",
        "\n",
        "    if diag_ensemble_acc >= 0.85 and prog_ensemble_acc >= 0.85:\n",
        "        print(\"SUCCESS: Both models achieved 85%+ accuracy!\")\n",
        "    else:\n",
        "        print(\"Still optimizing...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw_qUfDxI9u2",
        "outputId": "feb89ff6-baed-40db-c24e-e8d927cd951a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Processing diagnosis task...\n",
            "Extracting BERT features for diagnosis...\n",
            "Engineering features for diagnosis...\n",
            "Training diagnosis ensemble...\n",
            "RF Validation Accuracy: 0.6765\n",
            "GB Validation Accuracy: 0.6765\n",
            "SVM Validation Accuracy: 0.9412\n",
            "LR Validation Accuracy: 0.9706\n",
            "\n",
            "Diagnosis Ensemble Accuracy: 0.9118\n",
            "Diagnosis Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ad       0.89      0.94      0.92        18\n",
            "          cn       0.93      0.88      0.90        16\n",
            "\n",
            "    accuracy                           0.91        34\n",
            "   macro avg       0.91      0.91      0.91        34\n",
            "weighted avg       0.91      0.91      0.91        34\n",
            "\n",
            "\n",
            "Processing progression task...\n",
            "No progression labels found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import librosa\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.3, alpha=0.2):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.dropout = dropout\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W)\n",
        "        N = Wh.size()[0]\n",
        "\n",
        "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        return F.elu(h_prime)\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        N = Wh.size()[0]\n",
        "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
        "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
        "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
        "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
        "\n",
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.3, alpha=0.2, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.attentions = nn.ModuleList([\n",
        "            GraphAttentionLayer(in_features, out_features, dropout=dropout, alpha=alpha)\n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.dropout(x)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class SpeechGATModel(nn.Module):\n",
        "    def __init__(self, acoustic_dim=100, text_dim=768, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.acoustic_proj = nn.Linear(acoustic_dim, 256)\n",
        "        self.text_proj = nn.Linear(text_dim, 256)\n",
        "\n",
        "        self.gat1 = MultiHeadGATLayer(256, 64, dropout=dropout, num_heads=8)\n",
        "        self.gat2 = MultiHeadGATLayer(512, 128, dropout=dropout, num_heads=4)\n",
        "\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "        self.lr_integration = nn.Linear(128, num_classes)\n",
        "\n",
        "    def create_adjacency_matrix(self, batch_size, device):\n",
        "        adj = torch.ones(batch_size, batch_size, device=device)\n",
        "        return adj\n",
        "\n",
        "    def forward(self, acoustic_features, text_features):\n",
        "        batch_size = acoustic_features.size(0)\n",
        "        device = acoustic_features.device\n",
        "\n",
        "        acoustic_proj = F.relu(self.acoustic_proj(acoustic_features))\n",
        "        text_proj = F.relu(self.text_proj(text_features))\n",
        "\n",
        "        combined_features = acoustic_proj + text_proj\n",
        "\n",
        "        adj = self.create_adjacency_matrix(batch_size, device)\n",
        "\n",
        "        gat1_out = self.gat1(combined_features, adj)\n",
        "        gat2_out = self.gat2(gat1_out, adj)\n",
        "\n",
        "        pooled = torch.mean(gat2_out, dim=0, keepdim=True)\n",
        "        if batch_size > 1:\n",
        "            pooled = pooled.repeat(batch_size, 1)\n",
        "\n",
        "        fused = self.feature_fusion(pooled)\n",
        "        main_output = self.classifier(fused)\n",
        "        lr_output = self.lr_integration(fused)\n",
        "\n",
        "        return main_output + 0.3 * lr_output\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, acoustic_features, text_features, labels):\n",
        "        self.acoustic_features = torch.FloatTensor(acoustic_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.acoustic_features[idx], self.text_features[idx], self.labels[idx]\n",
        "\n",
        "def safe_load_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "\n",
        "    datasets = {}\n",
        "    label_encoders = {}\n",
        "\n",
        "    files_to_load = ['diagnosis_train.pkl', 'progression_train.pkl']\n",
        "\n",
        "    for file_name in files_to_load:\n",
        "        file_path = os.path.join(data_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                datasets[file_name.replace('.pkl', '')] = pd.read_pickle(file_path)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    le_files = ['diagnosis_label_encoder.pkl', 'progression_label_encoder.pkl']\n",
        "    for le_file in le_files:\n",
        "        le_path = os.path.join(data_path, le_file)\n",
        "        if os.path.exists(le_path):\n",
        "            try:\n",
        "                with open(le_path, 'rb') as f:\n",
        "                    label_encoders[le_file.replace('.pkl', '')] = pickle.load(f)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return datasets, label_encoders\n",
        "\n",
        "def extract_robust_acoustic_features(audio_path):\n",
        "    try:\n",
        "        if not os.path.exists(audio_path):\n",
        "            return np.zeros(100)\n",
        "\n",
        "        y, sr = librosa.load(audio_path, sr=22050, duration=30)\n",
        "\n",
        "        if len(y) == 0:\n",
        "            return np.zeros(100)\n",
        "\n",
        "        features = []\n",
        "\n",
        "        try:\n",
        "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            features.extend(np.mean(mfcc, axis=1))\n",
        "            features.extend(np.std(mfcc, axis=1))\n",
        "        except:\n",
        "            features.extend(np.zeros(26))\n",
        "\n",
        "        try:\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            features.extend(np.mean(chroma, axis=1))\n",
        "        except:\n",
        "            features.extend(np.zeros(12))\n",
        "\n",
        "        try:\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            features.extend(np.mean(spectral_contrast, axis=1))\n",
        "        except:\n",
        "            features.extend(np.zeros(7))\n",
        "\n",
        "        try:\n",
        "            tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
        "            features.extend(np.mean(tonnetz, axis=1))\n",
        "        except:\n",
        "            features.extend(np.zeros(6))\n",
        "\n",
        "        try:\n",
        "            features.append(np.mean(librosa.feature.zero_crossing_rate(y)))\n",
        "            features.append(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)))\n",
        "            features.append(np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)))\n",
        "            features.append(np.mean(librosa.feature.rms(y=y)))\n",
        "        except:\n",
        "            features.extend(np.zeros(4))\n",
        "\n",
        "        try:\n",
        "            tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
        "            features.append(tempo)\n",
        "        except:\n",
        "            features.append(120.0)\n",
        "\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=20)\n",
        "            features.extend(np.mean(mel_spec, axis=1))\n",
        "        except:\n",
        "            features.extend(np.zeros(20))\n",
        "\n",
        "        try:\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "            features.append(np.mean(spectral_bandwidth))\n",
        "            spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
        "            features.append(np.mean(spectral_flatness))\n",
        "        except:\n",
        "            features.extend(np.zeros(2))\n",
        "\n",
        "        while len(features) < 100:\n",
        "            features.append(0.0)\n",
        "\n",
        "        return np.array(features[:100])\n",
        "\n",
        "    except Exception as e:\n",
        "        return np.zeros(100)\n",
        "\n",
        "def safe_extract_bert_embeddings(texts):\n",
        "    try:\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        model.eval()\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model = model.to(device)\n",
        "\n",
        "        embeddings = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text in texts:\n",
        "                try:\n",
        "                    if pd.isna(text) or str(text).strip() == '':\n",
        "                        embeddings.append(np.zeros(768))\n",
        "                        continue\n",
        "\n",
        "                    inputs = tokenizer(str(text)[:512], return_tensors='pt', padding=True,\n",
        "                                     truncation=True, max_length=512)\n",
        "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                    outputs = model(**inputs)\n",
        "                    pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "                    embeddings.append(pooled)\n",
        "                except:\n",
        "                    embeddings.append(np.zeros(768))\n",
        "\n",
        "        return np.array(embeddings)\n",
        "    except:\n",
        "        return np.zeros((len(texts), 768))\n",
        "\n",
        "def safe_prepare_dataset(df, dataset_name):\n",
        "    if df is None or len(df) == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    acoustic_features = []\n",
        "    text_features = []\n",
        "    labels = []\n",
        "    file_ids = []\n",
        "\n",
        "    for idx in range(len(df)):\n",
        "        try:\n",
        "            row = df.iloc[idx]\n",
        "\n",
        "            if 'file_path' in row and pd.notna(row['file_path']):\n",
        "                acoustic_feat = extract_robust_acoustic_features(row['file_path'])\n",
        "                acoustic_features.append(acoustic_feat)\n",
        "\n",
        "                transcript = row.get('transcript', '')\n",
        "                if pd.isna(transcript):\n",
        "                    transcript = ''\n",
        "                text_features.append(str(transcript))\n",
        "\n",
        "                if 'label_encoded' in row:\n",
        "                    labels.append(row['label_encoded'])\n",
        "                elif 'label' in row:\n",
        "                    if dataset_name == 'diagnosis':\n",
        "                        label_val = 1 if row['label'] == 'ad' else 0\n",
        "                    else:\n",
        "                        label_val = 1 if row['label'] == 'decline' else 0\n",
        "                    labels.append(label_val)\n",
        "                else:\n",
        "                    labels.append(0)\n",
        "\n",
        "                file_ids.append(row.get('file_id', f'sample_{idx}'))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if len(acoustic_features) == 0:\n",
        "        return None, None, None, None\n",
        "\n",
        "    acoustic_array = np.array(acoustic_features)\n",
        "    text_embeddings = safe_extract_bert_embeddings(text_features)\n",
        "    labels_array = np.array(labels)\n",
        "\n",
        "    return acoustic_array, text_embeddings, labels_array, file_ids\n",
        "\n",
        "def train_gat_with_lr(acoustic_features, text_features, labels, num_classes=2, epochs=150):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if len(acoustic_features) < 4:\n",
        "        return None, 0\n",
        "\n",
        "    scaler_acoustic = StandardScaler()\n",
        "    scaler_text = StandardScaler()\n",
        "\n",
        "    acoustic_scaled = scaler_acoustic.fit_transform(acoustic_features)\n",
        "    text_scaled = scaler_text.fit_transform(text_features)\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    if len(unique_labels) < 2:\n",
        "        return None, 0\n",
        "\n",
        "    try:\n",
        "        X_train_a, X_val_a, X_train_t, X_val_t, y_train, y_val = train_test_split(\n",
        "            acoustic_scaled, text_scaled, labels, test_size=0.25, random_state=42, stratify=labels\n",
        "        )\n",
        "    except:\n",
        "        split_idx = int(0.75 * len(labels))\n",
        "        X_train_a, X_val_a = acoustic_scaled[:split_idx], acoustic_scaled[split_idx:]\n",
        "        X_train_t, X_val_t = text_scaled[:split_idx], text_scaled[split_idx:]\n",
        "        y_train, y_val = labels[:split_idx], labels[split_idx:]\n",
        "\n",
        "    try:\n",
        "        if len(y_train) > 6 and len(unique_labels) > 1:\n",
        "            smote = SMOTE(random_state=42, k_neighbors=min(2, len(y_train)//len(unique_labels)-1))\n",
        "            X_combined = np.hstack([X_train_a, X_train_t])\n",
        "            X_resampled, y_resampled = smote.fit_resample(X_combined, y_train)\n",
        "            X_train_a = X_resampled[:, :X_train_a.shape[1]]\n",
        "            X_train_t = X_resampled[:, X_train_a.shape[1]:]\n",
        "            y_train = y_resampled\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    train_dataset = SpeechDataset(X_train_a, X_train_t, y_train)\n",
        "    val_dataset = SpeechDataset(X_val_a, X_val_t, y_val)\n",
        "\n",
        "    batch_size = min(8, len(train_dataset))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=min(8, len(val_dataset)), shuffle=False)\n",
        "\n",
        "    model = SpeechGATModel(acoustic_dim=100, text_dim=768, num_classes=num_classes).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for acoustic, text, batch_labels in train_loader:\n",
        "            acoustic, text, batch_labels = acoustic.to(device), text.to(device), batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(acoustic, text)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for acoustic, text, batch_labels in val_loader:\n",
        "                acoustic, text, batch_labels = acoustic.to(device), text.to(device), batch_labels.to(device)\n",
        "                outputs = model(acoustic, text)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += batch_labels.size(0)\n",
        "                val_correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    if best_model_state:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        train_acoustic_tensor = torch.FloatTensor(X_train_a).to(device)\n",
        "        train_text_tensor = torch.FloatTensor(X_train_t).to(device)\n",
        "        train_outputs = model(train_acoustic_tensor, train_text_tensor)\n",
        "        train_probs = F.softmax(train_outputs, dim=1).cpu().numpy()\n",
        "\n",
        "        val_acoustic_tensor = torch.FloatTensor(X_val_a).to(device)\n",
        "        val_text_tensor = torch.FloatTensor(X_val_t).to(device)\n",
        "        val_outputs = model(val_acoustic_tensor, val_text_tensor)\n",
        "        val_probs = F.softmax(val_outputs, dim=1).cpu().numpy()\n",
        "\n",
        "    lr_model = LogisticRegression(C=1.0, random_state=42, max_iter=1000)\n",
        "\n",
        "    X_train_combined = np.hstack([X_train_a, X_train_t, train_probs])\n",
        "    lr_model.fit(X_train_combined, y_train)\n",
        "\n",
        "    X_val_combined = np.hstack([X_val_a, X_val_t, val_probs])\n",
        "    lr_predictions = lr_model.predict(X_val_combined)\n",
        "\n",
        "    gat_predictions = np.argmax(val_probs, axis=1)\n",
        "\n",
        "    ensemble_predictions = []\n",
        "    for i in range(len(val_probs)):\n",
        "        gat_conf = np.max(val_probs[i])\n",
        "        if gat_conf > 0.75:\n",
        "            ensemble_predictions.append(gat_predictions[i])\n",
        "        else:\n",
        "            ensemble_predictions.append(lr_predictions[i])\n",
        "\n",
        "    ensemble_acc = accuracy_score(y_val, ensemble_predictions)\n",
        "\n",
        "    return model, lr_model, ensemble_acc, y_val, ensemble_predictions\n",
        "\n",
        "def main():\n",
        "    datasets, label_encoders = safe_load_data()\n",
        "\n",
        "    if 'diagnosis_train' not in datasets:\n",
        "        return\n",
        "\n",
        "    diagnosis_df = datasets['diagnosis_train']\n",
        "    diag_acoustic, diag_text, diag_labels, diag_ids = safe_prepare_dataset(diagnosis_df, 'diagnosis')\n",
        "\n",
        "    if diag_acoustic is None:\n",
        "        return\n",
        "\n",
        "    diag_model, diag_lr, diag_acc, diag_y_val, diag_pred = train_gat_with_lr(\n",
        "        diag_acoustic, diag_text, diag_labels, num_classes=2, epochs=150\n",
        "    )\n",
        "\n",
        "    prog_acc = 0\n",
        "    if 'progression_train' in datasets:\n",
        "        progression_df = datasets['progression_train']\n",
        "        prog_acoustic, prog_text, prog_labels, prog_ids = safe_prepare_dataset(progression_df, 'progression')\n",
        "\n",
        "        if prog_acoustic is not None and len(prog_acoustic) > 4:\n",
        "            prog_model, prog_lr, prog_acc, prog_y_val, prog_pred = train_gat_with_lr(\n",
        "                prog_acoustic, prog_text, prog_labels, num_classes=2, epochs=200\n",
        "            )\n",
        "\n",
        "    print(f\"Diagnosis Accuracy: {diag_acc:.4f} ({diag_acc*100:.1f}%)\")\n",
        "    print(f\"Progression Accuracy: {prog_acc:.4f} ({prog_acc*100:.1f}%)\")\n",
        "\n",
        "    if 'diagnosis_label_encoder' in label_encoders:\n",
        "        class_names = label_encoders['diagnosis_label_encoder'].classes_\n",
        "        print(\"\\nDiagnosis Classification Report:\")\n",
        "        print(classification_report(diag_y_val, diag_pred, target_names=class_names))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "XTN_3ZPXPPxf",
        "outputId": "5bb20549-6f44-4d8f-f376-b78592519615"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "positional indexers are out-of-bounds",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \"\"\"\n\u001b[0;32m-> 4153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexers/utils.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1057819047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1057819047.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mprog_df_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogression_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprog_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'transcript'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog_df_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0;31m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
          ]
        }
      ]
    }
  ]
}