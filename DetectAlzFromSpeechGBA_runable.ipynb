{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/DetectAlzFromSpeechGBA_runable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJAKHEKhjm1R"
      },
      "source": [
        "# 1- paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI-ChIc4Zgx-",
        "outputId": "2637e7a6-aea1-4915-b3d9-4b3211c1c724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directories:\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test\n",
            "  /content/drive/MyDrive/Speech/linguistic_features\n",
            "  /content/drive/MyDrive/Speech/lightweight_features\n",
            "  /content/drive/MyDrive/Speech/transcripts\n",
            "  /content/drive/MyDrive/Speech/processed_datasets\n",
            "  /content/drive/MyDrive/Speech/gat_models\n",
            "  /content/drive/MyDrive/Speech/gat_results\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_decline\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_ad\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_cn\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_test\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_no_decline\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts\n",
            "\n",
            "Files:\n",
            "  /content/drive/MyDrive/Speech/diagnosis_model.pth\n",
            "  /content/drive/MyDrive/Speech/progression_model.pth\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/README.md\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso152.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso014.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso005.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso276.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso003.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso259.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso022.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso156.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso278.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso274.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso002.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso160.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso019.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso168.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso172.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso177.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso023.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso268.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso266.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso007.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso186.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso157.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso257.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso263.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso161.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso018.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso280.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso178.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso265.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso153.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso010.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso151.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso169.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso267.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso159.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso012.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso016.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso165.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso170.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso183.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso015.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso270.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso173.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso260.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso264.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso277.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso180.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso273.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso008.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso162.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso167.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso182.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso017.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso154.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso261.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso021.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso164.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso158.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso148.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso262.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso285.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso312.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso315.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso298.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso310.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso291.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso316.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso308.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso283.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso300.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso289.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso302.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso299.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso281.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso292.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso296.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso307.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso309.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso286.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso025.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso033.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso027.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso046.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso032.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso049.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso047.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso053.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso054.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso036.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso035.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso043.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso024.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso039.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso028.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso031.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso045.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso200.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso116.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso071.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso212.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso209.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso232.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso247.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso222.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso236.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso078.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso106.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso070.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso141.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso138.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso110.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso218.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso144.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso206.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso205.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso244.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso056.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso077.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso090.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso134.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso249.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso098.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso123.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso072.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso126.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso253.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso245.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso089.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso192.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso092.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso068.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso223.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso122.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso224.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso188.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso130.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso202.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso229.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso216.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso198.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso093.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso063.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso112.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso142.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso187.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso059.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso237.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso215.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso189.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso055.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso125.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso250.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso109.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso233.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso190.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso060.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso246.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso234.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso197.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso228.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso128.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso075.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso220.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso211.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso074.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso248.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso003.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso014.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso012.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso017.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso010.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso008.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso018.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso005.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso019.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso002.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso022.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso021.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso016.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso015.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso153.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso165.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso152.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso164.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso156.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso157.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso161.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso160.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso154.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso172.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso148.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso173.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso170.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso023.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso168.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso159.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso167.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso169.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso260.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso183.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso264.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso276.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso180.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso266.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso177.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso178.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso273.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso274.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso262.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso182.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso186.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso286.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso298.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso310.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso308.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso312.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso281.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso291.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso285.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso315.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso283.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso277.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso289.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso296.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso299.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso309.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso307.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso316.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso302.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso278.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso162.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso300.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso292.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso261.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso265.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso151.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso268.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso259.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso267.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso257.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso263.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso280.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso270.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso158.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso036.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso043.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso024.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso045.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso077.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso072.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso055.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso060.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso047.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso074.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso049.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso075.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso068.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso053.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso071.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso070.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso059.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso109.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso089.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso141.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso188.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso092.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso093.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso134.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso106.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso138.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso110.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso144.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso130.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso123.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso116.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso142.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso125.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso187.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso128.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso112.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso098.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso126.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso209.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso218.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso216.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso206.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso205.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso198.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso211.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso245.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso237.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso229.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso246.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso236.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso222.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso234.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso249.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso228.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso223.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso233.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso247.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso232.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso250.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso253.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso248.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso224.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso189.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso202.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso063.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso039.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso244.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso122.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso190.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso192.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso215.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso090.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso025.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso031.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso027.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso197.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso056.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso054.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso078.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso028.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso200.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso220.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso212.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso046.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso035.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso033.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso032.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/README.md\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp056.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp024.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp157.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp042.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp349.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp148.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp200.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp137.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp136.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp321.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp039.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp306.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp007.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp030.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp001.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp052.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp197.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp310.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp196.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp192.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp177.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp041.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp207.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp253.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp161.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp043.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp319.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp193.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp130.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp195.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp028.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp124.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp350.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp198.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp251.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp031.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp122.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp003.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp055.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp313.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp051.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp179.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp209.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp266.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp127.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp300.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp101.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp196.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp137.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp130.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp349.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp198.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp321.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp136.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp024.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp007.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp382.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp043.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp019.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp333.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp056.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp310.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp042.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp377.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp363.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp028.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp350.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp096.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp052.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp204.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp380.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp109.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp255.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp157.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp306.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp197.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp031.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp368.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp032.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp091.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp344.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp124.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp195.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp253.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp251.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp039.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp001.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp041.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp384.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp207.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp379.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp324.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp177.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp148.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp023.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp359.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp122.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp200.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp030.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp319.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp378.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp193.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp128.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp161.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp192.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp055.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp003.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp300.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp266.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp320.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp313.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp179.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp357.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp101.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp051.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp326.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp127.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp276.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp209.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp318.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/README\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/test_results_task3.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt29.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt13.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt21.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt1.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt18.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt10.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt23.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt26.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt17.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt11.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt2.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt12.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt9.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt15.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt24.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt26.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt5.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt20.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt22.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt32.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt11.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt7.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt2.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt8.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt18.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt17.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt12.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt3.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt28.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt4.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt6.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt15.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt21.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt19.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt31.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt29.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt14.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt13.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt23.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt10.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt30.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt16.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt27.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt1.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt25.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt24.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt9.wav\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/linguistic_features.json\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/linguistic_features.pkl\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/feature_summary.txt\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/feature_analysis.csv\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/diagnosis_ad_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/diagnosis_cn_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_decline_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_no_decline_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_test_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/diagnosis_ad_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/diagnosis_cn_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_decline_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_no_decline_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_test_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/all_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/final_stats.json\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/openpkl.py\n",
            "  /content/drive/MyDrive/Speech/transcripts/error_analysis.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/all_categories_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_results.pkl\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_summary.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_decline/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_ad/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_cn/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_test/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_no_decline/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso047.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso298.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso296.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso157.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso245.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso273.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso316.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso059.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso148.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso308.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso016.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso110.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso190.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso307.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso187.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso162.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso257.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso054.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso075.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso056.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso268.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso197.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso060.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso156.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso142.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso039.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso237.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso202.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso189.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso224.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso182.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso024.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso218.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso259.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso260.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso165.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso285.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso209.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso200.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso215.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso173.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso299.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso151.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso233.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso017.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso055.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso228.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso312.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso280.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso109.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso229.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso267.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso022.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso068.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso002.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso032.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso134.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso053.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso072.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso164.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso247.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso008.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso192.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso212.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso036.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso141.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso090.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso152.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso077.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso130.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso158.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso281.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso249.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso014.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso205.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso112.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso125.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso236.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso074.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso300.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso106.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso274.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso070.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso012.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso216.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso098.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso180.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso126.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso266.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso261.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso232.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso035.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso283.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso021.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso248.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso168.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso291.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso116.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso172.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso045.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso071.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso183.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso310.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso234.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso093.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso005.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso289.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso263.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso007.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso265.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso028.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso253.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso177.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso170.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso188.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso027.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso078.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso154.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso033.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso211.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso092.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso246.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso206.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso046.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso161.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso043.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso128.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso250.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso292.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso223.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso159.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso153.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso178.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso003.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso023.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso286.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso302.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso169.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso144.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso315.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso270.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso276.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso222.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso278.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso244.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso186.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso122.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso198.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso015.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso019.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso220.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso089.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso049.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso025.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso018.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso123.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso063.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso031.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso262.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso138.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp266.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp032.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp310.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp043.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp161.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp197.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt13.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp003.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp137.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt12.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp357.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp101.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp350.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt11.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp198.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp056.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp023.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp300.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp253.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso167.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt24.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt8.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp031.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt19.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp148.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp055.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp377.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp382.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp028.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt29.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp319.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt6.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt18.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp179.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp384.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp096.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp321.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp007.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp306.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp177.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp128.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp195.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp324.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp136.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp251.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt5.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp192.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp209.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp039.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp193.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt17.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt14.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp200.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt20.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp344.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt26.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp041.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso264.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp320.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp204.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso277.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp042.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt30.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp368.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt23.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt22.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt15.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp207.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt9.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp130.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt1.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp052.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp051.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp019.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt25.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp122.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt3.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp363.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp349.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp109.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso160.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt16.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt4.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp196.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp276.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp380.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt10.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt21.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt2.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp255.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp030.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp359.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp001.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp124.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso010.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp157.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp024.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso309.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt7.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp326.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp318.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt28.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt27.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp091.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp378.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp127.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp313.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp379.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp333.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt32.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt31.wav.txt\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_train.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_train.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_train.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_train.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_test.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_test.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/all_data.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/all_data.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/labels_dict.json\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_label_encoder.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_label_encoder.pkl\n",
            "  /content/drive/MyDrive/Speech/gat_models/diagnosis_gat_model.pth\n",
            "  /content/drive/MyDrive/Speech/gat_models/progression_gat_model.pth\n",
            "  /content/drive/MyDrive/Speech/gat_results/diagnosis_gat_results.pkl\n",
            "  /content/drive/MyDrive/Speech/gat_results/progression_gat_results.pkl\n",
            "\n",
            "Summary:\n",
            "  Total directories: 39\n",
            "  Total files: 819\n",
            "  Total paths: 858\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory\n",
        "base_directory = '/content/drive/MyDrive/Speech'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(base_directory):\n",
        "    # Separate lists for files and directories\n",
        "    file_paths = []\n",
        "    directory_paths = []\n",
        "\n",
        "    for root, directories, files in os.walk(base_directory):\n",
        "        # Add directory paths\n",
        "        for directory in directories:\n",
        "            dir_path = os.path.join(root, directory)\n",
        "            directory_paths.append(dir_path)\n",
        "\n",
        "        # Add file paths\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_paths.append(file_path)\n",
        "\n",
        "    print(\"Directories:\")\n",
        "    for directory in directory_paths:\n",
        "        print(f\"  {directory}\")\n",
        "\n",
        "    print(f\"\\nFiles:\")\n",
        "    for file in file_paths:\n",
        "        print(f\"  {file}\")\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"  Total directories: {len(directory_paths)}\")\n",
        "    print(f\"  Total files: {len(file_paths)}\")\n",
        "    print(f\"  Total paths: {len(file_paths) + len(directory_paths)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Directory {base_directory} does not exist!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXO3rUAgjpsn"
      },
      "source": [
        "# 2- extract label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzAC94TIb5u7",
        "outputId": "03074510-cb4f-49e1-fa86-91c806970514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting labels from directory structure...\n",
            "Loading existing features...\n",
            "Loading linguistic features...\n",
            "Loading transcripts...\n",
            "Dataset created with 271 samples\n",
            "Columns: ['file_id', 'dataset', 'label', 'file_path', 'transcript']\n",
            "Label distribution:\n",
            "label\n",
            "ad            87\n",
            "cn            79\n",
            "no_decline    58\n",
            "unknown       32\n",
            "decline       15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset Summary:\n",
            "Diagnosis Training: 166 samples\n",
            "  - AD: 87\n",
            "  - CN: 79\n",
            "Progression Training: 73 samples\n",
            "  - Decline: 15\n",
            "  - No Decline: 58\n",
            "Progression Test: 32 samples\n",
            "Datasets saved to /content/drive/MyDrive/Speech/processed_datasets\n",
            "\n",
            "Extracting audio features for a sample...\n",
            "Extracting mel-spectrograms and acoustic features...\n",
            "Processing adrso003...\n",
            "Processing adrso014...\n",
            "Processing adrso012...\n",
            "Processing adrso017...\n",
            "Processing adrso010...\n",
            "Successfully extracted features for 5 audio files\n",
            "Feature keys for adrso003: ['mel_spectrogram', 'log_mel_spectrogram', 'mel_mean', 'mel_std', 'log_mel_mean', 'log_mel_std', 'mfcc', 'chroma', 'spectral_contrast', 'tonnetz', 'zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']\n",
            "\n",
            "Processing complete!\n",
            "Available datasets:\n",
            "  - diagnosis_train: 166 samples\n",
            "  - progression_train: 73 samples\n",
            "  - progression_test: 32 samples\n",
            "  - all_data: 271 samples\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def extract_labels_from_directory_structure():\n",
        "    base_paths = {\n",
        "        'diagnosis_train': '/content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio',\n",
        "        'progression_train': '/content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio',\n",
        "        'progression_test': '/content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio'\n",
        "    }\n",
        "\n",
        "    labels = {}\n",
        "\n",
        "    for dataset_type, base_path in base_paths.items():\n",
        "        if dataset_type == 'progression_test':\n",
        "            files = os.listdir(base_path)\n",
        "            wav_files = [f for f in files if f.endswith('.wav')]\n",
        "            for wav_file in wav_files:\n",
        "                file_id = wav_file.replace('.wav', '')\n",
        "                labels[file_id] = {\n",
        "                    'dataset': 'progression_test',\n",
        "                    'label': 'unknown',\n",
        "                    'file_path': os.path.join(base_path, wav_file)\n",
        "                }\n",
        "        else:\n",
        "            subdirs = os.listdir(base_path)\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(base_path, subdir)\n",
        "                if os.path.isdir(subdir_path):\n",
        "                    wav_files = os.listdir(subdir_path)\n",
        "                    for wav_file in wav_files:\n",
        "                        if wav_file.endswith('.wav'):\n",
        "                            file_id = wav_file.replace('.wav', '')\n",
        "                            if dataset_type == 'diagnosis_train':\n",
        "                                label = 'ad' if subdir == 'ad' else 'cn'\n",
        "                            else:\n",
        "                                label = 'decline' if subdir == 'decline' else 'no_decline'\n",
        "\n",
        "                            labels[file_id] = {\n",
        "                                'dataset': dataset_type,\n",
        "                                'label': label,\n",
        "                                'file_path': os.path.join(subdir_path, wav_file)\n",
        "                            }\n",
        "\n",
        "    return labels\n",
        "\n",
        "def extract_mel_spectrogram(audio_path, n_mels=128, hop_length=512, n_fft=2048):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
        "                                                hop_length=hop_length, n_fft=n_fft)\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        return {\n",
        "            'mel_spectrogram': mel_spec,\n",
        "            'log_mel_spectrogram': log_mel_spec,\n",
        "            'mel_mean': np.mean(mel_spec, axis=1),\n",
        "            'mel_std': np.std(mel_spec, axis=1),\n",
        "            'log_mel_mean': np.mean(log_mel_spec, axis=1),\n",
        "            'log_mel_std': np.std(log_mel_spec, axis=1)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_wav2vec2_features(audio_path, model_name=\"facebook/wav2vec2-base-960h\"):\n",
        "    try:\n",
        "        processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "        model = Wav2Vec2Model.from_pretrained(model_name)\n",
        "\n",
        "        y, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        inputs = processor(y, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        features = last_hidden_states.squeeze().numpy()\n",
        "\n",
        "        return {\n",
        "            'wav2vec2_features': features,\n",
        "            'wav2vec2_mean': np.mean(features, axis=0),\n",
        "            'wav2vec2_std': np.std(features, axis=0),\n",
        "            'wav2vec2_max': np.max(features, axis=0),\n",
        "            'wav2vec2_min': np.min(features, axis=0)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing wav2vec2 for {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_acoustic_features(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        # Fix: Use correct librosa function names\n",
        "        features['mfcc'] = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
        "        features['chroma'] = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)  # Fixed function name\n",
        "        features['spectral_contrast'] = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr), axis=1)\n",
        "        features['tonnetz'] = np.mean(librosa.feature.tonnetz(y=y, sr=sr), axis=1)\n",
        "        features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "        features['rms_energy'] = np.mean(librosa.feature.rms(y=y))\n",
        "\n",
        "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "        features['tempo'] = tempo\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting acoustic features for {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_linguistic_features():\n",
        "    ling_features = {}\n",
        "\n",
        "    ling_path = '/content/drive/MyDrive/Speech/linguistic_features'\n",
        "    if os.path.exists(ling_path):\n",
        "        try:\n",
        "            with open(os.path.join(ling_path, 'linguistic_features.pkl'), 'rb') as f:\n",
        "                ling_features = pickle.load(f)\n",
        "        except:\n",
        "            try:\n",
        "                with open(os.path.join(ling_path, 'linguistic_features.json'), 'r') as f:\n",
        "                    ling_features = json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading linguistic features: {e}\")\n",
        "\n",
        "    return ling_features\n",
        "def load_transcripts():\n",
        "    transcripts = {}\n",
        "\n",
        "    transcript_files = [\n",
        "        '/content/drive/MyDrive/Speech/transcripts/all_categories_results.json',\n",
        "        '/content/drive/MyDrive/Speech/transcripts/transcription_results.json'\n",
        "    ]\n",
        "\n",
        "    for transcript_file in transcript_files:\n",
        "        if os.path.exists(transcript_file):\n",
        "            try:\n",
        "                with open(transcript_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        transcripts.update(data)\n",
        "                    elif isinstance(data, list):\n",
        "                        # Handle list format if that's what the file contains\n",
        "                        for item in data:\n",
        "                            if isinstance(item, dict) and len(item) >= 2:\n",
        "                                # Extract key-value pairs from list items\n",
        "                                keys = list(item.keys())\n",
        "                                transcripts[keys[0]] = item[keys[1]]\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON decode error in {transcript_file}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {transcript_file}: {e}\")\n",
        "\n",
        "    # Continue with individual transcript loading...\n",
        "\n",
        "    individual_transcript_path = '/content/drive/MyDrive/Speech/transcripts/individual_transcripts'\n",
        "    if os.path.exists(individual_transcript_path):\n",
        "        txt_files = [f for f in os.listdir(individual_transcript_path) if f.endswith('.txt')]\n",
        "        for txt_file in txt_files:\n",
        "            try:\n",
        "                file_id = txt_file.replace('.wav.txt', '')\n",
        "                with open(os.path.join(individual_transcript_path, txt_file), 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    transcripts[file_id] = content\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {txt_file}: {e}\")\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "# Define a placeholder function for load_existing_features\n",
        "def load_existing_features():\n",
        "    \"\"\"\n",
        "    Placeholder function to load existing features.\n",
        "    Replace with actual loading logic if needed.\n",
        "    \"\"\"\n",
        "    return {}\n",
        "\n",
        "\n",
        "def create_comprehensive_dataset():\n",
        "    print(\"Extracting labels from directory structure...\")\n",
        "    labels_dict = extract_labels_from_directory_structure()\n",
        "\n",
        "    print(\"Loading existing features...\")\n",
        "    existing_features = load_existing_features()\n",
        "\n",
        "    print(\"Loading linguistic features...\")\n",
        "    linguistic_features = load_linguistic_features()\n",
        "\n",
        "    print(\"Loading transcripts...\")\n",
        "    transcripts = load_transcripts()\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        row = {\n",
        "            'file_id': file_id,\n",
        "            'dataset': label_info['dataset'],\n",
        "            'label': label_info['label'],\n",
        "            'file_path': label_info['file_path']\n",
        "        }\n",
        "\n",
        "        if file_id in transcripts:\n",
        "            row['transcript'] = transcripts[file_id]\n",
        "\n",
        "        for category, features in existing_features.items():\n",
        "            if file_id in features:\n",
        "                feature_data = features[file_id]\n",
        "                if isinstance(feature_data, dict):\n",
        "                    for key, value in feature_data.items():\n",
        "                        row[f'{category}_{key}'] = value\n",
        "                else:\n",
        "                    row[f'{category}_features'] = feature_data\n",
        "\n",
        "        if file_id in linguistic_features:\n",
        "            ling_data = linguistic_features[file_id]\n",
        "            if isinstance(ling_data, dict):\n",
        "                for key, value in ling_data.items():\n",
        "                    row[f'linguistic_{key}'] = value\n",
        "\n",
        "        dataset.append(row)\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    print(f\"Dataset created with {len(df)} samples\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "    return df, labels_dict\n",
        "\n",
        "def extract_audio_features_batch(labels_dict, sample_limit=None):\n",
        "    print(\"Extracting mel-spectrograms and acoustic features...\")\n",
        "\n",
        "    audio_features = {}\n",
        "    processed_count = 0\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        if sample_limit and processed_count >= sample_limit:\n",
        "            break\n",
        "\n",
        "        audio_path = label_info['file_path']\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            print(f\"Processing {file_id}...\")\n",
        "\n",
        "            mel_features = extract_mel_spectrogram(audio_path)\n",
        "            acoustic_features = extract_acoustic_features(audio_path)\n",
        "\n",
        "            if mel_features and acoustic_features:\n",
        "                combined_features = {**mel_features, **acoustic_features}\n",
        "                audio_features[file_id] = combined_features\n",
        "                processed_count += 1\n",
        "        else:\n",
        "            print(f\"File not found: {audio_path}\")\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "def create_training_datasets():\n",
        "    df, labels_dict = create_comprehensive_dataset()\n",
        "\n",
        "    diagnosis_train = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "    progression_train = df[df['dataset'] == 'progression_train'].copy()\n",
        "    progression_test = df[df['dataset'] == 'progression_test'].copy()\n",
        "\n",
        "    diagnosis_le = LabelEncoder()\n",
        "    if len(diagnosis_train) > 0:\n",
        "        diagnosis_train['label_encoded'] = diagnosis_le.fit_transform(diagnosis_train['label'])\n",
        "\n",
        "    progression_le = LabelEncoder()\n",
        "    if len(progression_train) > 0:\n",
        "        progression_train['label_encoded'] = progression_le.fit_transform(progression_train['label'])\n",
        "\n",
        "    datasets = {\n",
        "        'diagnosis_train': diagnosis_train,\n",
        "        'progression_train': progression_train,\n",
        "        'progression_test': progression_test,\n",
        "        'all_data': df,\n",
        "        'labels_dict': labels_dict,\n",
        "        'diagnosis_label_encoder': diagnosis_le,\n",
        "        'progression_label_encoder': progression_le\n",
        "    }\n",
        "\n",
        "    print(\"\\nDataset Summary:\")\n",
        "    print(f\"Diagnosis Training: {len(diagnosis_train)} samples\")\n",
        "    if len(diagnosis_train) > 0:\n",
        "        print(f\"  - AD: {len(diagnosis_train[diagnosis_train['label'] == 'ad'])}\")\n",
        "        print(f\"  - CN: {len(diagnosis_train[diagnosis_train['label'] == 'cn'])}\")\n",
        "\n",
        "    print(f\"Progression Training: {len(progression_train)} samples\")\n",
        "    if len(progression_train) > 0:\n",
        "        print(f\"  - Decline: {len(progression_train[progression_train['label'] == 'decline'])}\")\n",
        "        print(f\"  - No Decline: {len(progression_train[progression_train['label'] == 'no_decline'])}\")\n",
        "\n",
        "    print(f\"Progression Test: {len(progression_test)} samples\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def save_datasets(datasets, output_path='/content/drive/MyDrive/Speech/processed_datasets'):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    for name, data in datasets.items():\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            data.to_csv(os.path.join(output_path, f'{name}.csv'), index=False)\n",
        "            data.to_pickle(os.path.join(output_path, f'{name}.pkl'))\n",
        "        elif name == 'labels_dict':\n",
        "            with open(os.path.join(output_path, 'labels_dict.json'), 'w') as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "        else:\n",
        "            with open(os.path.join(output_path, f'{name}.pkl'), 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "\n",
        "    print(f\"Datasets saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    datasets = create_training_datasets()\n",
        "    save_datasets(datasets)\n",
        "\n",
        "    print(\"\\nExtracting audio features for a sample...\")\n",
        "    sample_audio_features = extract_audio_features_batch(datasets['labels_dict'], sample_limit=5)\n",
        "\n",
        "    if sample_audio_features:\n",
        "        print(f\"Successfully extracted features for {len(sample_audio_features)} audio files\")\n",
        "        sample_id = list(sample_audio_features.keys())[0]\n",
        "        sample_features = sample_audio_features[sample_id]\n",
        "        print(f\"Feature keys for {sample_id}: {list(sample_features.keys())}\")\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    print(\"Available datasets:\")\n",
        "    for name in datasets.keys():\n",
        "        if isinstance(datasets[name], pd.DataFrame):\n",
        "            print(f\"  - {name}: {len(datasets[name])} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PdZYr4uj9Lc"
      },
      "source": [
        "# 3- Model- classification AD-CN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594,
          "referenced_widgets": [
            "d50dadc5b91b49fa81dcd3c04d08f7aa",
            "adbc4efc7127420db1e56bd6aa5b11ce",
            "8d5dace948724c5ca7caaf5b16142754",
            "d22bef0122b14d7a9f88ba18500353b2",
            "84adb669ceb44c58b083c054905bacbc",
            "70592dd2b68a4d2885f303a358cdcaa6",
            "1235658c89574c4481e68600115b4716",
            "f537cfcc7ef9442e8eeb87028266704a",
            "a496861664dd4ea29fc37e454082a90d",
            "c1d0247e51924a89be1594ac69e2120b",
            "aa068e337f534627b9b45d952aaf72fe",
            "02d5f9e1872e4db09517a189d8b5d23d",
            "38bb38fab68a4e95afe5eb31038b826c",
            "c011e7e492424807954dc914a21303a6",
            "4c098b0ddee14c6396f0552278153aa6",
            "dd706222d7b749329b0f2eb3f6f4c202",
            "3305c2c4da964d15906572aaa55a1df3",
            "d30f72f67afe440eb03a2336ab1beb26",
            "3c96aca3dbef4406bce82dbd69e5fabb",
            "883d55994f4440e4a1ff83ed733e0feb",
            "793d4125b5c548dc81bcd952f3928a9b",
            "ca29fcc32a3f4e4bb4981fc15dc401bf",
            "5672c263c35d48c087d0a9061545676c",
            "814b72be4ece4bcb8b40085a26af8bb9",
            "09d076367bfa441caa61cea97b82e957",
            "3b83039215f9415186f4a056a5d0b640",
            "0e0619ce3d024ed8a870c8513269ee6e",
            "4bee6d1325474e03b1de9e9e5431d2a3",
            "7b9b1ee123ea4a8a84617988336732ed",
            "3aafe5eb98b849cd8f223abaad3cb32f",
            "b15b3ed8fd1d42498afa3fdd7714d92c",
            "14e029f6cc724247812b5f85ad20c7ba",
            "6f8245ac3a814561855f2560ae95f46e",
            "617425d246e24aaeb6833e771390a185",
            "15eae9b2395c49689d62764a31f3a83e",
            "2f967574e0ef45ce87382c55776db60f",
            "737d7169d5c74d8fb1e071ebd942f5f0",
            "62fcf176c62c48ea92215edeeadade34",
            "0e277f888fe348609dd9708ae689022e",
            "5f662038cf734f56ac1d2095d60c4a77",
            "4c9c95dd6d844b15a2618ba9516f9dfd",
            "b99a857017724c7aa6598b3e111407b3",
            "64c4efcec8c94c668201badfa12ad71c",
            "383c2c9048c44b02bf8a45c090731ece",
            "f8042c0263404565a2652835b24ff79c",
            "c7f88dffbd0646f8b4eaa6705c2aaf89",
            "56be167fd1d04cf6ba5770e17e041adf",
            "70729595876d4187bf6872b167cf85b5",
            "cc237e702ba6462995a62a9dce160dab",
            "f8234340b00a438290965b7c2afaf3d3",
            "a64f9dd076ca44b38ff9e72918fb318c",
            "7f8ce07d9f0e43a784198c6e4b0873ee",
            "150556bc326248ecab38e2d28e1ab03a",
            "c211a8fe960447089a10219481021763",
            "92c20b9881ea449588cb0c2732c79679"
          ]
        },
        "id": "xw_qUfDxI9u2",
        "outputId": "688241dc-f02f-4af1-f8c8-c0a5834c5682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Processing diagnosis task...\n",
            "Extracting BERT features for diagnosis...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d50dadc5b91b49fa81dcd3c04d08f7aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02d5f9e1872e4db09517a189d8b5d23d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5672c263c35d48c087d0a9061545676c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "617425d246e24aaeb6833e771390a185",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8042c0263404565a2652835b24ff79c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Engineering features for diagnosis...\n",
            "Training diagnosis ensemble...\n",
            "RF Validation Accuracy: 0.6765\n",
            "GB Validation Accuracy: 0.6765\n",
            "SVM Validation Accuracy: 0.9412\n",
            "LR Validation Accuracy: 0.9706\n",
            "\n",
            "Diagnosis Ensemble Accuracy: 0.9118\n",
            "Diagnosis Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ad       0.89      0.94      0.92        18\n",
            "          cn       0.93      0.88      0.90        16\n",
            "\n",
            "    accuracy                           0.91        34\n",
            "   macro avg       0.91      0.91      0.91        34\n",
            "weighted avg       0.91      0.91      0.91        34\n",
            "\n",
            "\n",
            "Processing progression task...\n",
            "No progression labels found!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SimpleMultiModalModel(nn.Module):\n",
        "    def __init__(self, acoustic_dim=50, text_dim=768, hidden_dim=512, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.acoustic_encoder = nn.Sequential(\n",
        "            nn.Linear(acoustic_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, acoustic_features=None, text_features=None):\n",
        "        features = []\n",
        "\n",
        "        if acoustic_features is not None:\n",
        "            acoustic_out = self.acoustic_encoder(acoustic_features)\n",
        "            features.append(acoustic_out)\n",
        "\n",
        "        if text_features is not None:\n",
        "            text_out = self.text_encoder(text_features)\n",
        "            features.append(text_out)\n",
        "\n",
        "        if len(features) == 0:\n",
        "            raise ValueError(\"At least one input required\")\n",
        "        elif len(features) == 1:\n",
        "            combined = features[0]\n",
        "        else:\n",
        "            combined = torch.cat(features, dim=1)\n",
        "\n",
        "        return self.fusion(combined)\n",
        "\n",
        "def load_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "\n",
        "    diagnosis_train = pd.read_pickle(os.path.join(data_path, 'diagnosis_train.pkl'))\n",
        "    progression_train = pd.read_pickle(os.path.join(data_path, 'progression_train.pkl'))\n",
        "\n",
        "    with open(os.path.join(data_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "        diagnosis_le = pickle.load(f)\n",
        "    with open(os.path.join(data_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "        progression_le = pickle.load(f)\n",
        "\n",
        "    features = {}\n",
        "    for file in os.listdir(features_path):\n",
        "        if file.endswith('.pkl') and 'features' in file:\n",
        "            with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                features.update(data)\n",
        "\n",
        "    return diagnosis_train, progression_train, diagnosis_le, progression_le, features\n",
        "\n",
        "def extract_bert_features(texts, model_name='bert-base-uncased', max_length=128):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            if pd.isna(text) or text == '':\n",
        "                embeddings.append(np.zeros(768))\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(str(text), return_tensors='pt', padding=True,\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "            embeddings.append(pooled)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def prepare_features(df, features_dict):\n",
        "    acoustic_features = []\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            acoustic_feat = []\n",
        "\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        acoustic_feat.extend(feat.flatten())\n",
        "\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        if len(val) > 0:\n",
        "                            acoustic_feat.append(np.mean(val))\n",
        "                        else:\n",
        "                            acoustic_feat.append(0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val)\n",
        "\n",
        "            if len(acoustic_feat) > 100:\n",
        "                acoustic_feat = acoustic_feat[:100]\n",
        "            while len(acoustic_feat) < 100:\n",
        "                acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    return np.array(acoustic_features), valid_indices\n",
        "\n",
        "def train_ensemble_models(X_train, y_train, X_val, y_val):\n",
        "    models = {\n",
        "        'rf': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42,\n",
        "                                   class_weight='balanced', min_samples_split=5),\n",
        "        'gb': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42,\n",
        "                                       learning_rate=0.1),\n",
        "        'svm': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True, random_state=42),\n",
        "        'lr': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    trained_models = {}\n",
        "    val_scores = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        val_pred = model.predict(X_val)\n",
        "        val_acc = accuracy_score(y_val, val_pred)\n",
        "        trained_models[name] = model\n",
        "        val_scores[name] = val_acc\n",
        "        print(f\"{name.upper()} Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    return trained_models, val_scores\n",
        "\n",
        "def ensemble_predict(models, X_test, weights=None):\n",
        "    if weights is None:\n",
        "        weights = [1] * len(models)\n",
        "\n",
        "    predictions = []\n",
        "    for model in models.values():\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            pred_proba = model.predict_proba(X_test)\n",
        "            predictions.append(pred_proba)\n",
        "        else:\n",
        "            pred = model.predict(X_test)\n",
        "            pred_onehot = np.eye(len(np.unique(pred)))[pred]\n",
        "            predictions.append(pred_onehot)\n",
        "\n",
        "    weighted_avg = np.average(predictions, axis=0, weights=weights)\n",
        "    final_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    return final_predictions\n",
        "\n",
        "def enhanced_feature_engineering(acoustic_features, text_embeddings):\n",
        "    scaler = StandardScaler()\n",
        "    acoustic_scaled = scaler.fit_transform(acoustic_features)\n",
        "\n",
        "    text_scaled = StandardScaler().fit_transform(text_embeddings)\n",
        "\n",
        "    acoustic_stats = np.column_stack([\n",
        "        np.mean(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.std(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.max(acoustic_scaled, axis=1, keepdims=True),\n",
        "        np.min(acoustic_scaled, axis=1, keepdims=True)\n",
        "    ])\n",
        "\n",
        "    text_stats = np.column_stack([\n",
        "        np.mean(text_scaled, axis=1, keepdims=True),\n",
        "        np.std(text_scaled, axis=1, keepdims=True),\n",
        "        np.max(text_scaled, axis=1, keepdims=True),\n",
        "        np.min(text_scaled, axis=1, keepdims=True)\n",
        "    ])\n",
        "\n",
        "    text_stats_expanded = np.repeat(text_stats, acoustic_scaled.shape[1], axis=1)\n",
        "    acoustic_stats_expanded = np.repeat(acoustic_stats, text_scaled.shape[1], axis=1)\n",
        "\n",
        "    combined_features = np.hstack([\n",
        "        acoustic_scaled,\n",
        "        text_scaled,\n",
        "        acoustic_stats,\n",
        "        text_stats\n",
        "    ])\n",
        "\n",
        "    return combined_features, scaler\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "\n",
        "    print(\"Processing diagnosis task...\")\n",
        "    diag_acoustic, diag_indices = prepare_features(diagnosis_train, features_dict)\n",
        "    diag_df_filtered = diagnosis_train.iloc[diag_indices].reset_index(drop=True)\n",
        "    diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "    diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for diagnosis...\")\n",
        "    diag_text_features = extract_bert_features(diag_texts)\n",
        "\n",
        "    print(\"Engineering features for diagnosis...\")\n",
        "    diag_enhanced_features, diag_scaler = enhanced_feature_engineering(diag_acoustic, diag_text_features)\n",
        "\n",
        "    X_diag_train, X_diag_val, y_diag_train, y_diag_val = train_test_split(\n",
        "        diag_enhanced_features, diag_labels, test_size=0.2, random_state=42, stratify=diag_labels\n",
        "    )\n",
        "\n",
        "    print(\"Training diagnosis ensemble...\")\n",
        "    diag_models, diag_scores = train_ensemble_models(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "\n",
        "    best_diag_models = {k: v for k, v in diag_models.items() if diag_scores[k] > 0.6}\n",
        "    if not best_diag_models:\n",
        "        best_diag_models = diag_models\n",
        "\n",
        "    diag_ensemble_pred = ensemble_predict(best_diag_models, X_diag_val)\n",
        "    diag_ensemble_acc = accuracy_score(y_diag_val, diag_ensemble_pred)\n",
        "\n",
        "    print(f\"\\nDiagnosis Ensemble Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(\"Diagnosis Classification Report:\")\n",
        "    print(classification_report(y_diag_val, diag_ensemble_pred, target_names=diagnosis_le.classes_))\n",
        "\n",
        "    print(\"\\nProcessing progression task...\")\n",
        "    prog_acoustic, prog_indices = prepare_features(progression_train, features_dict)\n",
        "\n",
        "    if len(prog_indices) == 0 or len(prog_acoustic) == 0:\n",
        "        print(\"No valid progression features found!\")\n",
        "        return\n",
        "\n",
        "    prog_indices = [i for i in prog_indices if i < len(progression_train)]\n",
        "    prog_df_filtered = progression_train.iloc[prog_indices].reset_index(drop=True)\n",
        "    prog_texts = prog_df_filtered['transcript'].fillna('').tolist()\n",
        "    prog_labels = prog_df_filtered['label_encoded'].values\n",
        "\n",
        "    if len(prog_labels) == 0:\n",
        "        print(\"No progression labels found!\")\n",
        "        return\n",
        "\n",
        "    print(\"Extracting BERT features for progression...\")\n",
        "    prog_text_features = extract_bert_features(prog_texts)\n",
        "\n",
        "    if prog_text_features.size == 0 or prog_acoustic.size == 0:\n",
        "        print(\"Empty features detected, using simple approach...\")\n",
        "\n",
        "        simple_features = []\n",
        "        for idx, row in prog_df_filtered.iterrows():\n",
        "            file_id = row['file_id']\n",
        "            if file_id in features_dict:\n",
        "                feat_data = features_dict[file_id]\n",
        "                simple_feat = []\n",
        "                for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                    if key in feat_data:\n",
        "                        val = feat_data[key]\n",
        "                        if isinstance(val, (list, np.ndarray)) and len(val) > 0:\n",
        "                            simple_feat.append(np.mean(val))\n",
        "                        elif isinstance(val, (int, float)):\n",
        "                            simple_feat.append(val)\n",
        "                        else:\n",
        "                            simple_feat.append(0)\n",
        "                    else:\n",
        "                        simple_feat.append(0)\n",
        "\n",
        "                while len(simple_feat) < 20:\n",
        "                    simple_feat.append(0)\n",
        "                simple_features.append(simple_feat[:20])\n",
        "\n",
        "        prog_enhanced_features = StandardScaler().fit_transform(np.array(simple_features))\n",
        "    else:\n",
        "        print(\"Engineering features for progression...\")\n",
        "        prog_enhanced_features, prog_scaler = enhanced_feature_engineering(prog_acoustic, prog_text_features)\n",
        "\n",
        "    from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
        "    from imblearn.combine import SMOTEENN\n",
        "\n",
        "    try:\n",
        "        smote_enn = SMOTEENN(random_state=42)\n",
        "        prog_enhanced_balanced, prog_labels_balanced = smote_enn.fit_resample(prog_enhanced_features, prog_labels)\n",
        "    except:\n",
        "        try:\n",
        "            adasyn = ADASYN(random_state=42, n_neighbors=2)\n",
        "            prog_enhanced_balanced, prog_labels_balanced = adasyn.fit_resample(prog_enhanced_features, prog_labels)\n",
        "        except:\n",
        "            prog_enhanced_balanced, prog_labels_balanced = prog_enhanced_features, prog_labels\n",
        "\n",
        "    X_prog_train, X_prog_val, y_prog_train, y_prog_val = train_test_split(\n",
        "        prog_enhanced_balanced, prog_labels_balanced, test_size=0.2, random_state=42, stratify=prog_labels_balanced\n",
        "    )\n",
        "\n",
        "    print(\"Training progression ensemble...\")\n",
        "    prog_models, prog_scores = train_ensemble_models(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "\n",
        "    best_prog_models = {k: v for k, v in prog_models.items() if prog_scores[k] > 0.6}\n",
        "    if not best_prog_models:\n",
        "        best_prog_models = prog_models\n",
        "\n",
        "    prog_ensemble_pred = ensemble_predict(best_prog_models, X_prog_val)\n",
        "    prog_ensemble_acc = accuracy_score(y_prog_val, prog_ensemble_pred)\n",
        "\n",
        "    print(f\"\\nProgression Ensemble Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "    print(\"Progression Classification Report:\")\n",
        "    print(classification_report(y_prog_val, prog_ensemble_pred, target_names=progression_le.classes_))\n",
        "\n",
        "    if diag_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for diagnosis...\")\n",
        "        from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=min(200, X_diag_train.shape[1]//2))\n",
        "        X_diag_train_selected = selector.fit_transform(X_diag_train, y_diag_train)\n",
        "        X_diag_val_selected = selector.transform(X_diag_val)\n",
        "\n",
        "        diag_models_v2, diag_scores_v2 = train_ensemble_models(X_diag_train_selected, y_diag_train, X_diag_val_selected, y_diag_val)\n",
        "        diag_ensemble_pred_v2 = ensemble_predict(diag_models_v2, X_diag_val_selected)\n",
        "        diag_ensemble_acc_v2 = accuracy_score(y_diag_val, diag_ensemble_pred_v2)\n",
        "\n",
        "        print(f\"Diagnosis Improved Accuracy: {diag_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "        if diag_ensemble_acc_v2 > diag_ensemble_acc:\n",
        "            diag_ensemble_acc = diag_ensemble_acc_v2\n",
        "            diag_ensemble_pred = diag_ensemble_pred_v2\n",
        "\n",
        "    if prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for progression...\")\n",
        "        from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "        selector = SelectKBest(f_classif, k=min(150, X_prog_train.shape[1]//2))\n",
        "        X_prog_train_selected = selector.fit_transform(X_prog_train, y_prog_train)\n",
        "        X_prog_val_selected = selector.transform(X_prog_val)\n",
        "\n",
        "        prog_models_v2, prog_scores_v2 = train_ensemble_models(X_prog_train_selected, y_prog_train, X_prog_val_selected, y_prog_val)\n",
        "        prog_ensemble_pred_v2 = ensemble_predict(prog_models_v2, X_prog_val_selected)\n",
        "        prog_ensemble_acc_v2 = accuracy_score(y_prog_val, prog_ensemble_pred_v2)\n",
        "\n",
        "        print(f\"Progression Improved Accuracy: {prog_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "        if prog_ensemble_acc_v2 > prog_ensemble_acc:\n",
        "            prog_ensemble_acc = prog_ensemble_acc_v2\n",
        "            prog_ensemble_pred = prog_ensemble_pred_v2\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Diagnosis Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(f\"Progression Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying synthetic data generation...\")\n",
        "\n",
        "        from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "        def generate_synthetic_samples(X, y, target_samples=200):\n",
        "            synthetic_X = []\n",
        "            synthetic_y = []\n",
        "\n",
        "            for class_label in np.unique(y):\n",
        "                class_samples = X[y == class_label]\n",
        "\n",
        "                if len(class_samples) < 5:\n",
        "                    continue\n",
        "\n",
        "                nn_model = NearestNeighbors(n_neighbors=min(5, len(class_samples)))\n",
        "                nn_model.fit(class_samples)\n",
        "\n",
        "                for _ in range(target_samples // len(np.unique(y))):\n",
        "                    base_idx = np.random.randint(0, len(class_samples))\n",
        "                    base_sample = class_samples[base_idx]\n",
        "\n",
        "                    distances, indices = nn_model.kneighbors([base_sample])\n",
        "                    neighbor_idx = np.random.choice(indices[0])\n",
        "                    neighbor_sample = class_samples[neighbor_idx]\n",
        "\n",
        "                    alpha = np.random.uniform(0.2, 0.8)\n",
        "                    synthetic_sample = alpha * base_sample + (1 - alpha) * neighbor_sample\n",
        "\n",
        "                    noise = np.random.normal(0, 0.01, synthetic_sample.shape)\n",
        "                    synthetic_sample += noise\n",
        "\n",
        "                    synthetic_X.append(synthetic_sample)\n",
        "                    synthetic_y.append(class_label)\n",
        "\n",
        "            return np.array(synthetic_X), np.array(synthetic_y)\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            synth_X_diag, synth_y_diag = generate_synthetic_samples(X_diag_train, y_diag_train)\n",
        "            X_diag_augmented = np.vstack([X_diag_train, synth_X_diag])\n",
        "            y_diag_augmented = np.hstack([y_diag_train, synth_y_diag])\n",
        "\n",
        "            diag_models_synth, _ = train_ensemble_models(X_diag_augmented, y_diag_augmented, X_diag_val, y_diag_val)\n",
        "            diag_synth_pred = ensemble_predict(diag_models_synth, X_diag_val)\n",
        "            diag_synth_acc = accuracy_score(y_diag_val, diag_synth_pred)\n",
        "\n",
        "            print(f\"Diagnosis with Synthetic Data: {diag_synth_acc:.4f}\")\n",
        "\n",
        "            if diag_synth_acc > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_synth_acc\n",
        "                diag_ensemble_pred = diag_synth_pred\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            synth_X_prog, synth_y_prog = generate_synthetic_samples(X_prog_train, y_prog_train)\n",
        "            X_prog_augmented = np.vstack([X_prog_train, synth_X_prog])\n",
        "            y_prog_augmented = np.hstack([y_prog_train, synth_y_prog])\n",
        "\n",
        "            prog_models_synth, _ = train_ensemble_models(X_prog_augmented, y_prog_augmented, X_prog_val, y_prog_val)\n",
        "            prog_synth_pred = ensemble_predict(prog_models_synth, X_prog_val)\n",
        "            prog_synth_acc = accuracy_score(y_prog_val, prog_synth_pred)\n",
        "\n",
        "            print(f\"Progression with Synthetic Data: {prog_synth_acc:.4f}\")\n",
        "\n",
        "            if prog_synth_acc > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_synth_acc\n",
        "                prog_ensemble_pred = prog_synth_pred\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying neural network with heavy augmentation...\")\n",
        "\n",
        "        class HeavyAugmentationModel(nn.Module):\n",
        "            def __init__(self, input_dim, num_classes=2):\n",
        "                super().__init__()\n",
        "                self.layers = nn.Sequential(\n",
        "                    nn.Linear(input_dim, 1024),\n",
        "                    nn.BatchNorm1d(1024),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.5),\n",
        "\n",
        "                    nn.Linear(1024, 512),\n",
        "                    nn.BatchNorm1d(512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.4),\n",
        "\n",
        "                    nn.Linear(512, 256),\n",
        "                    nn.BatchNorm1d(256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3),\n",
        "\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.BatchNorm1d(128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.2),\n",
        "\n",
        "                    nn.Linear(128, num_classes)\n",
        "                )\n",
        "\n",
        "            def forward(self, x):\n",
        "                return self.layers(x)\n",
        "\n",
        "        def train_heavy_model(X_train, y_train, X_val, y_val, epochs=50):\n",
        "            model = HeavyAugmentationModel(X_train.shape[1]).to(device)\n",
        "\n",
        "            class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "            criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "            X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "            y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "            X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "            y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "            best_acc = 0\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "\n",
        "                batch_size = 32\n",
        "                num_batches = len(X_train_tensor) // batch_size + 1\n",
        "\n",
        "                total_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for i in range(num_batches):\n",
        "                    start_idx = i * batch_size\n",
        "                    end_idx = min((i + 1) * batch_size, len(X_train_tensor))\n",
        "\n",
        "                    if start_idx >= end_idx:\n",
        "                        break\n",
        "\n",
        "                    batch_X = X_train_tensor[start_idx:end_idx]\n",
        "                    batch_y = y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "                    if len(batch_X) == 0:\n",
        "                        continue\n",
        "\n",
        "                    noise = torch.randn_like(batch_X) * 0.01\n",
        "                    batch_X = batch_X + noise\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += batch_y.size(0)\n",
        "                    correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "                train_acc = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_outputs = model(X_val_tensor)\n",
        "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                    val_acc = 100 * (val_predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "            return model, best_acc / 100\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            print(\"Training heavy diagnosis model...\")\n",
        "            diag_heavy_model, diag_heavy_acc = train_heavy_model(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "            print(f\"Heavy Diagnosis Model Accuracy: {diag_heavy_acc:.4f}\")\n",
        "\n",
        "            if diag_heavy_acc > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_heavy_acc\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            print(\"Training heavy progression model...\")\n",
        "            prog_heavy_model, prog_heavy_acc = train_heavy_model(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "            print(f\"Heavy Progression Model Accuracy: {prog_heavy_acc:.4f}\")\n",
        "\n",
        "            if prog_heavy_acc > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_heavy_acc\n",
        "\n",
        "    print(f\"\\n=== FINAL RESULTS ===\")\n",
        "    print(f\"Diagnosis Task Accuracy: {diag_ensemble_acc:.4f} ({diag_ensemble_acc*100:.1f}%)\")\n",
        "    print(f\"Progression Task Accuracy: {prog_ensemble_acc:.4f} ({prog_ensemble_acc*100:.1f}%)\")\n",
        "\n",
        "    if diag_ensemble_acc >= 0.85 and prog_ensemble_acc >= 0.85:\n",
        "        print(\"SUCCESS: Both models achieved 85%+ accuracy!\")\n",
        "    else:\n",
        "        print(\"Still optimizing...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPPqnMqNoy45"
      },
      "source": [
        "# 4- Model- 100 Acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYpl5vGOjW3W",
        "outputId": "517b0fe3-bfad-4199-a6b1-285db3fc1b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directories:\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test\n",
            "  /content/drive/MyDrive/Speech/linguistic_features\n",
            "  /content/drive/MyDrive/Speech/lightweight_features\n",
            "  /content/drive/MyDrive/Speech/transcripts\n",
            "  /content/drive/MyDrive/Speech/processed_datasets\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_decline\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_ad\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_cn\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_test\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_no_decline\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts\n",
            "\n",
            "Files:\n",
            "  /content/drive/MyDrive/Speech/diagnosis_model.pth\n",
            "  /content/drive/MyDrive/Speech/progression_model.pth\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/README.md\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/adresso-train-mmse-scores.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso152.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso014.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso005.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso276.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso003.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso259.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso022.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso156.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso278.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso274.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso002.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso160.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso019.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso168.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso172.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso177.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso023.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso268.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso266.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso007.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso186.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso157.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso257.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso263.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso161.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso018.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso280.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso178.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso265.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso153.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso010.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso151.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso169.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso267.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso159.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso012.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso016.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso165.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso170.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso183.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso015.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso270.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso173.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso260.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso264.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso277.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso180.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso273.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso008.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso162.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso167.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso182.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso017.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso154.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso261.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso021.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso164.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso158.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso148.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso262.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso285.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso312.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso315.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso298.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso310.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso291.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso316.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso308.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso283.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso300.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso289.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso302.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso299.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso281.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso292.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso296.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso307.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso309.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn/adrso286.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso025.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso033.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso027.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso046.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso032.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso049.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso047.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso053.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso054.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso036.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso035.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso043.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso024.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso039.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso028.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso031.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso045.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso200.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso116.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso071.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso212.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso209.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso232.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso247.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso222.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso236.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso078.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso106.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso070.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso141.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso138.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso110.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso218.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso144.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso206.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso205.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso244.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso056.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso077.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso090.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso134.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso249.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso098.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso123.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso072.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso126.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso253.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso245.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso089.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso192.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso092.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso068.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso223.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso122.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso224.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso188.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso130.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso202.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso229.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso216.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso198.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso093.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso063.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso112.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso142.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso187.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso059.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso237.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso215.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso189.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso055.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso125.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso250.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso109.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso233.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso190.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso060.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso246.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso234.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso197.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso228.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso128.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso075.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso220.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso211.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso074.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/segmentation/ad/adrso248.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso003.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso014.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso012.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso017.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso010.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso008.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso018.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso005.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso019.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso002.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso022.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso007.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso021.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso016.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso015.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso153.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso165.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso152.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso164.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso156.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso157.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso161.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso160.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso154.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso172.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso148.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso173.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso170.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso023.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso168.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso159.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso167.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso169.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso260.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso183.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso264.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso276.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso180.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso266.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso177.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso178.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso273.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso274.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso262.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso182.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso186.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso286.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso298.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso310.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso308.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso312.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso281.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso291.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso285.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso315.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso283.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso277.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso289.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso296.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso299.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso309.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso307.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso316.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso302.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso278.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso162.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso300.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso292.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso261.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso265.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso151.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso268.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso259.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso267.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso257.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso263.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso280.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso270.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/cn/adrso158.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso036.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso043.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso024.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso045.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso077.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso072.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso055.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso060.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso047.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso074.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso049.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso075.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso068.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso053.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso071.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso070.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso059.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso109.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso089.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso141.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso188.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso092.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso093.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso134.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso106.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso138.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso110.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso144.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso130.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso123.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso116.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso142.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso125.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso187.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso128.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso112.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso098.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso126.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso209.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso218.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso216.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso206.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso205.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso198.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso211.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso245.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso237.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso229.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso246.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso236.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso222.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso234.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso249.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso228.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso223.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso233.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso247.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso232.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso250.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso253.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso248.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso224.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso189.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso202.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso063.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso039.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso244.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso122.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso190.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso192.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso215.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso090.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso025.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso031.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso027.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso197.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso056.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso054.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso078.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso028.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso200.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso220.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso212.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso046.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso035.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso033.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio/ad/adrso032.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/README.md\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp056.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp024.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp157.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp042.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp349.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp148.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp200.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp137.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp136.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp321.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp039.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp306.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp007.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp030.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp001.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp052.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp197.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp310.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp196.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp192.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp177.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp041.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp207.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp253.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp161.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp043.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp319.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp193.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp130.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp195.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp028.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp124.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp350.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp198.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp251.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp031.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/no_decline/adrsp122.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp003.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp055.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp313.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp051.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp179.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp209.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp266.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp127.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp300.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/segmentation/decline/adrsp101.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp196.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp137.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp130.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp349.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp198.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp321.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp136.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp024.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp007.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp382.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp043.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp019.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp333.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp056.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp310.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp042.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp377.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp363.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp028.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp350.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp096.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp052.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp204.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp380.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp109.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp255.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp157.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp306.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp197.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp031.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp368.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp032.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp091.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp344.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp124.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp195.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp253.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp251.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp039.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp001.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp041.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp384.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp207.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp379.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp324.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp177.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp148.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp023.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp359.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp122.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp200.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp030.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp319.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp378.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp193.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp128.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp161.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/no_decline/adrsp192.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp055.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp003.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp300.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp266.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp320.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp313.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp179.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp357.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp101.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp051.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp326.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp127.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp276.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp209.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio/decline/adrsp318.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/README\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/test_results_task3.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt29.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt13.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt21.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt1.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt18.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt10.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt23.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt26.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt17.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt11.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt2.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt12.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt9.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt15.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/segmentation/adrspt24.csv\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt26.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt5.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt20.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt22.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt32.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt11.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt7.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt2.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt8.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt18.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt17.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt12.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt3.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt28.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt4.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt6.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt15.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt21.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt19.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt31.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt29.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt14.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt13.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt23.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt10.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt30.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt16.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt27.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt1.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt25.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt24.wav\n",
            "  /content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio/adrspt9.wav\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/linguistic_features.json\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/linguistic_features.pkl\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/feature_summary.txt\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/feature_analysis.csv\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/diagnosis_ad_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/diagnosis_cn_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_decline_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_no_decline_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/linguistic_features/bert_embeddings/progression_test_embeddings.npz\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/diagnosis_ad_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/diagnosis_cn_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_decline_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_no_decline_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/progression_test_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/all_features.pkl\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/final_stats.json\n",
            "  /content/drive/MyDrive/Speech/lightweight_features/openpkl.py\n",
            "  /content/drive/MyDrive/Speech/transcripts/error_analysis.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/all_categories_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_results.pkl\n",
            "  /content/drive/MyDrive/Speech/transcripts/transcription_summary.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_decline/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_ad/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/diagnosis_cn/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_test/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/progression_no_decline/transcription_results.json\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso047.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso298.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso296.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso157.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso245.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso273.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso316.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso059.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso148.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso308.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso016.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso110.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso190.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso307.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso187.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso162.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso257.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso054.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso075.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso056.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso268.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso197.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso060.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso156.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso142.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso039.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso237.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso202.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso189.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso224.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso182.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso024.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso218.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso259.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso260.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso165.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso285.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso209.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso200.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso215.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso173.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso299.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso151.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso233.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso017.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso055.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso228.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso312.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso280.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso109.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso229.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso267.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso022.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso068.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso002.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso032.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso134.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso053.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso072.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso164.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso247.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso008.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso192.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso212.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso036.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso141.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso090.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso152.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso077.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso130.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso158.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso281.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso249.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso014.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso205.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso112.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso125.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso236.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso074.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso300.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso106.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso274.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso070.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso012.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso216.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso098.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso180.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso126.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso266.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso261.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso232.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso035.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso283.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso021.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso248.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso168.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso291.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso116.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso172.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso045.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso071.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso183.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso310.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso234.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso093.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso005.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso289.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso263.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso007.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso265.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso028.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso253.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso177.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso170.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso188.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso027.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso078.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso154.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso033.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso211.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso092.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso246.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso206.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso046.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso161.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso043.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso128.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso250.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso292.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso223.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso159.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso153.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso178.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso003.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso023.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso286.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso302.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso169.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso144.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso315.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso270.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso276.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso222.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso278.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso244.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso186.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso122.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso198.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso015.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso019.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso220.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso089.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso049.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso025.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso018.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso123.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso063.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso031.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso262.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso138.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp266.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp032.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp310.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp043.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp161.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp197.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt13.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp003.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp137.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt12.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp357.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp101.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp350.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt11.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp198.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp056.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp023.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp300.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp253.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso167.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt24.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt8.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp031.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt19.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp148.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp055.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp377.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp382.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp028.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt29.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp319.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt6.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt18.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp179.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp384.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp096.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp321.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp007.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp306.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp177.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp128.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp195.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp324.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp136.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp251.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt5.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp192.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp209.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp039.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp193.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt17.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt14.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp200.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt20.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp344.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt26.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp041.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso264.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp320.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp204.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso277.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp042.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt30.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp368.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt23.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt22.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt15.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp207.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt9.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp130.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt1.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp052.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp051.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp019.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt25.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp122.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt3.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp363.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp349.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp109.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso160.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt16.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt4.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp196.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp276.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp380.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt10.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt21.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt2.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp255.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp030.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp359.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp001.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp124.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso010.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp157.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp024.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrso309.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt7.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp326.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp318.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt28.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt27.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp091.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp378.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp127.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp313.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp379.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrsp333.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt32.wav.txt\n",
            "  /content/drive/MyDrive/Speech/transcripts/individual_transcripts/adrspt31.wav.txt\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_train.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_train.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_train.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_test.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/all_data.csv\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/all_data.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_train.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_test.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/labels_dict.json\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/progression_label_encoder.pkl\n",
            "  /content/drive/MyDrive/Speech/processed_datasets/diagnosis_label_encoder.pkl\n",
            "\n",
            "Summary:\n",
            "  Total directories: 37\n",
            "  Total files: 815\n",
            "  Total paths: 852\n",
            "Extracting labels from directory structure...\n",
            "Loading existing features...\n",
            "Loading linguistic features...\n",
            "Loading transcripts...\n",
            "Dataset created with 271 samples\n",
            "Columns: ['file_id', 'dataset', 'label', 'file_path', 'transcript', 'diagnosis_cn_log_mel', 'diagnosis_cn_wav2vec2', 'diagnosis_cn_file_id', 'diagnosis_cn_duration', 'diagnosis_cn_success', 'diagnosis_ad_log_mel', 'diagnosis_ad_wav2vec2', 'diagnosis_ad_file_id', 'diagnosis_ad_duration', 'diagnosis_ad_success', 'progression_no_decline_log_mel', 'progression_no_decline_wav2vec2', 'progression_no_decline_file_id', 'progression_no_decline_duration', 'progression_no_decline_success', 'progression_decline_log_mel', 'progression_decline_wav2vec2', 'progression_decline_file_id', 'progression_decline_duration', 'progression_decline_success', 'progression_test_log_mel', 'progression_test_wav2vec2', 'progression_test_file_id', 'progression_test_duration', 'progression_test_success']\n",
            "Label distribution:\n",
            "label\n",
            "ad            87\n",
            "cn            79\n",
            "no_decline    58\n",
            "unknown       32\n",
            "decline       15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset Summary:\n",
            "Diagnosis Training: 166 samples\n",
            "  - AD: 87\n",
            "  - CN: 79\n",
            "Progression Training: 73 samples\n",
            "  - Decline: 15\n",
            "  - No Decline: 58\n",
            "Progression Test: 32 samples\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import BertTokenizer, BertModel, Wav2Vec2Processor, Wav2Vec2Model\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_directory = '/content/drive/MyDrive/Speech'\n",
        "\n",
        "if os.path.exists(base_directory):\n",
        "    file_paths = []\n",
        "    directory_paths = []\n",
        "\n",
        "    for root, directories, files in os.walk(base_directory):\n",
        "        for directory in directories:\n",
        "            dir_path = os.path.join(root, directory)\n",
        "            directory_paths.append(dir_path)\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_paths.append(file_path)\n",
        "\n",
        "    print(\"Directories:\")\n",
        "    for directory in directory_paths:\n",
        "        print(f\"  {directory}\")\n",
        "\n",
        "    print(f\"\\nFiles:\")\n",
        "    for file in file_paths:\n",
        "        print(f\"  {file}\")\n",
        "\n",
        "    print(f\"\\nSummary:\")\n",
        "    print(f\"  Total directories: {len(directory_paths)}\")\n",
        "    print(f\"  Total files: {len(file_paths)}\")\n",
        "    print(f\"  Total paths: {len(file_paths) + len(directory_paths)}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Directory {base_directory} does not exist!\")\n",
        "\n",
        "def extract_labels_from_directory_structure():\n",
        "    base_paths = {\n",
        "        'diagnosis_train': '/content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio',\n",
        "        'progression_train': '/content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio',\n",
        "        'progression_test': '/content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio'\n",
        "    }\n",
        "\n",
        "    labels = {}\n",
        "\n",
        "    for dataset_type, base_path in base_paths.items():\n",
        "        if not os.path.exists(base_path):\n",
        "            continue\n",
        "\n",
        "        if dataset_type == 'progression_test':\n",
        "            files = os.listdir(base_path)\n",
        "            wav_files = [f for f in files if f.endswith('.wav')]\n",
        "            for wav_file in wav_files:\n",
        "                file_id = wav_file.replace('.wav', '')\n",
        "                labels[file_id] = {\n",
        "                    'dataset': 'progression_test',\n",
        "                    'label': 'unknown',\n",
        "                    'file_path': os.path.join(base_path, wav_file)\n",
        "                }\n",
        "        else:\n",
        "            subdirs = os.listdir(base_path)\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(base_path, subdir)\n",
        "                if os.path.isdir(subdir_path):\n",
        "                    wav_files = [f for f in os.listdir(subdir_path) if f.endswith('.wav')]\n",
        "                    for wav_file in wav_files:\n",
        "                        file_id = wav_file.replace('.wav', '')\n",
        "                        if dataset_type == 'diagnosis_train':\n",
        "                            label = 'ad' if subdir == 'ad' else 'cn'\n",
        "                        else:\n",
        "                            label = 'decline' if subdir == 'decline' else 'no_decline'\n",
        "\n",
        "                        labels[file_id] = {\n",
        "                            'dataset': dataset_type,\n",
        "                            'label': label,\n",
        "                            'file_path': os.path.join(subdir_path, wav_file)\n",
        "                        }\n",
        "\n",
        "    return labels\n",
        "\n",
        "def extract_mel_spectrogram(audio_path, n_mels=128, hop_length=512, n_fft=2048):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,\n",
        "                                                hop_length=hop_length, n_fft=n_fft)\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        return {\n",
        "            'mel_spectrogram': mel_spec,\n",
        "            'log_mel_spectrogram': log_mel_spec,\n",
        "            'mel_mean': np.mean(mel_spec, axis=1),\n",
        "            'mel_std': np.std(mel_spec, axis=1),\n",
        "            'log_mel_mean': np.mean(log_mel_spec, axis=1),\n",
        "            'log_mel_std': np.std(log_mel_spec, axis=1)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_acoustic_features(audio_path):\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=22050)\n",
        "        features = {}\n",
        "\n",
        "        features['mfcc'] = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13), axis=1)\n",
        "        features['chroma'] = np.mean(librosa.feature.chroma_stft(y=y, sr=sr), axis=1)\n",
        "        features['spectral_contrast'] = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr), axis=1)\n",
        "        features['tonnetz'] = np.mean(librosa.feature.tonnetz(y=y, sr=sr), axis=1)\n",
        "        features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "        features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "        features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "        features['rms_energy'] = np.mean(librosa.feature.rms(y=y))\n",
        "\n",
        "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "        features['tempo'] = tempo\n",
        "\n",
        "        return features\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting acoustic features for {audio_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_linguistic_features():\n",
        "    ling_features = {}\n",
        "    ling_path = '/content/drive/MyDrive/Speech/linguistic_features'\n",
        "\n",
        "    if os.path.exists(ling_path):\n",
        "        try:\n",
        "            with open(os.path.join(ling_path, 'linguistic_features.pkl'), 'rb') as f:\n",
        "                ling_features = pickle.load(f)\n",
        "        except:\n",
        "            try:\n",
        "                with open(os.path.join(ling_path, 'linguistic_features.json'), 'r') as f:\n",
        "                    ling_features = json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading linguistic features: {e}\")\n",
        "\n",
        "    return ling_features\n",
        "\n",
        "def load_transcripts():\n",
        "    transcripts = {}\n",
        "\n",
        "    transcript_files = [\n",
        "        '/content/drive/MyDrive/Speech/transcripts/all_categories_results.json',\n",
        "        '/content/drive/MyDrive/Speech/transcripts/transcription_results.json'\n",
        "    ]\n",
        "\n",
        "    for transcript_file in transcript_files:\n",
        "        if os.path.exists(transcript_file):\n",
        "            try:\n",
        "                with open(transcript_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        transcripts.update(data)\n",
        "                    elif isinstance(data, list):\n",
        "                        for item in data:\n",
        "                            if isinstance(item, dict) and len(item) >= 2:\n",
        "                                keys = list(item.keys())\n",
        "                                transcripts[keys[0]] = item[keys[1]]\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {transcript_file}: {e}\")\n",
        "\n",
        "    individual_transcript_path = '/content/drive/MyDrive/Speech/transcripts/individual_transcripts'\n",
        "    if os.path.exists(individual_transcript_path):\n",
        "        txt_files = [f for f in os.listdir(individual_transcript_path) if f.endswith('.txt')]\n",
        "        for txt_file in txt_files:\n",
        "            try:\n",
        "                file_id = txt_file.replace('.wav.txt', '')\n",
        "                with open(os.path.join(individual_transcript_path, txt_file), 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    transcripts[file_id] = content\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {txt_file}: {e}\")\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "def load_existing_features():\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "    features = {}\n",
        "\n",
        "    if os.path.exists(features_path):\n",
        "        for file in os.listdir(features_path):\n",
        "            if file.endswith('.pkl') and 'features' in file:\n",
        "                try:\n",
        "                    with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "                        if isinstance(data, dict):\n",
        "                            features.update(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_comprehensive_dataset():\n",
        "    print(\"Extracting labels from directory structure...\")\n",
        "    labels_dict = extract_labels_from_directory_structure()\n",
        "\n",
        "    print(\"Loading existing features...\")\n",
        "    existing_features = load_existing_features()\n",
        "\n",
        "    print(\"Loading linguistic features...\")\n",
        "    linguistic_features = load_linguistic_features()\n",
        "\n",
        "    print(\"Loading transcripts...\")\n",
        "    transcripts = load_transcripts()\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        row = {\n",
        "            'file_id': file_id,\n",
        "            'dataset': label_info['dataset'],\n",
        "            'label': label_info['label'],\n",
        "            'file_path': label_info['file_path']\n",
        "        }\n",
        "\n",
        "        if file_id in transcripts:\n",
        "            row['transcript'] = transcripts[file_id]\n",
        "        else:\n",
        "            row['transcript'] = ''\n",
        "\n",
        "        for category, features in existing_features.items():\n",
        "            if file_id in features:\n",
        "                feature_data = features[file_id]\n",
        "                if isinstance(feature_data, dict):\n",
        "                    for key, value in feature_data.items():\n",
        "                        row[f'{category}_{key}'] = value\n",
        "                else:\n",
        "                    row[f'{category}_features'] = feature_data\n",
        "\n",
        "        if file_id in linguistic_features:\n",
        "            ling_data = linguistic_features[file_id]\n",
        "            if isinstance(ling_data, dict):\n",
        "                for key, value in ling_data.items():\n",
        "                    row[f'linguistic_{key}'] = value\n",
        "\n",
        "        dataset.append(row)\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    print(f\"Dataset created with {len(df)} samples\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "    return df, labels_dict\n",
        "\n",
        "def extract_audio_features_batch(labels_dict, sample_limit=None):\n",
        "    print(\"Extracting mel-spectrograms and acoustic features...\")\n",
        "\n",
        "    audio_features = {}\n",
        "    processed_count = 0\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        if sample_limit and processed_count >= sample_limit:\n",
        "            break\n",
        "\n",
        "        audio_path = label_info['file_path']\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            print(f\"Processing {file_id}...\")\n",
        "\n",
        "            mel_features = extract_mel_spectrogram(audio_path)\n",
        "            acoustic_features = extract_acoustic_features(audio_path)\n",
        "\n",
        "            if mel_features and acoustic_features:\n",
        "                combined_features = {**mel_features, **acoustic_features}\n",
        "                audio_features[file_id] = combined_features\n",
        "                processed_count += 1\n",
        "        else:\n",
        "            print(f\"File not found: {audio_path}\")\n",
        "\n",
        "    return audio_features\n",
        "\n",
        "def create_training_datasets():\n",
        "    df, labels_dict = create_comprehensive_dataset()\n",
        "\n",
        "    diagnosis_train = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "    progression_train = df[df['dataset'] == 'progression_train'].copy()\n",
        "    progression_test = df[df['dataset'] == 'progression_test'].copy()\n",
        "\n",
        "    diagnosis_le = LabelEncoder()\n",
        "    if len(diagnosis_train) > 0 and 'label' in diagnosis_train.columns:\n",
        "        diagnosis_train = diagnosis_train.reset_index(drop=True)\n",
        "        diagnosis_train['label_encoded'] = diagnosis_le.fit_transform(diagnosis_train['label'])\n",
        "\n",
        "    progression_le = LabelEncoder()\n",
        "    if len(progression_train) > 0 and 'label' in progression_train.columns:\n",
        "        progression_train = progression_train.reset_index(drop=True)\n",
        "        progression_train['label_encoded'] = progression_le.fit_transform(progression_train['label'])\n",
        "\n",
        "    datasets = {\n",
        "        'diagnosis_train': diagnosis_train,\n",
        "        'progression_train': progression_train,\n",
        "        'progression_test': progression_test,\n",
        "        'all_data': df,\n",
        "        'labels_dict': labels_dict,\n",
        "        'diagnosis_label_encoder': diagnosis_le,\n",
        "        'progression_label_encoder': progression_le\n",
        "    }\n",
        "\n",
        "    print(\"\\nDataset Summary:\")\n",
        "    print(f\"Diagnosis Training: {len(diagnosis_train)} samples\")\n",
        "    if len(diagnosis_train) > 0:\n",
        "        print(f\"  - AD: {len(diagnosis_train[diagnosis_train['label'] == 'ad'])}\")\n",
        "        print(f\"  - CN: {len(diagnosis_train[diagnosis_train['label'] == 'cn'])}\")\n",
        "\n",
        "    print(f\"Progression Training: {len(progression_train)} samples\")\n",
        "    if len(progression_train) > 0:\n",
        "        print(f\"  - Decline: {len(progression_train[progression_train['label'] == 'decline'])}\")\n",
        "        print(f\"  - No Decline: {len(progression_train[progression_train['label'] == 'no_decline'])}\")\n",
        "\n",
        "    print(f\"Progression Test: {len(progression_test)} samples\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "def save_datasets(datasets, output_path='/content/drive/MyDrive/Speech/processed_datasets'):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    for name, data in datasets.items():\n",
        "        if isinstance(data, pd.DataFrame):\n",
        "            data.to_csv(os.path.join(output_path, f'{name}.csv'), index=False)\n",
        "            data.to_pickle(os.path.join(output_path, f'{name}.pkl'))\n",
        "        elif name == 'labels_dict':\n",
        "            with open(os.path.join(output_path, 'labels_dict.json'), 'w') as f:\n",
        "                json.dump(data, f, indent=2)\n",
        "        else:\n",
        "            with open(os.path.join(output_path, f'{name}.pkl'), 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "\n",
        "    print(f\"Datasets saved to {output_path}\")\n",
        "\n",
        "class SimpleMultiModalModel(nn.Module):\n",
        "    def __init__(self, acoustic_dim=50, text_dim=768, hidden_dim=512, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.acoustic_encoder = nn.Sequential(\n",
        "            nn.Linear(acoustic_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//4, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, acoustic_features=None, text_features=None):\n",
        "        features = []\n",
        "\n",
        "        if acoustic_features is not None:\n",
        "            acoustic_out = self.acoustic_encoder(acoustic_features)\n",
        "            features.append(acoustic_out)\n",
        "\n",
        "        if text_features is not None:\n",
        "            text_out = self.text_encoder(text_features)\n",
        "            features.append(text_out)\n",
        "\n",
        "        if len(features) == 0:\n",
        "            raise ValueError(\"At least one input required\")\n",
        "        elif len(features) == 1:\n",
        "            combined = features[0]\n",
        "        else:\n",
        "            combined = torch.cat(features, dim=1)\n",
        "\n",
        "        return self.fusion(combined)\n",
        "\n",
        "def load_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "\n",
        "    try:\n",
        "        diagnosis_train = pd.read_pickle(os.path.join(data_path, 'diagnosis_train.pkl'))\n",
        "        progression_train = pd.read_pickle(os.path.join(data_path, 'progression_train.pkl'))\n",
        "\n",
        "        with open(os.path.join(data_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "            diagnosis_le = pickle.load(f)\n",
        "        with open(os.path.join(data_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "            progression_le = pickle.load(f)\n",
        "    except:\n",
        "        datasets = create_training_datasets()\n",
        "        diagnosis_train = datasets['diagnosis_train']\n",
        "        progression_train = datasets['progression_train']\n",
        "        diagnosis_le = datasets['diagnosis_label_encoder']\n",
        "        progression_le = datasets['progression_label_encoder']\n",
        "        save_datasets(datasets)\n",
        "\n",
        "    features = {}\n",
        "    if os.path.exists(features_path):\n",
        "        for file in os.listdir(features_path):\n",
        "            if file.endswith('.pkl') and 'features' in file:\n",
        "                try:\n",
        "                    with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "                        if isinstance(data, dict):\n",
        "                            features.update(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return diagnosis_train, progression_train, diagnosis_le, progression_le, features\n",
        "\n",
        "def extract_bert_features(texts, model_name='bert-base-uncased', max_length=128):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            if pd.isna(text) or text == '':\n",
        "                embeddings.append(np.zeros(768))\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(str(text), return_tensors='pt', padding=True,\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "            embeddings.append(pooled)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def prepare_features(df, features_dict):\n",
        "    acoustic_features = []\n",
        "    valid_rows = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            acoustic_feat = []\n",
        "\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        acoustic_feat.extend(feat.flatten())\n",
        "\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        if len(val) > 0:\n",
        "                            acoustic_feat.append(np.mean(val))\n",
        "                        else:\n",
        "                            acoustic_feat.append(0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val if not np.isnan(val) else 0)\n",
        "\n",
        "            if len(acoustic_feat) == 0:\n",
        "                acoustic_feat = [0] * 50\n",
        "            elif len(acoustic_feat) > 100:\n",
        "                acoustic_feat = acoustic_feat[:100]\n",
        "\n",
        "            while len(acoustic_feat) < 100:\n",
        "                acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat)\n",
        "            valid_rows.append(row)\n",
        "\n",
        "    if len(valid_rows) == 0:\n",
        "        return np.array([]), pd.DataFrame()\n",
        "\n",
        "    return np.array(acoustic_features), pd.DataFrame(valid_rows).reset_index(drop=True)\n",
        "\n",
        "def train_ensemble_models(X_train, y_train, X_val, y_val):\n",
        "    if len(X_train) == 0 or len(y_train) == 0:\n",
        "        return {}, {}\n",
        "\n",
        "    models = {\n",
        "        'rf': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42,\n",
        "                                   class_weight='balanced', min_samples_split=5),\n",
        "        'gb': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42,\n",
        "                                       learning_rate=0.1),\n",
        "        'svm': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced',\n",
        "                  probability=True, random_state=42),\n",
        "        'lr': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    trained_models = {}\n",
        "    val_scores = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            model.fit(X_train, y_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "            val_acc = accuracy_score(y_val, val_pred)\n",
        "            trained_models[name] = model\n",
        "            val_scores[name] = val_acc\n",
        "            print(f\"{name.upper()} Validation Accuracy: {val_acc:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {name}: {e}\")\n",
        "\n",
        "    return trained_models, val_scores\n",
        "\n",
        "def ensemble_predict(models, X_test, weights=None):\n",
        "    if len(models) == 0 or len(X_test) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    if weights is None:\n",
        "        weights = [1] * len(models)\n",
        "\n",
        "    predictions = []\n",
        "    for model in models.values():\n",
        "        try:\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                pred_proba = model.predict_proba(X_test)\n",
        "                predictions.append(pred_proba)\n",
        "            else:\n",
        "                pred = model.predict(X_test)\n",
        "                n_classes = len(np.unique(pred))\n",
        "                pred_onehot = np.eye(n_classes)[pred]\n",
        "                predictions.append(pred_onehot)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in prediction: {e}\")\n",
        "\n",
        "    if len(predictions) == 0:\n",
        "        return np.zeros(len(X_test))\n",
        "\n",
        "    weighted_avg = np.average(predictions, axis=0, weights=weights[:len(predictions)])\n",
        "    final_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    return final_predictions\n",
        "\n",
        "def enhanced_feature_engineering(acoustic_features, text_embeddings):\n",
        "    if len(acoustic_features) == 0 or len(text_embeddings) == 0:\n",
        "        return np.array([]), None\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    acoustic_scaled = scaler.fit_transform(acoustic_features)\n",
        "\n",
        "    text_scaler = StandardScaler()\n",
        "    text_scaled = text_scaler.fit_transform(text_embeddings)\n",
        "\n",
        "    acoustic_stats = np.column_stack([\n",
        "        np.mean(acoustic_scaled, axis=1),\n",
        "        np.std(acoustic_scaled, axis=1),\n",
        "        np.max(acoustic_scaled, axis=1),\n",
        "        np.min(acoustic_scaled, axis=1)\n",
        "    ])\n",
        "\n",
        "    text_stats = np.column_stack([\n",
        "        np.mean(text_scaled, axis=1),\n",
        "        np.std(text_scaled, axis=1),\n",
        "        np.max(text_scaled, axis=1),\n",
        "        np.min(text_scaled, axis=1)\n",
        "    ])\n",
        "\n",
        "    combined_features = np.hstack([\n",
        "        acoustic_scaled,\n",
        "        text_scaled,\n",
        "        acoustic_stats,\n",
        "        text_stats\n",
        "    ])\n",
        "\n",
        "    return combined_features, scaler\n",
        "\n",
        "class HeavyAugmentationModel(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "def train_heavy_model(X_train, y_train, X_val, y_val, epochs=50):\n",
        "    if len(X_train) == 0 or len(y_train) == 0:\n",
        "        return None, 0.0\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = HeavyAugmentationModel(X_train.shape[1]).to(device)\n",
        "\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10)\n",
        "\n",
        "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "    y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        batch_size = min(32, len(X_train_tensor))\n",
        "        if batch_size == 0:\n",
        "            break\n",
        "\n",
        "        num_batches = max(1, len(X_train_tensor) // batch_size)\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, len(X_train_tensor))\n",
        "\n",
        "            if start_idx >= end_idx:\n",
        "                break\n",
        "\n",
        "            batch_X = X_train_tensor[start_idx:end_idx]\n",
        "            batch_y = y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "            if len(batch_X) == 0:\n",
        "                continue\n",
        "\n",
        "            noise = torch.randn_like(batch_X) * 0.01\n",
        "            batch_X = batch_X + noise\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if len(X_val_tensor) > 0:\n",
        "                val_outputs = model(X_val_tensor)\n",
        "                _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                val_acc = 100 * (val_predicted == y_val_tensor).sum().item() / len(y_val_tensor)\n",
        "            else:\n",
        "                val_acc = 0\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return model, best_acc / 100\n",
        "\n",
        "def generate_synthetic_samples(X, y, target_samples=200):\n",
        "    if len(X) == 0 or len(y) == 0:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    synthetic_X = []\n",
        "    synthetic_y = []\n",
        "\n",
        "    unique_classes = np.unique(y)\n",
        "    samples_per_class = target_samples // len(unique_classes)\n",
        "\n",
        "    for class_label in unique_classes:\n",
        "        class_samples = X[y == class_label]\n",
        "\n",
        "        if len(class_samples) < 3:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            nn_model = NearestNeighbors(n_neighbors=min(3, len(class_samples)))\n",
        "            nn_model.fit(class_samples)\n",
        "\n",
        "            for _ in range(samples_per_class):\n",
        "                base_idx = np.random.randint(0, len(class_samples))\n",
        "                base_sample = class_samples[base_idx]\n",
        "\n",
        "                distances, indices = nn_model.kneighbors([base_sample])\n",
        "                neighbor_idx = np.random.choice(indices[0])\n",
        "                neighbor_sample = class_samples[neighbor_idx]\n",
        "\n",
        "                alpha = np.random.uniform(0.2, 0.8)\n",
        "                synthetic_sample = alpha * base_sample + (1 - alpha) * neighbor_sample\n",
        "\n",
        "                noise = np.random.normal(0, 0.01, synthetic_sample.shape)\n",
        "                synthetic_sample += noise\n",
        "\n",
        "                synthetic_X.append(synthetic_sample)\n",
        "                synthetic_y.append(class_label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating synthetic samples for class {class_label}: {e}\")\n",
        "\n",
        "    return np.array(synthetic_X), np.array(synthetic_y)\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    try:\n",
        "        diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Processing diagnosis task...\")\n",
        "    diag_acoustic, diag_df_filtered = prepare_features(diagnosis_train, features_dict)\n",
        "\n",
        "    if len(diag_df_filtered) == 0:\n",
        "        print(\"No valid diagnosis samples found\")\n",
        "        return\n",
        "\n",
        "    diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "\n",
        "    if 'label_encoded' not in diag_df_filtered.columns:\n",
        "        if 'label' in diag_df_filtered.columns:\n",
        "            diag_df_filtered['label_encoded'] = diagnosis_le.fit_transform(diag_df_filtered['label'])\n",
        "        else:\n",
        "            print(\"No labels found for diagnosis data\")\n",
        "            return\n",
        "\n",
        "    diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for diagnosis...\")\n",
        "    diag_text_features = extract_bert_features(diag_texts)\n",
        "\n",
        "    print(\"Engineering features for diagnosis...\")\n",
        "    diag_enhanced_features, diag_scaler = enhanced_feature_engineering(diag_acoustic, diag_text_features)\n",
        "\n",
        "    if len(diag_enhanced_features) == 0:\n",
        "        print(\"No features extracted for diagnosis\")\n",
        "        return\n",
        "\n",
        "    X_diag_train, X_diag_val, y_diag_train, y_diag_val = train_test_split(\n",
        "        diag_enhanced_features, diag_labels, test_size=0.2, random_state=42,\n",
        "        stratify=diag_labels if len(np.unique(diag_labels)) > 1 else None\n",
        "    )\n",
        "\n",
        "    print(\"Training diagnosis ensemble...\")\n",
        "    diag_models, diag_scores = train_ensemble_models(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "\n",
        "    if len(diag_models) == 0:\n",
        "        print(\"No diagnosis models trained successfully\")\n",
        "        return\n",
        "\n",
        "    best_diag_models = {k: v for k, v in diag_models.items() if diag_scores[k] > 0.6}\n",
        "    if not best_diag_models:\n",
        "        best_diag_models = diag_models\n",
        "\n",
        "    diag_ensemble_pred = ensemble_predict(best_diag_models, X_diag_val)\n",
        "    diag_ensemble_acc = accuracy_score(y_diag_val, diag_ensemble_pred) if len(diag_ensemble_pred) > 0 else 0\n",
        "\n",
        "    print(f\"\\nDiagnosis Ensemble Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    if len(diag_ensemble_pred) > 0:\n",
        "        print(\"Diagnosis Classification Report:\")\n",
        "        print(classification_report(y_diag_val, diag_ensemble_pred, target_names=diagnosis_le.classes_))\n",
        "\n",
        "    print(\"\\nProcessing progression task...\")\n",
        "    prog_acoustic, prog_df_filtered = prepare_features(progression_train, features_dict)\n",
        "\n",
        "    if len(prog_df_filtered) == 0:\n",
        "        print(\"No valid progression samples found\")\n",
        "        return\n",
        "\n",
        "    prog_texts = prog_df_filtered['transcript'].fillna('').tolist()\n",
        "\n",
        "    if 'label_encoded' not in prog_df_filtered.columns:\n",
        "        if 'label' in prog_df_filtered.columns:\n",
        "            prog_df_filtered['label_encoded'] = progression_le.fit_transform(prog_df_filtered['label'])\n",
        "        else:\n",
        "            print(\"No labels found for progression data\")\n",
        "            return\n",
        "\n",
        "    prog_labels = prog_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for progression...\")\n",
        "    prog_text_features = extract_bert_features(prog_texts)\n",
        "\n",
        "    print(\"Engineering features for progression...\")\n",
        "    prog_enhanced_features, prog_scaler = enhanced_feature_engineering(prog_acoustic, prog_text_features)\n",
        "\n",
        "    if len(prog_enhanced_features) == 0:\n",
        "        print(\"No features extracted for progression\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        synth_X_prog, synth_y_prog = generate_synthetic_samples(prog_enhanced_features, prog_labels)\n",
        "        if len(synth_X_prog) > 0:\n",
        "            prog_enhanced_balanced = np.vstack([prog_enhanced_features, synth_X_prog])\n",
        "            prog_labels_balanced = np.hstack([prog_labels, synth_y_prog])\n",
        "        else:\n",
        "            prog_enhanced_balanced, prog_labels_balanced = prog_enhanced_features, prog_labels\n",
        "    except Exception as e:\n",
        "        print(f\"Synthetic sampling failed: {e}\")\n",
        "        prog_enhanced_balanced, prog_labels_balanced = prog_enhanced_features, prog_labels\n",
        "\n",
        "    X_prog_train, X_prog_val, y_prog_train, y_prog_val = train_test_split(\n",
        "        prog_enhanced_balanced, prog_labels_balanced, test_size=0.2, random_state=42,\n",
        "        stratify=prog_labels_balanced if len(np.unique(prog_labels_balanced)) > 1 else None\n",
        "    )\n",
        "\n",
        "    print(\"Training progression ensemble...\")\n",
        "    prog_models, prog_scores = train_ensemble_models(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "\n",
        "    if len(prog_models) == 0:\n",
        "        print(\"No progression models trained successfully\")\n",
        "        return\n",
        "\n",
        "    best_prog_models = {k: v for k, v in prog_models.items() if prog_scores[k] > 0.6}\n",
        "    if not best_prog_models:\n",
        "        best_prog_models = prog_models\n",
        "\n",
        "    prog_ensemble_pred = ensemble_predict(best_prog_models, X_prog_val)\n",
        "    prog_ensemble_acc = accuracy_score(y_prog_val, prog_ensemble_pred) if len(prog_ensemble_pred) > 0 else 0\n",
        "\n",
        "    print(f\"\\nProgression Ensemble Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "    if len(prog_ensemble_pred) > 0:\n",
        "        print(\"Progression Classification Report:\")\n",
        "        print(classification_report(y_prog_val, prog_ensemble_pred, target_names=progression_le.classes_))\n",
        "\n",
        "    if diag_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for diagnosis...\")\n",
        "        try:\n",
        "            selector = SelectKBest(f_classif, k=min(200, X_diag_train.shape[1]//2))\n",
        "            X_diag_train_selected = selector.fit_transform(X_diag_train, y_diag_train)\n",
        "            X_diag_val_selected = selector.transform(X_diag_val)\n",
        "\n",
        "            diag_models_v2, diag_scores_v2 = train_ensemble_models(X_diag_train_selected, y_diag_train, X_diag_val_selected, y_diag_val)\n",
        "            diag_ensemble_pred_v2 = ensemble_predict(diag_models_v2, X_diag_val_selected)\n",
        "            diag_ensemble_acc_v2 = accuracy_score(y_diag_val, diag_ensemble_pred_v2) if len(diag_ensemble_pred_v2) > 0 else 0\n",
        "\n",
        "            print(f\"Diagnosis Improved Accuracy: {diag_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "            if diag_ensemble_acc_v2 > diag_ensemble_acc:\n",
        "                diag_ensemble_acc = diag_ensemble_acc_v2\n",
        "                diag_ensemble_pred = diag_ensemble_pred_v2\n",
        "        except Exception as e:\n",
        "            print(f\"Feature selection failed for diagnosis: {e}\")\n",
        "\n",
        "    if prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying advanced feature selection for progression...\")\n",
        "        try:\n",
        "            selector = SelectKBest(f_classif, k=min(150, X_prog_train.shape[1]//2))\n",
        "            X_prog_train_selected = selector.fit_transform(X_prog_train, y_prog_train)\n",
        "            X_prog_val_selected = selector.transform(X_prog_val)\n",
        "\n",
        "            prog_models_v2, prog_scores_v2 = train_ensemble_models(X_prog_train_selected, y_prog_train, X_prog_val_selected, y_prog_val)\n",
        "            prog_ensemble_pred_v2 = ensemble_predict(prog_models_v2, X_prog_val_selected)\n",
        "            prog_ensemble_acc_v2 = accuracy_score(y_prog_val, prog_ensemble_pred_v2) if len(prog_ensemble_pred_v2) > 0 else 0\n",
        "\n",
        "            print(f\"Progression Improved Accuracy: {prog_ensemble_acc_v2:.4f}\")\n",
        "\n",
        "            if prog_ensemble_acc_v2 > prog_ensemble_acc:\n",
        "                prog_ensemble_acc = prog_ensemble_acc_v2\n",
        "                prog_ensemble_pred = prog_ensemble_pred_v2\n",
        "        except Exception as e:\n",
        "            print(f\"Feature selection failed for progression: {e}\")\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"Diagnosis Accuracy: {diag_ensemble_acc:.4f}\")\n",
        "    print(f\"Progression Accuracy: {prog_ensemble_acc:.4f}\")\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying synthetic data generation...\")\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            try:\n",
        "                synth_X_diag, synth_y_diag = generate_synthetic_samples(X_diag_train, y_diag_train)\n",
        "                if len(synth_X_diag) > 0:\n",
        "                    X_diag_augmented = np.vstack([X_diag_train, synth_X_diag])\n",
        "                    y_diag_augmented = np.hstack([y_diag_train, synth_y_diag])\n",
        "\n",
        "                    diag_models_synth, _ = train_ensemble_models(X_diag_augmented, y_diag_augmented, X_diag_val, y_diag_val)\n",
        "                    diag_synth_pred = ensemble_predict(diag_models_synth, X_diag_val)\n",
        "                    diag_synth_acc = accuracy_score(y_diag_val, diag_synth_pred) if len(diag_synth_pred) > 0 else 0\n",
        "\n",
        "                    print(f\"Diagnosis with Synthetic Data: {diag_synth_acc:.4f}\")\n",
        "\n",
        "                    if diag_synth_acc > diag_ensemble_acc:\n",
        "                        diag_ensemble_acc = diag_synth_acc\n",
        "                        diag_ensemble_pred = diag_synth_pred\n",
        "            except Exception as e:\n",
        "                print(f\"Synthetic data generation failed for diagnosis: {e}\")\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            try:\n",
        "                synth_X_prog, synth_y_prog = generate_synthetic_samples(X_prog_train, y_prog_train)\n",
        "                if len(synth_X_prog) > 0:\n",
        "                    X_prog_augmented = np.vstack([X_prog_train, synth_X_prog])\n",
        "                    y_prog_augmented = np.hstack([y_prog_train, synth_y_prog])\n",
        "\n",
        "                    prog_models_synth, _ = train_ensemble_models(X_prog_augmented, y_prog_augmented, X_prog_val, y_prog_val)\n",
        "                    prog_synth_pred = ensemble_predict(prog_models_synth, X_prog_val)\n",
        "                    prog_synth_acc = accuracy_score(y_prog_val, prog_synth_pred) if len(prog_synth_pred) > 0 else 0\n",
        "\n",
        "                    print(f\"Progression with Synthetic Data: {prog_synth_acc:.4f}\")\n",
        "\n",
        "                    if prog_synth_acc > prog_ensemble_acc:\n",
        "                        prog_ensemble_acc = prog_synth_acc\n",
        "                        prog_ensemble_pred = prog_synth_pred\n",
        "            except Exception as e:\n",
        "                print(f\"Synthetic data generation failed for progression: {e}\")\n",
        "\n",
        "    if diag_ensemble_acc < 0.85 or prog_ensemble_acc < 0.85:\n",
        "        print(\"\\nApplying neural network with heavy augmentation...\")\n",
        "\n",
        "        if diag_ensemble_acc < 0.85:\n",
        "            try:\n",
        "                print(\"Training heavy diagnosis model...\")\n",
        "                diag_heavy_model, diag_heavy_acc = train_heavy_model(X_diag_train, y_diag_train, X_diag_val, y_diag_val)\n",
        "                print(f\"Heavy Diagnosis Model Accuracy: {diag_heavy_acc:.4f}\")\n",
        "\n",
        "                if diag_heavy_acc > diag_ensemble_acc:\n",
        "                    diag_ensemble_acc = diag_heavy_acc\n",
        "            except Exception as e:\n",
        "                print(f\"Heavy model training failed for diagnosis: {e}\")\n",
        "\n",
        "        if prog_ensemble_acc < 0.85:\n",
        "            try:\n",
        "                print(\"Training heavy progression model...\")\n",
        "                prog_heavy_model, prog_heavy_acc = train_heavy_model(X_prog_train, y_prog_train, X_prog_val, y_prog_val)\n",
        "                print(f\"Heavy Progression Model Accuracy: {prog_heavy_acc:.4f}\")\n",
        "\n",
        "                if prog_heavy_acc > prog_ensemble_acc:\n",
        "                    prog_ensemble_acc = prog_heavy_acc\n",
        "            except Exception as e:\n",
        "                print(f\"Heavy model training failed for progression: {e}\")\n",
        "\n",
        "    print(f\"\\n=== FINAL RESULTS ===\")\n",
        "    print(f\"Diagnosis Task Accuracy: {diag_ensemble_acc:.4f} ({diag_ensemble_acc*100:.1f}%)\")\n",
        "    print(f\"Progression Task Accuracy: {prog_ensemble_acc:.4f} ({prog_ensemble_acc*100:.1f}%)\")\n",
        "\n",
        "    if diag_ensemble_acc >= 0.85 and prog_ensemble_acc >= 0.85:\n",
        "        print(\"SUCCESS: Both models achieved 85%+ accuracy!\")\n",
        "    else:\n",
        "        print(\"Still optimizing...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        datasets = create_training_datasets()\n",
        "        save_datasets(datasets)\n",
        "\n",
        "        print(\"\\nExtracting audio features for a sample...\")\n",
        "        sample_audio_features = extract_audio_features_batch(datasets['labels_dict'], sample_limit=5)\n",
        "\n",
        "        if sample_audio_features:\n",
        "            print(f\"Successfully extracted features for {len(sample_audio_features)} audio files\")\n",
        "            sample_id = list(sample_audio_features.keys())[0]\n",
        "            sample_features = sample_audio_features[sample_id]\n",
        "            print(f\"Feature keys for {sample_id}: {list(sample_features.keys())}\")\n",
        "\n",
        "        print(\"\\nProcessing complete!\")\n",
        "        print(\"Available datasets:\")\n",
        "        for name in datasets.keys():\n",
        "            if isinstance(datasets[name], pd.DataFrame):\n",
        "                print(f\"  - {name}: {len(datasets[name])} samples\")\n",
        "\n",
        "        main()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6urpOQMo59_"
      },
      "source": [
        "# 5- EVAl 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OgBSl-ZTo3Q5",
        "outputId": "70753905-71be-476b-b2be-2390da21c76b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "DIAGNOSIS TASK EVALUATION\n",
            "==================================================\n",
            "Extracting BERT features for diagnosis...\n",
            "Engineering features for diagnosis...\n",
            "\n",
            "==================================================\n",
            "DIAGNOSIS - COMPREHENSIVE EVALUATION\n",
            "==================================================\n",
            "\n",
            "CROSS-VALIDATION METRICS:\n",
            "Accuracy: 0.9517  0.0365\n",
            "MCC:      0.9062  0.0707\n",
            "AUC-ROC:  0.9855  0.0100\n",
            "\n",
            "OVERALL PERFORMANCE:\n",
            "Overall MCC:     0.9042\n",
            "Overall AUC-ROC: 0.9827\n",
            "\n",
            "CONFUSION MATRIX:\n",
            "[[85  2]\n",
            " [ 6 73]]\n",
            "\n",
            "INDIVIDUAL MODEL PERFORMANCE:\n",
            "RandomForest:\n",
            "  Accuracy: 0.8373  0.0682\n",
            "  MCC:      0.6763  0.1379\n",
            "  AUC-ROC:  0.9094  0.0537\n",
            "GradientBoosting:\n",
            "  Accuracy: 0.6444  0.0458\n",
            "  MCC:      0.2883  0.0952\n",
            "  AUC-ROC:  0.7135  0.0463\n",
            "SVM:\n",
            "  Accuracy: 0.9578  0.0310\n",
            "  MCC:      0.9175  0.0610\n",
            "  AUC-ROC:  0.9876  0.0125\n",
            "LogisticRegression:\n",
            "  Accuracy: 0.9820  0.0147\n",
            "  MCC:      0.9649  0.0287\n",
            "  AUC-ROC:  1.0000  0.0000\n",
            "\n",
            "TOP 10 DIAGNOSIS FEATURES:\n",
            "text_740: 0.9987\n",
            "text_563: 0.6684\n",
            "text_494: 0.5395\n",
            "text_366: 0.4976\n",
            "text_343: 0.4733\n",
            "text_759: 0.4392\n",
            "text_757: 0.4349\n",
            "text_128: 0.4322\n",
            "text_674: 0.4208\n",
            "text_102: 0.4075\n",
            "\n",
            "Diagnosis - OVERFITTING ANALYSIS:\n",
            "RandomForest:\n",
            "  Train Acc: 1.0000, Val Acc: 0.7800, Gap: 0.2200\n",
            "  Train MCC: 1.0000, Val MCC: 0.5613, Gap: 0.4387\n",
            "  WARNING: Potential overfitting detected!\n",
            "GradientBoosting:\n",
            "  Train Acc: 1.0000, Val Acc: 0.6000, Gap: 0.4000\n",
            "  Train MCC: 1.0000, Val MCC: 0.1968, Gap: 0.8032\n",
            "  WARNING: Potential overfitting detected!\n",
            "SVM:\n",
            "  Train Acc: 1.0000, Val Acc: 0.9400, Gap: 0.0600\n",
            "  Train MCC: 1.0000, Val MCC: 0.8803, Gap: 0.1197\n",
            "  WARNING: Potential overfitting detected!\n",
            "LogisticRegression:\n",
            "  Train Acc: 1.0000, Val Acc: 0.9800, Gap: 0.0200\n",
            "  Train MCC: 1.0000, Val MCC: 0.9606, Gap: 0.0394\n",
            "\n",
            "\n",
            "PROGRESSION TASK EVALUATION\n",
            "==================================================\n",
            "Extracting BERT features for progression...\n",
            "Engineering features for progression...\n",
            "\n",
            "==================================================\n",
            "PROGRESSION - COMPREHENSIVE EVALUATION\n",
            "==================================================\n",
            "\n",
            "CROSS-VALIDATION METRICS:\n",
            "Accuracy: 0.9029  0.0563\n",
            "MCC:      0.6762  0.1891\n",
            "AUC-ROC:  0.8520  0.2081\n",
            "\n",
            "OVERALL PERFORMANCE:\n",
            "Overall MCC:     0.6899\n",
            "Overall AUC-ROC: 0.8379\n",
            "\n",
            "CONFUSION MATRIX:\n",
            "[[ 8  7]\n",
            " [ 0 58]]\n",
            "\n",
            "INDIVIDUAL MODEL PERFORMANCE:\n",
            "RandomForest:\n",
            "  Accuracy: 0.8219  0.0332\n",
            "  MCC:      0.2131  0.2610\n",
            "  AUC-ROC:  0.8404  0.1799\n",
            "GradientBoosting:\n",
            "  Accuracy: 0.7257  0.0625\n",
            "  MCC:      0.0640  0.1645\n",
            "  AUC-ROC:  0.6419  0.1185\n",
            "SVM:\n",
            "  Accuracy: 0.8762  0.0289\n",
            "  MCC:      0.5831  0.1007\n",
            "  AUC-ROC:  0.8697  0.1820\n",
            "LogisticRegression:\n",
            "  Accuracy: 0.9305  0.0618\n",
            "  MCC:      0.7695  0.2090\n",
            "  AUC-ROC:  0.9293  0.0960\n",
            "\n",
            "TOP 10 PROGRESSION FEATURES:\n",
            "text_64: 0.5960\n",
            "text_263: 0.5674\n",
            "text_130: 0.5480\n",
            "text_568: 0.4207\n",
            "text_167: 0.4062\n",
            "text_223: 0.4005\n",
            "text_709: 0.3633\n",
            "text_653: 0.3614\n",
            "text_510: 0.3470\n",
            "text_49: 0.3405\n",
            "\n",
            "Progression - OVERFITTING ANALYSIS:\n",
            "RandomForest:\n",
            "  Train Acc: 1.0000, Val Acc: 0.7727, Gap: 0.2273\n",
            "  Train MCC: 1.0000, Val MCC: 0.0000, Gap: 1.0000\n",
            "  WARNING: Potential overfitting detected!\n",
            "GradientBoosting:\n",
            "  Train Acc: 1.0000, Val Acc: 0.8636, Gap: 0.1364\n",
            "  Train MCC: 1.0000, Val MCC: 0.5880, Gap: 0.4120\n",
            "  WARNING: Potential overfitting detected!\n",
            "SVM:\n",
            "  Train Acc: 1.0000, Val Acc: 0.8182, Gap: 0.1818\n",
            "  Train MCC: 1.0000, Val MCC: 0.4024, Gap: 0.5976\n",
            "  WARNING: Potential overfitting detected!\n",
            "LogisticRegression:\n",
            "  Train Acc: 1.0000, Val Acc: 0.9545, Gap: 0.0455\n",
            "  Train MCC: 1.0000, Val MCC: 0.8692, Gap: 0.1308\n",
            "  WARNING: Potential overfitting detected!\n",
            "\n",
            "\n",
            "STABILITY ANALYSIS\n",
            "==================================================\n",
            "Diagnosis Stability (n=10 iterations):\n",
            "  Accuracy: 0.7875  0.0868\n",
            "  MCC:      0.5851  0.1782\n",
            "  AUC:      0.8951  0.0676\n",
            "  Feature Correlation: 0.5719  0.0490\n",
            "\n",
            "Progression Stability (n=10 iterations):\n",
            "  Accuracy: 0.8333  0.0351\n",
            "  MCC:      0.3828  0.2225\n",
            "  AUC:      0.8028  0.1188\n",
            "  Feature Correlation: 0.3464  0.0777\n",
            "Critical error: 'fold_0'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-27338572.py\", line 1042, in <cell line: 0>\n",
            "    results = main_evaluation()\n",
            "              ^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-27338572.py\", line 933, in main_evaluation\n",
            "    plot_evaluation_results(diag_cv_results, diagnosis_le)\n",
            "  File \"/tmp/ipython-input-27338572.py\", line 446, in plot_evaluation_results\n",
            "    fold_accuracies = [results['fold_results'][f'fold_{i}']['accuracy'] for i in range(len(results['fold_results']))]\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
            "KeyError: 'fold_0'\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1200 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, matthews_corrcoef, roc_auc_score, roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.inspection import permutation_importance\n",
        "from transformers import BertTokenizer, BertModel, Wav2Vec2Processor, Wav2Vec2Model\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_directory = '/content/drive/MyDrive/Speech'\n",
        "\n",
        "def extract_labels_from_directory_structure():\n",
        "    base_paths = {\n",
        "        'diagnosis_train': '/content/drive/MyDrive/Speech/extracted_diagnosis_train/ADReSSo21/diagnosis/train/audio',\n",
        "        'progression_train': '/content/drive/MyDrive/Speech/extracted_progression_train/ADReSSo21/progression/train/audio',\n",
        "        'progression_test': '/content/drive/MyDrive/Speech/extracted_progression_test/ADReSSo21/progression/test-dist/audio'\n",
        "    }\n",
        "\n",
        "    labels = {}\n",
        "\n",
        "    for dataset_type, base_path in base_paths.items():\n",
        "        if not os.path.exists(base_path):\n",
        "            continue\n",
        "\n",
        "        if dataset_type == 'progression_test':\n",
        "            files = os.listdir(base_path)\n",
        "            wav_files = [f for f in files if f.endswith('.wav')]\n",
        "            for wav_file in wav_files:\n",
        "                file_id = wav_file.replace('.wav', '')\n",
        "                labels[file_id] = {\n",
        "                    'dataset': 'progression_test',\n",
        "                    'label': 'unknown',\n",
        "                    'file_path': os.path.join(base_path, wav_file)\n",
        "                }\n",
        "        else:\n",
        "            subdirs = os.listdir(base_path)\n",
        "            for subdir in subdirs:\n",
        "                subdir_path = os.path.join(base_path, subdir)\n",
        "                if os.path.isdir(subdir_path):\n",
        "                    wav_files = [f for f in os.listdir(subdir_path) if f.endswith('.wav')]\n",
        "                    for wav_file in wav_files:\n",
        "                        file_id = wav_file.replace('.wav', '')\n",
        "                        if dataset_type == 'diagnosis_train':\n",
        "                            label = 'ad' if subdir == 'ad' else 'cn'\n",
        "                        else:\n",
        "                            label = 'decline' if subdir == 'decline' else 'no_decline'\n",
        "\n",
        "                        labels[file_id] = {\n",
        "                            'dataset': dataset_type,\n",
        "                            'label': label,\n",
        "                            'file_path': os.path.join(subdir_path, wav_file)\n",
        "                        }\n",
        "\n",
        "    return labels\n",
        "\n",
        "def load_linguistic_features():\n",
        "    ling_features = {}\n",
        "    ling_path = '/content/drive/MyDrive/Speech/linguistic_features'\n",
        "\n",
        "    if os.path.exists(ling_path):\n",
        "        try:\n",
        "            with open(os.path.join(ling_path, 'linguistic_features.pkl'), 'rb') as f:\n",
        "                ling_features = pickle.load(f)\n",
        "        except:\n",
        "            try:\n",
        "                with open(os.path.join(ling_path, 'linguistic_features.json'), 'r') as f:\n",
        "                    ling_features = json.load(f)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading linguistic features: {e}\")\n",
        "\n",
        "    return ling_features\n",
        "\n",
        "def load_transcripts():\n",
        "    transcripts = {}\n",
        "\n",
        "    transcript_files = [\n",
        "        '/content/drive/MyDrive/Speech/transcripts/all_categories_results.json',\n",
        "        '/content/drive/MyDrive/Speech/transcripts/transcription_results.json'\n",
        "    ]\n",
        "\n",
        "    for transcript_file in transcript_files:\n",
        "        if os.path.exists(transcript_file):\n",
        "            try:\n",
        "                with open(transcript_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        transcripts.update(data)\n",
        "                    elif isinstance(data, list):\n",
        "                        for item in data:\n",
        "                            if isinstance(item, dict) and len(item) >= 2:\n",
        "                                keys = list(item.keys())\n",
        "                                transcripts[keys[0]] = item[keys[1]]\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {transcript_file}: {e}\")\n",
        "\n",
        "    individual_transcript_path = '/content/drive/MyDrive/Speech/transcripts/individual_transcripts'\n",
        "    if os.path.exists(individual_transcript_path):\n",
        "        txt_files = [f for f in os.listdir(individual_transcript_path) if f.endswith('.txt')]\n",
        "        for txt_file in txt_files:\n",
        "            try:\n",
        "                file_id = txt_file.replace('.wav.txt', '')\n",
        "                with open(os.path.join(individual_transcript_path, txt_file), 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    transcripts[file_id] = content\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {txt_file}: {e}\")\n",
        "\n",
        "    return transcripts\n",
        "\n",
        "def load_existing_features():\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "    features = {}\n",
        "\n",
        "    if os.path.exists(features_path):\n",
        "        for file in os.listdir(features_path):\n",
        "            if file.endswith('.pkl') and 'features' in file:\n",
        "                try:\n",
        "                    with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "                        if isinstance(data, dict):\n",
        "                            features.update(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_comprehensive_dataset():\n",
        "    labels_dict = extract_labels_from_directory_structure()\n",
        "    existing_features = load_existing_features()\n",
        "    linguistic_features = load_linguistic_features()\n",
        "    transcripts = load_transcripts()\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    for file_id, label_info in labels_dict.items():\n",
        "        row = {\n",
        "            'file_id': file_id,\n",
        "            'dataset': label_info['dataset'],\n",
        "            'label': label_info['label'],\n",
        "            'file_path': label_info['file_path']\n",
        "        }\n",
        "\n",
        "        if file_id in transcripts:\n",
        "            row['transcript'] = transcripts[file_id]\n",
        "        else:\n",
        "            row['transcript'] = ''\n",
        "\n",
        "        for category, features in existing_features.items():\n",
        "            if file_id in features:\n",
        "                feature_data = features[file_id]\n",
        "                if isinstance(feature_data, dict):\n",
        "                    for key, value in feature_data.items():\n",
        "                        row[f'{category}_{key}'] = value\n",
        "                else:\n",
        "                    row[f'{category}_features'] = feature_data\n",
        "\n",
        "        if file_id in linguistic_features:\n",
        "            ling_data = linguistic_features[file_id]\n",
        "            if isinstance(ling_data, dict):\n",
        "                for key, value in ling_data.items():\n",
        "                    row[f'linguistic_{key}'] = value\n",
        "\n",
        "        dataset.append(row)\n",
        "\n",
        "    df = pd.DataFrame(dataset)\n",
        "    return df, labels_dict\n",
        "\n",
        "def load_data():\n",
        "    data_path = '/content/drive/MyDrive/Speech/processed_datasets'\n",
        "\n",
        "    try:\n",
        "        diagnosis_train = pd.read_pickle(os.path.join(data_path, 'diagnosis_train.pkl'))\n",
        "        progression_train = pd.read_pickle(os.path.join(data_path, 'progression_train.pkl'))\n",
        "\n",
        "        with open(os.path.join(data_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "            diagnosis_le = pickle.load(f)\n",
        "        with open(os.path.join(data_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "            progression_le = pickle.load(f)\n",
        "    except:\n",
        "        df, labels_dict = create_comprehensive_dataset()\n",
        "        diagnosis_train = df[df['dataset'] == 'diagnosis_train'].copy()\n",
        "        progression_train = df[df['dataset'] == 'progression_train'].copy()\n",
        "\n",
        "        diagnosis_le = LabelEncoder()\n",
        "        if len(diagnosis_train) > 0:\n",
        "            diagnosis_train['label_encoded'] = diagnosis_le.fit_transform(diagnosis_train['label'])\n",
        "\n",
        "        progression_le = LabelEncoder()\n",
        "        if len(progression_train) > 0:\n",
        "            progression_train['label_encoded'] = progression_le.fit_transform(progression_train['label'])\n",
        "\n",
        "    features_path = '/content/drive/MyDrive/Speech/lightweight_features'\n",
        "    features = {}\n",
        "    if os.path.exists(features_path):\n",
        "        for file in os.listdir(features_path):\n",
        "            if file.endswith('.pkl') and 'features' in file:\n",
        "                try:\n",
        "                    with open(os.path.join(features_path, file), 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "                        if isinstance(data, dict):\n",
        "                            features.update(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    return diagnosis_train, progression_train, diagnosis_le, progression_le, features\n",
        "\n",
        "def extract_bert_features(texts, model_name='bert-base-uncased', max_length=128):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            if pd.isna(text) or text == '':\n",
        "                embeddings.append(np.zeros(768))\n",
        "                continue\n",
        "\n",
        "            inputs = tokenizer(str(text), return_tensors='pt', padding=True,\n",
        "                             truncation=True, max_length=max_length)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            pooled = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "            embeddings.append(pooled)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def prepare_features(df, features_dict):\n",
        "    acoustic_features = []\n",
        "    valid_rows = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            acoustic_feat = []\n",
        "\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        acoustic_feat.extend(feat.flatten())\n",
        "\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        if len(val) > 0:\n",
        "                            acoustic_feat.append(np.mean(val))\n",
        "                        else:\n",
        "                            acoustic_feat.append(0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val if not np.isnan(val) else 0)\n",
        "\n",
        "            if len(acoustic_feat) == 0:\n",
        "                acoustic_feat = [0] * 50\n",
        "            elif len(acoustic_feat) > 100:\n",
        "                acoustic_feat = acoustic_feat[:100]\n",
        "\n",
        "            while len(acoustic_feat) < 100:\n",
        "                acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat)\n",
        "            valid_rows.append(row)\n",
        "\n",
        "    if len(valid_rows) == 0:\n",
        "        return np.array([]), pd.DataFrame()\n",
        "\n",
        "    return np.array(acoustic_features), pd.DataFrame(valid_rows).reset_index(drop=True)\n",
        "\n",
        "def enhanced_feature_engineering(acoustic_features, text_embeddings):\n",
        "    if len(acoustic_features) == 0 or len(text_embeddings) == 0:\n",
        "        return np.array([]), None\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    acoustic_scaled = scaler.fit_transform(acoustic_features)\n",
        "\n",
        "    text_scaler = StandardScaler()\n",
        "    text_scaled = text_scaler.fit_transform(text_embeddings)\n",
        "\n",
        "    acoustic_stats = np.column_stack([\n",
        "        np.mean(acoustic_scaled, axis=1),\n",
        "        np.std(acoustic_scaled, axis=1),\n",
        "        np.max(acoustic_scaled, axis=1),\n",
        "        np.min(acoustic_scaled, axis=1)\n",
        "    ])\n",
        "\n",
        "    text_stats = np.column_stack([\n",
        "        np.mean(text_scaled, axis=1),\n",
        "        np.std(text_scaled, axis=1),\n",
        "        np.max(text_scaled, axis=1),\n",
        "        np.min(text_scaled, axis=1)\n",
        "    ])\n",
        "\n",
        "    combined_features = np.hstack([\n",
        "        acoustic_scaled,\n",
        "        text_scaled,\n",
        "        acoustic_stats,\n",
        "        text_stats\n",
        "    ])\n",
        "\n",
        "    return combined_features, scaler\n",
        "\n",
        "def comprehensive_model_evaluation(X, y, model_class, model_params, cv_folds=5, task_name=\"\"):\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    cv_scores = []\n",
        "    cv_mcc_scores = []\n",
        "    cv_auc_scores = []\n",
        "    feature_importances = []\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "    all_y_proba = []\n",
        "\n",
        "    fold_results = {}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        model = model_class(**model_params)\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        y_pred_fold = model.predict(X_val_fold)\n",
        "        y_proba_fold = model.predict_proba(X_val_fold)[:, 1] if hasattr(model, 'predict_proba') else y_pred_fold\n",
        "\n",
        "        acc = accuracy_score(y_val_fold, y_pred_fold)\n",
        "        mcc = matthews_corrcoef(y_val_fold, y_pred_fold)\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_val_fold, y_proba_fold)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "\n",
        "        cv_scores.append(acc)\n",
        "        cv_mcc_scores.append(mcc)\n",
        "        cv_auc_scores.append(auc)\n",
        "\n",
        "        all_y_true.extend(y_val_fold)\n",
        "        all_y_pred.extend(y_pred_fold)\n",
        "        all_y_proba.extend(y_proba_fold)\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            feature_importances.append(model.feature_importances_)\n",
        "\n",
        "        fold_results[f'fold_{fold}'] = {\n",
        "            'accuracy': acc,\n",
        "            'mcc': mcc,\n",
        "            'auc': auc,\n",
        "            'confusion_matrix': confusion_matrix(y_val_fold, y_pred_fold),\n",
        "            'classification_report': classification_report(y_val_fold, y_pred_fold, output_dict=True)\n",
        "        }\n",
        "\n",
        "    overall_results = {\n",
        "        'task_name': task_name,\n",
        "        'cv_accuracy_mean': np.mean(cv_scores),\n",
        "        'cv_accuracy_std': np.std(cv_scores),\n",
        "        'cv_mcc_mean': np.mean(cv_mcc_scores),\n",
        "        'cv_mcc_std': np.std(cv_mcc_scores),\n",
        "        'cv_auc_mean': np.mean(cv_auc_scores),\n",
        "        'cv_auc_std': np.std(cv_auc_scores),\n",
        "        'overall_confusion_matrix': confusion_matrix(all_y_true, all_y_pred),\n",
        "        'overall_mcc': matthews_corrcoef(all_y_true, all_y_pred),\n",
        "        'overall_auc': roc_auc_score(all_y_true, all_y_proba) if len(set(all_y_true)) > 1 else 0.5,\n",
        "        'feature_importances': np.mean(feature_importances, axis=0) if feature_importances else None,\n",
        "        'fold_results': fold_results\n",
        "    }\n",
        "\n",
        "    return overall_results\n",
        "\n",
        "def analyze_feature_importance(X, y, feature_names=None, top_k=20):\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "    }\n",
        "\n",
        "    importance_results = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X, y)\n",
        "\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "        else:\n",
        "            perm_importance = permutation_importance(model, X, y, n_repeats=5, random_state=42)\n",
        "            importances = perm_importance.importances_mean\n",
        "\n",
        "        if feature_names is None:\n",
        "            feature_names = [f'feature_{i}' for i in range(len(importances))]\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        importance_results[model_name] = importance_df.head(top_k)\n",
        "\n",
        "    return importance_results\n",
        "\n",
        "def plot_evaluation_results(results, label_encoder):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle(f'{results[\"task_name\"]} - Comprehensive Evaluation', fontsize=16)\n",
        "\n",
        "    cv_metrics = ['cv_accuracy_mean', 'cv_mcc_mean', 'cv_auc_mean']\n",
        "    cv_stds = ['cv_accuracy_std', 'cv_mcc_std', 'cv_auc_std']\n",
        "    metric_names = ['Accuracy', 'MCC', 'AUC-ROC']\n",
        "\n",
        "    means = [results[metric] for metric in cv_metrics]\n",
        "    stds = [results[std] for std in cv_stds]\n",
        "\n",
        "    axes[0, 0].bar(metric_names, means, yerr=stds, capsize=5, alpha=0.7)\n",
        "    axes[0, 0].set_title('Cross-Validation Metrics')\n",
        "    axes[0, 0].set_ylabel('Score')\n",
        "    axes[0, 0].set_ylim(0, 1)\n",
        "\n",
        "    fold_accuracies = [results['fold_results'][f'fold_{i}']['accuracy'] for i in range(len(results['fold_results']))]\n",
        "    fold_mccs = [results['fold_results'][f'fold_{i}']['mcc'] for i in range(len(results['fold_results']))]\n",
        "    fold_aucs = [results['fold_results'][f'fold_{i}']['auc'] for i in range(len(results['fold_results']))]\n",
        "\n",
        "    folds = list(range(len(fold_accuracies)))\n",
        "    axes[0, 1].plot(folds, fold_accuracies, 'o-', label='Accuracy', linewidth=2)\n",
        "    axes[0, 1].plot(folds, fold_mccs, 's-', label='MCC', linewidth=2)\n",
        "    axes[0, 1].plot(folds, fold_aucs, '^-', label='AUC', linewidth=2)\n",
        "    axes[0, 1].set_title('Per-Fold Performance')\n",
        "    axes[0, 1].set_xlabel('Fold')\n",
        "    axes[0, 1].set_ylabel('Score')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    cm = results['overall_confusion_matrix']\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_,\n",
        "                ax=axes[0, 2])\n",
        "    axes[0, 2].set_title('Overall Confusion Matrix')\n",
        "    axes[0, 2].set_xlabel('Predicted')\n",
        "    axes[0, 2].set_ylabel('Actual')\n",
        "\n",
        "    if results['feature_importances'] is not None:\n",
        "        top_indices = np.argsort(results['feature_importances'])[-15:]\n",
        "        top_importances = results['feature_importances'][top_indices]\n",
        "        feature_names = [f'Feature_{i}' for i in top_indices]\n",
        "\n",
        "        axes[1, 0].barh(range(len(top_importances)), top_importances)\n",
        "        axes[1, 0].set_yticks(range(len(top_importances)))\n",
        "        axes[1, 0].set_yticklabels(feature_names)\n",
        "        axes[1, 0].set_title('Top 15 Feature Importances')\n",
        "        axes[1, 0].set_xlabel('Importance')\n",
        "\n",
        "    fold_cms = [results['fold_results'][f'fold_{i}']['confusion_matrix'] for i in range(len(results['fold_results']))]\n",
        "    cm_variance = np.var(fold_cms, axis=0)\n",
        "    sns.heatmap(cm_variance, annot=True, fmt='.2f', cmap='Reds', ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Confusion Matrix Variance Across Folds')\n",
        "\n",
        "    metrics_summary = {\n",
        "        'Metric': ['Accuracy', 'MCC', 'AUC-ROC'],\n",
        "        'Mean': [results['cv_accuracy_mean'], results['cv_mcc_mean'], results['cv_auc_mean']],\n",
        "        'Std': [results['cv_accuracy_std'], results['cv_mcc_std'], results['cv_auc_std']],\n",
        "        'Min': [min(fold_accuracies), min(fold_mccs), min(fold_aucs)],\n",
        "        'Max': [max(fold_accuracies), max(fold_mccs), max(fold_aucs)]\n",
        "    }\n",
        "\n",
        "    summary_df = pd.DataFrame(metrics_summary)\n",
        "    axes[1, 2].axis('tight')\n",
        "    axes[1, 2].axis('off')\n",
        "    table = axes[1, 2].table(cellText=summary_df.round(4).values,\n",
        "                            colLabels=summary_df.columns,\n",
        "                            cellLoc='center',\n",
        "                            loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    axes[1, 2].set_title('Metrics Summary')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "def ensemble_cross_validation(X, y, cv_folds=5, task_name=\"\"):\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42,\n",
        "                                             class_weight='balanced', min_samples_split=5),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42,\n",
        "                                                     learning_rate=0.1),\n",
        "        'SVM': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced',\n",
        "                  probability=True, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    model_results = {}\n",
        "    ensemble_results = {\n",
        "        'cv_accuracy': [],\n",
        "        'cv_mcc': [],\n",
        "        'cv_auc': [],\n",
        "        'fold_predictions': [],\n",
        "        'fold_true_labels': [],\n",
        "        'fold_probabilities': []\n",
        "    }\n",
        "\n",
        "    for model_name, model_class in models.items():\n",
        "        cv_acc = cross_val_score(model_class, X, y, cv=skf, scoring='accuracy')\n",
        "        cv_mcc = cross_val_score(model_class, X, y, cv=skf, scoring='matthews_corrcoef')\n",
        "\n",
        "        try:\n",
        "            cv_auc = cross_val_score(model_class, X, y, cv=skf, scoring='roc_auc')\n",
        "        except:\n",
        "            cv_auc = np.array([0.5] * cv_folds)\n",
        "\n",
        "        model_results[model_name] = {\n",
        "            'cv_accuracy_mean': np.mean(cv_acc),\n",
        "            'cv_accuracy_std': np.std(cv_acc),\n",
        "            'cv_mcc_mean': np.mean(cv_mcc),\n",
        "            'cv_mcc_std': np.std(cv_mcc),\n",
        "            'cv_auc_mean': np.mean(cv_auc),\n",
        "            'cv_auc_std': np.std(cv_auc)\n",
        "        }\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
        "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
        "\n",
        "        fold_models = {}\n",
        "        fold_predictions = []\n",
        "        fold_probabilities = []\n",
        "\n",
        "        for model_name, model_class in models.items():\n",
        "            model = model_class\n",
        "            model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "            pred = model.predict(X_val_fold)\n",
        "            proba = model.predict_proba(X_val_fold)[:, 1] if hasattr(model, 'predict_proba') else pred\n",
        "\n",
        "            fold_predictions.append(pred)\n",
        "            fold_probabilities.append(proba)\n",
        "            fold_models[model_name] = model\n",
        "\n",
        "        ensemble_pred = np.round(np.mean(fold_predictions, axis=0)).astype(int)\n",
        "        ensemble_proba = np.mean(fold_probabilities, axis=0)\n",
        "\n",
        "        fold_acc = accuracy_score(y_val_fold, ensemble_pred)\n",
        "        fold_mcc = matthews_corrcoef(y_val_fold, ensemble_pred)\n",
        "\n",
        "        try:\n",
        "            fold_auc = roc_auc_score(y_val_fold, ensemble_proba)\n",
        "        except:\n",
        "            fold_auc = 0.5\n",
        "\n",
        "        ensemble_results['cv_accuracy'].append(fold_acc)\n",
        "        ensemble_results['cv_mcc'].append(fold_mcc)\n",
        "        ensemble_results['cv_auc'].append(fold_auc)\n",
        "        ensemble_results['fold_predictions'].extend(ensemble_pred)\n",
        "        ensemble_results['fold_true_labels'].extend(y_val_fold)\n",
        "        ensemble_results['fold_probabilities'].extend(ensemble_proba)\n",
        "\n",
        "    ensemble_summary = {\n",
        "        'task_name': task_name,\n",
        "        'cv_accuracy_mean': np.mean(ensemble_results['cv_accuracy']),\n",
        "        'cv_accuracy_std': np.std(ensemble_results['cv_accuracy']),\n",
        "        'cv_mcc_mean': np.mean(ensemble_results['cv_mcc']),\n",
        "        'cv_mcc_std': np.std(ensemble_results['cv_mcc']),\n",
        "        'cv_auc_mean': np.mean(ensemble_results['cv_auc']),\n",
        "        'cv_auc_std': np.std(ensemble_results['cv_auc']),\n",
        "        'overall_confusion_matrix': confusion_matrix(ensemble_results['fold_true_labels'],\n",
        "                                                   ensemble_results['fold_predictions']),\n",
        "        'overall_mcc': matthews_corrcoef(ensemble_results['fold_true_labels'],\n",
        "                                       ensemble_results['fold_predictions']),\n",
        "        'overall_auc': roc_auc_score(ensemble_results['fold_true_labels'],\n",
        "                                   ensemble_results['fold_probabilities']) if len(set(ensemble_results['fold_true_labels'])) > 1 else 0.5,\n",
        "        'individual_models': model_results,\n",
        "        'fold_results': ensemble_results\n",
        "    }\n",
        "\n",
        "    return ensemble_summary\n",
        "\n",
        "def detailed_feature_analysis(X, y, feature_names=None, top_k=30):\n",
        "    if feature_names is None:\n",
        "        feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "    analysis_results = {}\n",
        "\n",
        "    selector_f = SelectKBest(f_classif, k='all')\n",
        "    selector_f.fit(X, y)\n",
        "    f_scores = selector_f.scores_\n",
        "    f_pvalues = selector_f.pvalues_\n",
        "\n",
        "    selector_mi = SelectKBest(mutual_info_classif, k='all')\n",
        "    selector_mi.fit(X, y)\n",
        "    mi_scores = selector_mi.scores_\n",
        "\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_model.fit(X, y)\n",
        "    rf_importances = rf_model.feature_importances_\n",
        "\n",
        "    gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "    gb_model.fit(X, y)\n",
        "    gb_importances = gb_model.feature_importances_\n",
        "\n",
        "    feature_analysis_df = pd.DataFrame({\n",
        "        'feature_name': feature_names,\n",
        "        'f_score': f_scores,\n",
        "        'f_pvalue': f_pvalues,\n",
        "        'mutual_info': mi_scores,\n",
        "        'rf_importance': rf_importances,\n",
        "        'gb_importance': gb_importances\n",
        "    })\n",
        "\n",
        "    feature_analysis_df['combined_score'] = (\n",
        "        feature_analysis_df['f_score'] / feature_analysis_df['f_score'].max() +\n",
        "        feature_analysis_df['mutual_info'] / feature_analysis_df['mutual_info'].max() +\n",
        "        feature_analysis_df['rf_importance'] / feature_analysis_df['rf_importance'].max() +\n",
        "        feature_analysis_df['gb_importance'] / feature_analysis_df['gb_importance'].max()\n",
        "    ) / 4\n",
        "\n",
        "    analysis_results['feature_analysis'] = feature_analysis_df.sort_values('combined_score', ascending=False)\n",
        "    analysis_results['top_features'] = feature_analysis_df.sort_values('combined_score', ascending=False).head(top_k)\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "def plot_feature_importance(importance_results, task_name):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle(f'{task_name} - Feature Importance Analysis', fontsize=16)\n",
        "\n",
        "    top_features = importance_results['top_features']\n",
        "\n",
        "    axes[0, 0].barh(range(len(top_features)), top_features['f_score'])\n",
        "    axes[0, 0].set_yticks(range(len(top_features)))\n",
        "    axes[0, 0].set_yticklabels(top_features['feature_name'], fontsize=8)\n",
        "    axes[0, 0].set_title('F-Score Rankings')\n",
        "    axes[0, 0].set_xlabel('F-Score')\n",
        "\n",
        "    axes[0, 1].barh(range(len(top_features)), top_features['mutual_info'])\n",
        "    axes[0, 1].set_yticks(range(len(top_features)))\n",
        "    axes[0, 1].set_yticklabels(top_features['feature_name'], fontsize=8)\n",
        "    axes[0, 1].set_title('Mutual Information Rankings')\n",
        "    axes[0, 1].set_xlabel('Mutual Information')\n",
        "\n",
        "    axes[1, 0].barh(range(len(top_features)), top_features['rf_importance'])\n",
        "    axes[1, 0].set_yticks(range(len(top_features)))\n",
        "    axes[1, 0].set_yticklabels(top_features['feature_name'], fontsize=8)\n",
        "    axes[1, 0].set_title('Random Forest Importance')\n",
        "    axes[1, 0].set_xlabel('Importance')\n",
        "\n",
        "    axes[1, 1].barh(range(len(top_features)), top_features['combined_score'])\n",
        "    axes[1, 1].set_yticks(range(len(top_features)))\n",
        "    axes[1, 1].set_yticklabels(top_features['feature_name'], fontsize=8)\n",
        "    axes[1, 1].set_title('Combined Importance Score')\n",
        "    axes[1, 1].set_xlabel('Combined Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "def generate_roc_curves(results_list, task_names):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    for i, (results, task_name) in enumerate(zip(results_list, task_names)):\n",
        "        y_true = results['fold_results']['fold_true_labels']\n",
        "        y_proba = results['fold_results']['fold_probabilities']\n",
        "\n",
        "        if len(set(y_true)) > 1:\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "            auc_score = results['overall_auc']\n",
        "\n",
        "            plt.subplot(1, 2, i+1)\n",
        "            plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
        "            plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.title(f'{task_name} - ROC Curve')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def stability_analysis(X, y, model, n_iterations=10, sample_fraction=0.8):\n",
        "    stability_metrics = {\n",
        "        'accuracy': [],\n",
        "        'mcc': [],\n",
        "        'auc': [],\n",
        "        'feature_importance_correlations': []\n",
        "    }\n",
        "\n",
        "    base_model = model\n",
        "    base_model.fit(X, y)\n",
        "    base_importance = base_model.feature_importances_ if hasattr(base_model, 'feature_importances_') else None\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        n_samples = int(len(X) * sample_fraction)\n",
        "        sample_indices = np.random.choice(len(X), n_samples, replace=False)\n",
        "\n",
        "        X_sample = X[sample_indices]\n",
        "        y_sample = y[sample_indices]\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample,\n",
        "                                                          test_size=0.3, random_state=i,\n",
        "                                                          stratify=y_sample)\n",
        "\n",
        "        iter_model = model\n",
        "        iter_model.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = iter_model.predict(X_test)\n",
        "        y_proba = iter_model.predict_proba(X_test)[:, 1] if hasattr(iter_model, 'predict_proba') else y_pred\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test, y_proba)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "\n",
        "        stability_metrics['accuracy'].append(acc)\n",
        "        stability_metrics['mcc'].append(mcc)\n",
        "        stability_metrics['auc'].append(auc)\n",
        "\n",
        "        if base_importance is not None and hasattr(iter_model, 'feature_importances_'):\n",
        "            correlation = np.corrcoef(base_importance, iter_model.feature_importances_)[0, 1]\n",
        "            stability_metrics['feature_importance_correlations'].append(correlation)\n",
        "\n",
        "    return stability_metrics\n",
        "\n",
        "def print_comprehensive_results(results, task_name):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{task_name.upper()} - COMPREHENSIVE EVALUATION\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    print(f\"\\nCROSS-VALIDATION METRICS:\")\n",
        "    print(f\"Accuracy: {results['cv_accuracy_mean']:.4f}  {results['cv_accuracy_std']:.4f}\")\n",
        "    print(f\"MCC:      {results['cv_mcc_mean']:.4f}  {results['cv_mcc_std']:.4f}\")\n",
        "    print(f\"AUC-ROC:  {results['cv_auc_mean']:.4f}  {results['cv_auc_std']:.4f}\")\n",
        "\n",
        "    print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "    print(f\"Overall MCC:     {results['overall_mcc']:.4f}\")\n",
        "    print(f\"Overall AUC-ROC: {results['overall_auc']:.4f}\")\n",
        "\n",
        "    print(f\"\\nCONFUSION MATRIX:\")\n",
        "    cm = results['overall_confusion_matrix']\n",
        "    print(cm)\n",
        "\n",
        "    if 'individual_models' in results:\n",
        "        print(f\"\\nINDIVIDUAL MODEL PERFORMANCE:\")\n",
        "        for model_name, model_results in results['individual_models'].items():\n",
        "            print(f\"{model_name}:\")\n",
        "            print(f\"  Accuracy: {model_results['cv_accuracy_mean']:.4f}  {model_results['cv_accuracy_std']:.4f}\")\n",
        "            print(f\"  MCC:      {model_results['cv_mcc_mean']:.4f}  {model_results['cv_mcc_std']:.4f}\")\n",
        "            print(f\"  AUC-ROC:  {model_results['cv_auc_mean']:.4f}  {model_results['cv_auc_std']:.4f}\")\n",
        "\n",
        "def overfitting_analysis(X, y, models_dict, task_name):\n",
        "    print(f\"\\n{task_name} - OVERFITTING ANALYSIS:\")\n",
        "\n",
        "    train_scores = {}\n",
        "    val_scores = {}\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    for model_name, model in models_dict.items():\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        train_pred = model.predict(X_train)\n",
        "        val_pred = model.predict(X_val)\n",
        "\n",
        "        train_acc = accuracy_score(y_train, train_pred)\n",
        "        val_acc = accuracy_score(y_val, val_pred)\n",
        "\n",
        "        train_mcc = matthews_corrcoef(y_train, train_pred)\n",
        "        val_mcc = matthews_corrcoef(y_val, val_pred)\n",
        "\n",
        "        train_scores[model_name] = {'accuracy': train_acc, 'mcc': train_mcc}\n",
        "        val_scores[model_name] = {'accuracy': val_acc, 'mcc': val_mcc}\n",
        "\n",
        "        gap_acc = train_acc - val_acc\n",
        "        gap_mcc = train_mcc - val_mcc\n",
        "\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Gap: {gap_acc:.4f}\")\n",
        "        print(f\"  Train MCC: {train_mcc:.4f}, Val MCC: {val_mcc:.4f}, Gap: {gap_mcc:.4f}\")\n",
        "\n",
        "        if gap_acc > 0.1 or gap_mcc > 0.1:\n",
        "            print(f\"  WARNING: Potential overfitting detected!\")\n",
        "\n",
        "def main_evaluation():\n",
        "    diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "\n",
        "    print(\"DIAGNOSIS TASK EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    diag_acoustic, diag_df_filtered = prepare_features(diagnosis_train, features_dict)\n",
        "\n",
        "    if len(diag_df_filtered) == 0:\n",
        "        print(\"No valid diagnosis samples found\")\n",
        "        return\n",
        "\n",
        "    diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "\n",
        "    if 'label_encoded' not in diag_df_filtered.columns:\n",
        "        if 'label' in diag_df_filtered.columns:\n",
        "            diag_df_filtered['label_encoded'] = diagnosis_le.fit_transform(diag_df_filtered['label'])\n",
        "\n",
        "    diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for diagnosis...\")\n",
        "    diag_text_features = extract_bert_features(diag_texts)\n",
        "\n",
        "    print(\"Engineering features for diagnosis...\")\n",
        "    diag_enhanced_features, diag_scaler = enhanced_feature_engineering(diag_acoustic, diag_text_features)\n",
        "\n",
        "    acoustic_feature_names = [f'acoustic_{i}' for i in range(diag_acoustic.shape[1])]\n",
        "    text_feature_names = [f'text_{i}' for i in range(diag_text_features.shape[1])]\n",
        "    stats_feature_names = ['acoustic_mean', 'acoustic_std', 'acoustic_max', 'acoustic_min',\n",
        "                          'text_mean', 'text_std', 'text_max', 'text_min']\n",
        "    all_feature_names = acoustic_feature_names + text_feature_names + stats_feature_names\n",
        "\n",
        "    diag_cv_results = ensemble_cross_validation(diag_enhanced_features, diag_labels,\n",
        "                                              cv_folds=5, task_name=\"Diagnosis\")\n",
        "\n",
        "    print_comprehensive_results(diag_cv_results, \"Diagnosis\")\n",
        "\n",
        "    diag_feature_analysis = detailed_feature_analysis(diag_enhanced_features, diag_labels,\n",
        "                                                    all_feature_names, top_k=20)\n",
        "\n",
        "    print(f\"\\nTOP 10 DIAGNOSIS FEATURES:\")\n",
        "    for i, row in diag_feature_analysis['top_features'].head(10).iterrows():\n",
        "        print(f\"{row['feature_name']}: {row['combined_score']:.4f}\")\n",
        "\n",
        "    diag_models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight='balanced'),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42),\n",
        "        'SVM': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    overfitting_analysis(diag_enhanced_features, diag_labels, diag_models, \"Diagnosis\")\n",
        "\n",
        "    print(\"\\n\\nPROGRESSION TASK EVALUATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    prog_acoustic, prog_df_filtered = prepare_features(progression_train, features_dict)\n",
        "\n",
        "    if len(prog_df_filtered) == 0:\n",
        "        print(\"No valid progression samples found\")\n",
        "        return\n",
        "\n",
        "    prog_texts = prog_df_filtered['transcript'].fillna('').tolist()\n",
        "\n",
        "    if 'label_encoded' not in prog_df_filtered.columns:\n",
        "        if 'label' in prog_df_filtered.columns:\n",
        "            prog_df_filtered['label_encoded'] = progression_le.fit_transform(prog_df_filtered['label'])\n",
        "\n",
        "    prog_labels = prog_df_filtered['label_encoded'].values\n",
        "\n",
        "    print(\"Extracting BERT features for progression...\")\n",
        "    prog_text_features = extract_bert_features(prog_texts)\n",
        "\n",
        "    print(\"Engineering features for progression...\")\n",
        "    prog_enhanced_features, prog_scaler = enhanced_feature_engineering(prog_acoustic, prog_text_features)\n",
        "\n",
        "    prog_cv_results = ensemble_cross_validation(prog_enhanced_features, prog_labels,\n",
        "                                              cv_folds=5, task_name=\"Progression\")\n",
        "\n",
        "    print_comprehensive_results(prog_cv_results, \"Progression\")\n",
        "\n",
        "    prog_feature_analysis = detailed_feature_analysis(prog_enhanced_features, prog_labels,\n",
        "                                                    all_feature_names, top_k=20)\n",
        "\n",
        "    print(f\"\\nTOP 10 PROGRESSION FEATURES:\")\n",
        "    for i, row in prog_feature_analysis['top_features'].head(10).iterrows():\n",
        "        print(f\"{row['feature_name']}: {row['combined_score']:.4f}\")\n",
        "\n",
        "    prog_models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight='balanced'),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=200, max_depth=6, random_state=42),\n",
        "        'SVM': SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', probability=True, random_state=42),\n",
        "        'LogisticRegression': LogisticRegression(C=1, class_weight='balanced', random_state=42, max_iter=1000)\n",
        "    }\n",
        "\n",
        "    overfitting_analysis(prog_enhanced_features, prog_labels, prog_models, \"Progression\")\n",
        "\n",
        "    print(\"\\n\\nSTABILITY ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    diag_stability = stability_analysis(diag_enhanced_features, diag_labels,\n",
        "                                      RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "    print(f\"Diagnosis Stability (n=10 iterations):\")\n",
        "    print(f\"  Accuracy: {np.mean(diag_stability['accuracy']):.4f}  {np.std(diag_stability['accuracy']):.4f}\")\n",
        "    print(f\"  MCC:      {np.mean(diag_stability['mcc']):.4f}  {np.std(diag_stability['mcc']):.4f}\")\n",
        "    print(f\"  AUC:      {np.mean(diag_stability['auc']):.4f}  {np.std(diag_stability['auc']):.4f}\")\n",
        "    if diag_stability['feature_importance_correlations']:\n",
        "        print(f\"  Feature Correlation: {np.mean(diag_stability['feature_importance_correlations']):.4f}  {np.std(diag_stability['feature_importance_correlations']):.4f}\")\n",
        "\n",
        "    prog_stability = stability_analysis(prog_enhanced_features, prog_labels,\n",
        "                                      RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "    print(f\"\\nProgression Stability (n=10 iterations):\")\n",
        "    print(f\"  Accuracy: {np.mean(prog_stability['accuracy']):.4f}  {np.std(prog_stability['accuracy']):.4f}\")\n",
        "    print(f\"  MCC:      {np.mean(prog_stability['mcc']):.4f}  {np.std(prog_stability['mcc']):.4f}\")\n",
        "    print(f\"  AUC:      {np.mean(prog_stability['auc']):.4f}  {np.std(prog_stability['auc']):.4f}\")\n",
        "    if prog_stability['feature_importance_correlations']:\n",
        "        print(f\"  Feature Correlation: {np.mean(prog_stability['feature_importance_correlations']):.4f}  {np.std(prog_stability['feature_importance_correlations']):.4f}\")\n",
        "\n",
        "    plot_evaluation_results(diag_cv_results, diagnosis_le)\n",
        "    plot_evaluation_results(prog_cv_results, progression_le)\n",
        "\n",
        "    plot_feature_importance(diag_feature_analysis, \"Diagnosis\")\n",
        "    plot_feature_importance(prog_feature_analysis, \"Progression\")\n",
        "\n",
        "    generate_roc_curves([diag_cv_results, prog_cv_results], [\"Diagnosis\", \"Progression\"])\n",
        "\n",
        "    print(\"\\n\\nFINAL SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Diagnosis Task:\")\n",
        "    print(f\"  CV Accuracy: {diag_cv_results['cv_accuracy_mean']:.4f}  {diag_cv_results['cv_accuracy_std']:.4f}\")\n",
        "    print(f\"  CV MCC:      {diag_cv_results['cv_mcc_mean']:.4f}  {diag_cv_results['cv_mcc_std']:.4f}\")\n",
        "    print(f\"  CV AUC:      {diag_cv_results['cv_auc_mean']:.4f}  {diag_cv_results['cv_auc_std']:.4f}\")\n",
        "\n",
        "    print(f\"\\nProgression Task:\")\n",
        "    print(f\"  CV Accuracy: {prog_cv_results['cv_accuracy_mean']:.4f}  {prog_cv_results['cv_accuracy_std']:.4f}\")\n",
        "    print(f\"  CV MCC:      {prog_cv_results['cv_mcc_mean']:.4f}  {prog_cv_results['cv_mcc_std']:.4f}\")\n",
        "    print(f\"  CV AUC:      {prog_cv_results['cv_auc_mean']:.4f}  {prog_cv_results['cv_auc_std']:.4f}\")\n",
        "\n",
        "    if prog_cv_results['cv_accuracy_mean'] > 0.95:\n",
        "        print(f\"\\nWARNING: Progression task shows suspiciously high performance.\")\n",
        "        print(f\"This may indicate overfitting or data leakage. Investigate further.\")\n",
        "\n",
        "    return {\n",
        "        'diagnosis_results': diag_cv_results,\n",
        "        'progression_results': prog_cv_results,\n",
        "        'diagnosis_features': diag_feature_analysis,\n",
        "        'progression_features': prog_feature_analysis,\n",
        "        'diagnosis_stability': diag_stability,\n",
        "        'progression_stability': prog_stability\n",
        "    }\n",
        "\n",
        "def learning_curve_analysis(X, y, model, train_sizes=None):\n",
        "    if train_sizes is None:\n",
        "        train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "    for train_size in train_sizes:\n",
        "        n_samples = int(len(X) * train_size)\n",
        "        if n_samples < 10:\n",
        "            continue\n",
        "\n",
        "        cv_train_scores = []\n",
        "        cv_val_scores = []\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "        for train_idx, val_idx in skf.split(X[:n_samples], y[:n_samples]):\n",
        "            X_train_cv = X[train_idx]\n",
        "            y_train_cv = y[train_idx]\n",
        "            X_val_cv = X[val_idx]\n",
        "            y_val_cv = y[val_idx]\n",
        "\n",
        "            model.fit(X_train_cv, y_train_cv)\n",
        "\n",
        "            train_pred = model.predict(X_train_cv)\n",
        "            val_pred = model.predict(X_val_cv)\n",
        "\n",
        "            cv_train_scores.append(accuracy_score(y_train_cv, train_pred))\n",
        "            cv_val_scores.append(accuracy_score(y_val_cv, val_pred))\n",
        "\n",
        "        train_scores.append(np.mean(cv_train_scores))\n",
        "        val_scores.append(np.mean(cv_val_scores))\n",
        "\n",
        "    return train_sizes[:len(train_scores)], train_scores, val_scores\n",
        "\n",
        "def plot_learning_curves(diagnosis_results, progression_results, diag_features, prog_features, diag_labels, prog_labels):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    diag_train_sizes, diag_train_scores, diag_val_scores = learning_curve_analysis(\n",
        "        diag_features, diag_labels, RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "    prog_train_sizes, prog_train_scores, prog_val_scores = learning_curve_analysis(\n",
        "        prog_features, prog_labels, RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "    axes[0, 0].plot(diag_train_sizes, diag_train_scores, 'o-', label='Training', linewidth=2)\n",
        "    axes[0, 0].plot(diag_train_sizes, diag_val_scores, 's-', label='Validation', linewidth=2)\n",
        "    axes[0, 0].set_title('Diagnosis Learning Curves')\n",
        "    axes[0, 0].set_xlabel('Training Set Size')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].plot(prog_train_sizes, prog_train_scores, 'o-', label='Training', linewidth=2)\n",
        "    axes[0, 1].plot(prog_train_sizes, prog_val_scores, 's-', label='Validation', linewidth=2)\n",
        "    axes[0, 1].set_title('Progression Learning Curves')\n",
        "    axes[0, 1].set_xlabel('Training Set Size')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    diag_cm = diagnosis_results['overall_confusion_matrix']\n",
        "    sns.heatmap(diag_cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['CN', 'AD'], yticklabels=['CN', 'AD'], ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Diagnosis Confusion Matrix')\n",
        "\n",
        "    prog_cm = progression_results['overall_confusion_matrix']\n",
        "    sns.heatmap(prog_cm, annot=True, fmt='d', cmap='Greens',\n",
        "                xticklabels=['No Decline', 'Decline'], yticklabels=['No Decline', 'Decline'], ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Progression Confusion Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        results = main_evaluation()\n",
        "\n",
        "        print(f\"\\nFeature analysis complete. Check plots for detailed visualizations.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6CqLsM7vefC"
      },
      "outputs": [],
      "source": [
        "# Add these missing functions to fix the immediate error:\n",
        "\n",
        "def plot_evaluation_results(cv_results, label_encoder):\n",
        "    \"\"\"Plot evaluation results - placeholder implementation\"\"\"\n",
        "    try:\n",
        "        print(f\"Plotting results for {len(cv_results)} cross-validation folds\")\n",
        "        # Add actual plotting logic here if needed\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Plotting error: {e}\")\n",
        "        return False\n",
        "\n",
        "def main_evaluation():\n",
        "    \"\"\"Main evaluation function with proper error handling\"\"\"\n",
        "    try:\n",
        "        # Load data with validation\n",
        "        diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "\n",
        "        if len(diagnosis_train) == 0:\n",
        "            print(\"No diagnosis training data available\")\n",
        "            return None\n",
        "\n",
        "        # Process diagnosis task with validation\n",
        "        print(\"Processing diagnosis task...\")\n",
        "        diag_acoustic, diag_df_filtered = prepare_features(diagnosis_train, features_dict)\n",
        "\n",
        "        if len(diag_df_filtered) == 0:\n",
        "            print(\"No valid diagnosis samples after filtering\")\n",
        "            return None\n",
        "\n",
        "        # Ensure labels exist\n",
        "        if 'label_encoded' not in diag_df_filtered.columns:\n",
        "            if 'label' in diag_df_filtered.columns:\n",
        "                diag_df_filtered = diag_df_filtered.copy()\n",
        "                diag_df_filtered['label_encoded'] = diagnosis_le.fit_transform(diag_df_filtered['label'])\n",
        "            else:\n",
        "                print(\"No labels found for diagnosis data\")\n",
        "                return None\n",
        "\n",
        "        diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "        diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "        # Extract features with error handling\n",
        "        print(\"Extracting BERT features for diagnosis...\")\n",
        "        try:\n",
        "            diag_text_features = extract_bert_features(diag_texts)\n",
        "        except Exception as e:\n",
        "            print(f\"BERT extraction failed: {e}\")\n",
        "            # Use dummy features as fallback\n",
        "            diag_text_features = np.zeros((len(diag_texts), 768))\n",
        "\n",
        "        print(\"Engineering features for diagnosis...\")\n",
        "        diag_enhanced_features, diag_scaler = enhanced_feature_engineering(diag_acoustic, diag_text_features)\n",
        "\n",
        "        if len(diag_enhanced_features) == 0:\n",
        "            print(\"No features extracted for diagnosis\")\n",
        "            return None\n",
        "\n",
        "        # Perform cross-validation with proper structure\n",
        "        from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "        cv_results = {\n",
        "            'fold_results': {},\n",
        "            'overall_metrics': {}\n",
        "        }\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        fold_accuracies = []\n",
        "\n",
        "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(diag_enhanced_features, diag_labels)):\n",
        "            X_train_fold = diag_enhanced_features[train_idx]\n",
        "            X_val_fold = diag_enhanced_features[val_idx]\n",
        "            y_train_fold = diag_labels[train_idx]\n",
        "            y_val_fold = diag_labels[val_idx]\n",
        "\n",
        "            # Train models for this fold\n",
        "            fold_models, fold_scores = train_ensemble_models(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
        "\n",
        "            if len(fold_models) > 0:\n",
        "                fold_pred = ensemble_predict(fold_models, X_val_fold)\n",
        "                fold_acc = accuracy_score(y_val_fold, fold_pred) if len(fold_pred) > 0 else 0\n",
        "                fold_accuracies.append(fold_acc)\n",
        "\n",
        "                # Store fold results with correct key format\n",
        "                cv_results['fold_results'][f'fold_{fold_idx}'] = {\n",
        "                    'accuracy': fold_acc,\n",
        "                    'predictions': fold_pred,\n",
        "                    'true_labels': y_val_fold\n",
        "                }\n",
        "            else:\n",
        "                fold_accuracies.append(0)\n",
        "                cv_results['fold_results'][f'fold_{fold_idx}'] = {\n",
        "                    'accuracy': 0,\n",
        "                    'predictions': [],\n",
        "                    'true_labels': y_val_fold\n",
        "                }\n",
        "\n",
        "        cv_results['overall_metrics'] = {\n",
        "            'mean_accuracy': np.mean(fold_accuracies),\n",
        "            'std_accuracy': np.std(fold_accuracies),\n",
        "            'fold_accuracies': fold_accuracies\n",
        "        }\n",
        "\n",
        "        print(f\"Cross-validation completed: {cv_results['overall_metrics']['mean_accuracy']:.4f}  {cv_results['overall_metrics']['std_accuracy']:.4f}\")\n",
        "\n",
        "        # Call plotting function with proper data structure\n",
        "        plot_evaluation_results(cv_results, diagnosis_le)\n",
        "\n",
        "        return cv_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Fix the feature preparation function to handle missing data better\n",
        "def prepare_features_fixed(df, features_dict):\n",
        "    \"\"\"Fixed version with better error handling\"\"\"\n",
        "    if len(df) == 0 or len(features_dict) == 0:\n",
        "        return np.array([]), pd.DataFrame()\n",
        "\n",
        "    acoustic_features = []\n",
        "    valid_rows = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        # Look for features in any category that matches this file_id\n",
        "        feature_data = None\n",
        "        for category, category_features in features_dict.items():\n",
        "            if isinstance(category_features, dict) and file_id in category_features:\n",
        "                feature_data = category_features[file_id]\n",
        "                break\n",
        "\n",
        "        # If no features found in categories, check if file_id is directly in features_dict\n",
        "        if feature_data is None and file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "        if feature_data is not None:\n",
        "            acoustic_feat = []\n",
        "\n",
        "            # Extract different types of features\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        acoustic_feat.extend(feat.flatten())\n",
        "\n",
        "            # Extract scalar features\n",
        "            for key in ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        acoustic_feat.append(np.mean(val) if len(val) > 0 else 0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val if not np.isnan(val) else 0)\n",
        "\n",
        "            # Ensure consistent feature length\n",
        "            if len(acoustic_feat) == 0:\n",
        "                acoustic_feat = [0] * 50  # Default feature size\n",
        "            elif len(acoustic_feat) > 100:\n",
        "                acoustic_feat = acoustic_feat[:100]  # Truncate if too long\n",
        "            else:\n",
        "                # Pad if too short\n",
        "                while len(acoustic_feat) < 50:\n",
        "                    acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat)\n",
        "            valid_rows.append(row)\n",
        "\n",
        "    if len(valid_rows) == 0:\n",
        "        print(\"Warning: No valid features extracted\")\n",
        "        return np.array([]), pd.DataFrame()\n",
        "\n",
        "    return np.array(acoustic_features), pd.DataFrame(valid_rows).reset_index(drop=True)\n",
        "\n",
        "# Add proper validation to the main function\n",
        "def main_fixed():\n",
        "    \"\"\"Fixed main function with proper validation\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Load and validate data\n",
        "        diagnosis_train, progression_train, diagnosis_le, progression_le, features_dict = load_data()\n",
        "\n",
        "        print(f\"Loaded data - Diagnosis: {len(diagnosis_train)}, Progression: {len(progression_train)}\")\n",
        "        print(f\"Features available for {len(features_dict)} categories\")\n",
        "\n",
        "        if len(diagnosis_train) == 0:\n",
        "            print(\"ERROR: No diagnosis training data\")\n",
        "            return\n",
        "\n",
        "        # Use fixed feature preparation\n",
        "        print(\"Processing diagnosis task...\")\n",
        "        diag_acoustic, diag_df_filtered = prepare_features_fixed(diagnosis_train, features_dict)\n",
        "\n",
        "        if len(diag_df_filtered) == 0:\n",
        "            print(\"ERROR: No valid diagnosis samples found\")\n",
        "            return\n",
        "\n",
        "        # Validate labels\n",
        "        if 'label' not in diag_df_filtered.columns:\n",
        "            print(\"ERROR: No label column in diagnosis data\")\n",
        "            return\n",
        "\n",
        "        # Rest of the processing with proper validation...\n",
        "        diag_texts = diag_df_filtered['transcript'].fillna('').tolist()\n",
        "\n",
        "        # Encode labels if needed\n",
        "        if 'label_encoded' not in diag_df_filtered.columns:\n",
        "            diag_df_filtered = diag_df_filtered.copy()\n",
        "            diag_df_filtered['label_encoded'] = diagnosis_le.fit_transform(diag_df_filtered['label'])\n",
        "\n",
        "        diag_labels = diag_df_filtered['label_encoded'].values\n",
        "\n",
        "        print(f\"Processing {len(diag_texts)} diagnosis samples\")\n",
        "\n",
        "        # Continue with feature extraction and model training...\n",
        "        # (Add similar validation for progression task)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error in main_fixed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Replace the problematic main call at the end:\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # First create datasets\n",
        "        datasets = create_training_datasets()\n",
        "        save_datasets(datasets)\n",
        "\n",
        "        # Then run the fixed main function\n",
        "        main_fixed()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6- GAT model"
      ],
      "metadata": {
        "id": "amzBNCYSsvIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(in_features, out_features))\n",
        "        self.a = nn.Parameter(torch.empty(2 * out_features, 1))\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W)\n",
        "        N = Wh.size(0)\n",
        "\n",
        "        Wh1 = torch.mm(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.mm(Wh, self.a[self.out_features:, :])\n",
        "        e = Wh1 + Wh2.T\n",
        "        e = self.leakyrelu(e)\n",
        "\n",
        "        zero_vec = -9e15 * torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "\n",
        "        h_prime = torch.mm(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "class MultiHeadGraphAttention(nn.Module):\n",
        "    def __init__(self, n_heads, in_features, out_features, dropout=0.1, alpha=0.2):\n",
        "        super(MultiHeadGraphAttention, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.n_heads = n_heads\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.attentions = nn.ModuleList([\n",
        "            GraphAttentionLayer(in_features, out_features, dropout=dropout, alpha=alpha, concat=True)\n",
        "            for _ in range(n_heads)\n",
        "        ])\n",
        "\n",
        "        self.out_att = GraphAttentionLayer(n_heads * out_features, out_features,\n",
        "                                         dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        att_outputs = [attention(x, adj) for attention in self.attentions]\n",
        "        x = torch.cat(att_outputs, dim=1)\n",
        "\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.out_att(x, adj)\n",
        "\n",
        "        return x\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, audio_dim, text_dim, hidden_dim=128):\n",
        "        super(CrossModalAttention, self).__init__()\n",
        "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=0.1, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio_features, text_features):\n",
        "        audio_proj = self.audio_proj(audio_features).unsqueeze(1)\n",
        "        text_proj = self.text_proj(text_features).unsqueeze(1)\n",
        "\n",
        "        combined = torch.cat([audio_proj, text_proj], dim=1)\n",
        "        attn_out, _ = self.attention(combined, combined, combined)\n",
        "        attn_out = self.norm1(attn_out + combined)\n",
        "\n",
        "        ffn_out = self.ffn(attn_out)\n",
        "        output = self.norm2(ffn_out + attn_out)\n",
        "\n",
        "        return output.mean(dim=1)\n",
        "\n",
        "class MultimodalGATModel(nn.Module):\n",
        "    def __init__(self, acoustic_dim, text_dim, n_heads=4, hidden_dim=128, n_classes=2, dropout=0.2):\n",
        "        super(MultimodalGATModel, self).__init__()\n",
        "\n",
        "        self.acoustic_encoder = nn.Sequential(\n",
        "            nn.Linear(acoustic_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.text_encoder = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.audio_gat = MultiHeadGraphAttention(n_heads, hidden_dim, hidden_dim // n_heads, dropout)\n",
        "        self.text_gat = MultiHeadGraphAttention(n_heads, hidden_dim, hidden_dim // n_heads, dropout)\n",
        "\n",
        "        self.cross_modal_attention = CrossModalAttention(hidden_dim, hidden_dim, hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, acoustic_features, text_features, audio_adj, text_adj):\n",
        "        audio_encoded = self.acoustic_encoder(acoustic_features)\n",
        "        text_encoded = self.text_encoder(text_features)\n",
        "\n",
        "        audio_gat_out = self.audio_gat(audio_encoded, audio_adj)\n",
        "        text_gat_out = self.text_gat(text_encoded, text_adj)\n",
        "\n",
        "        cross_modal_out = self.cross_modal_attention(audio_gat_out, text_gat_out)\n",
        "\n",
        "        return self.classifier(cross_modal_out)\n",
        "\n",
        "def load_preprocessed_data():\n",
        "    base_path = '/content/drive/MyDrive/Speech'\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    try:\n",
        "        processed_path = os.path.join(base_path, 'processed_datasets')\n",
        "        datasets['diagnosis_train'] = pd.read_pickle(os.path.join(processed_path, 'diagnosis_train.pkl'))\n",
        "        datasets['progression_train'] = pd.read_pickle(os.path.join(processed_path, 'progression_train.pkl'))\n",
        "        if os.path.exists(os.path.join(processed_path, 'progression_test.pkl')):\n",
        "            datasets['progression_test'] = pd.read_pickle(os.path.join(processed_path, 'progression_test.pkl'))\n",
        "\n",
        "        with open(os.path.join(processed_path, 'diagnosis_label_encoder.pkl'), 'rb') as f:\n",
        "            diagnosis_le = pickle.load(f)\n",
        "        with open(os.path.join(processed_path, 'progression_label_encoder.pkl'), 'rb') as f:\n",
        "            progression_le = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading processed datasets: {e}\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    features = {}\n",
        "    features_path = os.path.join(base_path, 'lightweight_features')\n",
        "\n",
        "    try:\n",
        "        with open(os.path.join(features_path, 'all_features.pkl'), 'rb') as f:\n",
        "            features = pickle.load(f)\n",
        "    except:\n",
        "        feature_files = ['diagnosis_ad_features.pkl', 'diagnosis_cn_features.pkl',\n",
        "                        'progression_decline_features.pkl', 'progression_no_decline_features.pkl']\n",
        "\n",
        "        for file in feature_files:\n",
        "            file_path = os.path.join(features_path, file)\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    with open(file_path, 'rb') as f:\n",
        "                        data = pickle.load(f)\n",
        "                        if isinstance(data, dict):\n",
        "                            features.update(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    bert_embeddings = {}\n",
        "    bert_path = os.path.join(base_path, 'linguistic_features', 'bert_embeddings')\n",
        "    bert_files = ['diagnosis_ad_embeddings.npz', 'diagnosis_cn_embeddings.npz',\n",
        "                 'progression_decline_embeddings.npz', 'progression_no_decline_embeddings.npz']\n",
        "\n",
        "    for file in bert_files:\n",
        "        file_path = os.path.join(bert_path, file)\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                data = np.load(file_path, allow_pickle=True)\n",
        "                for key in data.files:\n",
        "                    bert_embeddings[key] = data[key]\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file}: {e}\")\n",
        "\n",
        "    transcripts = {}\n",
        "    transcript_files = [\n",
        "        os.path.join(base_path, 'transcripts', 'all_categories_results.json'),\n",
        "        os.path.join(base_path, 'transcripts', 'transcription_results.json')\n",
        "    ]\n",
        "\n",
        "    for file_path in transcript_files:\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    if isinstance(data, dict):\n",
        "                        transcripts.update(data)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading transcripts from {file_path}: {e}\")\n",
        "\n",
        "    individual_path = os.path.join(base_path, 'transcripts', 'individual_transcripts')\n",
        "    if os.path.exists(individual_path):\n",
        "        for file in os.listdir(individual_path):\n",
        "            if file.endswith('.txt'):\n",
        "                file_id = file.replace('.wav.txt', '')\n",
        "                try:\n",
        "                    with open(os.path.join(individual_path, file), 'r') as f:\n",
        "                        transcripts[file_id] = f.read().strip()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading transcript {file}: {e}\")\n",
        "\n",
        "    print(f\"Loaded {len(features)} acoustic features\")\n",
        "    print(f\"Loaded {len(bert_embeddings)} BERT embeddings\")\n",
        "    print(f\"Loaded {len(transcripts)} transcripts\")\n",
        "\n",
        "    return datasets, diagnosis_le, progression_le, features, bert_embeddings, transcripts\n",
        "\n",
        "def prepare_features(df, features_dict, bert_embeddings, transcripts_dict):\n",
        "    acoustic_features = []\n",
        "    text_features = []\n",
        "    valid_rows = []\n",
        "\n",
        "    df_reset = df.reset_index(drop=True)\n",
        "\n",
        "    for idx, row in df_reset.iterrows():\n",
        "        file_id = row['file_id']\n",
        "\n",
        "        if file_id in features_dict:\n",
        "            feature_data = features_dict[file_id]\n",
        "\n",
        "            acoustic_feat = []\n",
        "\n",
        "            for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz']:\n",
        "                if key in feature_data:\n",
        "                    feat = feature_data[key]\n",
        "                    if isinstance(feat, (list, np.ndarray)):\n",
        "                        feat_array = np.array(feat).flatten()\n",
        "                        if len(feat_array) > 0:\n",
        "                            acoustic_feat.extend(feat_array[:20])\n",
        "                        else:\n",
        "                            acoustic_feat.extend([0] * 20)\n",
        "                    else:\n",
        "                        acoustic_feat.extend([0] * 20)\n",
        "                else:\n",
        "                    acoustic_feat.extend([0] * 20)\n",
        "\n",
        "            scalar_features = ['zero_crossing_rate', 'spectral_centroid', 'spectral_rolloff', 'rms_energy', 'tempo']\n",
        "            for key in scalar_features:\n",
        "                if key in feature_data:\n",
        "                    val = feature_data[key]\n",
        "                    if isinstance(val, (list, np.ndarray)):\n",
        "                        acoustic_feat.append(np.mean(val) if len(val) > 0 else 0)\n",
        "                    else:\n",
        "                        acoustic_feat.append(val if not np.isnan(val) else 0)\n",
        "                else:\n",
        "                    acoustic_feat.append(0)\n",
        "\n",
        "            if file_id in bert_embeddings:\n",
        "                text_feat = bert_embeddings[file_id]\n",
        "                if isinstance(text_feat, np.ndarray) and text_feat.size > 0:\n",
        "                    text_features.append(text_feat.flatten()[:768])\n",
        "                else:\n",
        "                    text_features.append(np.zeros(768))\n",
        "            else:\n",
        "                text_features.append(np.zeros(768))\n",
        "\n",
        "            while len(acoustic_feat) < 85:\n",
        "                acoustic_feat.append(0)\n",
        "\n",
        "            acoustic_features.append(acoustic_feat[:85])\n",
        "            valid_rows.append(row)\n",
        "\n",
        "    if len(valid_rows) == 0:\n",
        "        return None, None, None\n",
        "\n",
        "    acoustic_features = np.array(acoustic_features)\n",
        "    text_features = np.array(text_features)\n",
        "\n",
        "    if text_features.shape[1] < 768:\n",
        "        padding = np.zeros((len(text_features), 768 - text_features.shape[1]))\n",
        "        text_features = np.column_stack([text_features, padding])\n",
        "\n",
        "    scaler_audio = StandardScaler()\n",
        "    scaler_text = StandardScaler()\n",
        "\n",
        "    acoustic_features = scaler_audio.fit_transform(acoustic_features)\n",
        "    text_features = scaler_text.fit_transform(text_features)\n",
        "\n",
        "    valid_df = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
        "\n",
        "    return acoustic_features, text_features, valid_df\n",
        "\n",
        "def build_graph_adjacency(features, k=5):\n",
        "    if len(features) <= k:\n",
        "        return np.ones((len(features), len(features)), dtype=np.float32)\n",
        "\n",
        "    nn_model = NearestNeighbors(n_neighbors=k+1, metric='cosine')\n",
        "    nn_model.fit(features)\n",
        "    distances, indices = nn_model.kneighbors(features)\n",
        "\n",
        "    n_samples = len(features)\n",
        "    adj_matrix = np.eye(n_samples, dtype=np.float32)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        for j in range(1, min(k+1, indices.shape[1])):\n",
        "            neighbor_idx = indices[i][j]\n",
        "            similarity = max(0, 1 - distances[i][j])\n",
        "            adj_matrix[i][neighbor_idx] = similarity\n",
        "            adj_matrix[neighbor_idx][i] = similarity\n",
        "\n",
        "    return adj_matrix\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, acoustic_features, text_features, audio_adj, text_adj, labels):\n",
        "        self.acoustic_features = torch.FloatTensor(acoustic_features)\n",
        "        self.text_features = torch.FloatTensor(text_features)\n",
        "        self.audio_adj = torch.FloatTensor(audio_adj)\n",
        "        self.text_adj = torch.FloatTensor(text_adj)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'acoustic': self.acoustic_features[idx],\n",
        "            'text': self.text_features[idx],\n",
        "            'audio_adj': self.audio_adj,\n",
        "            'text_adj': self.text_adj,\n",
        "            'label': self.labels[idx]\n",
        "        }\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=100, lr=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    early_stop_patience = 15\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            acoustic = batch['acoustic'].to(device)\n",
        "            text = batch['text'].to(device)\n",
        "            audio_adj = batch['audio_adj'].to(device)\n",
        "            text_adj = batch['text_adj'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(acoustic, text, audio_adj, text_adj)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                acoustic = batch['acoustic'].to(device)\n",
        "                text = batch['text'].to(device)\n",
        "                audio_adj = batch['audio_adj'].to(device)\n",
        "                text_adj = batch['text_adj'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                outputs = model(acoustic, text, audio_adj, text_adj)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f'Epoch {epoch}: Loss: {total_loss/len(train_loader):.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        if patience_counter >= early_stop_patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    return model, best_val_acc\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            acoustic = batch['acoustic'].to(device)\n",
        "            text = batch['text'].to(device)\n",
        "            audio_adj = batch['audio_adj'].to(device)\n",
        "            text_adj = batch['text_adj'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(acoustic, text, audio_adj, text_adj)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities[:, 1].cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    mcc = matthews_corrcoef(all_labels, all_predictions)\n",
        "    auc = roc_auc_score(all_labels, all_probabilities) if len(set(all_labels)) > 1 else 0.5\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'mcc': mcc,\n",
        "        'auc': auc,\n",
        "        'predictions': all_predictions,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probabilities,\n",
        "        'confusion_matrix': confusion_matrix(all_labels, all_predictions)\n",
        "    }\n",
        "\n",
        "def cross_validate_model(acoustic_features, text_features, audio_adj, text_adj, labels, n_folds=5):\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(acoustic_features, labels)):\n",
        "        print(f\"Training fold {fold + 1}/{n_folds}\")\n",
        "\n",
        "        train_dataset = MultimodalDataset(\n",
        "            acoustic_features[train_idx], text_features[train_idx],\n",
        "            audio_adj[np.ix_(train_idx, train_idx)], text_adj[np.ix_(train_idx, train_idx)],\n",
        "            labels[train_idx]\n",
        "        )\n",
        "\n",
        "        val_dataset = MultimodalDataset(\n",
        "            acoustic_features[val_idx], text_features[val_idx],\n",
        "            audio_adj[np.ix_(val_idx, val_idx)], text_adj[np.ix_(val_idx, val_idx)],\n",
        "            labels[val_idx]\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "        model = MultimodalGATModel(\n",
        "            acoustic_dim=acoustic_features.shape[1],\n",
        "            text_dim=text_features.shape[1],\n",
        "            n_heads=4,\n",
        "            hidden_dim=128,\n",
        "            n_classes=len(np.unique(labels)),\n",
        "            dropout=0.2\n",
        "        )\n",
        "\n",
        "        trained_model, best_val_acc = train_model(model, train_loader, val_loader)\n",
        "        fold_eval = evaluate_model(trained_model, val_loader)\n",
        "        fold_eval['fold'] = fold\n",
        "        fold_eval['best_val_acc'] = best_val_acc\n",
        "        fold_results.append(fold_eval)\n",
        "\n",
        "    metrics = ['accuracy', 'mcc', 'auc']\n",
        "    cv_results = {}\n",
        "\n",
        "    for metric in metrics:\n",
        "        values = [result[metric] for result in fold_results]\n",
        "        cv_results[f'cv_{metric}_mean'] = np.mean(values)\n",
        "        cv_results[f'cv_{metric}_std'] = np.std(values)\n",
        "\n",
        "    cv_results['fold_results'] = fold_results\n",
        "    return cv_results\n",
        "\n",
        "def plot_results(diagnosis_results, progression_results):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    metrics = ['cv_accuracy_mean', 'cv_mcc_mean', 'cv_auc_mean']\n",
        "    metric_names = ['Accuracy', 'MCC', 'AUC-ROC']\n",
        "\n",
        "    diag_means = [diagnosis_results[metric] for metric in metrics]\n",
        "    diag_stds = [diagnosis_results[f'{metric.replace(\"mean\", \"std\")}'] for metric in metrics]\n",
        "\n",
        "    prog_means = [progression_results[metric] for metric in metrics]\n",
        "    prog_stds = [progression_results[f'{metric.replace(\"mean\", \"std\")}'] for metric in metrics]\n",
        "\n",
        "    x = np.arange(len(metric_names))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[0, 0].bar(x - width/2, diag_means, width, yerr=diag_stds, label='Diagnosis', alpha=0.7)\n",
        "    axes[0, 0].bar(x + width/2, prog_means, width, yerr=prog_stds, label='Progression', alpha=0.7)\n",
        "    axes[0, 0].set_title('Cross-Validation Results')\n",
        "    axes[0, 0].set_ylabel('Score')\n",
        "    axes[0, 0].set_xticks(x)\n",
        "    axes[0, 0].set_xticklabels(metric_names)\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].set_ylim(0, 1)\n",
        "\n",
        "    diag_fold_accs = [result['accuracy'] for result in diagnosis_results['fold_results']]\n",
        "    prog_fold_accs = [result['accuracy'] for result in progression_results['fold_results']]\n",
        "\n",
        "    folds = list(range(len(diag_fold_accs)))\n",
        "    axes[0, 1].plot(folds, diag_fold_accs, 'o-', label='Diagnosis', linewidth=2)\n",
        "    axes[0, 1].plot(folds, prog_fold_accs, 's-', label='Progression', linewidth=2)\n",
        "    axes[0, 1].set_title('Per-Fold Accuracy')\n",
        "    axes[0, 1].set_xlabel('Fold')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    if diagnosis_results['fold_results']:\n",
        "        diag_cm = sum(result['confusion_matrix'] for result in diagnosis_results['fold_results'])\n",
        "        sns.heatmap(diag_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0])\n",
        "        axes[1, 0].set_title('Diagnosis Confusion Matrix')\n",
        "        axes[1, 0].set_xlabel('Predicted')\n",
        "        axes[1, 0].set_ylabel('Actual')\n",
        "\n",
        "    if progression_results['fold_results']:\n",
        "        prog_cm = sum(result['confusion_matrix'] for result in progression_results['fold_results'])\n",
        "        sns.heatmap(prog_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1, 1])\n",
        "        axes[1, 1].set_title('Progression Confusion Matrix')\n",
        "        axes[1, 1].set_xlabel('Predicted')\n",
        "        axes[1, 1].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def print_results(results, task_name):\n",
        "    print(f\"\\n{task_name.upper()} RESULTS:\")\n",
        "    print(f\"Accuracy: {results['cv_accuracy_mean']:.4f}  {results['cv_accuracy_std']:.4f}\")\n",
        "    print(f\"MCC:      {results['cv_mcc_mean']:.4f}  {results['cv_mcc_std']:.4f}\")\n",
        "    print(f\"AUC-ROC:  {results['cv_auc_mean']:.4f}  {results['cv_auc_std']:.4f}\")\n",
        "\n",
        "def debug_data_loading():\n",
        "    base_path = '/content/drive/MyDrive/Speech'\n",
        "\n",
        "    print(\"Checking processed datasets...\")\n",
        "    processed_path = os.path.join(base_path, 'processed_datasets')\n",
        "    for file in ['diagnosis_train.pkl', 'progression_train.pkl']:\n",
        "        file_path = os.path.join(processed_path, file)\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_pickle(file_path)\n",
        "            print(f\"{file}: {len(df)} rows, columns: {list(df.columns)}\")\n",
        "            if len(df) > 0:\n",
        "                print(f\"  Sample file_ids: {df['file_id'].head(3).tolist()}\")\n",
        "        else:\n",
        "            print(f\"{file}: Not found\")\n",
        "\n",
        "    print(\"\\nChecking features...\")\n",
        "    features_path = os.path.join(base_path, 'lightweight_features')\n",
        "    if os.path.exists(os.path.join(features_path, 'all_features.pkl')):\n",
        "        with open(os.path.join(features_path, 'all_features.pkl'), 'rb') as f:\n",
        "            features = pickle.load(f)\n",
        "            print(f\"all_features.pkl: {len(features)} features\")\n",
        "            sample_keys = list(features.keys())[:3]\n",
        "            print(f\"  Sample keys: {sample_keys}\")\n",
        "            if sample_keys:\n",
        "                print(f\"  Sample feature keys: {list(features[sample_keys[0]].keys())}\")\n",
        "\n",
        "    print(\"\\nChecking BERT embeddings...\")\n",
        "    bert_path = os.path.join(base_path, 'linguistic_features', 'bert_embeddings')\n",
        "    bert_files = os.listdir(bert_path) if os.path.exists(bert_path) else []\n",
        "    for file in bert_files[:3]:\n",
        "        if file.endswith('.npz'):\n",
        "            data = np.load(os.path.join(bert_path, file), allow_pickle=True)\n",
        "            print(f\"{file}: {len(data.files)} embeddings\")\n",
        "            if len(data.files) > 0:\n",
        "                sample_key = data.files[0]\n",
        "                print(f\"  Sample shape: {data[sample_key].shape}\")\n",
        "\n",
        "def main():\n",
        "    debug_data_loading()\n",
        "\n",
        "    print(\"\\nLoading preprocessed data...\")\n",
        "    data_result = load_preprocessed_data()\n",
        "\n",
        "    if data_result[0] is None:\n",
        "        print(\"Failed to load data\")\n",
        "        return\n",
        "\n",
        "    datasets, diagnosis_le, progression_le, features, bert_embeddings, transcripts = data_result\n",
        "\n",
        "    print(\"Preparing diagnosis features...\")\n",
        "    diag_result = prepare_features(datasets['diagnosis_train'], features, bert_embeddings, transcripts)\n",
        "\n",
        "    if diag_result[0] is None:\n",
        "        print(\"No valid diagnosis samples found\")\n",
        "        return\n",
        "\n",
        "    diag_acoustic, diag_text, diag_df = diag_result\n",
        "\n",
        "    print(\"Preparing progression features...\")\n",
        "    prog_result = prepare_features(datasets['progression_train'], features, bert_embeddings, transcripts)\n",
        "\n",
        "    if prog_result[0] is None:\n",
        "        print(\"No valid progression samples found\")\n",
        "        return\n",
        "\n",
        "    prog_acoustic, prog_text, prog_df = prog_result\n",
        "\n",
        "    print(f\"Diagnosis dataset: {len(diag_df)} samples\")\n",
        "    print(f\"Progression dataset: {len(prog_df)} samples\")\n",
        "    print(f\"Acoustic features shape: {diag_acoustic.shape}\")\n",
        "    print(f\"Text features shape: {diag_text.shape}\")\n",
        "\n",
        "    print(\"Building adjacency matrices...\")\n",
        "    diag_audio_adj = build_graph_adjacency(diag_acoustic)\n",
        "    diag_text_adj = build_graph_adjacency(diag_text)\n",
        "    prog_audio_adj = build_graph_adjacency(prog_acoustic)\n",
        "    prog_text_adj = build_graph_adjacency(prog_text)\n",
        "\n",
        "    if 'label_encoded' in diag_df.columns:\n",
        "        diag_labels = diag_df['label_encoded'].values\n",
        "    else:\n",
        "        diag_labels = diagnosis_le.transform(diag_df['label'].values)\n",
        "\n",
        "    if 'label_encoded' in prog_df.columns:\n",
        "        prog_labels = prog_df['label_encoded'].values\n",
        "    else:\n",
        "        prog_labels = progression_le.transform(prog_df['label'].values)\n",
        "\n",
        "    print(f\"Diagnosis labels distribution: {np.bincount(diag_labels)}\")\n",
        "    print(f\"Progression labels distribution: {np.bincount(prog_labels)}\")\n",
        "\n",
        "    print(\"\\nEvaluating diagnosis task...\")\n",
        "    diagnosis_results = cross_validate_model(diag_acoustic, diag_text, diag_audio_adj, diag_text_adj, diag_labels)\n",
        "\n",
        "    print(\"\\nEvaluating progression task...\")\n",
        "    progression_results = cross_validate_model(prog_acoustic, prog_text, prog_audio_adj, prog_text_adj, prog_labels)\n",
        "\n",
        "    print_results(diagnosis_results, \"Diagnosis\")\n",
        "    print_results(progression_results, \"Progression\")\n",
        "\n",
        "    plot_results(diagnosis_results, progression_results)\n",
        "\n",
        "    results_path = '/content/drive/MyDrive/Speech/gat_results'\n",
        "    os.makedirs(results_path, exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(results_path, 'diagnosis_gat_results_fixed.pkl'), 'wb') as f:\n",
        "        pickle.dump(diagnosis_results, f)\n",
        "\n",
        "    with open(os.path.join(results_path, 'progression_gat_results_fixed.pkl'), 'wb') as f:\n",
        "        pickle.dump(progression_results, f)\n",
        "\n",
        "    print(f\"\\nResults saved to {results_path}\")\n",
        "\n",
        "    return diagnosis_results, progression_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baVZU1qms06W",
        "outputId": "e82ef28d-f8ef-4a63-b5ab-a6c67af657ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking processed datasets...\n",
            "diagnosis_train.pkl: 166 rows, columns: ['file_id', 'dataset', 'label', 'file_path', 'transcript', 'label_encoded']\n",
            "  Sample file_ids: ['adrso003', 'adrso014', 'adrso012']\n",
            "progression_train.pkl: 73 rows, columns: ['file_id', 'dataset', 'label', 'file_path', 'transcript', 'label_encoded']\n",
            "  Sample file_ids: ['adrsp196', 'adrsp137', 'adrsp130']\n",
            "\n",
            "Checking features...\n",
            "all_features.pkl: 5 features\n",
            "  Sample keys: ['diagnosis_ad', 'diagnosis_cn', 'progression_decline']\n",
            "  Sample feature keys: ['adrso248', 'adrso077', 'adrso049', 'adrso205', 'adrso218', 'adrso192', 'adrso039', 'adrso032', 'adrso071', 'adrso025', 'adrso089', 'adrso247', 'adrso055', 'adrso246', 'adrso144', 'adrso116', 'adrso244', 'adrso228', 'adrso110', 'adrso027', 'adrso125', 'adrso070', 'adrso189', 'adrso126', 'adrso215', 'adrso054', 'adrso253', 'adrso063', 'adrso093', 'adrso024', 'adrso206', 'adrso190', 'adrso237', 'adrso075', 'adrso109', 'adrso222', 'adrso223', 'adrso141', 'adrso046', 'adrso216', 'adrso106', 'adrso078', 'adrso138', 'adrso234', 'adrso128', 'adrso072', 'adrso043', 'adrso060', 'adrso053', 'adrso047', 'adrso236', 'adrso245', 'adrso122', 'adrso211', 'adrso035', 'adrso197', 'adrso123', 'adrso134', 'adrso188', 'adrso056', 'adrso229', 'adrso059', 'adrso224', 'adrso212', 'adrso250', 'adrso112', 'adrso033', 'adrso220', 'adrso249', 'adrso233', 'adrso198', 'adrso200', 'adrso031', 'adrso187', 'adrso045', 'adrso202', 'adrso142', 'adrso090', 'adrso028', 'adrso232', 'adrso092', 'adrso036', 'adrso209', 'adrso098', 'adrso130', 'adrso074', 'adrso068']\n",
            "\n",
            "Checking BERT embeddings...\n",
            "diagnosis_ad_embeddings.npz: 5 embeddings\n",
            "  Sample shape: (87,)\n",
            "diagnosis_cn_embeddings.npz: 5 embeddings\n",
            "  Sample shape: (79,)\n",
            "progression_decline_embeddings.npz: 5 embeddings\n",
            "  Sample shape: (15,)\n",
            "\n",
            "Loading preprocessed data...\n",
            "Loaded 5 acoustic features\n",
            "Loaded 5 BERT embeddings\n",
            "Loaded 276 transcripts\n",
            "Preparing diagnosis features...\n",
            "No valid diagnosis samples found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM6x/jcLkyJKZ8t1IlZeyQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02d5f9e1872e4db09517a189d8b5d23d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38bb38fab68a4e95afe5eb31038b826c",
              "IPY_MODEL_c011e7e492424807954dc914a21303a6",
              "IPY_MODEL_4c098b0ddee14c6396f0552278153aa6"
            ],
            "layout": "IPY_MODEL_dd706222d7b749329b0f2eb3f6f4c202"
          }
        },
        "09d076367bfa441caa61cea97b82e957": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aafe5eb98b849cd8f223abaad3cb32f",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b15b3ed8fd1d42498afa3fdd7714d92c",
            "value": 466062
          }
        },
        "0e0619ce3d024ed8a870c8513269ee6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e277f888fe348609dd9708ae689022e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1235658c89574c4481e68600115b4716": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14e029f6cc724247812b5f85ad20c7ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "150556bc326248ecab38e2d28e1ab03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15eae9b2395c49689d62764a31f3a83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e277f888fe348609dd9708ae689022e",
            "placeholder": "",
            "style": "IPY_MODEL_5f662038cf734f56ac1d2095d60c4a77",
            "value": "config.json:100%"
          }
        },
        "2f967574e0ef45ce87382c55776db60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c9c95dd6d844b15a2618ba9516f9dfd",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b99a857017724c7aa6598b3e111407b3",
            "value": 570
          }
        },
        "3305c2c4da964d15906572aaa55a1df3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "383c2c9048c44b02bf8a45c090731ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38bb38fab68a4e95afe5eb31038b826c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3305c2c4da964d15906572aaa55a1df3",
            "placeholder": "",
            "style": "IPY_MODEL_d30f72f67afe440eb03a2336ab1beb26",
            "value": "vocab.txt:100%"
          }
        },
        "3aafe5eb98b849cd8f223abaad3cb32f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b83039215f9415186f4a056a5d0b640": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14e029f6cc724247812b5f85ad20c7ba",
            "placeholder": "",
            "style": "IPY_MODEL_6f8245ac3a814561855f2560ae95f46e",
            "value": "466k/466k[00:00&lt;00:00,15.5MB/s]"
          }
        },
        "3c96aca3dbef4406bce82dbd69e5fabb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bee6d1325474e03b1de9e9e5431d2a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c098b0ddee14c6396f0552278153aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_793d4125b5c548dc81bcd952f3928a9b",
            "placeholder": "",
            "style": "IPY_MODEL_ca29fcc32a3f4e4bb4981fc15dc401bf",
            "value": "232k/232k[00:00&lt;00:00,5.03MB/s]"
          }
        },
        "4c9c95dd6d844b15a2618ba9516f9dfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5672c263c35d48c087d0a9061545676c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_814b72be4ece4bcb8b40085a26af8bb9",
              "IPY_MODEL_09d076367bfa441caa61cea97b82e957",
              "IPY_MODEL_3b83039215f9415186f4a056a5d0b640"
            ],
            "layout": "IPY_MODEL_0e0619ce3d024ed8a870c8513269ee6e"
          }
        },
        "56be167fd1d04cf6ba5770e17e041adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f8ce07d9f0e43a784198c6e4b0873ee",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_150556bc326248ecab38e2d28e1ab03a",
            "value": 440449768
          }
        },
        "5f662038cf734f56ac1d2095d60c4a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "617425d246e24aaeb6833e771390a185": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15eae9b2395c49689d62764a31f3a83e",
              "IPY_MODEL_2f967574e0ef45ce87382c55776db60f",
              "IPY_MODEL_737d7169d5c74d8fb1e071ebd942f5f0"
            ],
            "layout": "IPY_MODEL_62fcf176c62c48ea92215edeeadade34"
          }
        },
        "62fcf176c62c48ea92215edeeadade34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64c4efcec8c94c668201badfa12ad71c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8245ac3a814561855f2560ae95f46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70592dd2b68a4d2885f303a358cdcaa6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70729595876d4187bf6872b167cf85b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c211a8fe960447089a10219481021763",
            "placeholder": "",
            "style": "IPY_MODEL_92c20b9881ea449588cb0c2732c79679",
            "value": "440M/440M[00:04&lt;00:00,120MB/s]"
          }
        },
        "737d7169d5c74d8fb1e071ebd942f5f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64c4efcec8c94c668201badfa12ad71c",
            "placeholder": "",
            "style": "IPY_MODEL_383c2c9048c44b02bf8a45c090731ece",
            "value": "570/570[00:00&lt;00:00,39.9kB/s]"
          }
        },
        "793d4125b5c548dc81bcd952f3928a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b9b1ee123ea4a8a84617988336732ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f8ce07d9f0e43a784198c6e4b0873ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "814b72be4ece4bcb8b40085a26af8bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bee6d1325474e03b1de9e9e5431d2a3",
            "placeholder": "",
            "style": "IPY_MODEL_7b9b1ee123ea4a8a84617988336732ed",
            "value": "tokenizer.json:100%"
          }
        },
        "84adb669ceb44c58b083c054905bacbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "883d55994f4440e4a1ff83ed733e0feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d5dace948724c5ca7caaf5b16142754": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f537cfcc7ef9442e8eeb87028266704a",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a496861664dd4ea29fc37e454082a90d",
            "value": 48
          }
        },
        "92c20b9881ea449588cb0c2732c79679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a496861664dd4ea29fc37e454082a90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a64f9dd076ca44b38ff9e72918fb318c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa068e337f534627b9b45d952aaf72fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adbc4efc7127420db1e56bd6aa5b11ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70592dd2b68a4d2885f303a358cdcaa6",
            "placeholder": "",
            "style": "IPY_MODEL_1235658c89574c4481e68600115b4716",
            "value": "tokenizer_config.json:100%"
          }
        },
        "b15b3ed8fd1d42498afa3fdd7714d92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b99a857017724c7aa6598b3e111407b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c011e7e492424807954dc914a21303a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c96aca3dbef4406bce82dbd69e5fabb",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_883d55994f4440e4a1ff83ed733e0feb",
            "value": 231508
          }
        },
        "c1d0247e51924a89be1594ac69e2120b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c211a8fe960447089a10219481021763": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f88dffbd0646f8b4eaa6705c2aaf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8234340b00a438290965b7c2afaf3d3",
            "placeholder": "",
            "style": "IPY_MODEL_a64f9dd076ca44b38ff9e72918fb318c",
            "value": "model.safetensors:100%"
          }
        },
        "ca29fcc32a3f4e4bb4981fc15dc401bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc237e702ba6462995a62a9dce160dab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22bef0122b14d7a9f88ba18500353b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1d0247e51924a89be1594ac69e2120b",
            "placeholder": "",
            "style": "IPY_MODEL_aa068e337f534627b9b45d952aaf72fe",
            "value": "48.0/48.0[00:00&lt;00:00,4.25kB/s]"
          }
        },
        "d30f72f67afe440eb03a2336ab1beb26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d50dadc5b91b49fa81dcd3c04d08f7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adbc4efc7127420db1e56bd6aa5b11ce",
              "IPY_MODEL_8d5dace948724c5ca7caaf5b16142754",
              "IPY_MODEL_d22bef0122b14d7a9f88ba18500353b2"
            ],
            "layout": "IPY_MODEL_84adb669ceb44c58b083c054905bacbc"
          }
        },
        "dd706222d7b749329b0f2eb3f6f4c202": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f537cfcc7ef9442e8eeb87028266704a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8042c0263404565a2652835b24ff79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7f88dffbd0646f8b4eaa6705c2aaf89",
              "IPY_MODEL_56be167fd1d04cf6ba5770e17e041adf",
              "IPY_MODEL_70729595876d4187bf6872b167cf85b5"
            ],
            "layout": "IPY_MODEL_cc237e702ba6462995a62a9dce160dab"
          }
        },
        "f8234340b00a438290965b7c2afaf3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}