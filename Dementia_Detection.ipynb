{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUunNzVXPJRXjBhYQQ5tgl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Dementia_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb8c6oC9R01c",
        "outputId": "22ff430c-7152-4d05-fb4d-d4bbe0a81014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multimodal Deep Learning System for Alzheimer's Disease Classification\n",
        "# ADReSSo21 Dataset - Acoustic and Linguistic Feature Integration\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import transformers\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, AutoTokenizer, AutoModel\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import speech_recognition as sr\n",
        "import textstat\n",
        "import re\n",
        "from collections import Counter\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Install required packages for Colab\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages for Google Colab\"\"\"\n",
        "    packages = [\n",
        "        'librosa',\n",
        "        'transformers',\n",
        "        'torch-geometric',\n",
        "        'SpeechRecognition',\n",
        "        'textstat',\n",
        "        'networkx',\n",
        "        'pyaudio',\n",
        "        'pydub'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            os.system(f'pip install -q {package}')\n",
        "        except:\n",
        "            print(f\"Failed to install {package}\")\n",
        "\n",
        "# Data paths configuration\n",
        "class DataConfig:\n",
        "    def __init__(self):\n",
        "        self.base_path = \"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"\n",
        "        self.diagnosis_paths = {\n",
        "            'ad': f\"{self.base_path}/diagnosis/train/audio/ad\",\n",
        "            'cn': f\"{self.base_path}/diagnosis/train/audio/cn\"\n",
        "        }\n",
        "        self.progression_paths = {\n",
        "            'decline': f\"{self.base_path}/progression/train/audio/decline\",\n",
        "            'no_decline': f\"{self.base_path}/progression/train/audio/no_decline\"\n",
        "        }\n",
        "        self.test_path = f\"{self.base_path}/progression/test-dist/audio\"\n",
        "\n",
        "# Acoustic Feature Extractor\n",
        "class AcousticFeatureExtractor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=128, n_fft=2048, hop_length=512):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "        # Initialize Wav2Vec2 processor and model\n",
        "        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
        "\n",
        "    def extract_log_mel_spectrogram(self, audio, sr):\n",
        "        \"\"\"Extract Log-Mel spectrogram features\"\"\"\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=audio, sr=sr, n_mels=self.n_mels, n_fft=self.n_fft, hop_length=self.hop_length\n",
        "        )\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "        return log_mel_spec\n",
        "\n",
        "    def extract_delta_features(self, log_mel_spec):\n",
        "        \"\"\"Extract delta and delta-delta features\"\"\"\n",
        "        delta = librosa.feature.delta(log_mel_spec)\n",
        "        delta_delta = librosa.feature.delta(log_mel_spec, order=2)\n",
        "        return delta, delta_delta\n",
        "\n",
        "    def extract_wav2vec_embeddings(self, audio):\n",
        "        \"\"\"Extract Wav2Vec2 embeddings\"\"\"\n",
        "        try:\n",
        "            # Ensure audio is the right format\n",
        "            if len(audio.shape) > 1:\n",
        "                audio = audio.mean(axis=0)\n",
        "\n",
        "            # Resample to 16kHz if needed\n",
        "            if self.sample_rate != 16000:\n",
        "                audio = librosa.resample(audio, orig_sr=self.sample_rate, target_sr=16000)\n",
        "\n",
        "            inputs = self.wav2vec_processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.wav2vec_model(**inputs)\n",
        "                embeddings = outputs.last_hidden_state.squeeze().cpu().numpy()\n",
        "\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting Wav2Vec embeddings: {e}\")\n",
        "            return np.zeros((100, 768))  # Fallback\n",
        "\n",
        "    def extract_all_features(self, audio_path):\n",
        "        \"\"\"Extract all acoustic features from audio file\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "\n",
        "            # Extract features\n",
        "            log_mel_spec = self.extract_log_mel_spectrogram(audio, sr)\n",
        "            delta, delta_delta = self.extract_delta_features(log_mel_spec)\n",
        "            wav2vec_embeddings = self.extract_wav2vec_embeddings(audio)\n",
        "\n",
        "            return {\n",
        "                'log_mel_spec': log_mel_spec,\n",
        "                'delta': delta,\n",
        "                'delta_delta': delta_delta,\n",
        "                'wav2vec_embeddings': wav2vec_embeddings,\n",
        "                'audio_length': len(audio) / sr\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "# Automatic Speech Recognition\n",
        "class ASRTranscriber:\n",
        "    def __init__(self):\n",
        "        self.recognizer = sr.Recognizer()\n",
        "\n",
        "    def transcribe_audio(self, audio_path):\n",
        "        \"\"\"Transcribe audio to text using Google Speech Recognition\"\"\"\n",
        "        try:\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                audio_data = self.recognizer.record(source)\n",
        "                text = self.recognizer.recognize_google(audio_data)\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"ASR Error for {audio_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "# Linguistic Feature Extractor\n",
        "class LinguisticFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        # Initialize transformer models\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model = AutoModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "    def extract_transformer_features(self, text):\n",
        "        \"\"\"Extract BERT embeddings\"\"\"\n",
        "        try:\n",
        "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True,\n",
        "                                  padding=True, max_length=512)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert_model(**inputs)\n",
        "                embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting transformer features: {e}\")\n",
        "            return np.zeros(768)\n",
        "\n",
        "    def extract_psycholinguistic_features(self, text):\n",
        "        \"\"\"Extract psycholinguistic features using textstat\"\"\"\n",
        "        if not text.strip():\n",
        "            return np.zeros(10)\n",
        "\n",
        "        features = [\n",
        "            textstat.flesch_reading_ease(text),\n",
        "            textstat.flesch_kincaid_grade(text),\n",
        "            textstat.gunning_fog(text),\n",
        "            textstat.coleman_liau_index(text),\n",
        "            textstat.automated_readability_index(text),\n",
        "            textstat.avg_sentence_length(text),\n",
        "            textstat.avg_syllables_per_word(text),\n",
        "            textstat.sentence_count(text),\n",
        "            len(text.split()),  # word count\n",
        "            len(text)  # character count\n",
        "        ]\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def extract_repetitiveness_features(self, text):\n",
        "        \"\"\"Extract repetitiveness and fluency features\"\"\"\n",
        "        if not text.strip():\n",
        "            return np.zeros(5)\n",
        "\n",
        "        words = text.lower().split()\n",
        "        word_counts = Counter(words)\n",
        "\n",
        "        # Repetitiveness metrics\n",
        "        total_words = len(words)\n",
        "        unique_words = len(set(words))\n",
        "        repetition_ratio = 1 - (unique_words / max(total_words, 1))\n",
        "\n",
        "        # Filler words\n",
        "        filler_words = ['um', 'uh', 'er', 'ah', 'well', 'you know', 'like']\n",
        "        filler_count = sum(word_counts.get(filler, 0) for filler in filler_words)\n",
        "        filler_ratio = filler_count / max(total_words, 1)\n",
        "\n",
        "        # Pause indicators (simplified)\n",
        "        pause_indicators = text.count('...') + text.count('..') + text.count(' - ')\n",
        "        pause_ratio = pause_indicators / max(len(text.split('.')), 1)\n",
        "\n",
        "        return np.array([\n",
        "            repetition_ratio,\n",
        "            filler_ratio,\n",
        "            pause_ratio,\n",
        "            unique_words / max(total_words, 1),  # lexical diversity\n",
        "            len(set(words)) / max(len(words), 1)  # type-token ratio\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def extract_lexical_complexity_features(self, text):\n",
        "        \"\"\"Extract lexical complexity features\"\"\"\n",
        "        if not text.strip():\n",
        "            return np.zeros(8)\n",
        "\n",
        "        words = text.split()\n",
        "        sentences = text.split('.')\n",
        "\n",
        "        # Basic complexity metrics\n",
        "        avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
        "        avg_sentence_length = np.mean([len(sent.split()) for sent in sentences if sent.strip()]) if sentences else 0\n",
        "\n",
        "        # Syllable complexity (simplified)\n",
        "        def count_syllables(word):\n",
        "            word = word.lower().strip('.,!?\";')\n",
        "            count = 0\n",
        "            vowels = 'aeiouy'\n",
        "            if word and word[0] in vowels:\n",
        "                count += 1\n",
        "            for i in range(1, len(word)):\n",
        "                if word[i] in vowels and word[i-1] not in vowels:\n",
        "                    count += 1\n",
        "            if word.endswith('e'):\n",
        "                count -= 1\n",
        "            return max(count, 1)\n",
        "\n",
        "        syllable_counts = [count_syllables(word) for word in words]\n",
        "        avg_syllables = np.mean(syllable_counts) if syllable_counts else 0\n",
        "\n",
        "        return np.array([\n",
        "            avg_word_length,\n",
        "            avg_sentence_length,\n",
        "            avg_syllables,\n",
        "            len(words),  # total words\n",
        "            len(sentences),  # total sentences\n",
        "            len(set(words)),  # unique words\n",
        "            textstat.difficult_words(text),\n",
        "            textstat.polysyllabcount(text)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def extract_all_features(self, text):\n",
        "        \"\"\"Extract all linguistic features\"\"\"\n",
        "        transformer_features = self.extract_transformer_features(text)\n",
        "        psycholinguistic_features = self.extract_psycholinguistic_features(text)\n",
        "        repetitiveness_features = self.extract_repetitiveness_features(text)\n",
        "        lexical_complexity_features = self.extract_lexical_complexity_features(text)\n",
        "\n",
        "        return {\n",
        "            'transformer_features': transformer_features,\n",
        "            'psycholinguistic_features': psycholinguistic_features,\n",
        "            'repetitiveness_features': repetitiveness_features,\n",
        "            'lexical_complexity_features': lexical_complexity_features\n",
        "        }\n",
        "\n",
        "# Graph Neural Network for Feature Visualization\n",
        "class FeatureGNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=32):\n",
        "        super(FeatureGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Vision Transformer with Graph-based Attention\n",
        "class GraphViT(nn.Module):\n",
        "    def __init__(self, acoustic_dim, linguistic_dim, num_heads=8, num_layers=6):\n",
        "        super(GraphViT, self).__init__()\n",
        "        self.acoustic_dim = acoustic_dim\n",
        "        self.linguistic_dim = linguistic_dim\n",
        "        self.d_model = 512\n",
        "\n",
        "        # Projection layers\n",
        "        self.acoustic_proj = nn.Linear(acoustic_dim, self.d_model)\n",
        "        self.linguistic_proj = nn.Linear(linguistic_dim, self.d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1000, self.d_model))\n",
        "\n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Graph attention\n",
        "        self.graph_attention = nn.MultiheadAttention(self.d_model, num_heads)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.d_model, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)  # AD vs CN\n",
        "        )\n",
        "\n",
        "    def forward(self, acoustic_features, linguistic_features):\n",
        "        # Project features to common dimension\n",
        "        acoustic_proj = self.acoustic_proj(acoustic_features)\n",
        "        linguistic_proj = self.linguistic_proj(linguistic_features)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([acoustic_proj, linguistic_proj], dim=1)\n",
        "        seq_len = combined.size(1)\n",
        "\n",
        "        # Add positional encoding\n",
        "        combined = combined + self.pos_encoding[:seq_len]\n",
        "\n",
        "        # Transformer encoding\n",
        "        encoded = self.transformer(combined.transpose(0, 1)).transpose(0, 1)\n",
        "\n",
        "        # Graph-based attention between acoustic and linguistic features\n",
        "        acoustic_len = acoustic_proj.size(1)\n",
        "        acoustic_encoded = encoded[:, :acoustic_len]\n",
        "        linguistic_encoded = encoded[:, acoustic_len:]\n",
        "\n",
        "        # Cross-modal attention\n",
        "        attended, _ = self.graph_attention(\n",
        "            acoustic_encoded.transpose(0, 1),\n",
        "            linguistic_encoded.transpose(0, 1),\n",
        "            linguistic_encoded.transpose(0, 1)\n",
        "        )\n",
        "        attended = attended.transpose(0, 1)\n",
        "\n",
        "        # Global average pooling\n",
        "        pooled = torch.mean(attended, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(pooled)\n",
        "        return output\n",
        "\n",
        "# Dataset Class\n",
        "class ADReSSoDataset(Dataset):\n",
        "    def __init__(self, data_paths, config, mode='train'):\n",
        "        self.data_paths = data_paths\n",
        "        self.config = config\n",
        "        self.mode = mode\n",
        "        self.samples = []\n",
        "\n",
        "        # Initialize feature extractors\n",
        "        self.acoustic_extractor = AcousticFeatureExtractor()\n",
        "        self.asr_transcriber = ASRTranscriber()\n",
        "        self.linguistic_extractor = LinguisticFeatureExtractor()\n",
        "\n",
        "        # Load data samples\n",
        "        self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        \"\"\"Load all audio file paths and labels\"\"\"\n",
        "        if self.mode == 'diagnosis':\n",
        "            for label, path in self.data_paths.items():\n",
        "                if os.path.exists(path):\n",
        "                    for file in os.listdir(path):\n",
        "                        if file.endswith(('.wav', '.mp3', '.flac')):\n",
        "                            self.samples.append({\n",
        "                                'path': os.path.join(path, file),\n",
        "                                'label': 1 if label == 'ad' else 0,\n",
        "                                'task': 'diagnosis'\n",
        "                            })\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Extract acoustic features\n",
        "        acoustic_features = self.acoustic_extractor.extract_all_features(sample['path'])\n",
        "        if acoustic_features is None:\n",
        "            return None\n",
        "\n",
        "        # Transcribe audio to text\n",
        "        transcript = self.asr_transcriber.transcribe_audio(sample['path'])\n",
        "\n",
        "        # Extract linguistic features\n",
        "        linguistic_features = self.linguistic_extractor.extract_all_features(transcript)\n",
        "\n",
        "        return {\n",
        "            'acoustic_features': acoustic_features,\n",
        "            'linguistic_features': linguistic_features,\n",
        "            'transcript': transcript,\n",
        "            'label': sample['label'],\n",
        "            'file_path': sample['path']\n",
        "        }\n",
        "\n",
        "# Feature Visualization and Graph Creation\n",
        "class FeatureVisualizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def create_feature_graph(self, acoustic_features, linguistic_features):\n",
        "        \"\"\"Create a graph representing feature relationships\"\"\"\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # Add acoustic feature nodes\n",
        "        acoustic_nodes = ['log_mel', 'delta', 'delta_delta', 'wav2vec']\n",
        "        for node in acoustic_nodes:\n",
        "            G.add_node(node, type='acoustic')\n",
        "\n",
        "        # Add linguistic feature nodes\n",
        "        linguistic_nodes = ['transformer', 'psycholinguistic', 'repetitiveness', 'lexical']\n",
        "        for node in linguistic_nodes:\n",
        "            G.add_node(node, type='linguistic')\n",
        "\n",
        "        # Add edges based on feature correlations (simplified)\n",
        "        # In practice, you would compute actual correlations\n",
        "        edges = [\n",
        "            ('log_mel', 'transformer', {'weight': 0.3}),\n",
        "            ('wav2vec', 'transformer', {'weight': 0.7}),\n",
        "            ('delta', 'psycholinguistic', {'weight': 0.4}),\n",
        "            ('repetitiveness', 'delta_delta', {'weight': 0.2}),\n",
        "            ('lexical', 'log_mel', {'weight': 0.1})\n",
        "        ]\n",
        "\n",
        "        G.add_edges_from(edges)\n",
        "        return G\n",
        "\n",
        "    def visualize_feature_graph(self, G, title=\"Feature Interaction Graph\"):\n",
        "        \"\"\"Visualize the feature interaction graph\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Define colors for different node types\n",
        "        node_colors = []\n",
        "        for node in G.nodes():\n",
        "            if G.nodes[node]['type'] == 'acoustic':\n",
        "                node_colors.append('lightblue')\n",
        "            else:\n",
        "                node_colors.append('lightcoral')\n",
        "\n",
        "        # Create layout\n",
        "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
        "\n",
        "        # Draw the graph\n",
        "        nx.draw(G, pos,\n",
        "               node_color=node_colors,\n",
        "               node_size=2000,\n",
        "               with_labels=True,\n",
        "               font_size=10,\n",
        "               font_weight='bold',\n",
        "               edge_color='gray',\n",
        "               width=2)\n",
        "\n",
        "        # Add edge labels\n",
        "        edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)\n",
        "\n",
        "        plt.title(title, fontsize=16, fontweight='bold')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_feature_correlations(self, features_dict, title=\"Feature Correlations\"):\n",
        "        \"\"\"Plot correlation matrix of features\"\"\"\n",
        "        # Flatten all features into a single array\n",
        "        all_features = []\n",
        "        feature_names = []\n",
        "\n",
        "        for category, features in features_dict.items():\n",
        "            if isinstance(features, np.ndarray):\n",
        "                if features.ndim == 1:\n",
        "                    all_features.extend(features[:10])  # Limit to first 10 features\n",
        "                    feature_names.extend([f\"{category}_{i}\" for i in range(min(10, len(features)))])\n",
        "                else:\n",
        "                    flattened = features.flatten()[:10]\n",
        "                    all_features.extend(flattened)\n",
        "                    feature_names.extend([f\"{category}_{i}\" for i in range(len(flattened))])\n",
        "\n",
        "        # Create correlation matrix (simplified example)\n",
        "        if len(all_features) > 1:\n",
        "            feature_matrix = np.array(all_features).reshape(1, -1)\n",
        "            correlation_matrix = np.corrcoef(feature_matrix.T)\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(correlation_matrix,\n",
        "                       xticklabels=feature_names[:correlation_matrix.shape[0]],\n",
        "                       yticklabels=feature_names[:correlation_matrix.shape[1]],\n",
        "                       annot=True,\n",
        "                       cmap='coolwarm',\n",
        "                       center=0)\n",
        "            plt.title(title)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Training Pipeline\n",
        "class ADClassificationTrainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare training and validation datasets\"\"\"\n",
        "        # Create dataset for diagnosis task\n",
        "        diagnosis_paths = self.config.diagnosis_paths\n",
        "        dataset = ADReSSoDataset(diagnosis_paths, self.config, mode='diagnosis')\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            dataset, [train_size, val_size]\n",
        "        )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n",
        "                                collate_fn=self.collate_fn)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,\n",
        "                               collate_fn=self.collate_fn)\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"Custom collate function to handle variable-length sequences\"\"\"\n",
        "        # Filter out None samples\n",
        "        batch = [item for item in batch if item is not None]\n",
        "        if not batch:\n",
        "            return None\n",
        "\n",
        "        # Process acoustic features\n",
        "        acoustic_features = []\n",
        "        linguistic_features = []\n",
        "        labels = []\n",
        "\n",
        "        for item in batch:\n",
        "            # Flatten and concatenate acoustic features\n",
        "            acoustic = np.concatenate([\n",
        "                item['acoustic_features']['log_mel_spec'].flatten()[:1000],\n",
        "                item['acoustic_features']['delta'].flatten()[:1000],\n",
        "                item['acoustic_features']['delta_delta'].flatten()[:1000],\n",
        "                item['acoustic_features']['wav2vec_embeddings'].flatten()[:768]\n",
        "            ])\n",
        "\n",
        "            # Concatenate linguistic features\n",
        "            linguistic = np.concatenate([\n",
        "                item['linguistic_features']['transformer_features'].flatten()[:768],\n",
        "                item['linguistic_features']['psycholinguistic_features'],\n",
        "                item['linguistic_features']['repetitiveness_features'],\n",
        "                item['linguistic_features']['lexical_complexity_features']\n",
        "            ])\n",
        "\n",
        "            acoustic_features.append(acoustic)\n",
        "            linguistic_features.append(linguistic)\n",
        "            labels.append(item['label'])\n",
        "\n",
        "        return {\n",
        "            'acoustic': torch.FloatTensor(acoustic_features).to(self.device),\n",
        "            'linguistic': torch.FloatTensor(linguistic_features).to(self.device),\n",
        "            'labels': torch.LongTensor(labels).to(self.device)\n",
        "        }\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=50):\n",
        "        \"\"\"Train the multimodal model\"\"\"\n",
        "        # Initialize model\n",
        "        acoustic_dim = 3768  # log_mel + delta + delta_delta + wav2vec\n",
        "        linguistic_dim = 791  # transformer + psycholinguistic + repetitiveness + lexical\n",
        "\n",
        "        model = GraphViT(acoustic_dim, linguistic_dim).to(self.device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch in train_loader:\n",
        "                if batch is None:\n",
        "                    continue\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(batch['acoustic'], batch['linguistic'])\n",
        "                loss = criterion(outputs, batch['labels'])\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += batch['labels'].size(0)\n",
        "                train_correct += (predicted == batch['labels']).sum().item()\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    if batch is None:\n",
        "                        continue\n",
        "\n",
        "                    outputs = model(batch['acoustic'], batch['linguistic'])\n",
        "                    loss = criterion(outputs, batch['labels'])\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += batch['labels'].size(0)\n",
        "                    val_correct += (predicted == batch['labels']).sum().item()\n",
        "\n",
        "            # Calculate averages\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            train_acc = 100 * train_correct / train_total\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Save best model\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                torch.save(model.state_dict(), 'best_ad_model.pth')\n",
        "\n",
        "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "            print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "            print('-' * 60)\n",
        "\n",
        "        return model, train_losses, val_losses\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    \"\"\"Main function to run the entire pipeline\"\"\"\n",
        "    print(\"Initializing AD Classification System...\")\n",
        "\n",
        "    # Install requirements (uncomment for first run)\n",
        "    # install_requirements()\n",
        "\n",
        "    # Mount Google Drive (uncomment for Colab)\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "\n",
        "    # Initialize configuration\n",
        "    config = DataConfig()\n",
        "\n",
        "    # Initialize visualizer\n",
        "    visualizer = FeatureVisualizer()\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = ADClassificationTrainer(config)\n",
        "\n",
        "    print(\"Preparing data...\")\n",
        "    train_loader, val_loader = trainer.prepare_data()\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    model, train_losses, val_losses = trainer.train_model(train_loader, val_loader)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Create example feature graph\n",
        "    example_acoustic = {'log_mel': np.random.randn(128, 100)}\n",
        "    example_linguistic = {'transformer': np.random.randn(768)}\n",
        "    G = visualizer.create_feature_graph(example_acoustic, example_linguistic)\n",
        "    visualizer.visualize_feature_graph(G)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Training completed! Model saved as 'best_ad_model.pth'\")\n",
        "\n",
        "# Example usage for testing individual components\n",
        "def test_feature_extraction():\n",
        "    \"\"\"Test feature extraction on a single audio file\"\"\"\n",
        "    # Initialize extractors\n",
        "    acoustic_extractor = AcousticFeatureExtractor()\n",
        "    asr_transcriber = ASRTranscriber()\n",
        "    linguistic_extractor = LinguisticFeatureExtractor()\n",
        "    visualizer = FeatureVisualizer()\n",
        "\n",
        "    # Example audio file path (replace with actual path)\n",
        "    audio_path = \"/content/drive/MyDrive/Voice/extracted/ADReSSo21/diagnosis/train/audio/ad/sample.wav\"\n",
        "\n",
        "    if os.path.exists(audio_path):\n",
        "        print(f\"Processing: {audio_path}\")\n",
        "\n",
        "        # Extract acoustic features\n",
        "        acoustic_features = acoustic_extractor.extract_all_features(audio_path)\n",
        "        print(f\"Acoustic features extracted: {list(acoustic_features.keys())}\")\n",
        "\n",
        "        # Transcribe audio\n",
        "        transcript = asr_transcriber.transcribe_audio(audio_path)\n",
        "        print(f\"Transcript: {transcript[:100]}...\")\n",
        "\n",
        "        # Extract linguistic features\n",
        "        linguistic_features = linguistic_extractor.extract_all_features(transcript)\n",
        "        print(f\"Linguistic features extracted: {list(linguistic_features.keys())}\")\n",
        "\n",
        "        # Visualize feature relationships\n",
        "        G = visualizer.create_feature_graph(acoustic_features, linguistic_features)\n",
        "        visualizer.visualize_feature_graph(G, \"Sample Feature Interaction Graph\")\n",
        "\n",
        "        # Plot feature correlations\n",
        "        all_features = {**acoustic_features, **linguistic_features}\n",
        "        visualizer.plot_feature_correlations(all_features, \"Sample Feature Correlations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "FDkfks6bVDd0",
        "outputId": "c02d6a48-f50f-4f5e-a179-4388f4d31970"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'speech_recognition'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c1075e255899>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspeech_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextstat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'speech_recognition'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}