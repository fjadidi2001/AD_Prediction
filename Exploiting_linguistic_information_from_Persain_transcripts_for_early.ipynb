{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLRJrNu7bh3ZN1HAz50MeU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Exploiting_linguistic_information_from_Persain_transcripts_for_early.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H4z7R6CX61x",
        "outputId": "4755757f-8df6-4cab-b164-58517bc6d820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:❌ No potential dataset files found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Step 1 encountered errors. Please check the logs above.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 1: Data Acquisition and Preparation for Alzheimer's Disease Detection\n",
        "Pipeline using ADReSSo21 Dataset\n",
        "\n",
        "This script handles the extraction, organization, and initial preparation of\n",
        "transcripts from the ADReSSo21 dataset for early AD detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import csv\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ADReSSo21DataProcessor:\n",
        "    \"\"\"\n",
        "    Class to handle ADReSSo21 dataset extraction and preparation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str = \"/content/drive/MyDrive/Voice/ADReSSo21\"):\n",
        "        \"\"\"\n",
        "        Initialize the data processor\n",
        "\n",
        "        Args:\n",
        "            base_path (str): Base path where ADReSSo21 data will be stored\n",
        "        \"\"\"\n",
        "        self.base_path = Path(base_path)\n",
        "        self.extracted_path = self.base_path / \"extracted\"\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
        "        self.extracted_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Dataset file names\n",
        "        self.dataset_files = {\n",
        "            'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "            'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "            'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "        }\n",
        "\n",
        "        # Directory structure mapping\n",
        "        self.directory_structure = {\n",
        "            'progression_train': {\n",
        "                'segmentation': ['no_decline', 'decline'],\n",
        "                'audio': ['no_decline', 'decline']\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'segmentation': [''],  # test-dist has no subdirectories\n",
        "                'audio': ['']\n",
        "            },\n",
        "            'diagnosis_train': {\n",
        "                'segmentation': ['cn', 'ad'],\n",
        "                'audio': ['cn', 'ad']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def mount_google_drive(self):\n",
        "        \"\"\"\n",
        "        Mount Google Drive in Colab environment\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "\n",
        "            # Check if already mounted\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Google Drive already mounted\")\n",
        "                return True\n",
        "\n",
        "            drive.mount('/content/drive')\n",
        "            logger.info(\"Google Drive mounted successfully\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            logger.warning(\"Not running in Google Colab environment\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error mounting Google Drive: {e}\")\n",
        "            # Try to continue anyway if drive is accessible\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Drive appears to be accessible despite error\")\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "    def find_dataset_files(self):\n",
        "        \"\"\"\n",
        "        Search for ADReSSo21 dataset files in various formats and locations\n",
        "        \"\"\"\n",
        "        logger.info(f\"Searching for dataset files in: {self.base_path}\")\n",
        "\n",
        "        # Check if base directory exists\n",
        "        if not self.base_path.exists():\n",
        "            logger.error(f\"Base directory does not exist: {self.base_path}\")\n",
        "            logger.info(\"Checking parent directories...\")\n",
        "\n",
        "            # Check common alternative paths\n",
        "            alternative_paths = [\n",
        "                Path(\"/content/drive/MyDrive/Voice/\"),\n",
        "                Path(\"/content/drive/MyDrive/\"),\n",
        "                Path(\"/content/drive/\"),\n",
        "                Path(\"/content/\")\n",
        "            ]\n",
        "\n",
        "            for alt_path in alternative_paths:\n",
        "                if alt_path.exists():\n",
        "                    logger.info(f\"Found directory: {alt_path}\")\n",
        "                    # List contents\n",
        "                    items = list(alt_path.glob(\"*\"))\n",
        "                    for item in items[:10]:  # Show first 10 items\n",
        "                        logger.info(f\"  {item.name}\")\n",
        "                    if len(items) > 10:\n",
        "                        logger.info(f\"  ... and {len(items) - 10} more items\")\n",
        "\n",
        "            return None, []\n",
        "\n",
        "        # List all files in the base directory\n",
        "        all_files = list(self.base_path.glob(\"*\"))\n",
        "        logger.info(f\"Found {len(all_files)} items in base directory:\")\n",
        "\n",
        "        dataset_files = []\n",
        "\n",
        "        for item in all_files:\n",
        "            if item.is_file():\n",
        "                size_mb = item.stat().st_size / (1024*1024)\n",
        "                logger.info(f\"  FILE: {item.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "                # Check for various dataset file formats\n",
        "                if any(keyword in item.name.lower() for keyword in ['adresso', 'alzheimer', 'dementia']):\n",
        "                    dataset_files.append(item)\n",
        "            else:\n",
        "                logger.info(f\"  DIR:  {item.name}/\")\n",
        "\n",
        "        # Look for specific file types\n",
        "        file_types = {\n",
        "            '.tgz': list(self.base_path.glob(\"*.tgz\")),\n",
        "            '.tar.gz': list(self.base_path.glob(\"*.tar.gz\")),\n",
        "            '.zip': list(self.base_path.glob(\"*.zip\")),\n",
        "            '.rar': list(self.base_path.glob(\"*.rar\")),\n",
        "            '.7z': list(self.base_path.glob(\"*.7z\"))\n",
        "        }\n",
        "\n",
        "        found_archives = []\n",
        "        for file_type, files in file_types.items():\n",
        "            if files:\n",
        "                logger.info(f\"Found {len(files)} {file_type} files:\")\n",
        "                for file in files:\n",
        "                    logger.info(f\"  {file.name}\")\n",
        "                    found_archives.extend(files)\n",
        "\n",
        "        if dataset_files:\n",
        "            logger.info(f\"Found {len(dataset_files)} potential dataset files:\")\n",
        "            for file in dataset_files:\n",
        "                logger.info(f\"  {file.name}\")\n",
        "\n",
        "        return found_archives, dataset_files\n",
        "\n",
        "    def interactive_file_selection(self, found_archives, dataset_files):\n",
        "        \"\"\"\n",
        "        Help user identify and select the correct dataset files\n",
        "        \"\"\"\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"DATASET FILE DETECTION RESULTS\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        if not found_archives and not dataset_files:\n",
        "            logger.error(\"❌ No archive files or potential dataset files found!\")\n",
        "            logger.info(\"\\n📋 TROUBLESHOOTING STEPS:\")\n",
        "            logger.info(\"1. Verify you've uploaded the ADReSSo21 dataset files\")\n",
        "            logger.info(\"2. Check if files are in a different directory\")\n",
        "            logger.info(\"3. Ensure files are properly uploaded to Google Drive\")\n",
        "            logger.info(\"4. Check if files have different names or extensions\")\n",
        "            return None\n",
        "\n",
        "        logger.info(\"🔍 FOUND FILES ANALYSIS:\")\n",
        "\n",
        "        # Analyze found files\n",
        "        likely_candidates = []\n",
        "\n",
        "        for file in found_archives + dataset_files:\n",
        "            score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # Check file name for ADReSSo21 indicators\n",
        "            name_lower = file.name.lower()\n",
        "            if 'adresso' in name_lower:\n",
        "                score += 5\n",
        "                reasons.append(\"Contains 'ADReSSo'\")\n",
        "            if 'progression' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'progression'\")\n",
        "            if 'diagnosis' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'diagnosis'\")\n",
        "            if 'train' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'train'\")\n",
        "            if 'test' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'test'\")\n",
        "\n",
        "            # Check file size (ADReSSo21 files should be reasonably large)\n",
        "            size_mb = file.stat().st_size / (1024*1024)\n",
        "            if size_mb > 10:  # Larger than 10MB\n",
        "                score += 2\n",
        "                reasons.append(f\"Good size ({size_mb:.1f} MB)\")\n",
        "            elif size_mb > 1:\n",
        "                score += 1\n",
        "                reasons.append(f\"Moderate size ({size_mb:.1f} MB)\")\n",
        "\n",
        "            if score > 0:\n",
        "                likely_candidates.append((file, score, reasons))\n",
        "\n",
        "        # Sort by score\n",
        "        likely_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if likely_candidates:\n",
        "            logger.info(f\"🎯 TOP CANDIDATES (sorted by likelihood):\")\n",
        "            for i, (file, score, reasons) in enumerate(likely_candidates[:5]):\n",
        "                logger.info(f\"  {i+1}. {file.name} (Score: {score})\")\n",
        "                logger.info(f\"     Reasons: {', '.join(reasons)}\")\n",
        "                logger.info(f\"     Path: {file}\")\n",
        "\n",
        "        # Return the most likely candidate for automatic processing\n",
        "        if likely_candidates and likely_candidates[0][1] >= 5:\n",
        "            return likely_candidates[0][0]\n",
        "\n",
        "        return None\n",
        "    def extract_any_archive(self, file_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Extract archive files in various formats\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to the archive file\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_extension = file_path.suffix.lower()\n",
        "            extract_dir = self.extracted_path / file_path.stem\n",
        "            extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            logger.info(f\"Attempting to extract: {file_path.name}\")\n",
        "\n",
        "            if file_extension in ['.tgz', '.gz'] or file_path.name.endswith('.tar.gz'):\n",
        "                # Handle .tgz and .tar.gz files\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            elif file_extension == '.zip':\n",
        "                # Handle .zip files\n",
        "                import zipfile\n",
        "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_dir)\n",
        "\n",
        "            elif file_extension == '.tar':\n",
        "                # Handle .tar files\n",
        "                with tarfile.open(file_path, 'r') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            else:\n",
        "                logger.error(f\"Unsupported archive format: {file_extension}\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully extracted {file_path.name} to {extract_dir}\")\n",
        "\n",
        "            # List extracted contents\n",
        "            extracted_items = list(extract_dir.rglob(\"*\"))\n",
        "            logger.info(f\"Extracted {len(extracted_items)} items\")\n",
        "\n",
        "            # Show directory structure\n",
        "            dirs = [item for item in extracted_items if item.is_dir()]\n",
        "            files = [item for item in extracted_items if item.is_file()]\n",
        "\n",
        "            logger.info(f\"  Directories: {len(dirs)}\")\n",
        "            logger.info(f\"  Files: {len(files)}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting {file_path}: {e}\")\n",
        "            return False\n",
        "    def extract_tgz_files(self) -> bool:\n",
        "        \"\"\"\n",
        "        Extract all archive files to the extraction directory\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Find potential dataset files\n",
        "            found_archives, dataset_files = self.find_dataset_files()\n",
        "\n",
        "            # Try to identify the best candidates\n",
        "            best_candidate = self.interactive_file_selection(found_archives, dataset_files)\n",
        "\n",
        "            if best_candidate:\n",
        "                logger.info(f\"🎯 Attempting to extract most likely candidate: {best_candidate.name}\")\n",
        "                if self.extract_any_archive(best_candidate):\n",
        "                    return True\n",
        "\n",
        "            # If no clear candidate, try all archive files\n",
        "            if found_archives:\n",
        "                logger.info(\"🔄 Trying to extract all found archive files...\")\n",
        "                extracted_any = False\n",
        "\n",
        "                for archive in found_archives:\n",
        "                    if self.extract_any_archive(archive):\n",
        "                        extracted_any = True\n",
        "\n",
        "                return extracted_any\n",
        "\n",
        "            # Fallback: try the original method for exact file names\n",
        "            logger.info(\"🔄 Trying original extraction method...\")\n",
        "            extracted_any = False\n",
        "\n",
        "            for dataset_name, filename in self.dataset_files.items():\n",
        "                file_path = self.base_path / filename\n",
        "\n",
        "                if not file_path.exists():\n",
        "                    logger.warning(f\"Expected file not found: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Extracting {filename}...\")\n",
        "\n",
        "                # Extract to specific subdirectory\n",
        "                extract_dir = self.extracted_path / dataset_name\n",
        "                extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "                logger.info(f\"Successfully extracted {filename}\")\n",
        "                extracted_any = True\n",
        "\n",
        "            return extracted_any\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {e}\")\n",
        "            return False\n",
        "\n",
        "    def verify_directory_structure(self) -> Dict[str, bool]:\n",
        "        \"\"\"\n",
        "        Verify that the extracted directories match expected structure\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, bool]: Status of each dataset extraction\n",
        "        \"\"\"\n",
        "        verification_results = {}\n",
        "\n",
        "        for dataset_name, structure in self.directory_structure.items():\n",
        "            dataset_path = self.extracted_path / dataset_name / \"ADReSSo21\"\n",
        "\n",
        "            # Check if main dataset directory exists\n",
        "            if not dataset_path.exists():\n",
        "                verification_results[dataset_name] = False\n",
        "                logger.error(f\"Dataset directory not found: {dataset_path}\")\n",
        "                continue\n",
        "\n",
        "            # Verify subdirectories\n",
        "            all_dirs_exist = True\n",
        "\n",
        "            for data_type, subdirs in structure.items():\n",
        "                if dataset_name == 'progression_test':\n",
        "                    # Special case for test data\n",
        "                    seg_path = dataset_path / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "                    audio_path = dataset_path / \"progression\" / \"test-dist\" / \"audio\"\n",
        "\n",
        "                    if not (seg_path.exists() and audio_path.exists()):\n",
        "                        all_dirs_exist = False\n",
        "                        logger.error(f\"Test directories missing in {dataset_name}\")\n",
        "                else:\n",
        "                    # Regular structure for train data\n",
        "                    base_type_path = dataset_path / (\"progression\" if \"progression\" in dataset_name else \"diagnosis\")\n",
        "\n",
        "                    for subdir in subdirs:\n",
        "                        if subdir:  # Skip empty strings\n",
        "                            seg_path = base_type_path / \"train\" / \"segmentation\" / subdir\n",
        "                            audio_path = base_type_path / \"train\" / \"audio\" / subdir\n",
        "\n",
        "                            if not (seg_path.exists() and audio_path.exists()):\n",
        "                                all_dirs_exist = False\n",
        "                                logger.error(f\"Missing directories for {dataset_name}/{subdir}\")\n",
        "\n",
        "            verification_results[dataset_name] = all_dirs_exist\n",
        "\n",
        "            if all_dirs_exist:\n",
        "                logger.info(f\"✓ Directory structure verified for {dataset_name}\")\n",
        "            else:\n",
        "                logger.warning(f\"✗ Directory structure issues found for {dataset_name}\")\n",
        "\n",
        "        return verification_results\n",
        "\n",
        "    def extract_transcripts_from_csv(self, csv_file_path: Path) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract transcript data from a single CSV file following CHAT protocol\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (Path): Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of transcript segments with metadata\n",
        "        \"\"\"\n",
        "        transcripts = []\n",
        "\n",
        "        try:\n",
        "            # Try different encodings as CSV files might have various encodings\n",
        "            encodings = ['utf-8', 'latin-1', 'cp1252']\n",
        "            df = None\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_file_path, encoding=encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                logger.error(f\"Could not read CSV file with any encoding: {csv_file_path}\")\n",
        "                return transcripts\n",
        "\n",
        "            # Log column names for inspection\n",
        "            logger.info(f\"CSV columns in {csv_file_path.name}: {list(df.columns)}\")\n",
        "\n",
        "            # Extract relevant columns (adjust based on actual CSV structure)\n",
        "            # Common CHAT protocol columns might include: speaker, utterance, time, etc.\n",
        "            for index, row in df.iterrows():\n",
        "                transcript_entry = {\n",
        "                    'file_id': csv_file_path.stem,\n",
        "                    'row_index': index,\n",
        "                    'data': dict(row)  # Store all columns for now\n",
        "                }\n",
        "\n",
        "                # Look for text/utterance columns (common names in CHAT protocol)\n",
        "                text_columns = ['utterance', 'text', 'transcript', 'speech', 'content']\n",
        "                for col in text_columns:\n",
        "                    if col in df.columns and pd.notna(row[col]):\n",
        "                        transcript_entry['transcript'] = str(row[col])\n",
        "                        break\n",
        "\n",
        "                transcripts.append(transcript_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing CSV file {csv_file_path}: {e}\")\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def collect_all_transcripts(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Collect all transcripts from segmentation CSV files\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Dict]]: Organized transcripts by category\n",
        "        \"\"\"\n",
        "        all_transcripts = {\n",
        "            'progression_train_no_decline': [],\n",
        "            'progression_train_decline': [],\n",
        "            'progression_test': [],\n",
        "            'diagnosis_train_cn': [],\n",
        "            'diagnosis_train_ad': []\n",
        "        }\n",
        "\n",
        "        # Process progression training data\n",
        "        prog_train_path = self.extracted_path / \"progression_train\" / \"ADReSSo21\" / \"progression\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['no_decline', 'decline']:\n",
        "            csv_dir = prog_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'progression_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process progression test data\n",
        "        prog_test_path = self.extracted_path / \"progression_test\" / \"ADReSSo21\" / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "        if prog_test_path.exists():\n",
        "            for csv_file in prog_test_path.glob('*.csv'):\n",
        "                transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                all_transcripts['progression_test'].extend(transcripts)\n",
        "                logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process diagnosis training data\n",
        "        diag_train_path = self.extracted_path / \"diagnosis_train\" / \"ADReSSo21\" / \"diagnosis\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['cn', 'ad']:\n",
        "            csv_dir = diag_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'diagnosis_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        return all_transcripts\n",
        "\n",
        "    def save_transcripts_summary(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Save a summary of extracted transcripts for review\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the saved summary file\n",
        "        \"\"\"\n",
        "        summary_file = self.base_path / \"transcripts_summary.txt\"\n",
        "\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"ADReSSo21 Dataset Transcripts Summary\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            total_transcripts = 0\n",
        "            for category, transcript_list in transcripts.items():\n",
        "                count = len(transcript_list)\n",
        "                total_transcripts += count\n",
        "                f.write(f\"{category}: {count} transcript entries\\n\")\n",
        "\n",
        "                # Show sample transcript if available\n",
        "                if transcript_list and 'transcript' in transcript_list[0]:\n",
        "                    sample = transcript_list[0]['transcript'][:100] + \"...\" if len(transcript_list[0]['transcript']) > 100 else transcript_list[0]['transcript']\n",
        "                    f.write(f\"  Sample: {sample}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(f\"Total transcript entries: {total_transcripts}\\n\")\n",
        "            f.write(\"\\nNote: These English transcripts need to be translated to Persian for the study.\\n\")\n",
        "            f.write(\"Translation should be done manually by native Persian speakers as per the methodology.\\n\")\n",
        "\n",
        "        logger.info(f\"Transcripts summary saved to: {summary_file}\")\n",
        "        return summary_file\n",
        "\n",
        "    def prepare_for_translation(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Prepare transcript files for manual Persian translation\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the translation directory\n",
        "        \"\"\"\n",
        "        translation_dir = self.base_path / \"for_translation\"\n",
        "        translation_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        for category, transcript_list in transcripts.items():\n",
        "            if not transcript_list:\n",
        "                continue\n",
        "\n",
        "            # Create CSV file for translation\n",
        "            csv_file = translation_dir / f\"{category}_for_translation.csv\"\n",
        "\n",
        "            with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['ID', 'Original_English', 'Persian_Translation', 'Notes'])\n",
        "\n",
        "                for i, entry in enumerate(transcript_list):\n",
        "                    if 'transcript' in entry:\n",
        "                        transcript_id = f\"{category}_{i+1}\"\n",
        "                        english_text = entry['transcript']\n",
        "                        writer.writerow([transcript_id, english_text, '', ''])\n",
        "\n",
        "            logger.info(f\"Created translation file: {csv_file}\")\n",
        "\n",
        "        # Create translation instructions\n",
        "        instructions_file = translation_dir / \"TRANSLATION_INSTRUCTIONS.txt\"\n",
        "        with open(instructions_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"PERSIAN TRANSLATION INSTRUCTIONS\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "            f.write(\"IMPORTANT REQUIREMENTS:\\n\")\n",
        "            f.write(\"1. Translation must be done by native Persian speakers\\n\")\n",
        "            f.write(\"2. Translator should have at least 13 years of formal Persian education\\n\")\n",
        "            f.write(\"3. Translation should be verified by an independent linguistic expert\\n\")\n",
        "            f.write(\"4. PRESERVE ALL linguistic features:\\n\")\n",
        "            f.write(\"   - Pause words (uhm, uhh, etc.) - translate equivalent Persian pause words\\n\")\n",
        "            f.write(\"   - Repetitions - keep all repetitions\\n\")\n",
        "            f.write(\"   - Linguistic errors - preserve grammatical/syntactic errors\\n\")\n",
        "            f.write(\"   - Syntactic errors - maintain sentence structure issues\\n\")\n",
        "            f.write(\"5. EXCLUDE annotations like [clears throat], [laughs], etc.\\n\")\n",
        "            f.write(\"6. Do NOT use machine translation - manual translation only\\n\")\n",
        "            f.write(\"7. Capture cultural and linguistic nuances specific to Persian\\n\\n\")\n",
        "            f.write(\"Fill in the 'Persian_Translation' column in each CSV file.\\n\")\n",
        "            f.write(\"Use 'Notes' column for any translation decisions or concerns.\\n\")\n",
        "\n",
        "        logger.info(f\"Translation instructions saved to: {instructions_file}\")\n",
        "        return translation_dir\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute Step 1 of the pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "\n",
        "    # Initialize the data processor with the correct path based on your error\n",
        "    processor = ADReSSo21DataProcessor()\n",
        "\n",
        "    # Step 1.1: Mount Google Drive (if in Colab)\n",
        "    logger.info(\"Step 1.1: Mounting Google Drive...\")\n",
        "    drive_mounted = processor.mount_google_drive()\n",
        "\n",
        "    # Step 1.2: Search for dataset files\n",
        "    logger.info(\"Step 1.2: Searching for dataset files...\")\n",
        "    found_archives, dataset_files = processor.find_dataset_files()\n",
        "\n",
        "    if not found_archives and not dataset_files:\n",
        "        logger.error(\"❌ No potential dataset files found!\")\n",
        "        logger.info(\"\\n📋 PLEASE CHECK:\")\n",
        "        logger.info(\"1. Are the ADReSSo21 files uploaded to Google Drive?\")\n",
        "        logger.info(\"2. Are they in the correct directory?\")\n",
        "        logger.info(f\"   Expected location: {processor.base_path}\")\n",
        "        logger.info(\"3. Do they have the expected names:\")\n",
        "        logger.info(\"   - ADReSSo21-progression-train.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-progression-test.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-diagnosis-train.tgz\")\n",
        "        logger.info(\"4. Or are they in a different format (.zip, .rar, etc.)?\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.3: Extract dataset files\n",
        "    logger.info(\"Step 1.3: Extracting dataset files...\")\n",
        "    extraction_success = processor.extract_tgz_files()\n",
        "\n",
        "    if not extraction_success:\n",
        "        logger.error(\"❌ Failed to extract any dataset files.\")\n",
        "        logger.info(\"\\n🔧 POSSIBLE SOLUTIONS:\")\n",
        "        logger.info(\"1. Check if files are corrupted - try re-downloading\")\n",
        "        logger.info(\"2. Try extracting files manually first\")\n",
        "        logger.info(\"3. Ensure files are not password protected\")\n",
        "        logger.info(\"4. Check if files are in an unsupported format\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.4: Verify directory structure\n",
        "    logger.info(\"Step 1.4: Verifying directory structure...\")\n",
        "    verification_results = processor.verify_directory_structure()\n",
        "\n",
        "    successful_extractions = [k for k, v in verification_results.items() if v]\n",
        "\n",
        "    if not successful_extractions:\n",
        "        logger.error(\"No datasets were successfully extracted and verified.\")\n",
        "        return False\n",
        "\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "\n",
        "    # Step 1.5: Extract transcripts from CSV files\n",
        "    logger.info(\"Step 1.5: Extracting transcripts from segmentation CSV files...\")\n",
        "    all_transcripts = processor.collect_all_transcripts()\n",
        "\n",
        "    # Check if we actually got any transcripts\n",
        "    total_transcripts = sum(len(transcripts) for transcripts in all_transcripts.values())\n",
        "\n",
        "    if total_transcripts == 0:\n",
        "        logger.error(\"No transcripts were extracted from CSV files!\")\n",
        "        logger.info(\"This might indicate:\")\n",
        "        logger.info(\"  - CSV files are in a different format than expected\")\n",
        "        logger.info(\"  - Directory structure is different\")\n",
        "        logger.info(\"  - Files are corrupted\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.6: Save summary and prepare for translation\n",
        "    logger.info(\"Step 1.6: Saving transcripts summary...\")\n",
        "    processor.save_transcripts_summary(all_transcripts)\n",
        "\n",
        "    logger.info(\"Step 1.7: Preparing files for Persian translation...\")\n",
        "    translation_dir = processor.prepare_for_translation(all_transcripts)\n",
        "\n",
        "    # Final summary\n",
        "    logger.info(f\"\\n{'='*50}\")\n",
        "    logger.info(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "    logger.info(f\"Total transcript entries extracted: {total_transcripts}\")\n",
        "    logger.info(f\"Translation files prepared in: {translation_dir}\")\n",
        "    logger.info(\"NEXT STEPS:\")\n",
        "    logger.info(\"1. Have native Persian speakers translate the CSV files\")\n",
        "    logger.info(\"2. Verify translations with linguistic expert\")\n",
        "    logger.info(\"3. Return translated files for Step 2 (Data Preprocessing)\")\n",
        "    logger.info(f\"{'='*50}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if success:\n",
        "        print(\"\\n✅ Step 1 completed successfully!\")\n",
        "        print(\"📁 Check the translation directory for files to be translated to Persian\")\n",
        "        print(\"🔄 Once translation is complete, you can proceed to Step 2\")\n",
        "    else:\n",
        "        print(\"\\n❌ Step 1 encountered errors. Please check the logs above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1fFwtsjbfA_",
        "outputId": "74e92c53-6e31-4b99-ccb6-d0b074144ffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/MyDrive/Voice'\n",
        "extracted_base_path = '/content/drive/MyDrive/Voice/Extracted_dataset'\n",
        "\n",
        "# Dataset file paths\n",
        "dataset_files = {\n",
        "    'diagnosis_train': '/content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train.tgz',\n",
        "    'progression_test': '/content/drive/MyDrive/Voice/ADReSSo21-progression-test.tgz',\n",
        "    'progression_train': '/content/drive/MyDrive/Voice/ADReSSo21-progression-train.tgz'\n",
        "}\n",
        "\n",
        "def create_directory_structure():\n",
        "    \"\"\"Create the directory structure for extracted datasets\"\"\"\n",
        "    print(\"Creating directory structure...\")\n",
        "\n",
        "    # Create main extracted dataset folder\n",
        "    os.makedirs(extracted_base_path, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for each dataset\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_dir = os.path.join(extracted_base_path, dataset_name)\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Directory structure created at: {extracted_base_path}\")\n",
        "\n",
        "def extract_dataset(tgz_path, extract_to_path, dataset_name):\n",
        "    \"\"\"Extract a .tgz file to the specified directory\"\"\"\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "\n",
        "    if not os.path.exists(tgz_path):\n",
        "        print(f\"ERROR: File not found - {tgz_path}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
        "            tar.extractall(path=extract_to_path)\n",
        "        print(f\"Successfully extracted {dataset_name} to {extract_to_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR extracting {dataset_name}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def inspect_extracted_structure():\n",
        "    \"\"\"Inspect the extracted directory structure and report findings\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INSPECTING EXTRACTED DATASET STRUCTURE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        print(f\"\\n--- {dataset_name.upper()} ---\")\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            # Walk through the directory structure\n",
        "            for root, dirs, files in os.walk(dataset_path):\n",
        "                level = root.replace(dataset_path, '').count(os.sep)\n",
        "                indent = ' ' * 2 * level\n",
        "                print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "                # Show first few files in each directory\n",
        "                sub_indent = ' ' * 2 * (level + 1)\n",
        "                for file in files[:5]:  # Show first 5 files\n",
        "                    print(f\"{sub_indent}{file}\")\n",
        "                if len(files) > 5:\n",
        "                    print(f\"{sub_indent}... and {len(files) - 5} more files\")\n",
        "        else:\n",
        "            print(f\"Directory not found: {dataset_path}\")\n",
        "\n",
        "def find_and_inspect_csv_files():\n",
        "    \"\"\"Find CSV files containing transcripts and inspect their structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INSPECTING CSV FILES (TRANSCRIPTS)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    csv_files_found = []\n",
        "\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        # Look for CSV files in segmentation directories\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.csv'):\n",
        "                        csv_path = os.path.join(root, file)\n",
        "                        csv_files_found.append((dataset_name, root, file, csv_path))\n",
        "\n",
        "    print(f\"Found {len(csv_files_found)} CSV files total\")\n",
        "\n",
        "    # Inspect structure of first few CSV files\n",
        "    for i, (dataset_name, directory, filename, full_path) in enumerate(csv_files_found[:3]):\n",
        "        print(f\"\\n--- CSV File {i+1}: {filename} ---\")\n",
        "        print(f\"Dataset: {dataset_name}\")\n",
        "        print(f\"Directory: {directory}\")\n",
        "\n",
        "        try:\n",
        "            # Read and inspect CSV structure\n",
        "            df = pd.read_csv(full_path)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "            print(\"First few rows:\")\n",
        "            print(df.head(2).to_string())\n",
        "\n",
        "            # Check for transcript-like content\n",
        "            for col in df.columns:\n",
        "                if any(keyword in col.lower() for keyword in ['transcript', 'text', 'utterance', 'speech']):\n",
        "                    print(f\"\\nSample content from '{col}':\")\n",
        "                    sample_content = df[col].dropna().head(2).tolist()\n",
        "                    for content in sample_content:\n",
        "                        print(f\"  '{str(content)[:100]}...'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV: {str(e)}\")\n",
        "\n",
        "    return csv_files_found\n",
        "\n",
        "def generate_summary_report(csv_files_found):\n",
        "    \"\"\"Generate a summary report of the extracted data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Count files by dataset and category\n",
        "    summary = {}\n",
        "    for dataset_name, directory, filename, full_path in csv_files_found:\n",
        "        if dataset_name not in summary:\n",
        "            summary[dataset_name] = {'total_csv': 0, 'categories': {}}\n",
        "\n",
        "        summary[dataset_name]['total_csv'] += 1\n",
        "\n",
        "        # Determine category (cn, ad, decline, no_decline)\n",
        "        category = 'unknown'\n",
        "        if '/cn/' in directory:\n",
        "            category = 'cn'\n",
        "        elif '/ad/' in directory:\n",
        "            category = 'ad'\n",
        "        elif '/decline/' in directory:\n",
        "            category = 'decline'\n",
        "        elif '/no_decline/' in directory:\n",
        "            category = 'no_decline'\n",
        "\n",
        "        if category not in summary[dataset_name]['categories']:\n",
        "            summary[dataset_name]['categories'][category] = 0\n",
        "        summary[dataset_name]['categories'][category] += 1\n",
        "\n",
        "    # Print summary\n",
        "    for dataset_name, data in summary.items():\n",
        "        print(f\"\\n{dataset_name.upper()}:\")\n",
        "        print(f\"  Total CSV files: {data['total_csv']}\")\n",
        "        print(\"  Categories:\")\n",
        "        for category, count in data['categories'].items():\n",
        "            print(f\"    {category}: {count} files\")\n",
        "\n",
        "    print(f\"\\nExtracted dataset location: {extracted_base_path}\")\n",
        "    print(\"Ready for Step 2: Translation to Persian\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1.1: Create directory structure\n",
        "    create_directory_structure()\n",
        "\n",
        "    # Step 1.2: Extract all datasets\n",
        "    print(\"\\nExtracting datasets...\")\n",
        "    extraction_success = True\n",
        "\n",
        "    for dataset_name, tgz_path in dataset_files.items():\n",
        "        extract_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        success = extract_dataset(tgz_path, extract_path, dataset_name)\n",
        "        if not success:\n",
        "            extraction_success = False\n",
        "\n",
        "    if not extraction_success:\n",
        "        print(\"\\nERROR: Some extractions failed. Please check the file paths and try again.\")\n",
        "        return\n",
        "\n",
        "    # Step 1.3: Inspect extracted structure\n",
        "    inspect_extracted_structure()\n",
        "\n",
        "    # Step 1.4: Find and inspect CSV files (transcripts)\n",
        "    csv_files_found = find_and_inspect_csv_files()\n",
        "\n",
        "    # Step 1.5: Generate summary report\n",
        "    generate_summary_report(csv_files_found)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Next step: Manual translation of English transcripts to Persian\")\n",
        "    print(\"Note: The paper emphasizes using native Persian speakers for translation\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpuJVWoUjYeC",
        "outputId": "b425378f-9915-4640-dfe5-cb8f0898f54c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting Step 1: Data Acquisition and Preparation\n",
            "============================================================\n",
            "Creating directory structure...\n",
            "Directory structure created at: /content/drive/MyDrive/Voice/Extracted_dataset\n",
            "\n",
            "Extracting datasets...\n",
            "Extracting diagnosis_train...\n",
            "Successfully extracted diagnosis_train to /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train\n",
            "Extracting progression_test...\n",
            "Successfully extracted progression_test to /content/drive/MyDrive/Voice/Extracted_dataset/progression_test\n",
            "Extracting progression_train...\n",
            "Successfully extracted progression_train to /content/drive/MyDrive/Voice/Extracted_dataset/progression_train\n",
            "\n",
            "============================================================\n",
            "INSPECTING EXTRACTED DATASET STRUCTURE\n",
            "============================================================\n",
            "\n",
            "--- DIAGNOSIS_TRAIN ---\n",
            "diagnosis_train/\n",
            "  ADReSSo21/\n",
            "    diagnosis/\n",
            "      README.md\n",
            "      train/\n",
            "        adresso-train-mmse-scores.csv\n",
            "        segmentation/\n",
            "          cn/\n",
            "            adrso281.csv\n",
            "            adrso308.csv\n",
            "            adrso270.csv\n",
            "            adrso022.csv\n",
            "            adrso298.csv\n",
            "            ... and 74 more files\n",
            "          ad/\n",
            "            adrso229.csv\n",
            "            adrso106.csv\n",
            "            adrso144.csv\n",
            "            adrso049.csv\n",
            "            adrso078.csv\n",
            "            ... and 82 more files\n",
            "        audio/\n",
            "          cn/\n",
            "            adrso173.wav\n",
            "            adrso015.wav\n",
            "            adrso307.wav\n",
            "            adrso283.wav\n",
            "            adrso167.wav\n",
            "            ... and 74 more files\n",
            "          ad/\n",
            "            adrso047.wav\n",
            "            adrso128.wav\n",
            "            adrso045.wav\n",
            "            adrso110.wav\n",
            "            adrso036.wav\n",
            "            ... and 82 more files\n",
            "\n",
            "--- PROGRESSION_TEST ---\n",
            "progression_test/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      test-dist/\n",
            "        test_results_task3.csv\n",
            "        README\n",
            "        segmentation/\n",
            "          adrspt24.csv\n",
            "          adrspt15.csv\n",
            "          adrspt12.csv\n",
            "          adrspt2.csv\n",
            "          adrspt9.csv\n",
            "          ... and 10 more files\n",
            "        audio/\n",
            "          adrspt20.wav\n",
            "          adrspt15.wav\n",
            "          adrspt4.wav\n",
            "          adrspt28.wav\n",
            "          adrspt16.wav\n",
            "          ... and 27 more files\n",
            "\n",
            "--- PROGRESSION_TRAIN ---\n",
            "progression_train/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      README.md\n",
            "      train/\n",
            "        segmentation/\n",
            "          no_decline/\n",
            "            adrsp195.csv\n",
            "            adrsp041.csv\n",
            "            adrsp030.csv\n",
            "            adrsp052.csv\n",
            "            adrsp349.csv\n",
            "            ... and 32 more files\n",
            "          decline/\n",
            "            adrsp051.csv\n",
            "            adrsp313.csv\n",
            "            adrsp101.csv\n",
            "            adrsp055.csv\n",
            "            adrsp179.csv\n",
            "            ... and 5 more files\n",
            "        audio/\n",
            "          no_decline/\n",
            "            adrsp196.wav\n",
            "            adrsp137.wav\n",
            "            adrsp130.wav\n",
            "            adrsp349.wav\n",
            "            adrsp198.wav\n",
            "            ... and 53 more files\n",
            "          decline/\n",
            "            adrsp055.wav\n",
            "            adrsp003.wav\n",
            "            adrsp266.wav\n",
            "            adrsp300.wav\n",
            "            adrsp320.wav\n",
            "            ... and 10 more files\n",
            "\n",
            "============================================================\n",
            "INSPECTING CSV FILES (TRANSCRIPTS)\n",
            "============================================================\n",
            "Found 228 CSV files total\n",
            "\n",
            "--- CSV File 1: adrso281.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (18, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  2129\n",
            "1           2     PAR   2129  5471\n",
            "\n",
            "--- CSV File 2: adrso308.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (24, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  4873\n",
            "1           2     PAR   4873  7270\n",
            "\n",
            "--- CSV File 3: adrso270.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (20, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     INV      0   997\n",
            "1           2     PAR    997  2294\n",
            "\n",
            "============================================================\n",
            "SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            "DIAGNOSIS_TRAIN:\n",
            "  Total CSV files: 166\n",
            "  Categories:\n",
            "    unknown: 166 files\n",
            "\n",
            "PROGRESSION_TEST:\n",
            "  Total CSV files: 15\n",
            "  Categories:\n",
            "    unknown: 15 files\n",
            "\n",
            "PROGRESSION_TRAIN:\n",
            "  Total CSV files: 47\n",
            "  Categories:\n",
            "    unknown: 47 files\n",
            "\n",
            "Extracted dataset location: /content/drive/MyDrive/Voice/Extracted_dataset\n",
            "Ready for Step 2: Translation to Persian\n",
            "\n",
            "============================================================\n",
            "STEP 1 COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Next step: Manual translation of English transcripts to Persian\n",
            "Note: The paper emphasizes using native Persian speakers for translation\n"
          ]
        }
      ]
    }
  ]
}