{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNenodYjwwl3zerpoT8EPPy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Exploiting_linguistic_information_from_Persain_transcripts_for_early.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H4z7R6CX61x",
        "outputId": "4755757f-8df6-4cab-b164-58517bc6d820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:❌ No potential dataset files found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Step 1 encountered errors. Please check the logs above.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 1: Data Acquisition and Preparation for Alzheimer's Disease Detection\n",
        "Pipeline using ADReSSo21 Dataset\n",
        "\n",
        "This script handles the extraction, organization, and initial preparation of\n",
        "transcripts from the ADReSSo21 dataset for early AD detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import csv\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ADReSSo21DataProcessor:\n",
        "    \"\"\"\n",
        "    Class to handle ADReSSo21 dataset extraction and preparation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str = \"/content/drive/MyDrive/Voice/ADReSSo21\"):\n",
        "        \"\"\"\n",
        "        Initialize the data processor\n",
        "\n",
        "        Args:\n",
        "            base_path (str): Base path where ADReSSo21 data will be stored\n",
        "        \"\"\"\n",
        "        self.base_path = Path(base_path)\n",
        "        self.extracted_path = self.base_path / \"extracted\"\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
        "        self.extracted_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Dataset file names\n",
        "        self.dataset_files = {\n",
        "            'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "            'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "            'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "        }\n",
        "\n",
        "        # Directory structure mapping\n",
        "        self.directory_structure = {\n",
        "            'progression_train': {\n",
        "                'segmentation': ['no_decline', 'decline'],\n",
        "                'audio': ['no_decline', 'decline']\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'segmentation': [''],  # test-dist has no subdirectories\n",
        "                'audio': ['']\n",
        "            },\n",
        "            'diagnosis_train': {\n",
        "                'segmentation': ['cn', 'ad'],\n",
        "                'audio': ['cn', 'ad']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def mount_google_drive(self):\n",
        "        \"\"\"\n",
        "        Mount Google Drive in Colab environment\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "\n",
        "            # Check if already mounted\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Google Drive already mounted\")\n",
        "                return True\n",
        "\n",
        "            drive.mount('/content/drive')\n",
        "            logger.info(\"Google Drive mounted successfully\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            logger.warning(\"Not running in Google Colab environment\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error mounting Google Drive: {e}\")\n",
        "            # Try to continue anyway if drive is accessible\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Drive appears to be accessible despite error\")\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "    def find_dataset_files(self):\n",
        "        \"\"\"\n",
        "        Search for ADReSSo21 dataset files in various formats and locations\n",
        "        \"\"\"\n",
        "        logger.info(f\"Searching for dataset files in: {self.base_path}\")\n",
        "\n",
        "        # Check if base directory exists\n",
        "        if not self.base_path.exists():\n",
        "            logger.error(f\"Base directory does not exist: {self.base_path}\")\n",
        "            logger.info(\"Checking parent directories...\")\n",
        "\n",
        "            # Check common alternative paths\n",
        "            alternative_paths = [\n",
        "                Path(\"/content/drive/MyDrive/Voice/\"),\n",
        "                Path(\"/content/drive/MyDrive/\"),\n",
        "                Path(\"/content/drive/\"),\n",
        "                Path(\"/content/\")\n",
        "            ]\n",
        "\n",
        "            for alt_path in alternative_paths:\n",
        "                if alt_path.exists():\n",
        "                    logger.info(f\"Found directory: {alt_path}\")\n",
        "                    # List contents\n",
        "                    items = list(alt_path.glob(\"*\"))\n",
        "                    for item in items[:10]:  # Show first 10 items\n",
        "                        logger.info(f\"  {item.name}\")\n",
        "                    if len(items) > 10:\n",
        "                        logger.info(f\"  ... and {len(items) - 10} more items\")\n",
        "\n",
        "            return None, []\n",
        "\n",
        "        # List all files in the base directory\n",
        "        all_files = list(self.base_path.glob(\"*\"))\n",
        "        logger.info(f\"Found {len(all_files)} items in base directory:\")\n",
        "\n",
        "        dataset_files = []\n",
        "\n",
        "        for item in all_files:\n",
        "            if item.is_file():\n",
        "                size_mb = item.stat().st_size / (1024*1024)\n",
        "                logger.info(f\"  FILE: {item.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "                # Check for various dataset file formats\n",
        "                if any(keyword in item.name.lower() for keyword in ['adresso', 'alzheimer', 'dementia']):\n",
        "                    dataset_files.append(item)\n",
        "            else:\n",
        "                logger.info(f\"  DIR:  {item.name}/\")\n",
        "\n",
        "        # Look for specific file types\n",
        "        file_types = {\n",
        "            '.tgz': list(self.base_path.glob(\"*.tgz\")),\n",
        "            '.tar.gz': list(self.base_path.glob(\"*.tar.gz\")),\n",
        "            '.zip': list(self.base_path.glob(\"*.zip\")),\n",
        "            '.rar': list(self.base_path.glob(\"*.rar\")),\n",
        "            '.7z': list(self.base_path.glob(\"*.7z\"))\n",
        "        }\n",
        "\n",
        "        found_archives = []\n",
        "        for file_type, files in file_types.items():\n",
        "            if files:\n",
        "                logger.info(f\"Found {len(files)} {file_type} files:\")\n",
        "                for file in files:\n",
        "                    logger.info(f\"  {file.name}\")\n",
        "                    found_archives.extend(files)\n",
        "\n",
        "        if dataset_files:\n",
        "            logger.info(f\"Found {len(dataset_files)} potential dataset files:\")\n",
        "            for file in dataset_files:\n",
        "                logger.info(f\"  {file.name}\")\n",
        "\n",
        "        return found_archives, dataset_files\n",
        "\n",
        "    def interactive_file_selection(self, found_archives, dataset_files):\n",
        "        \"\"\"\n",
        "        Help user identify and select the correct dataset files\n",
        "        \"\"\"\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"DATASET FILE DETECTION RESULTS\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        if not found_archives and not dataset_files:\n",
        "            logger.error(\"❌ No archive files or potential dataset files found!\")\n",
        "            logger.info(\"\\n📋 TROUBLESHOOTING STEPS:\")\n",
        "            logger.info(\"1. Verify you've uploaded the ADReSSo21 dataset files\")\n",
        "            logger.info(\"2. Check if files are in a different directory\")\n",
        "            logger.info(\"3. Ensure files are properly uploaded to Google Drive\")\n",
        "            logger.info(\"4. Check if files have different names or extensions\")\n",
        "            return None\n",
        "\n",
        "        logger.info(\"🔍 FOUND FILES ANALYSIS:\")\n",
        "\n",
        "        # Analyze found files\n",
        "        likely_candidates = []\n",
        "\n",
        "        for file in found_archives + dataset_files:\n",
        "            score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # Check file name for ADReSSo21 indicators\n",
        "            name_lower = file.name.lower()\n",
        "            if 'adresso' in name_lower:\n",
        "                score += 5\n",
        "                reasons.append(\"Contains 'ADReSSo'\")\n",
        "            if 'progression' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'progression'\")\n",
        "            if 'diagnosis' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'diagnosis'\")\n",
        "            if 'train' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'train'\")\n",
        "            if 'test' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'test'\")\n",
        "\n",
        "            # Check file size (ADReSSo21 files should be reasonably large)\n",
        "            size_mb = file.stat().st_size / (1024*1024)\n",
        "            if size_mb > 10:  # Larger than 10MB\n",
        "                score += 2\n",
        "                reasons.append(f\"Good size ({size_mb:.1f} MB)\")\n",
        "            elif size_mb > 1:\n",
        "                score += 1\n",
        "                reasons.append(f\"Moderate size ({size_mb:.1f} MB)\")\n",
        "\n",
        "            if score > 0:\n",
        "                likely_candidates.append((file, score, reasons))\n",
        "\n",
        "        # Sort by score\n",
        "        likely_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if likely_candidates:\n",
        "            logger.info(f\"🎯 TOP CANDIDATES (sorted by likelihood):\")\n",
        "            for i, (file, score, reasons) in enumerate(likely_candidates[:5]):\n",
        "                logger.info(f\"  {i+1}. {file.name} (Score: {score})\")\n",
        "                logger.info(f\"     Reasons: {', '.join(reasons)}\")\n",
        "                logger.info(f\"     Path: {file}\")\n",
        "\n",
        "        # Return the most likely candidate for automatic processing\n",
        "        if likely_candidates and likely_candidates[0][1] >= 5:\n",
        "            return likely_candidates[0][0]\n",
        "\n",
        "        return None\n",
        "    def extract_any_archive(self, file_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Extract archive files in various formats\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to the archive file\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_extension = file_path.suffix.lower()\n",
        "            extract_dir = self.extracted_path / file_path.stem\n",
        "            extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            logger.info(f\"Attempting to extract: {file_path.name}\")\n",
        "\n",
        "            if file_extension in ['.tgz', '.gz'] or file_path.name.endswith('.tar.gz'):\n",
        "                # Handle .tgz and .tar.gz files\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            elif file_extension == '.zip':\n",
        "                # Handle .zip files\n",
        "                import zipfile\n",
        "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_dir)\n",
        "\n",
        "            elif file_extension == '.tar':\n",
        "                # Handle .tar files\n",
        "                with tarfile.open(file_path, 'r') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            else:\n",
        "                logger.error(f\"Unsupported archive format: {file_extension}\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully extracted {file_path.name} to {extract_dir}\")\n",
        "\n",
        "            # List extracted contents\n",
        "            extracted_items = list(extract_dir.rglob(\"*\"))\n",
        "            logger.info(f\"Extracted {len(extracted_items)} items\")\n",
        "\n",
        "            # Show directory structure\n",
        "            dirs = [item for item in extracted_items if item.is_dir()]\n",
        "            files = [item for item in extracted_items if item.is_file()]\n",
        "\n",
        "            logger.info(f\"  Directories: {len(dirs)}\")\n",
        "            logger.info(f\"  Files: {len(files)}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting {file_path}: {e}\")\n",
        "            return False\n",
        "    def extract_tgz_files(self) -> bool:\n",
        "        \"\"\"\n",
        "        Extract all archive files to the extraction directory\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Find potential dataset files\n",
        "            found_archives, dataset_files = self.find_dataset_files()\n",
        "\n",
        "            # Try to identify the best candidates\n",
        "            best_candidate = self.interactive_file_selection(found_archives, dataset_files)\n",
        "\n",
        "            if best_candidate:\n",
        "                logger.info(f\"🎯 Attempting to extract most likely candidate: {best_candidate.name}\")\n",
        "                if self.extract_any_archive(best_candidate):\n",
        "                    return True\n",
        "\n",
        "            # If no clear candidate, try all archive files\n",
        "            if found_archives:\n",
        "                logger.info(\"🔄 Trying to extract all found archive files...\")\n",
        "                extracted_any = False\n",
        "\n",
        "                for archive in found_archives:\n",
        "                    if self.extract_any_archive(archive):\n",
        "                        extracted_any = True\n",
        "\n",
        "                return extracted_any\n",
        "\n",
        "            # Fallback: try the original method for exact file names\n",
        "            logger.info(\"🔄 Trying original extraction method...\")\n",
        "            extracted_any = False\n",
        "\n",
        "            for dataset_name, filename in self.dataset_files.items():\n",
        "                file_path = self.base_path / filename\n",
        "\n",
        "                if not file_path.exists():\n",
        "                    logger.warning(f\"Expected file not found: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Extracting {filename}...\")\n",
        "\n",
        "                # Extract to specific subdirectory\n",
        "                extract_dir = self.extracted_path / dataset_name\n",
        "                extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "                logger.info(f\"Successfully extracted {filename}\")\n",
        "                extracted_any = True\n",
        "\n",
        "            return extracted_any\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {e}\")\n",
        "            return False\n",
        "\n",
        "    def verify_directory_structure(self) -> Dict[str, bool]:\n",
        "        \"\"\"\n",
        "        Verify that the extracted directories match expected structure\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, bool]: Status of each dataset extraction\n",
        "        \"\"\"\n",
        "        verification_results = {}\n",
        "\n",
        "        for dataset_name, structure in self.directory_structure.items():\n",
        "            dataset_path = self.extracted_path / dataset_name / \"ADReSSo21\"\n",
        "\n",
        "            # Check if main dataset directory exists\n",
        "            if not dataset_path.exists():\n",
        "                verification_results[dataset_name] = False\n",
        "                logger.error(f\"Dataset directory not found: {dataset_path}\")\n",
        "                continue\n",
        "\n",
        "            # Verify subdirectories\n",
        "            all_dirs_exist = True\n",
        "\n",
        "            for data_type, subdirs in structure.items():\n",
        "                if dataset_name == 'progression_test':\n",
        "                    # Special case for test data\n",
        "                    seg_path = dataset_path / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "                    audio_path = dataset_path / \"progression\" / \"test-dist\" / \"audio\"\n",
        "\n",
        "                    if not (seg_path.exists() and audio_path.exists()):\n",
        "                        all_dirs_exist = False\n",
        "                        logger.error(f\"Test directories missing in {dataset_name}\")\n",
        "                else:\n",
        "                    # Regular structure for train data\n",
        "                    base_type_path = dataset_path / (\"progression\" if \"progression\" in dataset_name else \"diagnosis\")\n",
        "\n",
        "                    for subdir in subdirs:\n",
        "                        if subdir:  # Skip empty strings\n",
        "                            seg_path = base_type_path / \"train\" / \"segmentation\" / subdir\n",
        "                            audio_path = base_type_path / \"train\" / \"audio\" / subdir\n",
        "\n",
        "                            if not (seg_path.exists() and audio_path.exists()):\n",
        "                                all_dirs_exist = False\n",
        "                                logger.error(f\"Missing directories for {dataset_name}/{subdir}\")\n",
        "\n",
        "            verification_results[dataset_name] = all_dirs_exist\n",
        "\n",
        "            if all_dirs_exist:\n",
        "                logger.info(f\"✓ Directory structure verified for {dataset_name}\")\n",
        "            else:\n",
        "                logger.warning(f\"✗ Directory structure issues found for {dataset_name}\")\n",
        "\n",
        "        return verification_results\n",
        "\n",
        "    def extract_transcripts_from_csv(self, csv_file_path: Path) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract transcript data from a single CSV file following CHAT protocol\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (Path): Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of transcript segments with metadata\n",
        "        \"\"\"\n",
        "        transcripts = []\n",
        "\n",
        "        try:\n",
        "            # Try different encodings as CSV files might have various encodings\n",
        "            encodings = ['utf-8', 'latin-1', 'cp1252']\n",
        "            df = None\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_file_path, encoding=encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                logger.error(f\"Could not read CSV file with any encoding: {csv_file_path}\")\n",
        "                return transcripts\n",
        "\n",
        "            # Log column names for inspection\n",
        "            logger.info(f\"CSV columns in {csv_file_path.name}: {list(df.columns)}\")\n",
        "\n",
        "            # Extract relevant columns (adjust based on actual CSV structure)\n",
        "            # Common CHAT protocol columns might include: speaker, utterance, time, etc.\n",
        "            for index, row in df.iterrows():\n",
        "                transcript_entry = {\n",
        "                    'file_id': csv_file_path.stem,\n",
        "                    'row_index': index,\n",
        "                    'data': dict(row)  # Store all columns for now\n",
        "                }\n",
        "\n",
        "                # Look for text/utterance columns (common names in CHAT protocol)\n",
        "                text_columns = ['utterance', 'text', 'transcript', 'speech', 'content']\n",
        "                for col in text_columns:\n",
        "                    if col in df.columns and pd.notna(row[col]):\n",
        "                        transcript_entry['transcript'] = str(row[col])\n",
        "                        break\n",
        "\n",
        "                transcripts.append(transcript_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing CSV file {csv_file_path}: {e}\")\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def collect_all_transcripts(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Collect all transcripts from segmentation CSV files\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Dict]]: Organized transcripts by category\n",
        "        \"\"\"\n",
        "        all_transcripts = {\n",
        "            'progression_train_no_decline': [],\n",
        "            'progression_train_decline': [],\n",
        "            'progression_test': [],\n",
        "            'diagnosis_train_cn': [],\n",
        "            'diagnosis_train_ad': []\n",
        "        }\n",
        "\n",
        "        # Process progression training data\n",
        "        prog_train_path = self.extracted_path / \"progression_train\" / \"ADReSSo21\" / \"progression\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['no_decline', 'decline']:\n",
        "            csv_dir = prog_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'progression_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process progression test data\n",
        "        prog_test_path = self.extracted_path / \"progression_test\" / \"ADReSSo21\" / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "        if prog_test_path.exists():\n",
        "            for csv_file in prog_test_path.glob('*.csv'):\n",
        "                transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                all_transcripts['progression_test'].extend(transcripts)\n",
        "                logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process diagnosis training data\n",
        "        diag_train_path = self.extracted_path / \"diagnosis_train\" / \"ADReSSo21\" / \"diagnosis\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['cn', 'ad']:\n",
        "            csv_dir = diag_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'diagnosis_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        return all_transcripts\n",
        "\n",
        "    def save_transcripts_summary(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Save a summary of extracted transcripts for review\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the saved summary file\n",
        "        \"\"\"\n",
        "        summary_file = self.base_path / \"transcripts_summary.txt\"\n",
        "\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"ADReSSo21 Dataset Transcripts Summary\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            total_transcripts = 0\n",
        "            for category, transcript_list in transcripts.items():\n",
        "                count = len(transcript_list)\n",
        "                total_transcripts += count\n",
        "                f.write(f\"{category}: {count} transcript entries\\n\")\n",
        "\n",
        "                # Show sample transcript if available\n",
        "                if transcript_list and 'transcript' in transcript_list[0]:\n",
        "                    sample = transcript_list[0]['transcript'][:100] + \"...\" if len(transcript_list[0]['transcript']) > 100 else transcript_list[0]['transcript']\n",
        "                    f.write(f\"  Sample: {sample}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(f\"Total transcript entries: {total_transcripts}\\n\")\n",
        "            f.write(\"\\nNote: These English transcripts need to be translated to Persian for the study.\\n\")\n",
        "            f.write(\"Translation should be done manually by native Persian speakers as per the methodology.\\n\")\n",
        "\n",
        "        logger.info(f\"Transcripts summary saved to: {summary_file}\")\n",
        "        return summary_file\n",
        "\n",
        "    def prepare_for_translation(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Prepare transcript files for manual Persian translation\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the translation directory\n",
        "        \"\"\"\n",
        "        translation_dir = self.base_path / \"for_translation\"\n",
        "        translation_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        for category, transcript_list in transcripts.items():\n",
        "            if not transcript_list:\n",
        "                continue\n",
        "\n",
        "            # Create CSV file for translation\n",
        "            csv_file = translation_dir / f\"{category}_for_translation.csv\"\n",
        "\n",
        "            with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['ID', 'Original_English', 'Persian_Translation', 'Notes'])\n",
        "\n",
        "                for i, entry in enumerate(transcript_list):\n",
        "                    if 'transcript' in entry:\n",
        "                        transcript_id = f\"{category}_{i+1}\"\n",
        "                        english_text = entry['transcript']\n",
        "                        writer.writerow([transcript_id, english_text, '', ''])\n",
        "\n",
        "            logger.info(f\"Created translation file: {csv_file}\")\n",
        "\n",
        "        # Create translation instructions\n",
        "        instructions_file = translation_dir / \"TRANSLATION_INSTRUCTIONS.txt\"\n",
        "        with open(instructions_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"PERSIAN TRANSLATION INSTRUCTIONS\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "            f.write(\"IMPORTANT REQUIREMENTS:\\n\")\n",
        "            f.write(\"1. Translation must be done by native Persian speakers\\n\")\n",
        "            f.write(\"2. Translator should have at least 13 years of formal Persian education\\n\")\n",
        "            f.write(\"3. Translation should be verified by an independent linguistic expert\\n\")\n",
        "            f.write(\"4. PRESERVE ALL linguistic features:\\n\")\n",
        "            f.write(\"   - Pause words (uhm, uhh, etc.) - translate equivalent Persian pause words\\n\")\n",
        "            f.write(\"   - Repetitions - keep all repetitions\\n\")\n",
        "            f.write(\"   - Linguistic errors - preserve grammatical/syntactic errors\\n\")\n",
        "            f.write(\"   - Syntactic errors - maintain sentence structure issues\\n\")\n",
        "            f.write(\"5. EXCLUDE annotations like [clears throat], [laughs], etc.\\n\")\n",
        "            f.write(\"6. Do NOT use machine translation - manual translation only\\n\")\n",
        "            f.write(\"7. Capture cultural and linguistic nuances specific to Persian\\n\\n\")\n",
        "            f.write(\"Fill in the 'Persian_Translation' column in each CSV file.\\n\")\n",
        "            f.write(\"Use 'Notes' column for any translation decisions or concerns.\\n\")\n",
        "\n",
        "        logger.info(f\"Translation instructions saved to: {instructions_file}\")\n",
        "        return translation_dir\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute Step 1 of the pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "\n",
        "    # Initialize the data processor with the correct path based on your error\n",
        "    processor = ADReSSo21DataProcessor()\n",
        "\n",
        "    # Step 1.1: Mount Google Drive (if in Colab)\n",
        "    logger.info(\"Step 1.1: Mounting Google Drive...\")\n",
        "    drive_mounted = processor.mount_google_drive()\n",
        "\n",
        "    # Step 1.2: Search for dataset files\n",
        "    logger.info(\"Step 1.2: Searching for dataset files...\")\n",
        "    found_archives, dataset_files = processor.find_dataset_files()\n",
        "\n",
        "    if not found_archives and not dataset_files:\n",
        "        logger.error(\"❌ No potential dataset files found!\")\n",
        "        logger.info(\"\\n📋 PLEASE CHECK:\")\n",
        "        logger.info(\"1. Are the ADReSSo21 files uploaded to Google Drive?\")\n",
        "        logger.info(\"2. Are they in the correct directory?\")\n",
        "        logger.info(f\"   Expected location: {processor.base_path}\")\n",
        "        logger.info(\"3. Do they have the expected names:\")\n",
        "        logger.info(\"   - ADReSSo21-progression-train.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-progression-test.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-diagnosis-train.tgz\")\n",
        "        logger.info(\"4. Or are they in a different format (.zip, .rar, etc.)?\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.3: Extract dataset files\n",
        "    logger.info(\"Step 1.3: Extracting dataset files...\")\n",
        "    extraction_success = processor.extract_tgz_files()\n",
        "\n",
        "    if not extraction_success:\n",
        "        logger.error(\"❌ Failed to extract any dataset files.\")\n",
        "        logger.info(\"\\n🔧 POSSIBLE SOLUTIONS:\")\n",
        "        logger.info(\"1. Check if files are corrupted - try re-downloading\")\n",
        "        logger.info(\"2. Try extracting files manually first\")\n",
        "        logger.info(\"3. Ensure files are not password protected\")\n",
        "        logger.info(\"4. Check if files are in an unsupported format\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.4: Verify directory structure\n",
        "    logger.info(\"Step 1.4: Verifying directory structure...\")\n",
        "    verification_results = processor.verify_directory_structure()\n",
        "\n",
        "    successful_extractions = [k for k, v in verification_results.items() if v]\n",
        "\n",
        "    if not successful_extractions:\n",
        "        logger.error(\"No datasets were successfully extracted and verified.\")\n",
        "        return False\n",
        "\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "\n",
        "    # Step 1.5: Extract transcripts from CSV files\n",
        "    logger.info(\"Step 1.5: Extracting transcripts from segmentation CSV files...\")\n",
        "    all_transcripts = processor.collect_all_transcripts()\n",
        "\n",
        "    # Check if we actually got any transcripts\n",
        "    total_transcripts = sum(len(transcripts) for transcripts in all_transcripts.values())\n",
        "\n",
        "    if total_transcripts == 0:\n",
        "        logger.error(\"No transcripts were extracted from CSV files!\")\n",
        "        logger.info(\"This might indicate:\")\n",
        "        logger.info(\"  - CSV files are in a different format than expected\")\n",
        "        logger.info(\"  - Directory structure is different\")\n",
        "        logger.info(\"  - Files are corrupted\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.6: Save summary and prepare for translation\n",
        "    logger.info(\"Step 1.6: Saving transcripts summary...\")\n",
        "    processor.save_transcripts_summary(all_transcripts)\n",
        "\n",
        "    logger.info(\"Step 1.7: Preparing files for Persian translation...\")\n",
        "    translation_dir = processor.prepare_for_translation(all_transcripts)\n",
        "\n",
        "    # Final summary\n",
        "    logger.info(f\"\\n{'='*50}\")\n",
        "    logger.info(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "    logger.info(f\"Total transcript entries extracted: {total_transcripts}\")\n",
        "    logger.info(f\"Translation files prepared in: {translation_dir}\")\n",
        "    logger.info(\"NEXT STEPS:\")\n",
        "    logger.info(\"1. Have native Persian speakers translate the CSV files\")\n",
        "    logger.info(\"2. Verify translations with linguistic expert\")\n",
        "    logger.info(\"3. Return translated files for Step 2 (Data Preprocessing)\")\n",
        "    logger.info(f\"{'='*50}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if success:\n",
        "        print(\"\\n✅ Step 1 completed successfully!\")\n",
        "        print(\"📁 Check the translation directory for files to be translated to Persian\")\n",
        "        print(\"🔄 Once translation is complete, you can proceed to Step 2\")\n",
        "    else:\n",
        "        print(\"\\n❌ Step 1 encountered errors. Please check the logs above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1fFwtsjbfA_",
        "outputId": "74e92c53-6e31-4b99-ccb6-d0b074144ffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/MyDrive/Voice'\n",
        "extracted_base_path = '/content/drive/MyDrive/Voice/Extracted_dataset'\n",
        "\n",
        "# Dataset file paths\n",
        "dataset_files = {\n",
        "    'diagnosis_train': '/content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train.tgz',\n",
        "    'progression_test': '/content/drive/MyDrive/Voice/ADReSSo21-progression-test.tgz',\n",
        "    'progression_train': '/content/drive/MyDrive/Voice/ADReSSo21-progression-train.tgz'\n",
        "}\n",
        "\n",
        "def create_directory_structure():\n",
        "    \"\"\"Create the directory structure for extracted datasets\"\"\"\n",
        "    print(\"Creating directory structure...\")\n",
        "\n",
        "    # Create main extracted dataset folder\n",
        "    os.makedirs(extracted_base_path, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for each dataset\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_dir = os.path.join(extracted_base_path, dataset_name)\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Directory structure created at: {extracted_base_path}\")\n",
        "\n",
        "def extract_dataset(tgz_path, extract_to_path, dataset_name):\n",
        "    \"\"\"Extract a .tgz file to the specified directory\"\"\"\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "\n",
        "    if not os.path.exists(tgz_path):\n",
        "        print(f\"ERROR: File not found - {tgz_path}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
        "            tar.extractall(path=extract_to_path)\n",
        "        print(f\"Successfully extracted {dataset_name} to {extract_to_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR extracting {dataset_name}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def inspect_extracted_structure():\n",
        "    \"\"\"Inspect the extracted directory structure and report findings\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INSPECTING EXTRACTED DATASET STRUCTURE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        print(f\"\\n--- {dataset_name.upper()} ---\")\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            # Walk through the directory structure\n",
        "            for root, dirs, files in os.walk(dataset_path):\n",
        "                level = root.replace(dataset_path, '').count(os.sep)\n",
        "                indent = ' ' * 2 * level\n",
        "                print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "                # Show first few files in each directory\n",
        "                sub_indent = ' ' * 2 * (level + 1)\n",
        "                for file in files[:5]:  # Show first 5 files\n",
        "                    print(f\"{sub_indent}{file}\")\n",
        "                if len(files) > 5:\n",
        "                    print(f\"{sub_indent}... and {len(files) - 5} more files\")\n",
        "        else:\n",
        "            print(f\"Directory not found: {dataset_path}\")\n",
        "\n",
        "def find_and_inspect_csv_files():\n",
        "    \"\"\"Find CSV files containing transcripts and inspect their structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INSPECTING CSV FILES (TRANSCRIPTS)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    csv_files_found = []\n",
        "\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        # Look for CSV files in segmentation directories\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.csv'):\n",
        "                        csv_path = os.path.join(root, file)\n",
        "                        csv_files_found.append((dataset_name, root, file, csv_path))\n",
        "\n",
        "    print(f\"Found {len(csv_files_found)} CSV files total\")\n",
        "\n",
        "    # Inspect structure of first few CSV files\n",
        "    for i, (dataset_name, directory, filename, full_path) in enumerate(csv_files_found[:3]):\n",
        "        print(f\"\\n--- CSV File {i+1}: {filename} ---\")\n",
        "        print(f\"Dataset: {dataset_name}\")\n",
        "        print(f\"Directory: {directory}\")\n",
        "\n",
        "        try:\n",
        "            # Read and inspect CSV structure\n",
        "            df = pd.read_csv(full_path)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "            print(\"First few rows:\")\n",
        "            print(df.head(2).to_string())\n",
        "\n",
        "            # Check for transcript-like content\n",
        "            for col in df.columns:\n",
        "                if any(keyword in col.lower() for keyword in ['transcript', 'text', 'utterance', 'speech']):\n",
        "                    print(f\"\\nSample content from '{col}':\")\n",
        "                    sample_content = df[col].dropna().head(2).tolist()\n",
        "                    for content in sample_content:\n",
        "                        print(f\"  '{str(content)[:100]}...'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV: {str(e)}\")\n",
        "\n",
        "    return csv_files_found\n",
        "\n",
        "def generate_summary_report(csv_files_found):\n",
        "    \"\"\"Generate a summary report of the extracted data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Count files by dataset and category\n",
        "    summary = {}\n",
        "    for dataset_name, directory, filename, full_path in csv_files_found:\n",
        "        if dataset_name not in summary:\n",
        "            summary[dataset_name] = {'total_csv': 0, 'categories': {}}\n",
        "\n",
        "        summary[dataset_name]['total_csv'] += 1\n",
        "\n",
        "        # Determine category (cn, ad, decline, no_decline)\n",
        "        category = 'unknown'\n",
        "        if '/cn/' in directory:\n",
        "            category = 'cn'\n",
        "        elif '/ad/' in directory:\n",
        "            category = 'ad'\n",
        "        elif '/decline/' in directory:\n",
        "            category = 'decline'\n",
        "        elif '/no_decline/' in directory:\n",
        "            category = 'no_decline'\n",
        "\n",
        "        if category not in summary[dataset_name]['categories']:\n",
        "            summary[dataset_name]['categories'][category] = 0\n",
        "        summary[dataset_name]['categories'][category] += 1\n",
        "\n",
        "    # Print summary\n",
        "    for dataset_name, data in summary.items():\n",
        "        print(f\"\\n{dataset_name.upper()}:\")\n",
        "        print(f\"  Total CSV files: {data['total_csv']}\")\n",
        "        print(\"  Categories:\")\n",
        "        for category, count in data['categories'].items():\n",
        "            print(f\"    {category}: {count} files\")\n",
        "\n",
        "    print(f\"\\nExtracted dataset location: {extracted_base_path}\")\n",
        "    print(\"Ready for Step 2: Translation to Persian\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1.1: Create directory structure\n",
        "    create_directory_structure()\n",
        "\n",
        "    # Step 1.2: Extract all datasets\n",
        "    print(\"\\nExtracting datasets...\")\n",
        "    extraction_success = True\n",
        "\n",
        "    for dataset_name, tgz_path in dataset_files.items():\n",
        "        extract_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        success = extract_dataset(tgz_path, extract_path, dataset_name)\n",
        "        if not success:\n",
        "            extraction_success = False\n",
        "\n",
        "    if not extraction_success:\n",
        "        print(\"\\nERROR: Some extractions failed. Please check the file paths and try again.\")\n",
        "        return\n",
        "\n",
        "    # Step 1.3: Inspect extracted structure\n",
        "    inspect_extracted_structure()\n",
        "\n",
        "    # Step 1.4: Find and inspect CSV files (transcripts)\n",
        "    csv_files_found = find_and_inspect_csv_files()\n",
        "\n",
        "    # Step 1.5: Generate summary report\n",
        "    generate_summary_report(csv_files_found)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Next step: Manual translation of English transcripts to Persian\")\n",
        "    print(\"Note: The paper emphasizes using native Persian speakers for translation\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpuJVWoUjYeC",
        "outputId": "b425378f-9915-4640-dfe5-cb8f0898f54c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting Step 1: Data Acquisition and Preparation\n",
            "============================================================\n",
            "Creating directory structure...\n",
            "Directory structure created at: /content/drive/MyDrive/Voice/Extracted_dataset\n",
            "\n",
            "Extracting datasets...\n",
            "Extracting diagnosis_train...\n",
            "Successfully extracted diagnosis_train to /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train\n",
            "Extracting progression_test...\n",
            "Successfully extracted progression_test to /content/drive/MyDrive/Voice/Extracted_dataset/progression_test\n",
            "Extracting progression_train...\n",
            "Successfully extracted progression_train to /content/drive/MyDrive/Voice/Extracted_dataset/progression_train\n",
            "\n",
            "============================================================\n",
            "INSPECTING EXTRACTED DATASET STRUCTURE\n",
            "============================================================\n",
            "\n",
            "--- DIAGNOSIS_TRAIN ---\n",
            "diagnosis_train/\n",
            "  ADReSSo21/\n",
            "    diagnosis/\n",
            "      README.md\n",
            "      train/\n",
            "        adresso-train-mmse-scores.csv\n",
            "        segmentation/\n",
            "          cn/\n",
            "            adrso281.csv\n",
            "            adrso308.csv\n",
            "            adrso270.csv\n",
            "            adrso022.csv\n",
            "            adrso298.csv\n",
            "            ... and 74 more files\n",
            "          ad/\n",
            "            adrso229.csv\n",
            "            adrso106.csv\n",
            "            adrso144.csv\n",
            "            adrso049.csv\n",
            "            adrso078.csv\n",
            "            ... and 82 more files\n",
            "        audio/\n",
            "          cn/\n",
            "            adrso173.wav\n",
            "            adrso015.wav\n",
            "            adrso307.wav\n",
            "            adrso283.wav\n",
            "            adrso167.wav\n",
            "            ... and 74 more files\n",
            "          ad/\n",
            "            adrso047.wav\n",
            "            adrso128.wav\n",
            "            adrso045.wav\n",
            "            adrso110.wav\n",
            "            adrso036.wav\n",
            "            ... and 82 more files\n",
            "\n",
            "--- PROGRESSION_TEST ---\n",
            "progression_test/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      test-dist/\n",
            "        test_results_task3.csv\n",
            "        README\n",
            "        segmentation/\n",
            "          adrspt24.csv\n",
            "          adrspt15.csv\n",
            "          adrspt12.csv\n",
            "          adrspt2.csv\n",
            "          adrspt9.csv\n",
            "          ... and 10 more files\n",
            "        audio/\n",
            "          adrspt20.wav\n",
            "          adrspt15.wav\n",
            "          adrspt4.wav\n",
            "          adrspt28.wav\n",
            "          adrspt16.wav\n",
            "          ... and 27 more files\n",
            "\n",
            "--- PROGRESSION_TRAIN ---\n",
            "progression_train/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      README.md\n",
            "      train/\n",
            "        segmentation/\n",
            "          no_decline/\n",
            "            adrsp195.csv\n",
            "            adrsp041.csv\n",
            "            adrsp030.csv\n",
            "            adrsp052.csv\n",
            "            adrsp349.csv\n",
            "            ... and 32 more files\n",
            "          decline/\n",
            "            adrsp051.csv\n",
            "            adrsp313.csv\n",
            "            adrsp101.csv\n",
            "            adrsp055.csv\n",
            "            adrsp179.csv\n",
            "            ... and 5 more files\n",
            "        audio/\n",
            "          no_decline/\n",
            "            adrsp196.wav\n",
            "            adrsp137.wav\n",
            "            adrsp130.wav\n",
            "            adrsp349.wav\n",
            "            adrsp198.wav\n",
            "            ... and 53 more files\n",
            "          decline/\n",
            "            adrsp055.wav\n",
            "            adrsp003.wav\n",
            "            adrsp266.wav\n",
            "            adrsp300.wav\n",
            "            adrsp320.wav\n",
            "            ... and 10 more files\n",
            "\n",
            "============================================================\n",
            "INSPECTING CSV FILES (TRANSCRIPTS)\n",
            "============================================================\n",
            "Found 228 CSV files total\n",
            "\n",
            "--- CSV File 1: adrso281.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (18, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  2129\n",
            "1           2     PAR   2129  5471\n",
            "\n",
            "--- CSV File 2: adrso308.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (24, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  4873\n",
            "1           2     PAR   4873  7270\n",
            "\n",
            "--- CSV File 3: adrso270.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (20, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     INV      0   997\n",
            "1           2     PAR    997  2294\n",
            "\n",
            "============================================================\n",
            "SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            "DIAGNOSIS_TRAIN:\n",
            "  Total CSV files: 166\n",
            "  Categories:\n",
            "    unknown: 166 files\n",
            "\n",
            "PROGRESSION_TEST:\n",
            "  Total CSV files: 15\n",
            "  Categories:\n",
            "    unknown: 15 files\n",
            "\n",
            "PROGRESSION_TRAIN:\n",
            "  Total CSV files: 47\n",
            "  Categories:\n",
            "    unknown: 47 files\n",
            "\n",
            "Extracted dataset location: /content/drive/MyDrive/Voice/Extracted_dataset\n",
            "Ready for Step 2: Translation to Persian\n",
            "\n",
            "============================================================\n",
            "STEP 1 COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Next step: Manual translation of English transcripts to Persian\n",
            "Note: The paper emphasizes using native Persian speakers for translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Define paths\n",
        "extracted_base_path = '/content/drive/MyDrive/Voice/Extracted_dataset'\n",
        "persian_translated_path = '/content/drive/MyDrive/Voice/Persian_Translated_Dataset'\n",
        "\n",
        "def create_persian_dataset_structure():\n",
        "    \"\"\"Create directory structure for Persian translated dataset\"\"\"\n",
        "    print(\"Creating Persian dataset directory structure...\")\n",
        "\n",
        "    # Create main Persian dataset folder\n",
        "    os.makedirs(persian_translated_path, exist_ok=True)\n",
        "\n",
        "    # Mirror the original structure\n",
        "    dataset_types = ['diagnosis_train', 'progression_test', 'progression_train']\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        original_path = os.path.join(extracted_base_path, dataset_type)\n",
        "        persian_path = os.path.join(persian_translated_path, dataset_type)\n",
        "\n",
        "        # Walk through original structure and create mirror structure\n",
        "        for root, dirs, files in os.walk(original_path):\n",
        "            # Create corresponding Persian directory\n",
        "            relative_path = os.path.relpath(root, original_path)\n",
        "            if relative_path == '.':\n",
        "                new_path = persian_path\n",
        "            else:\n",
        "                new_path = os.path.join(persian_path, relative_path)\n",
        "            os.makedirs(new_path, exist_ok=True)\n",
        "\n",
        "    print(f\"Persian dataset structure created at: {persian_translated_path}\")\n",
        "\n",
        "def find_transcript_columns(df):\n",
        "    \"\"\"Identify columns that likely contain transcript text\"\"\"\n",
        "    transcript_cols = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        # Look for common transcript column names\n",
        "        if any(keyword in col_lower for keyword in ['transcript', 'text', 'utterance', 'speech', 'content', 'words']):\n",
        "            transcript_cols.append(col)\n",
        "        # Also check if column contains string data that looks like speech\n",
        "        elif df[col].dtype == 'object':\n",
        "            sample_data = df[col].dropna().head(5)\n",
        "            if len(sample_data) > 0:\n",
        "                # Check if it contains typical speech patterns\n",
        "                sample_text = ' '.join(sample_data.astype(str))\n",
        "                if any(word in sample_text.lower() for word in ['the', 'and', 'is', 'are', 'this', 'that', 'uhm', 'uh']):\n",
        "                    transcript_cols.append(col)\n",
        "\n",
        "    return transcript_cols\n",
        "\n",
        "def analyze_linguistic_features(text):\n",
        "    \"\"\"Analyze text for linguistic features that need to be preserved\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'has_pause_words': False,\n",
        "            'has_repetitions': False,\n",
        "            'pause_words': [],\n",
        "            'word_count': 0,\n",
        "            'requires_translation': False\n",
        "        }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Common English pause words/fillers that need to be preserved\n",
        "    pause_words = ['uhm', 'uh', 'um', 'er', 'ah', 'hmm', 'mm']\n",
        "    found_pause_words = [pw for pw in pause_words if pw in text_lower]\n",
        "\n",
        "    # Simple repetition detection (same word appearing consecutively)\n",
        "    words = text.split()\n",
        "    has_repetitions = any(i > 0 and words[i].lower() == words[i-1].lower() for i in range(1, len(words)))\n",
        "\n",
        "    # Check if text contains English content (basic check)\n",
        "    english_indicators = ['the', 'and', 'is', 'are', 'this', 'that', 'with', 'for', 'to', 'of', 'in', 'on']\n",
        "    requires_translation = any(word in text_lower for word in english_indicators)\n",
        "\n",
        "    return {\n",
        "        'has_pause_words': len(found_pause_words) > 0,\n",
        "        'has_repetitions': has_repetitions,\n",
        "        'pause_words': found_pause_words,\n",
        "        'word_count': len(words),\n",
        "        'requires_translation': requires_translation\n",
        "    }\n",
        "\n",
        "def create_translation_template(csv_files_info):\n",
        "    \"\"\"Create a structured template for translation\"\"\"\n",
        "    translation_data = []\n",
        "\n",
        "    print(\"Creating translation template...\")\n",
        "\n",
        "    for dataset_name, directory, filename, full_path in csv_files_info:\n",
        "        try:\n",
        "            df = pd.read_csv(full_path)\n",
        "            transcript_cols = find_transcript_columns(df)\n",
        "\n",
        "            if transcript_cols:\n",
        "                print(f\"Processing {filename} - Found transcript columns: {transcript_cols}\")\n",
        "\n",
        "                for idx, row in df.iterrows():\n",
        "                    for col in transcript_cols:\n",
        "                        original_text = row[col]\n",
        "                        if pd.notna(original_text) and isinstance(original_text, str) and original_text.strip():\n",
        "\n",
        "                            features = analyze_linguistic_features(original_text)\n",
        "\n",
        "                            if features['requires_translation']:\n",
        "                                translation_entry = {\n",
        "                                    'dataset': dataset_name,\n",
        "                                    'file': filename,\n",
        "                                    'directory': directory,\n",
        "                                    'row_index': idx,\n",
        "                                    'column': col,\n",
        "                                    'original_english': original_text,\n",
        "                                    'persian_translation': '',  # To be filled\n",
        "                                    'has_pause_words': features['has_pause_words'],\n",
        "                                    'pause_words': features['pause_words'],\n",
        "                                    'has_repetitions': features['has_repetitions'],\n",
        "                                    'word_count': features['word_count'],\n",
        "                                    'translation_notes': '',\n",
        "                                    'translation_status': 'pending'\n",
        "                                }\n",
        "                                translation_data.append(translation_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    return translation_data\n",
        "\n",
        "def save_translation_template(translation_data):\n",
        "    \"\"\"Save translation template for manual translation\"\"\"\n",
        "    template_path = os.path.join(persian_translated_path, 'translation_template.json')\n",
        "\n",
        "    # Save as JSON for easy editing\n",
        "    with open(template_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(translation_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Also create a CSV version for easier viewing/editing\n",
        "    csv_template_path = os.path.join(persian_translated_path, 'translation_template.csv')\n",
        "    df_template = pd.DataFrame(translation_data)\n",
        "    df_template.to_csv(csv_template_path, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"Translation template saved:\")\n",
        "    print(f\"  JSON format: {template_path}\")\n",
        "    print(f\"  CSV format: {csv_template_path}\")\n",
        "\n",
        "    return template_path, csv_template_path\n",
        "\n",
        "def create_translation_guidelines():\n",
        "    \"\"\"Create detailed translation guidelines based on the paper's methodology\"\"\"\n",
        "    guidelines = \"\"\"\n",
        "PERSIAN TRANSLATION GUIDELINES\n",
        "==============================\n",
        "\n",
        "CRITICAL REQUIREMENTS (Based on Research Paper):\n",
        "\n",
        "1. PRESERVE ALL LINGUISTIC FEATURES:\n",
        "   - Keep ALL pause words: \"uhm\", \"uh\", \"um\", \"er\", \"ah\" etc.\n",
        "   - Translate to Persian equivalents: \"اوم\", \"اه\", \"ام\", \"ار\", \"آه\"\n",
        "   - MAINTAIN repetitions exactly as they appear\n",
        "   - PRESERVE all linguistic and syntactic errors\n",
        "   - Keep hesitations and false starts\n",
        "\n",
        "2. EXCLUDE NON-LINGUISTIC ANNOTATIONS:\n",
        "   - Remove: [clears throat], [laughs], [coughs]\n",
        "   - Remove: [inaudible], [unclear]\n",
        "   - Keep only actual speech content\n",
        "\n",
        "3. TRANSLATION PRINCIPLES:\n",
        "   - Translate meaning while preserving linguistic characteristics\n",
        "   - Maintain natural Persian flow where possible\n",
        "   - Keep cultural context appropriate to Persian speakers\n",
        "   - Preserve sentence structure patterns when possible\n",
        "\n",
        "4. SPECIFIC EXAMPLES:\n",
        "   English: \"The woman is uhm she is washing dishes\"\n",
        "   Persian: \"زن اوم او ظرف می‌شوید\" (keeping the pause word and structure)\n",
        "\n",
        "   English: \"The the boy is running\"\n",
        "   Persian: \"پسر پسر دارد می‌دود\" (preserving repetition)\n",
        "\n",
        "5. QUALITY CONTROL:\n",
        "   - Double-check each translation\n",
        "   - Ensure pause words are correctly placed\n",
        "   - Verify repetitions are maintained\n",
        "   - Check that errors are preserved appropriately\n",
        "\n",
        "PAUSE WORD EQUIVALENTS:\n",
        "- \"uhm\" → \"اوم\"\n",
        "- \"uh\" → \"اه\"\n",
        "- \"um\" → \"ام\"\n",
        "- \"er\" → \"ار\"\n",
        "- \"ah\" → \"آه\"\n",
        "- \"hmm\" → \"هوم\"\n",
        "\"\"\"\n",
        "\n",
        "    guidelines_path = os.path.join(persian_translated_path, 'translation_guidelines.txt')\n",
        "    with open(guidelines_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(guidelines)\n",
        "\n",
        "    print(f\"Translation guidelines saved: {guidelines_path}\")\n",
        "    return guidelines_path\n",
        "\n",
        "def load_and_apply_translations(template_path):\n",
        "    \"\"\"Load completed translations and apply them to create Persian dataset\"\"\"\n",
        "    print(\"Loading translations and creating Persian dataset...\")\n",
        "\n",
        "    # Load translation data\n",
        "    with open(template_path, 'r', encoding='utf-8') as f:\n",
        "        translation_data = json.load(f)\n",
        "\n",
        "    # Group by file for processing\n",
        "    files_to_process = {}\n",
        "    for entry in translation_data:\n",
        "        key = (entry['dataset'], entry['file'], entry['directory'])\n",
        "        if key not in files_to_process:\n",
        "            files_to_process[key] = []\n",
        "        files_to_process[key].append(entry)\n",
        "\n",
        "    translated_files_count = 0\n",
        "\n",
        "    for (dataset_name, filename, directory), translations in files_to_process.items():\n",
        "        # Load original CSV\n",
        "        original_path = os.path.join(directory, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(original_path)\n",
        "            df_persian = df.copy()\n",
        "\n",
        "            # Apply translations\n",
        "            for translation in translations:\n",
        "                if translation['persian_translation'].strip():  # Only if translation exists\n",
        "                    row_idx = translation['row_index']\n",
        "                    col = translation['column']\n",
        "                    df_persian.at[row_idx, col] = translation['persian_translation']\n",
        "\n",
        "            # Save Persian version\n",
        "            relative_dir = os.path.relpath(directory, extracted_base_path)\n",
        "            persian_dir = os.path.join(persian_translated_path, relative_dir)\n",
        "            os.makedirs(persian_dir, exist_ok=True)\n",
        "\n",
        "            persian_file_path = os.path.join(persian_dir, f\"persian_{filename}\")\n",
        "            df_persian.to_csv(persian_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "            translated_files_count += 1\n",
        "            print(f\"Created Persian version: {persian_file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    print(f\"Successfully created {translated_files_count} Persian dataset files\")\n",
        "\n",
        "def get_translation_statistics(csv_files_info):\n",
        "    \"\"\"Get statistics about translation requirements\"\"\"\n",
        "    total_entries = 0\n",
        "    entries_needing_translation = 0\n",
        "    total_words = 0\n",
        "    files_with_transcripts = 0\n",
        "\n",
        "    for dataset_name, directory, filename, full_path in csv_files_info:\n",
        "        try:\n",
        "            df = pd.read_csv(full_path)\n",
        "            transcript_cols = find_transcript_columns(df)\n",
        "\n",
        "            if transcript_cols:\n",
        "                files_with_transcripts += 1\n",
        "                for col in transcript_cols:\n",
        "                    for text in df[col].dropna():\n",
        "                        if isinstance(text, str) and text.strip():\n",
        "                            total_entries += 1\n",
        "                            features = analyze_linguistic_features(text)\n",
        "                            if features['requires_translation']:\n",
        "                                entries_needing_translation += 1\n",
        "                                total_words += features['word_count']\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        'total_files_with_transcripts': files_with_transcripts,\n",
        "        'total_entries': total_entries,\n",
        "        'entries_needing_translation': entries_needing_translation,\n",
        "        'estimated_total_words': total_words\n",
        "    }\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    print(\"Starting Step 2: Translation to Persian\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 2.1: Create Persian dataset structure\n",
        "    create_persian_dataset_structure()\n",
        "\n",
        "    # Step 2.2: Find all CSV files from Step 1\n",
        "    print(\"\\nScanning for CSV files...\")\n",
        "    csv_files_info = []\n",
        "\n",
        "    for dataset_name in ['diagnosis_train', 'progression_test', 'progression_train']:\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.csv'):\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        csv_files_info.append((dataset_name, root, file, full_path))\n",
        "\n",
        "    print(f\"Found {len(csv_files_info)} CSV files to analyze\")\n",
        "\n",
        "    # Step 2.3: Get translation statistics\n",
        "    stats = get_translation_statistics(csv_files_info)\n",
        "    print(f\"\\nTranslation Statistics:\")\n",
        "    print(f\"  Files with transcripts: {stats['total_files_with_transcripts']}\")\n",
        "    print(f\"  Total text entries: {stats['total_entries']}\")\n",
        "    print(f\"  Entries needing translation: {stats['entries_needing_translation']}\")\n",
        "    print(f\"  Estimated total words: {stats['estimated_total_words']}\")\n",
        "\n",
        "    # Step 2.4: Create translation template\n",
        "    translation_data = create_translation_template(csv_files_info)\n",
        "\n",
        "    # Step 2.5: Save translation template\n",
        "    template_path, csv_template_path = save_translation_template(translation_data)\n",
        "\n",
        "    # Step 2.6: Create translation guidelines\n",
        "    guidelines_path = create_translation_guidelines()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 2 SETUP COMPLETED!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Found {len(translation_data)} text entries requiring translation\")\n",
        "    print(f\"\\nNext Actions Required:\")\n",
        "    print(f\"1. Review translation guidelines: {guidelines_path}\")\n",
        "    print(f\"2. Open translation template: {csv_template_path}\")\n",
        "    print(f\"3. Fill in 'persian_translation' column for each entry\")\n",
        "    print(f\"4. Save the completed translations\")\n",
        "    print(f\"5. Run the apply_translations() function\")\n",
        "\n",
        "    print(f\"\\nIMPORTANT REMINDERS:\")\n",
        "    print(f\"- Preserve ALL pause words (uhm, uh, etc.)\")\n",
        "    print(f\"- Keep repetitions exactly as they appear\")\n",
        "    print(f\"- Maintain linguistic errors and hesitations\")\n",
        "    print(f\"- Use native Persian speaker expertise for accuracy\")\n",
        "\n",
        "    return template_path, csv_template_path, guidelines_path\n",
        "\n",
        "def apply_completed_translations():\n",
        "    \"\"\"Call this function after completing manual translations\"\"\"\n",
        "    template_path = os.path.join(persian_translated_path, 'translation_template.json')\n",
        "\n",
        "    if os.path.exists(template_path):\n",
        "        load_and_apply_translations(template_path)\n",
        "        print(\"\\nPersian dataset creation completed!\")\n",
        "        print(\"Ready for Step 3: Data Preprocessing\")\n",
        "    else:\n",
        "        print(\"Translation template not found. Please complete Step 2 setup first.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    template_path, csv_template_path, guidelines_path = main()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"READY FOR TRANSLATION!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Please open this file to start translating:\")\n",
        "    print(f\"{csv_template_path}\")\n",
        "    print(f\"\\nAfter completing translations, run:\")\n",
        "    print(f\"apply_completed_translations()\")\n",
        "\n",
        "# Quick function to check translation progress\n",
        "def check_translation_progress():\n",
        "    \"\"\"Check how many translations have been completed\"\"\"\n",
        "    template_path = os.path.join(persian_translated_path, 'translation_template.json')\n",
        "\n",
        "    if os.path.exists(template_path):\n",
        "        with open(template_path, 'r', encoding='utf-8') as f:\n",
        "            translation_data = json.load(f)\n",
        "\n",
        "        total = len(translation_data)\n",
        "        completed = sum(1 for entry in translation_data if entry['persian_translation'].strip())\n",
        "\n",
        "        print(f\"Translation Progress: {completed}/{total} ({completed/total*100:.1f}%)\")\n",
        "        return completed, total\n",
        "    else:\n",
        "        print(\"Translation template not found. Run main() first.\")\n",
        "        return 0, 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9MoaTzKm4X2",
        "outputId": "65c117b2-b44b-4042-bc49-f64df3253c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Step 2: Translation to Persian\n",
            "============================================================\n",
            "Creating Persian dataset directory structure...\n",
            "Persian dataset structure created at: /content/drive/MyDrive/Voice/Persian_Translated_Dataset\n",
            "\n",
            "Scanning for CSV files...\n",
            "Found 228 CSV files to analyze\n",
            "\n",
            "Translation Statistics:\n",
            "  Files with transcripts: 0\n",
            "  Total text entries: 0\n",
            "  Entries needing translation: 0\n",
            "  Estimated total words: 0\n",
            "Creating translation template...\n",
            "Translation template saved:\n",
            "  JSON format: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.json\n",
            "  CSV format: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.csv\n",
            "Translation guidelines saved: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_guidelines.txt\n",
            "\n",
            "============================================================\n",
            "STEP 2 SETUP COMPLETED!\n",
            "============================================================\n",
            "Found 0 text entries requiring translation\n",
            "\n",
            "Next Actions Required:\n",
            "1. Review translation guidelines: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_guidelines.txt\n",
            "2. Open translation template: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.csv\n",
            "3. Fill in 'persian_translation' column for each entry\n",
            "4. Save the completed translations\n",
            "5. Run the apply_translations() function\n",
            "\n",
            "IMPORTANT REMINDERS:\n",
            "- Preserve ALL pause words (uhm, uh, etc.)\n",
            "- Keep repetitions exactly as they appear\n",
            "- Maintain linguistic errors and hesitations\n",
            "- Use native Persian speaker expertise for accuracy\n",
            "\n",
            "============================================================\n",
            "READY FOR TRANSLATION!\n",
            "============================================================\n",
            "Please open this file to start translating:\n",
            "/content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.csv\n",
            "\n",
            "After completing translations, run:\n",
            "apply_completed_translations()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define paths\n",
        "extracted_base_path = '/content/drive/MyDrive/Voice/Extracted_dataset'\n",
        "\n",
        "def inspect_csv_files_detailed():\n",
        "    \"\"\"Detailed inspection of CSV files to understand their structure\"\"\"\n",
        "    print(\"DETAILED CSV STRUCTURE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    csv_files_info = []\n",
        "\n",
        "    # Collect all CSV files\n",
        "    for dataset_name in ['diagnosis_train', 'progression_test', 'progression_train']:\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.csv'):\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        csv_files_info.append((dataset_name, root, file, full_path))\n",
        "\n",
        "    print(f\"Found {len(csv_files_info)} CSV files\")\n",
        "\n",
        "    # Analyze first 5 files in detail\n",
        "    for i, (dataset_name, directory, filename, full_path) in enumerate(csv_files_info[:5]):\n",
        "        print(f\"\\n--- ANALYZING FILE {i+1}: {filename} ---\")\n",
        "        print(f\"Dataset: {dataset_name}\")\n",
        "        print(f\"Path: {directory}\")\n",
        "\n",
        "        try:\n",
        "            # Read CSV\n",
        "            df = pd.read_csv(full_path)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "            # Show all column details\n",
        "            print(\"\\nColumn Analysis:\")\n",
        "            for col in df.columns:\n",
        "                print(f\"  '{col}':\")\n",
        "                print(f\"    Data type: {df[col].dtype}\")\n",
        "                print(f\"    Non-null count: {df[col].count()}/{len(df)}\")\n",
        "\n",
        "                # Show sample values\n",
        "                sample_values = df[col].dropna().head(3).tolist()\n",
        "                print(f\"    Sample values: {sample_values}\")\n",
        "\n",
        "                # Check if this looks like transcript data\n",
        "                if df[col].dtype == 'object' and df[col].count() > 0:\n",
        "                    sample_text = ' '.join(str(v) for v in sample_values if pd.notna(v))\n",
        "                    if len(sample_text) > 20:  # If there's substantial text\n",
        "                        print(f\"    Sample text (first 100 chars): '{sample_text[:100]}...'\")\n",
        "\n",
        "            print(f\"\\nFirst 2 rows of data:\")\n",
        "            print(df.head(2).to_string())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    return csv_files_info\n",
        "\n",
        "def find_actual_transcript_data(csv_files_info):\n",
        "    \"\"\"Find actual transcript data by examining content patterns\"\"\"\n",
        "    print(f\"\\nSEARCHING FOR TRANSCRIPT DATA PATTERNS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    transcript_files = []\n",
        "\n",
        "    for dataset_name, directory, filename, full_path in csv_files_info[:10]:  # Check first 10 files\n",
        "        try:\n",
        "            df = pd.read_csv(full_path)\n",
        "\n",
        "            # Check each column for transcript-like content\n",
        "            for col in df.columns:\n",
        "                if df[col].dtype == 'object':\n",
        "                    # Get non-null values\n",
        "                    values = df[col].dropna()\n",
        "\n",
        "                    if len(values) > 0:\n",
        "                        # Join sample values to analyze\n",
        "                        sample_text = ' '.join(str(v) for v in values.head(10))\n",
        "\n",
        "                        # Look for English words that commonly appear in speech\n",
        "                        english_words = ['the', 'and', 'is', 'are', 'this', 'that', 'with', 'woman', 'man', 'boy', 'girl',\n",
        "                                       'kitchen', 'water', 'cookie', 'jar', 'falling', 'washing', 'dishes']\n",
        "\n",
        "                        # Look for speech characteristics\n",
        "                        speech_indicators = ['uhm', 'uh', 'um', 'well', 'you know', 'like']\n",
        "\n",
        "                        word_matches = sum(1 for word in english_words if word.lower() in sample_text.lower())\n",
        "                        speech_matches = sum(1 for indicator in speech_indicators if indicator.lower() in sample_text.lower())\n",
        "\n",
        "                        if word_matches >= 3 or speech_matches >= 1:\n",
        "                            transcript_files.append({\n",
        "                                'dataset': dataset_name,\n",
        "                                'file': filename,\n",
        "                                'directory': directory,\n",
        "                                'column': col,\n",
        "                                'word_matches': word_matches,\n",
        "                                'speech_matches': speech_matches,\n",
        "                                'sample_text': sample_text[:200],\n",
        "                                'total_entries': len(values)\n",
        "                            })\n",
        "\n",
        "                            print(f\"FOUND TRANSCRIPT DATA:\")\n",
        "                            print(f\"  File: {filename}\")\n",
        "                            print(f\"  Column: '{col}'\")\n",
        "                            print(f\"  English words found: {word_matches}\")\n",
        "                            print(f\"  Speech indicators: {speech_matches}\")\n",
        "                            print(f\"  Entries: {len(values)}\")\n",
        "                            print(f\"  Sample: '{sample_text[:150]}...'\")\n",
        "                            print()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing {filename}: {str(e)}\")\n",
        "\n",
        "    return transcript_files\n",
        "\n",
        "def check_adresso_format():\n",
        "    \"\"\"Check if files follow ADReSSo challenge format\"\"\"\n",
        "    print(f\"\\nCHECKING ADReSSo CHALLENGE FORMAT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ADReSSo files typically have specific naming patterns\n",
        "    for dataset_name in ['diagnosis_train', 'progression_test', 'progression_train']:\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        print(f\"\\n{dataset_name.upper()}:\")\n",
        "\n",
        "        # Check directory structure\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            level = root.replace(dataset_path, '').count(os.sep)\n",
        "            indent = '  ' * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "            # Show file patterns\n",
        "            if files:\n",
        "                # Group files by extension\n",
        "                extensions = {}\n",
        "                for file in files:\n",
        "                    ext = os.path.splitext(file)[1]\n",
        "                    if ext not in extensions:\n",
        "                        extensions[ext] = []\n",
        "                    extensions[ext].append(file)\n",
        "\n",
        "                for ext, file_list in extensions.items():\n",
        "                    print(f\"{indent}  {ext} files: {len(file_list)}\")\n",
        "                    # Show first few filenames\n",
        "                    for file in file_list[:3]:\n",
        "                        print(f\"{indent}    {file}\")\n",
        "                    if len(file_list) > 3:\n",
        "                        print(f\"{indent}    ... and {len(file_list)-3} more\")\n",
        "\n",
        "def manual_csv_inspection():\n",
        "    \"\"\"Let user manually specify which files/columns contain transcripts\"\"\"\n",
        "    print(f\"\\nMANUAL CSV INSPECTION\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Let's manually check a few files to understand the format...\")\n",
        "\n",
        "    # Pick a few specific files to examine\n",
        "    test_files = []\n",
        "\n",
        "    for dataset_name in ['diagnosis_train']:  # Start with diagnosis train\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root and files:\n",
        "                # Take first CSV file from each category\n",
        "                for file in files[:1]:  # Just first file\n",
        "                    if file.endswith('.csv'):\n",
        "                        test_files.append(os.path.join(root, file))\n",
        "                        break\n",
        "                if len(test_files) >= 2:  # Limit to 2 files for detailed inspection\n",
        "                    break\n",
        "\n",
        "    for file_path in test_files:\n",
        "        print(f\"\\nDETAILED INSPECTION: {os.path.basename(file_path)}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        try:\n",
        "            # Try different encodings\n",
        "            encodings = ['utf-8', 'latin-1', 'cp1252']\n",
        "            df = None\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, encoding=encoding)\n",
        "                    print(f\"Successfully read with {encoding} encoding\")\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if df is not None:\n",
        "                print(f\"Shape: {df.shape}\")\n",
        "                print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "                # Show full content of small files\n",
        "                if len(df) <= 10:\n",
        "                    print(\"\\nFULL FILE CONTENT:\")\n",
        "                    print(df.to_string())\n",
        "                else:\n",
        "                    print(\"\\nFIRST 5 ROWS:\")\n",
        "                    print(df.head().to_string())\n",
        "\n",
        "                # Look for the most promising transcript column\n",
        "                best_col = None\n",
        "                max_text_length = 0\n",
        "\n",
        "                for col in df.columns:\n",
        "                    if df[col].dtype == 'object':\n",
        "                        sample = df[col].dropna()\n",
        "                        if len(sample) > 0:\n",
        "                            avg_length = sample.astype(str).str.len().mean()\n",
        "                            if avg_length > max_text_length:\n",
        "                                max_text_length = avg_length\n",
        "                                best_col = col\n",
        "\n",
        "                if best_col:\n",
        "                    print(f\"\\nMOST LIKELY TRANSCRIPT COLUMN: '{best_col}'\")\n",
        "                    print(\"Sample entries:\")\n",
        "                    for i, entry in enumerate(df[best_col].dropna().head(3)):\n",
        "                        print(f\"  {i+1}: '{entry}'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "\n",
        "# Main diagnostic function\n",
        "def main():\n",
        "    print(\"CSV DIAGNOSTIC ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1: Detailed CSV inspection\n",
        "    csv_files_info = inspect_csv_files_detailed()\n",
        "\n",
        "    # Step 2: Search for transcript patterns\n",
        "    transcript_files = find_actual_transcript_data(csv_files_info)\n",
        "\n",
        "    # Step 3: Check ADReSSo format\n",
        "    check_adresso_format()\n",
        "\n",
        "    # Step 4: Manual inspection\n",
        "    manual_csv_inspection()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DIAGNOSTIC SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total CSV files found: {len(csv_files_info)}\")\n",
        "    print(f\"Files with potential transcript data: {len(transcript_files)}\")\n",
        "\n",
        "    if transcript_files:\n",
        "        print(\"\\nPotential transcript columns found:\")\n",
        "        for tf in transcript_files:\n",
        "            print(f\"  {tf['file']}: column '{tf['column']}' ({tf['total_entries']} entries)\")\n",
        "    else:\n",
        "        print(\"\\nNo clear transcript data detected automatically.\")\n",
        "        print(\"Manual inspection results above should help identify the correct format.\")\n",
        "\n",
        "    return csv_files_info, transcript_files\n",
        "\n",
        "# Run diagnostic\n",
        "if __name__ == \"__main__\":\n",
        "    csv_files_info, transcript_files = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bXjN6h_nYNz",
        "outputId": "eed1bc02-e4d5-42ae-91d2-9097f1184762"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV DIAGNOSTIC ANALYSIS\n",
            "============================================================\n",
            "DETAILED CSV STRUCTURE ANALYSIS\n",
            "============================================================\n",
            "Found 228 CSV files\n",
            "\n",
            "--- ANALYZING FILE 1: adrso281.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Path: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (18, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column Analysis:\n",
            "  'Unnamed: 0':\n",
            "    Data type: int64\n",
            "    Non-null count: 18/18\n",
            "    Sample values: [1, 2, 3]\n",
            "  'speaker':\n",
            "    Data type: object\n",
            "    Non-null count: 18/18\n",
            "    Sample values: ['PAR', 'PAR', 'PAR']\n",
            "  'begin':\n",
            "    Data type: int64\n",
            "    Non-null count: 18/18\n",
            "    Sample values: [0, 2129, 5471]\n",
            "  'end':\n",
            "    Data type: int64\n",
            "    Non-null count: 18/18\n",
            "    Sample values: [2129, 5471, 7249]\n",
            "\n",
            "First 2 rows of data:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  2129\n",
            "1           2     PAR   2129  5471\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- ANALYZING FILE 2: adrso308.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Path: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (24, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column Analysis:\n",
            "  'Unnamed: 0':\n",
            "    Data type: int64\n",
            "    Non-null count: 24/24\n",
            "    Sample values: [1, 2, 3]\n",
            "  'speaker':\n",
            "    Data type: object\n",
            "    Non-null count: 24/24\n",
            "    Sample values: ['PAR', 'PAR', 'PAR']\n",
            "  'begin':\n",
            "    Data type: int64\n",
            "    Non-null count: 24/24\n",
            "    Sample values: [0, 4873, 7270]\n",
            "  'end':\n",
            "    Data type: int64\n",
            "    Non-null count: 24/24\n",
            "    Sample values: [4873, 7270, 10991]\n",
            "\n",
            "First 2 rows of data:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  4873\n",
            "1           2     PAR   4873  7270\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- ANALYZING FILE 3: adrso270.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Path: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (20, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column Analysis:\n",
            "  'Unnamed: 0':\n",
            "    Data type: int64\n",
            "    Non-null count: 20/20\n",
            "    Sample values: [1, 2, 3]\n",
            "  'speaker':\n",
            "    Data type: object\n",
            "    Non-null count: 20/20\n",
            "    Sample values: ['INV', 'PAR', 'PAR']\n",
            "  'begin':\n",
            "    Data type: int64\n",
            "    Non-null count: 20/20\n",
            "    Sample values: [0, 997, 2294]\n",
            "  'end':\n",
            "    Data type: int64\n",
            "    Non-null count: 20/20\n",
            "    Sample values: [997, 2294, 10541]\n",
            "\n",
            "First 2 rows of data:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     INV      0   997\n",
            "1           2     PAR    997  2294\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- ANALYZING FILE 4: adrso022.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Path: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (8, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column Analysis:\n",
            "  'Unnamed: 0':\n",
            "    Data type: int64\n",
            "    Non-null count: 8/8\n",
            "    Sample values: [1, 2, 3]\n",
            "  'speaker':\n",
            "    Data type: object\n",
            "    Non-null count: 8/8\n",
            "    Sample values: ['INV', 'INV', 'INV']\n",
            "  'begin':\n",
            "    Data type: int64\n",
            "    Non-null count: 8/8\n",
            "    Sample values: [0, 24311, 26761]\n",
            "  'end':\n",
            "    Data type: int64\n",
            "    Non-null count: 8/8\n",
            "    Sample values: [5944, 26761, 27700]\n",
            "\n",
            "First 2 rows of data:\n",
            "   Unnamed: 0 speaker  begin    end\n",
            "0           1     INV      0   5944\n",
            "1           2     INV  24311  26761\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- ANALYZING FILE 5: adrso298.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Path: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (14, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column Analysis:\n",
            "  'Unnamed: 0':\n",
            "    Data type: int64\n",
            "    Non-null count: 14/14\n",
            "    Sample values: [1, 2, 3]\n",
            "  'speaker':\n",
            "    Data type: object\n",
            "    Non-null count: 14/14\n",
            "    Sample values: ['PAR', 'PAR', 'PAR']\n",
            "  'begin':\n",
            "    Data type: int64\n",
            "    Non-null count: 14/14\n",
            "    Sample values: [0, 7565, 10429]\n",
            "  'end':\n",
            "    Data type: int64\n",
            "    Non-null count: 14/14\n",
            "    Sample values: [7565, 10429, 17899]\n",
            "\n",
            "First 2 rows of data:\n",
            "   Unnamed: 0 speaker  begin    end\n",
            "0           1     PAR      0   7565\n",
            "1           2     PAR   7565  10429\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SEARCHING FOR TRANSCRIPT DATA PATTERNS\n",
            "============================================================\n",
            "\n",
            "CHECKING ADReSSo CHALLENGE FORMAT\n",
            "============================================================\n",
            "\n",
            "DIAGNOSIS_TRAIN:\n",
            "diagnosis_train/\n",
            "  ADReSSo21/\n",
            "    diagnosis/\n",
            "      .md files: 1\n",
            "        README.md\n",
            "      train/\n",
            "        .csv files: 1\n",
            "          adresso-train-mmse-scores.csv\n",
            "        segmentation/\n",
            "          cn/\n",
            "            .csv files: 79\n",
            "              adrso281.csv\n",
            "              adrso308.csv\n",
            "              adrso270.csv\n",
            "              ... and 76 more\n",
            "          ad/\n",
            "            .csv files: 87\n",
            "              adrso229.csv\n",
            "              adrso106.csv\n",
            "              adrso144.csv\n",
            "              ... and 84 more\n",
            "        audio/\n",
            "          cn/\n",
            "            .wav files: 79\n",
            "              adrso173.wav\n",
            "              adrso015.wav\n",
            "              adrso307.wav\n",
            "              ... and 76 more\n",
            "          ad/\n",
            "            .wav files: 87\n",
            "              adrso047.wav\n",
            "              adrso128.wav\n",
            "              adrso045.wav\n",
            "              ... and 84 more\n",
            "\n",
            "PROGRESSION_TEST:\n",
            "progression_test/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      test-dist/\n",
            "        .csv files: 1\n",
            "          test_results_task3.csv\n",
            "         files: 1\n",
            "          README\n",
            "        segmentation/\n",
            "          .csv files: 15\n",
            "            adrspt24.csv\n",
            "            adrspt15.csv\n",
            "            adrspt12.csv\n",
            "            ... and 12 more\n",
            "        audio/\n",
            "          .wav files: 32\n",
            "            adrspt20.wav\n",
            "            adrspt15.wav\n",
            "            adrspt4.wav\n",
            "            ... and 29 more\n",
            "\n",
            "PROGRESSION_TRAIN:\n",
            "progression_train/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      .md files: 1\n",
            "        README.md\n",
            "      train/\n",
            "        segmentation/\n",
            "          no_decline/\n",
            "            .csv files: 37\n",
            "              adrsp195.csv\n",
            "              adrsp041.csv\n",
            "              adrsp030.csv\n",
            "              ... and 34 more\n",
            "          decline/\n",
            "            .csv files: 10\n",
            "              adrsp051.csv\n",
            "              adrsp313.csv\n",
            "              adrsp101.csv\n",
            "              ... and 7 more\n",
            "        audio/\n",
            "          no_decline/\n",
            "            .wav files: 58\n",
            "              adrsp196.wav\n",
            "              adrsp137.wav\n",
            "              adrsp130.wav\n",
            "              ... and 55 more\n",
            "          decline/\n",
            "            .wav files: 15\n",
            "              adrsp055.wav\n",
            "              adrsp003.wav\n",
            "              adrsp266.wav\n",
            "              ... and 12 more\n",
            "\n",
            "MANUAL CSV INSPECTION\n",
            "============================================================\n",
            "Let's manually check a few files to understand the format...\n",
            "\n",
            "DETAILED INSPECTION: adrso281.csv\n",
            "----------------------------------------\n",
            "Successfully read with utf-8 encoding\n",
            "Shape: (18, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "FIRST 5 ROWS:\n",
            "   Unnamed: 0 speaker  begin    end\n",
            "0           1     PAR      0   2129\n",
            "1           2     PAR   2129   5471\n",
            "2           3     PAR   5471   7249\n",
            "3           4     PAR   7249  11821\n",
            "4           5     PAR  11821  13351\n",
            "\n",
            "MOST LIKELY TRANSCRIPT COLUMN: 'speaker'\n",
            "Sample entries:\n",
            "  1: 'PAR'\n",
            "  2: 'PAR'\n",
            "  3: 'PAR'\n",
            "\n",
            "DETAILED INSPECTION: adrso229.csv\n",
            "----------------------------------------\n",
            "Successfully read with utf-8 encoding\n",
            "Shape: (34, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "FIRST 5 ROWS:\n",
            "   Unnamed: 0 speaker  begin    end\n",
            "0           1     INV      0   2475\n",
            "1           2     INV   2475   6232\n",
            "2           3     PAR   6232  13316\n",
            "3           4     PAR  13316  20997\n",
            "4           5     PAR  20997  24654\n",
            "\n",
            "MOST LIKELY TRANSCRIPT COLUMN: 'speaker'\n",
            "Sample entries:\n",
            "  1: 'INV'\n",
            "  2: 'INV'\n",
            "  3: 'PAR'\n",
            "\n",
            "============================================================\n",
            "DIAGNOSTIC SUMMARY\n",
            "============================================================\n",
            "Total CSV files found: 228\n",
            "Files with potential transcript data: 0\n",
            "\n",
            "No clear transcript data detected automatically.\n",
            "Manual inspection results above should help identify the correct format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def comprehensive_dataset_analysis(extracted_base_path):\n",
        "    \"\"\"\n",
        "    Comprehensive analysis to find actual transcript data in ADReSSo21 dataset\n",
        "    \"\"\"\n",
        "    print(\"COMPREHENSIVE DATASET ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    analysis_results = {\n",
        "        'csv_files': [],\n",
        "        'text_files': [],\n",
        "        'json_files': [],\n",
        "        'other_files': [],\n",
        "        'audio_files': [],\n",
        "        'potential_transcript_sources': []\n",
        "    }\n",
        "\n",
        "    # Walk through all directories\n",
        "    for root, dirs, files in os.walk(extracted_base_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_ext = os.path.splitext(file)[1].lower()\n",
        "\n",
        "            # Categorize files\n",
        "            if file_ext == '.csv':\n",
        "                analysis_results['csv_files'].append(file_path)\n",
        "            elif file_ext in ['.txt', '.text']:\n",
        "                analysis_results['text_files'].append(file_path)\n",
        "            elif file_ext == '.json':\n",
        "                analysis_results['json_files'].append(file_path)\n",
        "            elif file_ext == '.wav':\n",
        "                analysis_results['audio_files'].append(file_path)\n",
        "            elif file_ext in ['.md', '.readme']:\n",
        "                analysis_results['other_files'].append(file_path)\n",
        "\n",
        "    print(f\"Files found:\")\n",
        "    print(f\"  CSV files: {len(analysis_results['csv_files'])}\")\n",
        "    print(f\"  Text files: {len(analysis_results['text_files'])}\")\n",
        "    print(f\"  JSON files: {len(analysis_results['json_files'])}\")\n",
        "    print(f\"  Audio files: {len(analysis_results['audio_files'])}\")\n",
        "    print(f\"  Other files: {len(analysis_results['other_files'])}\")\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "def analyze_csv_content_detailed(csv_files, max_files=5):\n",
        "    \"\"\"\n",
        "    Detailed analysis of CSV file contents to find transcript data\n",
        "    \"\"\"\n",
        "    print(f\"\\nDETAILED CSV CONTENT ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    transcript_candidates = []\n",
        "\n",
        "    for i, csv_file in enumerate(csv_files[:max_files]):\n",
        "        print(f\"\\n--- Analyzing CSV {i+1}: {os.path.basename(csv_file)} ---\")\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "            # Check each column for potential transcript content\n",
        "            for col in df.columns:\n",
        "                print(f\"\\nColumn '{col}':\")\n",
        "                print(f\"  Data type: {df[col].dtype}\")\n",
        "                print(f\"  Non-null values: {df[col].count()} / {len(df)}\")\n",
        "\n",
        "                # Get sample values\n",
        "                sample_values = df[col].dropna().head(3).tolist()\n",
        "                print(f\"  Sample values: {sample_values}\")\n",
        "\n",
        "                # Check if this could be transcript content\n",
        "                if df[col].dtype == 'object':  # String data\n",
        "                    # Look for actual text content (not just labels)\n",
        "                    text_samples = [str(val) for val in sample_values if len(str(val)) > 10]\n",
        "                    if text_samples:\n",
        "                        print(f\"  >>> POTENTIAL TRANSCRIPT CONTENT FOUND <<<\")\n",
        "                        transcript_candidates.append({\n",
        "                            'file': csv_file,\n",
        "                            'column': col,\n",
        "                            'samples': text_samples[:2]\n",
        "                        })\n",
        "\n",
        "                        # Show more detail for potential transcripts\n",
        "                        for j, sample in enumerate(text_samples[:2]):\n",
        "                            print(f\"    Sample {j+1}: '{sample[:100]}...'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {csv_file}: {str(e)}\")\n",
        "\n",
        "    return transcript_candidates\n",
        "\n",
        "def analyze_text_files(text_files, max_files=10):\n",
        "    \"\"\"\n",
        "    Analyze text files for transcript content\n",
        "    \"\"\"\n",
        "    print(f\"\\nTEXT FILES ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    transcript_text_files = []\n",
        "\n",
        "    for text_file in text_files[:max_files]:\n",
        "        print(f\"\\n--- Analyzing: {os.path.basename(text_file)} ---\")\n",
        "\n",
        "        try:\n",
        "            with open(text_file, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            print(f\"File size: {len(content)} characters\")\n",
        "\n",
        "            if len(content) > 50:  # Skip very short files\n",
        "                # Show first 200 characters\n",
        "                preview = content[:200].replace('\\n', ' ').strip()\n",
        "                print(f\"Preview: '{preview}...'\")\n",
        "\n",
        "                # Check if this looks like transcript content\n",
        "                if any(indicator in content.lower() for indicator in\n",
        "                       ['participant:', 'investigator:', 'speaker:', 'transcript', 'utterance']):\n",
        "                    print(\">>> POTENTIAL TRANSCRIPT FILE <<<\")\n",
        "                    transcript_text_files.append(text_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {text_file}: {str(e)}\")\n",
        "\n",
        "    return transcript_text_files\n",
        "\n",
        "def check_readme_files(other_files):\n",
        "    \"\"\"\n",
        "    Check README and documentation files for dataset structure info\n",
        "    \"\"\"\n",
        "    print(f\"\\nREADME/DOCUMENTATION ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for file_path in other_files:\n",
        "        if 'readme' in os.path.basename(file_path).lower():\n",
        "            print(f\"\\n--- {os.path.basename(file_path)} ---\")\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                print(content[:500] + \"...\" if len(content) > 500 else content)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading: {str(e)}\")\n",
        "\n",
        "def analyze_mmse_scores_file(extracted_base_path):\n",
        "    \"\"\"\n",
        "    Analyze the MMSE scores file which might contain additional data\n",
        "    \"\"\"\n",
        "    print(f\"\\nMMSE SCORES FILE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Look for MMSE scores file\n",
        "    mmse_files = []\n",
        "    for root, dirs, files in os.walk(extracted_base_path):\n",
        "        for file in files:\n",
        "            if 'mmse' in file.lower() and file.endswith('.csv'):\n",
        "                mmse_files.append(os.path.join(root, file))\n",
        "\n",
        "    for mmse_file in mmse_files:\n",
        "        print(f\"\\n--- Analyzing: {os.path.basename(mmse_file)} ---\")\n",
        "        try:\n",
        "            df = pd.read_csv(mmse_file)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "            print(\"First few rows:\")\n",
        "            print(df.head().to_string())\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "\n",
        "def generate_recommendations(analysis_results, transcript_candidates, transcript_text_files):\n",
        "    \"\"\"\n",
        "    Generate recommendations based on analysis\n",
        "    \"\"\"\n",
        "    print(f\"\\nRECOMMENDAIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if transcript_candidates:\n",
        "        print(\"✓ Found potential transcript data in CSV files:\")\n",
        "        for candidate in transcript_candidates:\n",
        "            print(f\"  - {os.path.basename(candidate['file'])}, column: {candidate['column']}\")\n",
        "    else:\n",
        "        print(\"✗ No clear transcript data found in CSV files\")\n",
        "\n",
        "    if transcript_text_files:\n",
        "        print(\"✓ Found potential transcript text files:\")\n",
        "        for txt_file in transcript_text_files:\n",
        "            print(f\"  - {os.path.basename(txt_file)}\")\n",
        "    else:\n",
        "        print(\"✗ No transcript text files found\")\n",
        "\n",
        "    if not transcript_candidates and not transcript_text_files:\n",
        "        print(\"\\n🔍 NEXT STEPS:\")\n",
        "        print(\"1. The CSV files appear to contain only segmentation data (timestamps)\")\n",
        "        print(\"2. You may need to:\")\n",
        "        print(\"   a) Look for transcript files in a different location\")\n",
        "        print(\"   b) Use speech-to-text on the audio files\")\n",
        "        print(\"   c) Check if transcripts are embedded in the audio metadata\")\n",
        "        print(\"   d) Contact the dataset providers for transcript access\")\n",
        "\n",
        "        print(f\"\\n📊 DATASET SUMMARY:\")\n",
        "        print(f\"   Audio files available: {len(analysis_results['audio_files'])}\")\n",
        "        print(f\"   These can be processed with speech-to-text tools\")\n",
        "\n",
        "# Main analysis function\n",
        "def run_comprehensive_analysis(extracted_base_path='/content/drive/MyDrive/Voice/Extracted_dataset'):\n",
        "    \"\"\"\n",
        "    Run comprehensive analysis to find transcript data\n",
        "    \"\"\"\n",
        "    print(\"STARTING COMPREHENSIVE ANALYSIS OF ADReSSo21 DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Get overview of all files\n",
        "    analysis_results = comprehensive_dataset_analysis(extracted_base_path)\n",
        "\n",
        "    # Step 2: Detailed CSV analysis\n",
        "    transcript_candidates = analyze_csv_content_detailed(analysis_results['csv_files'])\n",
        "\n",
        "    # Step 3: Text files analysis\n",
        "    transcript_text_files = analyze_text_files(analysis_results['text_files'])\n",
        "\n",
        "    # Step 4: Check documentation\n",
        "    check_readme_files(analysis_results['other_files'])\n",
        "\n",
        "    # Step 5: MMSE scores analysis\n",
        "    analyze_mmse_scores_file(extracted_base_path)\n",
        "\n",
        "    # Step 6: Generate recommendations\n",
        "    generate_recommendations(analysis_results, transcript_candidates, transcript_text_files)\n",
        "\n",
        "    return {\n",
        "        'analysis_results': analysis_results,\n",
        "        'transcript_candidates': transcript_candidates,\n",
        "        'transcript_text_files': transcript_text_files\n",
        "    }\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_comprehensive_analysis()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEQgGOJGrR5T",
        "outputId": "0fc50ab6-71d2-4249-99f9-eff62c306905"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING COMPREHENSIVE ANALYSIS OF ADReSSo21 DATASET\n",
            "================================================================================\n",
            "COMPREHENSIVE DATASET ANALYSIS\n",
            "============================================================\n",
            "Files found:\n",
            "  CSV files: 230\n",
            "  Text files: 0\n",
            "  JSON files: 0\n",
            "  Audio files: 271\n",
            "  Other files: 2\n",
            "\n",
            "DETAILED CSV CONTENT ANALYSIS\n",
            "============================================================\n",
            "\n",
            "--- Analyzing CSV 1: adresso-train-mmse-scores.csv ---\n",
            "Shape: (166, 4)\n",
            "Columns: ['Unnamed: 0', 'adressfname', 'mmse', 'dx']\n",
            "\n",
            "Column 'Unnamed: 0':\n",
            "  Data type: int64\n",
            "  Non-null values: 166 / 166\n",
            "  Sample values: [23, 24, 25]\n",
            "\n",
            "Column 'adressfname':\n",
            "  Data type: object\n",
            "  Non-null values: 166 / 166\n",
            "  Sample values: ['adrso024', 'adrso025', 'adrso027']\n",
            "\n",
            "Column 'mmse':\n",
            "  Data type: int64\n",
            "  Non-null values: 166 / 166\n",
            "  Sample values: [20, 11, 18]\n",
            "\n",
            "Column 'dx':\n",
            "  Data type: object\n",
            "  Non-null values: 166 / 166\n",
            "  Sample values: ['ad', 'ad', 'ad']\n",
            "\n",
            "--- Analyzing CSV 2: adrso281.csv ---\n",
            "Shape: (18, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column 'Unnamed: 0':\n",
            "  Data type: int64\n",
            "  Non-null values: 18 / 18\n",
            "  Sample values: [1, 2, 3]\n",
            "\n",
            "Column 'speaker':\n",
            "  Data type: object\n",
            "  Non-null values: 18 / 18\n",
            "  Sample values: ['PAR', 'PAR', 'PAR']\n",
            "\n",
            "Column 'begin':\n",
            "  Data type: int64\n",
            "  Non-null values: 18 / 18\n",
            "  Sample values: [0, 2129, 5471]\n",
            "\n",
            "Column 'end':\n",
            "  Data type: int64\n",
            "  Non-null values: 18 / 18\n",
            "  Sample values: [2129, 5471, 7249]\n",
            "\n",
            "--- Analyzing CSV 3: adrso308.csv ---\n",
            "Shape: (24, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column 'Unnamed: 0':\n",
            "  Data type: int64\n",
            "  Non-null values: 24 / 24\n",
            "  Sample values: [1, 2, 3]\n",
            "\n",
            "Column 'speaker':\n",
            "  Data type: object\n",
            "  Non-null values: 24 / 24\n",
            "  Sample values: ['PAR', 'PAR', 'PAR']\n",
            "\n",
            "Column 'begin':\n",
            "  Data type: int64\n",
            "  Non-null values: 24 / 24\n",
            "  Sample values: [0, 4873, 7270]\n",
            "\n",
            "Column 'end':\n",
            "  Data type: int64\n",
            "  Non-null values: 24 / 24\n",
            "  Sample values: [4873, 7270, 10991]\n",
            "\n",
            "--- Analyzing CSV 4: adrso270.csv ---\n",
            "Shape: (20, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column 'Unnamed: 0':\n",
            "  Data type: int64\n",
            "  Non-null values: 20 / 20\n",
            "  Sample values: [1, 2, 3]\n",
            "\n",
            "Column 'speaker':\n",
            "  Data type: object\n",
            "  Non-null values: 20 / 20\n",
            "  Sample values: ['INV', 'PAR', 'PAR']\n",
            "\n",
            "Column 'begin':\n",
            "  Data type: int64\n",
            "  Non-null values: 20 / 20\n",
            "  Sample values: [0, 997, 2294]\n",
            "\n",
            "Column 'end':\n",
            "  Data type: int64\n",
            "  Non-null values: 20 / 20\n",
            "  Sample values: [997, 2294, 10541]\n",
            "\n",
            "--- Analyzing CSV 5: adrso022.csv ---\n",
            "Shape: (8, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "\n",
            "Column 'Unnamed: 0':\n",
            "  Data type: int64\n",
            "  Non-null values: 8 / 8\n",
            "  Sample values: [1, 2, 3]\n",
            "\n",
            "Column 'speaker':\n",
            "  Data type: object\n",
            "  Non-null values: 8 / 8\n",
            "  Sample values: ['INV', 'INV', 'INV']\n",
            "\n",
            "Column 'begin':\n",
            "  Data type: int64\n",
            "  Non-null values: 8 / 8\n",
            "  Sample values: [0, 24311, 26761]\n",
            "\n",
            "Column 'end':\n",
            "  Data type: int64\n",
            "  Non-null values: 8 / 8\n",
            "  Sample values: [5944, 26761, 27700]\n",
            "\n",
            "TEXT FILES ANALYSIS\n",
            "============================================================\n",
            "\n",
            "README/DOCUMENTATION ANALYSIS\n",
            "============================================================\n",
            "\n",
            "--- README.md ---\n",
            "# This directory contains the training data for the [IS2021 ADReSSo Challenge](https://edin.ac/3p1cyaI).\n",
            "\n",
            "The data are in the following directories structure and files:\n",
            "\n",
            "```\n",
            "diagnosis\n",
            "└── train\n",
            "    ├── audio\n",
            "    │   ├── ad\n",
            "    │   └── cn\n",
            "    └── segmentation\n",
            "        ├── ad\n",
            "        └── cn\n",
            "```\n",
            "\n",
            "They contain the enhanced, volume normalised audio data for the\n",
            "diagnosis and MMSE score prediction tasks, and a table of MMSE scores\n",
            "for model training (adresso-train-mmse-scores.csv). The abbreviation\n",
            "'cn...\n",
            "\n",
            "--- README.md ---\n",
            "# This directory contains the training data for the [IS2021 ADReSSo Challenge](https://edin.ac/3p1cyaI).\n",
            "\n",
            "The data are in the following directories structure and files:\n",
            "\n",
            "```\n",
            "progression\n",
            "└── train\n",
            "    ├── audio\n",
            "        ├── decline\n",
            "        └── no_decline\n",
            "```\n",
            "\n",
            "This directory contains the data for the disease progression\n",
            "(cognitive decline) prediction task. \n",
            "\n",
            "The data consist of enhanced, volume normalised audio (.wav) files and\n",
            "utterance segmentation files (diarisation), in CSV format. The latter\n",
            "a...\n",
            "\n",
            "MMSE SCORES FILE ANALYSIS\n",
            "============================================================\n",
            "\n",
            "--- Analyzing: adresso-train-mmse-scores.csv ---\n",
            "Shape: (166, 4)\n",
            "Columns: ['Unnamed: 0', 'adressfname', 'mmse', 'dx']\n",
            "First few rows:\n",
            "   Unnamed: 0 adressfname  mmse  dx\n",
            "0          23    adrso024    20  ad\n",
            "1          24    adrso025    11  ad\n",
            "2          25    adrso027    18  ad\n",
            "3          26    adrso028    18  ad\n",
            "4          28    adrso031    26  ad\n",
            "\n",
            "RECOMMENDAIONS\n",
            "============================================================\n",
            "✗ No clear transcript data found in CSV files\n",
            "✗ No transcript text files found\n",
            "\n",
            "🔍 NEXT STEPS:\n",
            "1. The CSV files appear to contain only segmentation data (timestamps)\n",
            "2. You may need to:\n",
            "   a) Look for transcript files in a different location\n",
            "   b) Use speech-to-text on the audio files\n",
            "   c) Check if transcripts are embedded in the audio metadata\n",
            "   d) Contact the dataset providers for transcript access\n",
            "\n",
            "📊 DATASET SUMMARY:\n",
            "   Audio files available: 271\n",
            "   These can be processed with speech-to-text tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1"
      ],
      "metadata": {
        "id": "rHsSaQxgs2kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa SpeechRecognition pydub openai-whisper\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfQZsvWBs747",
        "outputId": "4d32593c-b143-41e1-d107-816f6ae38efa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading speechrecognition-3.14.3-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=4fe360674941f183d832542890191c60f7d54eb47362d908eba2b6f3c9f59027\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: pydub, SpeechRecognition, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed SpeechRecognition-3.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages (run these in separate cells first)\n",
        "\"\"\"\n",
        "!pip install librosa\n",
        "!pip install SpeechRecognition\n",
        "!pip install pydub\n",
        "!pip install google-cloud-speech  # Optional: for better accuracy\n",
        "!pip install openai-whisper       # Alternative: OpenAI Whisper\n",
        "\"\"\"\n",
        "\n",
        "class ADReSSoTranscriptionPipeline:\n",
        "    def __init__(self, extracted_base_path, output_path):\n",
        "        self.extracted_base_path = extracted_base_path\n",
        "        self.output_path = output_path\n",
        "        self.recognizer = sr.Recognizer()\n",
        "        self.transcription_results = []\n",
        "\n",
        "        # Create output directory structure\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    def find_audio_and_segmentation_pairs(self):\n",
        "        \"\"\"Find matching audio files and their segmentation CSV files\"\"\"\n",
        "        print(\"Finding audio-segmentation file pairs...\")\n",
        "\n",
        "        pairs = []\n",
        "\n",
        "        # Walk through the dataset structure\n",
        "        for root, dirs, files in os.walk(self.extracted_base_path):\n",
        "            # Look for audio directories\n",
        "            if 'audio' in root and any(f.endswith('.wav') for f in files):\n",
        "                audio_dir = root\n",
        "\n",
        "                # Find corresponding segmentation directory\n",
        "                seg_dir = audio_dir.replace('/audio/', '/segmentation/')\n",
        "\n",
        "                if os.path.exists(seg_dir):\n",
        "                    # Match audio files with CSV files\n",
        "                    for audio_file in files:\n",
        "                        if audio_file.endswith('.wav'):\n",
        "                            base_name = os.path.splitext(audio_file)[0]\n",
        "                            csv_file = f\"{base_name}.csv\"\n",
        "                            csv_path = os.path.join(seg_dir, csv_file)\n",
        "\n",
        "                            if os.path.exists(csv_path):\n",
        "                                pairs.append({\n",
        "                                    'audio_path': os.path.join(audio_dir, audio_file),\n",
        "                                    'csv_path': csv_path,\n",
        "                                    'file_id': base_name,\n",
        "                                    'category': self._get_category_from_path(audio_dir)\n",
        "                                })\n",
        "\n",
        "        print(f\"Found {len(pairs)} audio-segmentation pairs\")\n",
        "        return pairs\n",
        "\n",
        "    def _get_category_from_path(self, path):\n",
        "        \"\"\"Extract category (ad/cn/decline/no_decline) from file path\"\"\"\n",
        "        if '/ad/' in path:\n",
        "            return 'ad'\n",
        "        elif '/cn/' in path:\n",
        "            return 'cn'\n",
        "        elif '/decline/' in path:\n",
        "            return 'decline'\n",
        "        elif '/no_decline/' in path:\n",
        "            return 'no_decline'\n",
        "        else:\n",
        "            return 'unknown'\n",
        "\n",
        "    def segment_audio_by_speaker(self, audio_path, csv_path):\n",
        "        \"\"\"Segment audio file based on CSV segmentation data\"\"\"\n",
        "        print(f\"Processing: {os.path.basename(audio_path)}\")\n",
        "\n",
        "        try:\n",
        "            # Load segmentation data\n",
        "            seg_df = pd.read_csv(csv_path)\n",
        "\n",
        "            # Load audio file\n",
        "            audio = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "            segments = []\n",
        "\n",
        "            for idx, row in seg_df.iterrows():\n",
        "                speaker = row['speaker']\n",
        "                start_ms = int(row['begin'])\n",
        "                end_ms = int(row['end'])\n",
        "\n",
        "                # Extract audio segment\n",
        "                segment = audio[start_ms:end_ms]\n",
        "\n",
        "                segments.append({\n",
        "                    'segment_id': idx,\n",
        "                    'speaker': speaker,\n",
        "                    'start_ms': start_ms,\n",
        "                    'end_ms': end_ms,\n",
        "                    'duration_ms': end_ms - start_ms,\n",
        "                    'audio_segment': segment\n",
        "                })\n",
        "\n",
        "            return segments\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error segmenting {audio_path}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def transcribe_segment_whisper(self, audio_segment):\n",
        "        \"\"\"Transcribe audio segment using OpenAI Whisper (most accurate)\"\"\"\n",
        "        try:\n",
        "            import whisper\n",
        "\n",
        "            # Save temporary audio file\n",
        "            temp_path = \"/tmp/temp_segment.wav\"\n",
        "            audio_segment.export(temp_path, format=\"wav\")\n",
        "\n",
        "            # Load Whisper model (you can choose: tiny, base, small, medium, large)\n",
        "            model = whisper.load_model(\"base\")\n",
        "\n",
        "            # Transcribe\n",
        "            result = model.transcribe(temp_path)\n",
        "\n",
        "            # Clean up\n",
        "            os.remove(temp_path)\n",
        "\n",
        "            return result[\"text\"].strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Whisper transcription error: {str(e)}\")\n",
        "            return self.transcribe_segment_google(audio_segment)\n",
        "\n",
        "    def transcribe_segment_google(self, audio_segment):\n",
        "        \"\"\"Transcribe audio segment using Google Speech Recognition\"\"\"\n",
        "        try:\n",
        "            # Convert to required format\n",
        "            temp_path = \"/tmp/temp_segment.wav\"\n",
        "            audio_segment.export(temp_path, format=\"wav\", parameters=[\"-ac\", \"1\", \"-ar\", \"16000\"])\n",
        "\n",
        "            # Transcribe using Google\n",
        "            with sr.AudioFile(temp_path) as source:\n",
        "                audio_data = self.recognizer.record(source)\n",
        "                text = self.recognizer.recognize_google(audio_data, language='en-US')\n",
        "\n",
        "            # Clean up\n",
        "            os.remove(temp_path)\n",
        "\n",
        "            return text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Google transcription error: {str(e)}\")\n",
        "            return \"[TRANSCRIPTION_FAILED]\"\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path, csv_path, file_id, category):\n",
        "        \"\"\"Transcribe entire audio file using segmentation\"\"\"\n",
        "        print(f\"Transcribing {file_id}...\")\n",
        "\n",
        "        # Segment audio\n",
        "        segments = self.segment_audio_by_speaker(audio_path, csv_path)\n",
        "\n",
        "        if not segments:\n",
        "            return None\n",
        "\n",
        "        # Transcribe each segment\n",
        "        transcription_data = {\n",
        "            'file_id': file_id,\n",
        "            'category': category,\n",
        "            'audio_path': audio_path,\n",
        "            'csv_path': csv_path,\n",
        "            'segments': []\n",
        "        }\n",
        "\n",
        "        for segment in segments:\n",
        "            # Skip very short segments (less than 1 second)\n",
        "            if segment['duration_ms'] < 1000:\n",
        "                continue\n",
        "\n",
        "            # Transcribe the segment\n",
        "            transcript = self.transcribe_segment_whisper(segment['audio_segment'])\n",
        "\n",
        "            segment_data = {\n",
        "                'segment_id': segment['segment_id'],\n",
        "                'speaker': segment['speaker'],\n",
        "                'start_ms': segment['start_ms'],\n",
        "                'end_ms': segment['end_ms'],\n",
        "                'duration_ms': segment['duration_ms'],\n",
        "                'transcript_english': transcript,\n",
        "                'transcript_persian': '',  # To be filled later\n",
        "                'needs_translation': len(transcript) > 0 and transcript != \"[TRANSCRIPTION_FAILED]\"\n",
        "            }\n",
        "\n",
        "            transcription_data['segments'].append(segment_data)\n",
        "\n",
        "            print(f\"  Segment {segment['segment_id']} ({segment['speaker']}): {transcript[:50]}...\")\n",
        "\n",
        "        return transcription_data\n",
        "\n",
        "    def process_all_files(self, max_files=None):\n",
        "        \"\"\"Process all audio files and create transcriptions\"\"\"\n",
        "        pairs = self.find_audio_and_segmentation_pairs()\n",
        "\n",
        "        if max_files:\n",
        "            pairs = pairs[:max_files]\n",
        "\n",
        "        print(f\"Processing {len(pairs)} files...\")\n",
        "\n",
        "        all_transcriptions = []\n",
        "\n",
        "        for i, pair in enumerate(pairs):\n",
        "            print(f\"\\n--- Processing {i+1}/{len(pairs)} ---\")\n",
        "\n",
        "            transcription = self.transcribe_audio_file(\n",
        "                pair['audio_path'],\n",
        "                pair['csv_path'],\n",
        "                pair['file_id'],\n",
        "                pair['category']\n",
        "            )\n",
        "\n",
        "            if transcription:\n",
        "                all_transcriptions.append(transcription)\n",
        "\n",
        "                # Save individual result\n",
        "                output_file = os.path.join(self.output_path, f\"{pair['file_id']}_transcription.json\")\n",
        "                with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(transcription, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Save combined results\n",
        "        combined_file = os.path.join(self.output_path, 'all_transcriptions.json')\n",
        "        with open(combined_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_transcriptions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\nCompleted! Transcribed {len(all_transcriptions)} files\")\n",
        "        print(f\"Results saved to: {self.output_path}\")\n",
        "\n",
        "        return all_transcriptions\n",
        "\n",
        "    def create_translation_template(self, transcriptions):\n",
        "        \"\"\"Create template for Persian translation from transcriptions\"\"\"\n",
        "        print(\"Creating Persian translation template...\")\n",
        "\n",
        "        translation_entries = []\n",
        "\n",
        "        for transcription in transcriptions:\n",
        "            for segment in transcription['segments']:\n",
        "                if segment['needs_translation']:\n",
        "                    entry = {\n",
        "                        'file_id': transcription['file_id'],\n",
        "                        'category': transcription['category'],\n",
        "                        'segment_id': segment['segment_id'],\n",
        "                        'speaker': segment['speaker'],\n",
        "                        'start_ms': segment['start_ms'],\n",
        "                        'end_ms': segment['end_ms'],\n",
        "                        'original_english': segment['transcript_english'],\n",
        "                        'persian_translation': '',\n",
        "                        'translation_notes': '',\n",
        "                        'linguistic_features': self._analyze_linguistic_features(segment['transcript_english']),\n",
        "                        'translation_status': 'pending'\n",
        "                    }\n",
        "                    translation_entries.append(entry)\n",
        "\n",
        "        # Save translation template\n",
        "        template_path = os.path.join(self.output_path, 'persian_translation_template.json')\n",
        "        with open(template_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(translation_entries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Also create CSV version\n",
        "        csv_path = os.path.join(self.output_path, 'persian_translation_template.csv')\n",
        "        df = pd.DataFrame(translation_entries)\n",
        "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "        print(f\"Translation template created:\")\n",
        "        print(f\"  JSON: {template_path}\")\n",
        "        print(f\"  CSV: {csv_path}\")\n",
        "        print(f\"  Total entries to translate: {len(translation_entries)}\")\n",
        "\n",
        "        return template_path, csv_path\n",
        "\n",
        "    def _analyze_linguistic_features(self, text):\n",
        "        \"\"\"Analyze text for linguistic features important for Persian translation\"\"\"\n",
        "        if not text or text == \"[TRANSCRIPTION_FAILED]\":\n",
        "            return {}\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Detect pause words/fillers\n",
        "        pause_words = ['uhm', 'uh', 'um', 'er', 'ah', 'hmm', 'mm', 'well']\n",
        "        found_pauses = [pw for pw in pause_words if pw in text_lower]\n",
        "\n",
        "        # Detect repetitions\n",
        "        words = text.split()\n",
        "        repetitions = []\n",
        "        for i in range(1, len(words)):\n",
        "            if words[i].lower() == words[i-1].lower():\n",
        "                repetitions.append(words[i])\n",
        "\n",
        "        return {\n",
        "            'word_count': len(words),\n",
        "            'has_pause_words': len(found_pauses) > 0,\n",
        "            'pause_words': found_pauses,\n",
        "            'has_repetitions': len(repetitions) > 0,\n",
        "            'repeated_words': repetitions,\n",
        "            'sentence_fragments': text.count('.') < len([s for s in text.split('.') if s.strip()]) - 1\n",
        "        }\n",
        "\n",
        "    def create_persian_translation_guidelines(self):\n",
        "        \"\"\"Create detailed guidelines for Persian translation\"\"\"\n",
        "        guidelines = \"\"\"\n",
        "پرشین ترانسلیشن گایدلاینز / PERSIAN TRANSLATION GUIDELINES\n",
        "================================================================\n",
        "\n",
        "مهم‌ترین اصول / CRITICAL PRINCIPLES:\n",
        "\n",
        "1. حفظ ویژگی‌های زبانی / PRESERVE LINGUISTIC FEATURES:\n",
        "   - تمام کلمات مکث را حفظ کنید / Keep ALL pause words:\n",
        "     \"uhm\" → \"اوم\", \"uh\" → \"اه\", \"um\" → \"ام\", \"er\" → \"ار\"\n",
        "   - تکرارها را دقیقاً حفظ کنید / Maintain repetitions exactly\n",
        "   - اشتباهات و تردیدها را نگه دارید / Keep errors and hesitations\n",
        "\n",
        "2. ترجمه طبیعی به فارسی / NATURAL PERSIAN TRANSLATION:\n",
        "   - معنی را حفظ کنید ولی ساختار فارسی را رعایت کنید\n",
        "   - از کلمات و عبارات طبیعی فارسی استفاده کنید\n",
        "   - فرهنگ ایرانی را در نظر بگیرید\n",
        "\n",
        "3. مثال‌ها / EXAMPLES:\n",
        "   English: \"The woman is uhm she is washing dishes\"\n",
        "   Persian: \"زن اوم او ظرف می‌شوید\"\n",
        "\n",
        "   English: \"I I don't know\"\n",
        "   Persian: \"من من نمی‌دانم\"\n",
        "\n",
        "4. کنترل کیفیت / QUALITY CONTROL:\n",
        "   - هر ترجمه را دوبار بررسی کنید\n",
        "   - کلمات مکث را در جای درست قرار دهید\n",
        "   - تکرارها را حفظ کنید\n",
        "   - معنی کلی را حفظ کنید\n",
        "\n",
        "معادل‌های کلمات مکث / PAUSE WORD EQUIVALENTS:\n",
        "- \"uhm\" → \"اوم\"\n",
        "- \"uh\" → \"اه\"\n",
        "- \"um\" → \"ام\"\n",
        "- \"er\" → \"ار\"\n",
        "- \"ah\" → \"آه\"\n",
        "- \"hmm\" → \"هوم\"\n",
        "- \"well\" → \"خوب\"\n",
        "\"\"\"\n",
        "\n",
        "        guidelines_path = os.path.join(self.output_path, 'persian_translation_guidelines.txt')\n",
        "        with open(guidelines_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(guidelines)\n",
        "\n",
        "        print(f\"Translation guidelines saved: {guidelines_path}\")\n",
        "        return guidelines_path\n",
        "\n",
        "# Usage Functions\n",
        "def setup_transcription_pipeline(extracted_base_path='/content/drive/MyDrive/Voice/Extracted_dataset'):\n",
        "    \"\"\"Setup and run the complete transcription pipeline\"\"\"\n",
        "\n",
        "    output_path = '/content/drive/MyDrive/Voice/Transcriptions_and_Translations'\n",
        "\n",
        "    print(\"SETTING UP AUDIO-TO-PERSIAN TRANSLATION PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize pipeline\n",
        "    pipeline = ADReSSoTranscriptionPipeline(extracted_base_path, output_path)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def run_sample_transcription(pipeline, num_files=3):\n",
        "    \"\"\"Run transcription on a small sample first\"\"\"\n",
        "    print(f\"Running sample transcription on {num_files} files...\")\n",
        "\n",
        "    transcriptions = pipeline.process_all_files(max_files=num_files)\n",
        "\n",
        "    if transcriptions:\n",
        "        # Create translation template\n",
        "        template_path, csv_path = pipeline.create_translation_template(transcriptions)\n",
        "\n",
        "        # Create guidelines\n",
        "        guidelines_path = pipeline.create_persian_translation_guidelines()\n",
        "\n",
        "        print(f\"\\nSample transcription completed!\")\n",
        "        print(f\"Review the results and then run full transcription\")\n",
        "\n",
        "        return template_path, csv_path, guidelines_path\n",
        "    else:\n",
        "        print(\"No transcriptions were successful. Check audio files and dependencies.\")\n",
        "        return None\n",
        "\n",
        "def run_full_transcription(pipeline):\n",
        "    \"\"\"Run transcription on all files\"\"\"\n",
        "    print(\"Running full transcription on all audio files...\")\n",
        "\n",
        "    transcriptions = pipeline.process_all_files()\n",
        "\n",
        "    if transcriptions:\n",
        "        # Create translation template\n",
        "        template_path, csv_path = pipeline.create_translation_template(transcriptions)\n",
        "\n",
        "        # Create guidelines\n",
        "        guidelines_path = pipeline.create_persian_translation_guidelines()\n",
        "\n",
        "        print(f\"\\nFull transcription completed!\")\n",
        "        print(f\"Total files transcribed: {len(transcriptions)}\")\n",
        "        print(f\"Ready for Persian translation!\")\n",
        "\n",
        "        return template_path, csv_path, guidelines_path\n",
        "    else:\n",
        "        print(\"No transcriptions were successful. Check audio files and dependencies.\")\n",
        "        return None\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ADReSSo21 Audio-to-Persian Translation Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Step 1: Install required packages (run in separate cells):\")\n",
        "    print(\"!pip install librosa SpeechRecognition pydub openai-whisper\")\n",
        "    print(\"\\nStep 2: Setup pipeline:\")\n",
        "    print(\"pipeline = setup_transcription_pipeline()\")\n",
        "    print(\"\\nStep 3: Test with sample:\")\n",
        "    print(\"run_sample_transcription(pipeline, num_files=3)\")\n",
        "    print(\"\\nStep 4: Run full transcription:\")\n",
        "    print(\"run_full_transcription(pipeline)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eY72-QYs3Zi",
        "outputId": "bbafe8bf-c99a-434a-8211-1347853246b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADReSSo21 Audio-to-Persian Translation Pipeline\n",
            "============================================================\n",
            "Step 1: Install required packages (run in separate cells):\n",
            "!pip install librosa SpeechRecognition pydub openai-whisper\n",
            "\n",
            "Step 2: Setup pipeline:\n",
            "pipeline = setup_transcription_pipeline()\n",
            "\n",
            "Step 3: Test with sample:\n",
            "run_sample_transcription(pipeline, num_files=3)\n",
            "\n",
            "Step 4: Run full transcription:\n",
            "run_full_transcription(pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whG_mVYms9Wg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}