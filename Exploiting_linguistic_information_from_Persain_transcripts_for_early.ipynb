{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/P0l8q0wvJixWve9MYm9d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Exploiting_linguistic_information_from_Persain_transcripts_for_early.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H4z7R6CX61x",
        "outputId": "4755757f-8df6-4cab-b164-58517bc6d820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:❌ No potential dataset files found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Step 1 encountered errors. Please check the logs above.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 1: Data Acquisition and Preparation for Alzheimer's Disease Detection\n",
        "Pipeline using ADReSSo21 Dataset\n",
        "\n",
        "This script handles the extraction, organization, and initial preparation of\n",
        "transcripts from the ADReSSo21 dataset for early AD detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import csv\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ADReSSo21DataProcessor:\n",
        "    \"\"\"\n",
        "    Class to handle ADReSSo21 dataset extraction and preparation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str = \"/content/drive/MyDrive/Voice/ADReSSo21\"):\n",
        "        \"\"\"\n",
        "        Initialize the data processor\n",
        "\n",
        "        Args:\n",
        "            base_path (str): Base path where ADReSSo21 data will be stored\n",
        "        \"\"\"\n",
        "        self.base_path = Path(base_path)\n",
        "        self.extracted_path = self.base_path / \"extracted\"\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
        "        self.extracted_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Dataset file names\n",
        "        self.dataset_files = {\n",
        "            'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "            'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "            'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "        }\n",
        "\n",
        "        # Directory structure mapping\n",
        "        self.directory_structure = {\n",
        "            'progression_train': {\n",
        "                'segmentation': ['no_decline', 'decline'],\n",
        "                'audio': ['no_decline', 'decline']\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'segmentation': [''],  # test-dist has no subdirectories\n",
        "                'audio': ['']\n",
        "            },\n",
        "            'diagnosis_train': {\n",
        "                'segmentation': ['cn', 'ad'],\n",
        "                'audio': ['cn', 'ad']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def mount_google_drive(self):\n",
        "        \"\"\"\n",
        "        Mount Google Drive in Colab environment\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "\n",
        "            # Check if already mounted\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Google Drive already mounted\")\n",
        "                return True\n",
        "\n",
        "            drive.mount('/content/drive')\n",
        "            logger.info(\"Google Drive mounted successfully\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            logger.warning(\"Not running in Google Colab environment\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error mounting Google Drive: {e}\")\n",
        "            # Try to continue anyway if drive is accessible\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Drive appears to be accessible despite error\")\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "    def find_dataset_files(self):\n",
        "        \"\"\"\n",
        "        Search for ADReSSo21 dataset files in various formats and locations\n",
        "        \"\"\"\n",
        "        logger.info(f\"Searching for dataset files in: {self.base_path}\")\n",
        "\n",
        "        # Check if base directory exists\n",
        "        if not self.base_path.exists():\n",
        "            logger.error(f\"Base directory does not exist: {self.base_path}\")\n",
        "            logger.info(\"Checking parent directories...\")\n",
        "\n",
        "            # Check common alternative paths\n",
        "            alternative_paths = [\n",
        "                Path(\"/content/drive/MyDrive/Voice/\"),\n",
        "                Path(\"/content/drive/MyDrive/\"),\n",
        "                Path(\"/content/drive/\"),\n",
        "                Path(\"/content/\")\n",
        "            ]\n",
        "\n",
        "            for alt_path in alternative_paths:\n",
        "                if alt_path.exists():\n",
        "                    logger.info(f\"Found directory: {alt_path}\")\n",
        "                    # List contents\n",
        "                    items = list(alt_path.glob(\"*\"))\n",
        "                    for item in items[:10]:  # Show first 10 items\n",
        "                        logger.info(f\"  {item.name}\")\n",
        "                    if len(items) > 10:\n",
        "                        logger.info(f\"  ... and {len(items) - 10} more items\")\n",
        "\n",
        "            return None, []\n",
        "\n",
        "        # List all files in the base directory\n",
        "        all_files = list(self.base_path.glob(\"*\"))\n",
        "        logger.info(f\"Found {len(all_files)} items in base directory:\")\n",
        "\n",
        "        dataset_files = []\n",
        "\n",
        "        for item in all_files:\n",
        "            if item.is_file():\n",
        "                size_mb = item.stat().st_size / (1024*1024)\n",
        "                logger.info(f\"  FILE: {item.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "                # Check for various dataset file formats\n",
        "                if any(keyword in item.name.lower() for keyword in ['adresso', 'alzheimer', 'dementia']):\n",
        "                    dataset_files.append(item)\n",
        "            else:\n",
        "                logger.info(f\"  DIR:  {item.name}/\")\n",
        "\n",
        "        # Look for specific file types\n",
        "        file_types = {\n",
        "            '.tgz': list(self.base_path.glob(\"*.tgz\")),\n",
        "            '.tar.gz': list(self.base_path.glob(\"*.tar.gz\")),\n",
        "            '.zip': list(self.base_path.glob(\"*.zip\")),\n",
        "            '.rar': list(self.base_path.glob(\"*.rar\")),\n",
        "            '.7z': list(self.base_path.glob(\"*.7z\"))\n",
        "        }\n",
        "\n",
        "        found_archives = []\n",
        "        for file_type, files in file_types.items():\n",
        "            if files:\n",
        "                logger.info(f\"Found {len(files)} {file_type} files:\")\n",
        "                for file in files:\n",
        "                    logger.info(f\"  {file.name}\")\n",
        "                    found_archives.extend(files)\n",
        "\n",
        "        if dataset_files:\n",
        "            logger.info(f\"Found {len(dataset_files)} potential dataset files:\")\n",
        "            for file in dataset_files:\n",
        "                logger.info(f\"  {file.name}\")\n",
        "\n",
        "        return found_archives, dataset_files\n",
        "\n",
        "    def interactive_file_selection(self, found_archives, dataset_files):\n",
        "        \"\"\"\n",
        "        Help user identify and select the correct dataset files\n",
        "        \"\"\"\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"DATASET FILE DETECTION RESULTS\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        if not found_archives and not dataset_files:\n",
        "            logger.error(\"❌ No archive files or potential dataset files found!\")\n",
        "            logger.info(\"\\n📋 TROUBLESHOOTING STEPS:\")\n",
        "            logger.info(\"1. Verify you've uploaded the ADReSSo21 dataset files\")\n",
        "            logger.info(\"2. Check if files are in a different directory\")\n",
        "            logger.info(\"3. Ensure files are properly uploaded to Google Drive\")\n",
        "            logger.info(\"4. Check if files have different names or extensions\")\n",
        "            return None\n",
        "\n",
        "        logger.info(\"🔍 FOUND FILES ANALYSIS:\")\n",
        "\n",
        "        # Analyze found files\n",
        "        likely_candidates = []\n",
        "\n",
        "        for file in found_archives + dataset_files:\n",
        "            score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # Check file name for ADReSSo21 indicators\n",
        "            name_lower = file.name.lower()\n",
        "            if 'adresso' in name_lower:\n",
        "                score += 5\n",
        "                reasons.append(\"Contains 'ADReSSo'\")\n",
        "            if 'progression' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'progression'\")\n",
        "            if 'diagnosis' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'diagnosis'\")\n",
        "            if 'train' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'train'\")\n",
        "            if 'test' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'test'\")\n",
        "\n",
        "            # Check file size (ADReSSo21 files should be reasonably large)\n",
        "            size_mb = file.stat().st_size / (1024*1024)\n",
        "            if size_mb > 10:  # Larger than 10MB\n",
        "                score += 2\n",
        "                reasons.append(f\"Good size ({size_mb:.1f} MB)\")\n",
        "            elif size_mb > 1:\n",
        "                score += 1\n",
        "                reasons.append(f\"Moderate size ({size_mb:.1f} MB)\")\n",
        "\n",
        "            if score > 0:\n",
        "                likely_candidates.append((file, score, reasons))\n",
        "\n",
        "        # Sort by score\n",
        "        likely_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if likely_candidates:\n",
        "            logger.info(f\"🎯 TOP CANDIDATES (sorted by likelihood):\")\n",
        "            for i, (file, score, reasons) in enumerate(likely_candidates[:5]):\n",
        "                logger.info(f\"  {i+1}. {file.name} (Score: {score})\")\n",
        "                logger.info(f\"     Reasons: {', '.join(reasons)}\")\n",
        "                logger.info(f\"     Path: {file}\")\n",
        "\n",
        "        # Return the most likely candidate for automatic processing\n",
        "        if likely_candidates and likely_candidates[0][1] >= 5:\n",
        "            return likely_candidates[0][0]\n",
        "\n",
        "        return None\n",
        "    def extract_any_archive(self, file_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Extract archive files in various formats\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to the archive file\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_extension = file_path.suffix.lower()\n",
        "            extract_dir = self.extracted_path / file_path.stem\n",
        "            extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            logger.info(f\"Attempting to extract: {file_path.name}\")\n",
        "\n",
        "            if file_extension in ['.tgz', '.gz'] or file_path.name.endswith('.tar.gz'):\n",
        "                # Handle .tgz and .tar.gz files\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            elif file_extension == '.zip':\n",
        "                # Handle .zip files\n",
        "                import zipfile\n",
        "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_dir)\n",
        "\n",
        "            elif file_extension == '.tar':\n",
        "                # Handle .tar files\n",
        "                with tarfile.open(file_path, 'r') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            else:\n",
        "                logger.error(f\"Unsupported archive format: {file_extension}\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully extracted {file_path.name} to {extract_dir}\")\n",
        "\n",
        "            # List extracted contents\n",
        "            extracted_items = list(extract_dir.rglob(\"*\"))\n",
        "            logger.info(f\"Extracted {len(extracted_items)} items\")\n",
        "\n",
        "            # Show directory structure\n",
        "            dirs = [item for item in extracted_items if item.is_dir()]\n",
        "            files = [item for item in extracted_items if item.is_file()]\n",
        "\n",
        "            logger.info(f\"  Directories: {len(dirs)}\")\n",
        "            logger.info(f\"  Files: {len(files)}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting {file_path}: {e}\")\n",
        "            return False\n",
        "    def extract_tgz_files(self) -> bool:\n",
        "        \"\"\"\n",
        "        Extract all archive files to the extraction directory\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Find potential dataset files\n",
        "            found_archives, dataset_files = self.find_dataset_files()\n",
        "\n",
        "            # Try to identify the best candidates\n",
        "            best_candidate = self.interactive_file_selection(found_archives, dataset_files)\n",
        "\n",
        "            if best_candidate:\n",
        "                logger.info(f\"🎯 Attempting to extract most likely candidate: {best_candidate.name}\")\n",
        "                if self.extract_any_archive(best_candidate):\n",
        "                    return True\n",
        "\n",
        "            # If no clear candidate, try all archive files\n",
        "            if found_archives:\n",
        "                logger.info(\"🔄 Trying to extract all found archive files...\")\n",
        "                extracted_any = False\n",
        "\n",
        "                for archive in found_archives:\n",
        "                    if self.extract_any_archive(archive):\n",
        "                        extracted_any = True\n",
        "\n",
        "                return extracted_any\n",
        "\n",
        "            # Fallback: try the original method for exact file names\n",
        "            logger.info(\"🔄 Trying original extraction method...\")\n",
        "            extracted_any = False\n",
        "\n",
        "            for dataset_name, filename in self.dataset_files.items():\n",
        "                file_path = self.base_path / filename\n",
        "\n",
        "                if not file_path.exists():\n",
        "                    logger.warning(f\"Expected file not found: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Extracting {filename}...\")\n",
        "\n",
        "                # Extract to specific subdirectory\n",
        "                extract_dir = self.extracted_path / dataset_name\n",
        "                extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "                logger.info(f\"Successfully extracted {filename}\")\n",
        "                extracted_any = True\n",
        "\n",
        "            return extracted_any\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {e}\")\n",
        "            return False\n",
        "\n",
        "    def verify_directory_structure(self) -> Dict[str, bool]:\n",
        "        \"\"\"\n",
        "        Verify that the extracted directories match expected structure\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, bool]: Status of each dataset extraction\n",
        "        \"\"\"\n",
        "        verification_results = {}\n",
        "\n",
        "        for dataset_name, structure in self.directory_structure.items():\n",
        "            dataset_path = self.extracted_path / dataset_name / \"ADReSSo21\"\n",
        "\n",
        "            # Check if main dataset directory exists\n",
        "            if not dataset_path.exists():\n",
        "                verification_results[dataset_name] = False\n",
        "                logger.error(f\"Dataset directory not found: {dataset_path}\")\n",
        "                continue\n",
        "\n",
        "            # Verify subdirectories\n",
        "            all_dirs_exist = True\n",
        "\n",
        "            for data_type, subdirs in structure.items():\n",
        "                if dataset_name == 'progression_test':\n",
        "                    # Special case for test data\n",
        "                    seg_path = dataset_path / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "                    audio_path = dataset_path / \"progression\" / \"test-dist\" / \"audio\"\n",
        "\n",
        "                    if not (seg_path.exists() and audio_path.exists()):\n",
        "                        all_dirs_exist = False\n",
        "                        logger.error(f\"Test directories missing in {dataset_name}\")\n",
        "                else:\n",
        "                    # Regular structure for train data\n",
        "                    base_type_path = dataset_path / (\"progression\" if \"progression\" in dataset_name else \"diagnosis\")\n",
        "\n",
        "                    for subdir in subdirs:\n",
        "                        if subdir:  # Skip empty strings\n",
        "                            seg_path = base_type_path / \"train\" / \"segmentation\" / subdir\n",
        "                            audio_path = base_type_path / \"train\" / \"audio\" / subdir\n",
        "\n",
        "                            if not (seg_path.exists() and audio_path.exists()):\n",
        "                                all_dirs_exist = False\n",
        "                                logger.error(f\"Missing directories for {dataset_name}/{subdir}\")\n",
        "\n",
        "            verification_results[dataset_name] = all_dirs_exist\n",
        "\n",
        "            if all_dirs_exist:\n",
        "                logger.info(f\"✓ Directory structure verified for {dataset_name}\")\n",
        "            else:\n",
        "                logger.warning(f\"✗ Directory structure issues found for {dataset_name}\")\n",
        "\n",
        "        return verification_results\n",
        "\n",
        "    def extract_transcripts_from_csv(self, csv_file_path: Path) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract transcript data from a single CSV file following CHAT protocol\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (Path): Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of transcript segments with metadata\n",
        "        \"\"\"\n",
        "        transcripts = []\n",
        "\n",
        "        try:\n",
        "            # Try different encodings as CSV files might have various encodings\n",
        "            encodings = ['utf-8', 'latin-1', 'cp1252']\n",
        "            df = None\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_file_path, encoding=encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                logger.error(f\"Could not read CSV file with any encoding: {csv_file_path}\")\n",
        "                return transcripts\n",
        "\n",
        "            # Log column names for inspection\n",
        "            logger.info(f\"CSV columns in {csv_file_path.name}: {list(df.columns)}\")\n",
        "\n",
        "            # Extract relevant columns (adjust based on actual CSV structure)\n",
        "            # Common CHAT protocol columns might include: speaker, utterance, time, etc.\n",
        "            for index, row in df.iterrows():\n",
        "                transcript_entry = {\n",
        "                    'file_id': csv_file_path.stem,\n",
        "                    'row_index': index,\n",
        "                    'data': dict(row)  # Store all columns for now\n",
        "                }\n",
        "\n",
        "                # Look for text/utterance columns (common names in CHAT protocol)\n",
        "                text_columns = ['utterance', 'text', 'transcript', 'speech', 'content']\n",
        "                for col in text_columns:\n",
        "                    if col in df.columns and pd.notna(row[col]):\n",
        "                        transcript_entry['transcript'] = str(row[col])\n",
        "                        break\n",
        "\n",
        "                transcripts.append(transcript_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing CSV file {csv_file_path}: {e}\")\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def collect_all_transcripts(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Collect all transcripts from segmentation CSV files\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Dict]]: Organized transcripts by category\n",
        "        \"\"\"\n",
        "        all_transcripts = {\n",
        "            'progression_train_no_decline': [],\n",
        "            'progression_train_decline': [],\n",
        "            'progression_test': [],\n",
        "            'diagnosis_train_cn': [],\n",
        "            'diagnosis_train_ad': []\n",
        "        }\n",
        "\n",
        "        # Process progression training data\n",
        "        prog_train_path = self.extracted_path / \"progression_train\" / \"ADReSSo21\" / \"progression\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['no_decline', 'decline']:\n",
        "            csv_dir = prog_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'progression_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process progression test data\n",
        "        prog_test_path = self.extracted_path / \"progression_test\" / \"ADReSSo21\" / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "        if prog_test_path.exists():\n",
        "            for csv_file in prog_test_path.glob('*.csv'):\n",
        "                transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                all_transcripts['progression_test'].extend(transcripts)\n",
        "                logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process diagnosis training data\n",
        "        diag_train_path = self.extracted_path / \"diagnosis_train\" / \"ADReSSo21\" / \"diagnosis\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['cn', 'ad']:\n",
        "            csv_dir = diag_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'diagnosis_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        return all_transcripts\n",
        "\n",
        "    def save_transcripts_summary(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Save a summary of extracted transcripts for review\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the saved summary file\n",
        "        \"\"\"\n",
        "        summary_file = self.base_path / \"transcripts_summary.txt\"\n",
        "\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"ADReSSo21 Dataset Transcripts Summary\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            total_transcripts = 0\n",
        "            for category, transcript_list in transcripts.items():\n",
        "                count = len(transcript_list)\n",
        "                total_transcripts += count\n",
        "                f.write(f\"{category}: {count} transcript entries\\n\")\n",
        "\n",
        "                # Show sample transcript if available\n",
        "                if transcript_list and 'transcript' in transcript_list[0]:\n",
        "                    sample = transcript_list[0]['transcript'][:100] + \"...\" if len(transcript_list[0]['transcript']) > 100 else transcript_list[0]['transcript']\n",
        "                    f.write(f\"  Sample: {sample}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(f\"Total transcript entries: {total_transcripts}\\n\")\n",
        "            f.write(\"\\nNote: These English transcripts need to be translated to Persian for the study.\\n\")\n",
        "            f.write(\"Translation should be done manually by native Persian speakers as per the methodology.\\n\")\n",
        "\n",
        "        logger.info(f\"Transcripts summary saved to: {summary_file}\")\n",
        "        return summary_file\n",
        "\n",
        "    def prepare_for_translation(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Prepare transcript files for manual Persian translation\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the translation directory\n",
        "        \"\"\"\n",
        "        translation_dir = self.base_path / \"for_translation\"\n",
        "        translation_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        for category, transcript_list in transcripts.items():\n",
        "            if not transcript_list:\n",
        "                continue\n",
        "\n",
        "            # Create CSV file for translation\n",
        "            csv_file = translation_dir / f\"{category}_for_translation.csv\"\n",
        "\n",
        "            with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['ID', 'Original_English', 'Persian_Translation', 'Notes'])\n",
        "\n",
        "                for i, entry in enumerate(transcript_list):\n",
        "                    if 'transcript' in entry:\n",
        "                        transcript_id = f\"{category}_{i+1}\"\n",
        "                        english_text = entry['transcript']\n",
        "                        writer.writerow([transcript_id, english_text, '', ''])\n",
        "\n",
        "            logger.info(f\"Created translation file: {csv_file}\")\n",
        "\n",
        "        # Create translation instructions\n",
        "        instructions_file = translation_dir / \"TRANSLATION_INSTRUCTIONS.txt\"\n",
        "        with open(instructions_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"PERSIAN TRANSLATION INSTRUCTIONS\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "            f.write(\"IMPORTANT REQUIREMENTS:\\n\")\n",
        "            f.write(\"1. Translation must be done by native Persian speakers\\n\")\n",
        "            f.write(\"2. Translator should have at least 13 years of formal Persian education\\n\")\n",
        "            f.write(\"3. Translation should be verified by an independent linguistic expert\\n\")\n",
        "            f.write(\"4. PRESERVE ALL linguistic features:\\n\")\n",
        "            f.write(\"   - Pause words (uhm, uhh, etc.) - translate equivalent Persian pause words\\n\")\n",
        "            f.write(\"   - Repetitions - keep all repetitions\\n\")\n",
        "            f.write(\"   - Linguistic errors - preserve grammatical/syntactic errors\\n\")\n",
        "            f.write(\"   - Syntactic errors - maintain sentence structure issues\\n\")\n",
        "            f.write(\"5. EXCLUDE annotations like [clears throat], [laughs], etc.\\n\")\n",
        "            f.write(\"6. Do NOT use machine translation - manual translation only\\n\")\n",
        "            f.write(\"7. Capture cultural and linguistic nuances specific to Persian\\n\\n\")\n",
        "            f.write(\"Fill in the 'Persian_Translation' column in each CSV file.\\n\")\n",
        "            f.write(\"Use 'Notes' column for any translation decisions or concerns.\\n\")\n",
        "\n",
        "        logger.info(f\"Translation instructions saved to: {instructions_file}\")\n",
        "        return translation_dir\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute Step 1 of the pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "\n",
        "    # Initialize the data processor with the correct path based on your error\n",
        "    processor = ADReSSo21DataProcessor()\n",
        "\n",
        "    # Step 1.1: Mount Google Drive (if in Colab)\n",
        "    logger.info(\"Step 1.1: Mounting Google Drive...\")\n",
        "    drive_mounted = processor.mount_google_drive()\n",
        "\n",
        "    # Step 1.2: Search for dataset files\n",
        "    logger.info(\"Step 1.2: Searching for dataset files...\")\n",
        "    found_archives, dataset_files = processor.find_dataset_files()\n",
        "\n",
        "    if not found_archives and not dataset_files:\n",
        "        logger.error(\"❌ No potential dataset files found!\")\n",
        "        logger.info(\"\\n📋 PLEASE CHECK:\")\n",
        "        logger.info(\"1. Are the ADReSSo21 files uploaded to Google Drive?\")\n",
        "        logger.info(\"2. Are they in the correct directory?\")\n",
        "        logger.info(f\"   Expected location: {processor.base_path}\")\n",
        "        logger.info(\"3. Do they have the expected names:\")\n",
        "        logger.info(\"   - ADReSSo21-progression-train.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-progression-test.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-diagnosis-train.tgz\")\n",
        "        logger.info(\"4. Or are they in a different format (.zip, .rar, etc.)?\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.3: Extract dataset files\n",
        "    logger.info(\"Step 1.3: Extracting dataset files...\")\n",
        "    extraction_success = processor.extract_tgz_files()\n",
        "\n",
        "    if not extraction_success:\n",
        "        logger.error(\"❌ Failed to extract any dataset files.\")\n",
        "        logger.info(\"\\n🔧 POSSIBLE SOLUTIONS:\")\n",
        "        logger.info(\"1. Check if files are corrupted - try re-downloading\")\n",
        "        logger.info(\"2. Try extracting files manually first\")\n",
        "        logger.info(\"3. Ensure files are not password protected\")\n",
        "        logger.info(\"4. Check if files are in an unsupported format\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.4: Verify directory structure\n",
        "    logger.info(\"Step 1.4: Verifying directory structure...\")\n",
        "    verification_results = processor.verify_directory_structure()\n",
        "\n",
        "    successful_extractions = [k for k, v in verification_results.items() if v]\n",
        "\n",
        "    if not successful_extractions:\n",
        "        logger.error(\"No datasets were successfully extracted and verified.\")\n",
        "        return False\n",
        "\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "\n",
        "    # Step 1.5: Extract transcripts from CSV files\n",
        "    logger.info(\"Step 1.5: Extracting transcripts from segmentation CSV files...\")\n",
        "    all_transcripts = processor.collect_all_transcripts()\n",
        "\n",
        "    # Check if we actually got any transcripts\n",
        "    total_transcripts = sum(len(transcripts) for transcripts in all_transcripts.values())\n",
        "\n",
        "    if total_transcripts == 0:\n",
        "        logger.error(\"No transcripts were extracted from CSV files!\")\n",
        "        logger.info(\"This might indicate:\")\n",
        "        logger.info(\"  - CSV files are in a different format than expected\")\n",
        "        logger.info(\"  - Directory structure is different\")\n",
        "        logger.info(\"  - Files are corrupted\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.6: Save summary and prepare for translation\n",
        "    logger.info(\"Step 1.6: Saving transcripts summary...\")\n",
        "    processor.save_transcripts_summary(all_transcripts)\n",
        "\n",
        "    logger.info(\"Step 1.7: Preparing files for Persian translation...\")\n",
        "    translation_dir = processor.prepare_for_translation(all_transcripts)\n",
        "\n",
        "    # Final summary\n",
        "    logger.info(f\"\\n{'='*50}\")\n",
        "    logger.info(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "    logger.info(f\"Total transcript entries extracted: {total_transcripts}\")\n",
        "    logger.info(f\"Translation files prepared in: {translation_dir}\")\n",
        "    logger.info(\"NEXT STEPS:\")\n",
        "    logger.info(\"1. Have native Persian speakers translate the CSV files\")\n",
        "    logger.info(\"2. Verify translations with linguistic expert\")\n",
        "    logger.info(\"3. Return translated files for Step 2 (Data Preprocessing)\")\n",
        "    logger.info(f\"{'='*50}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if success:\n",
        "        print(\"\\n✅ Step 1 completed successfully!\")\n",
        "        print(\"📁 Check the translation directory for files to be translated to Persian\")\n",
        "        print(\"🔄 Once translation is complete, you can proceed to Step 2\")\n",
        "    else:\n",
        "        print(\"\\n❌ Step 1 encountered errors. Please check the logs above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1fFwtsjbfA_",
        "outputId": "74e92c53-6e31-4b99-ccb6-d0b074144ffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}