{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZ7WobDRMJAuY0G9ZD7Ci4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Exploiting_linguistic_information_from_Persain_transcripts_for_early.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H4z7R6CX61x",
        "outputId": "4755757f-8df6-4cab-b164-58517bc6d820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:❌ No potential dataset files found!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "❌ Step 1 encountered errors. Please check the logs above.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 1: Data Acquisition and Preparation for Alzheimer's Disease Detection\n",
        "Pipeline using ADReSSo21 Dataset\n",
        "\n",
        "This script handles the extraction, organization, and initial preparation of\n",
        "transcripts from the ADReSSo21 dataset for early AD detection.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import csv\n",
        "from typing import Dict, List, Tuple\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ADReSSo21DataProcessor:\n",
        "    \"\"\"\n",
        "    Class to handle ADReSSo21 dataset extraction and preparation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str = \"/content/drive/MyDrive/Voice/ADReSSo21\"):\n",
        "        \"\"\"\n",
        "        Initialize the data processor\n",
        "\n",
        "        Args:\n",
        "            base_path (str): Base path where ADReSSo21 data will be stored\n",
        "        \"\"\"\n",
        "        self.base_path = Path(base_path)\n",
        "        self.extracted_path = self.base_path / \"extracted\"\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
        "        self.extracted_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Dataset file names\n",
        "        self.dataset_files = {\n",
        "            'progression_train': 'ADReSSo21-progression-train.tgz',\n",
        "            'progression_test': 'ADReSSo21-progression-test.tgz',\n",
        "            'diagnosis_train': 'ADReSSo21-diagnosis-train.tgz'\n",
        "        }\n",
        "\n",
        "        # Directory structure mapping\n",
        "        self.directory_structure = {\n",
        "            'progression_train': {\n",
        "                'segmentation': ['no_decline', 'decline'],\n",
        "                'audio': ['no_decline', 'decline']\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'segmentation': [''],  # test-dist has no subdirectories\n",
        "                'audio': ['']\n",
        "            },\n",
        "            'diagnosis_train': {\n",
        "                'segmentation': ['cn', 'ad'],\n",
        "                'audio': ['cn', 'ad']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def mount_google_drive(self):\n",
        "        \"\"\"\n",
        "        Mount Google Drive in Colab environment\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "\n",
        "            # Check if already mounted\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Google Drive already mounted\")\n",
        "                return True\n",
        "\n",
        "            drive.mount('/content/drive')\n",
        "            logger.info(\"Google Drive mounted successfully\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            logger.warning(\"Not running in Google Colab environment\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error mounting Google Drive: {e}\")\n",
        "            # Try to continue anyway if drive is accessible\n",
        "            if os.path.exists('/content/drive'):\n",
        "                logger.info(\"Drive appears to be accessible despite error\")\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "    def find_dataset_files(self):\n",
        "        \"\"\"\n",
        "        Search for ADReSSo21 dataset files in various formats and locations\n",
        "        \"\"\"\n",
        "        logger.info(f\"Searching for dataset files in: {self.base_path}\")\n",
        "\n",
        "        # Check if base directory exists\n",
        "        if not self.base_path.exists():\n",
        "            logger.error(f\"Base directory does not exist: {self.base_path}\")\n",
        "            logger.info(\"Checking parent directories...\")\n",
        "\n",
        "            # Check common alternative paths\n",
        "            alternative_paths = [\n",
        "                Path(\"/content/drive/MyDrive/Voice/\"),\n",
        "                Path(\"/content/drive/MyDrive/\"),\n",
        "                Path(\"/content/drive/\"),\n",
        "                Path(\"/content/\")\n",
        "            ]\n",
        "\n",
        "            for alt_path in alternative_paths:\n",
        "                if alt_path.exists():\n",
        "                    logger.info(f\"Found directory: {alt_path}\")\n",
        "                    # List contents\n",
        "                    items = list(alt_path.glob(\"*\"))\n",
        "                    for item in items[:10]:  # Show first 10 items\n",
        "                        logger.info(f\"  {item.name}\")\n",
        "                    if len(items) > 10:\n",
        "                        logger.info(f\"  ... and {len(items) - 10} more items\")\n",
        "\n",
        "            return None, []\n",
        "\n",
        "        # List all files in the base directory\n",
        "        all_files = list(self.base_path.glob(\"*\"))\n",
        "        logger.info(f\"Found {len(all_files)} items in base directory:\")\n",
        "\n",
        "        dataset_files = []\n",
        "\n",
        "        for item in all_files:\n",
        "            if item.is_file():\n",
        "                size_mb = item.stat().st_size / (1024*1024)\n",
        "                logger.info(f\"  FILE: {item.name} ({size_mb:.1f} MB)\")\n",
        "\n",
        "                # Check for various dataset file formats\n",
        "                if any(keyword in item.name.lower() for keyword in ['adresso', 'alzheimer', 'dementia']):\n",
        "                    dataset_files.append(item)\n",
        "            else:\n",
        "                logger.info(f\"  DIR:  {item.name}/\")\n",
        "\n",
        "        # Look for specific file types\n",
        "        file_types = {\n",
        "            '.tgz': list(self.base_path.glob(\"*.tgz\")),\n",
        "            '.tar.gz': list(self.base_path.glob(\"*.tar.gz\")),\n",
        "            '.zip': list(self.base_path.glob(\"*.zip\")),\n",
        "            '.rar': list(self.base_path.glob(\"*.rar\")),\n",
        "            '.7z': list(self.base_path.glob(\"*.7z\"))\n",
        "        }\n",
        "\n",
        "        found_archives = []\n",
        "        for file_type, files in file_types.items():\n",
        "            if files:\n",
        "                logger.info(f\"Found {len(files)} {file_type} files:\")\n",
        "                for file in files:\n",
        "                    logger.info(f\"  {file.name}\")\n",
        "                    found_archives.extend(files)\n",
        "\n",
        "        if dataset_files:\n",
        "            logger.info(f\"Found {len(dataset_files)} potential dataset files:\")\n",
        "            for file in dataset_files:\n",
        "                logger.info(f\"  {file.name}\")\n",
        "\n",
        "        return found_archives, dataset_files\n",
        "\n",
        "    def interactive_file_selection(self, found_archives, dataset_files):\n",
        "        \"\"\"\n",
        "        Help user identify and select the correct dataset files\n",
        "        \"\"\"\n",
        "        logger.info(\"\\n\" + \"=\"*60)\n",
        "        logger.info(\"DATASET FILE DETECTION RESULTS\")\n",
        "        logger.info(\"=\"*60)\n",
        "\n",
        "        if not found_archives and not dataset_files:\n",
        "            logger.error(\"❌ No archive files or potential dataset files found!\")\n",
        "            logger.info(\"\\n📋 TROUBLESHOOTING STEPS:\")\n",
        "            logger.info(\"1. Verify you've uploaded the ADReSSo21 dataset files\")\n",
        "            logger.info(\"2. Check if files are in a different directory\")\n",
        "            logger.info(\"3. Ensure files are properly uploaded to Google Drive\")\n",
        "            logger.info(\"4. Check if files have different names or extensions\")\n",
        "            return None\n",
        "\n",
        "        logger.info(\"🔍 FOUND FILES ANALYSIS:\")\n",
        "\n",
        "        # Analyze found files\n",
        "        likely_candidates = []\n",
        "\n",
        "        for file in found_archives + dataset_files:\n",
        "            score = 0\n",
        "            reasons = []\n",
        "\n",
        "            # Check file name for ADReSSo21 indicators\n",
        "            name_lower = file.name.lower()\n",
        "            if 'adresso' in name_lower:\n",
        "                score += 5\n",
        "                reasons.append(\"Contains 'ADReSSo'\")\n",
        "            if 'progression' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'progression'\")\n",
        "            if 'diagnosis' in name_lower:\n",
        "                score += 3\n",
        "                reasons.append(\"Contains 'diagnosis'\")\n",
        "            if 'train' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'train'\")\n",
        "            if 'test' in name_lower:\n",
        "                score += 2\n",
        "                reasons.append(\"Contains 'test'\")\n",
        "\n",
        "            # Check file size (ADReSSo21 files should be reasonably large)\n",
        "            size_mb = file.stat().st_size / (1024*1024)\n",
        "            if size_mb > 10:  # Larger than 10MB\n",
        "                score += 2\n",
        "                reasons.append(f\"Good size ({size_mb:.1f} MB)\")\n",
        "            elif size_mb > 1:\n",
        "                score += 1\n",
        "                reasons.append(f\"Moderate size ({size_mb:.1f} MB)\")\n",
        "\n",
        "            if score > 0:\n",
        "                likely_candidates.append((file, score, reasons))\n",
        "\n",
        "        # Sort by score\n",
        "        likely_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        if likely_candidates:\n",
        "            logger.info(f\"🎯 TOP CANDIDATES (sorted by likelihood):\")\n",
        "            for i, (file, score, reasons) in enumerate(likely_candidates[:5]):\n",
        "                logger.info(f\"  {i+1}. {file.name} (Score: {score})\")\n",
        "                logger.info(f\"     Reasons: {', '.join(reasons)}\")\n",
        "                logger.info(f\"     Path: {file}\")\n",
        "\n",
        "        # Return the most likely candidate for automatic processing\n",
        "        if likely_candidates and likely_candidates[0][1] >= 5:\n",
        "            return likely_candidates[0][0]\n",
        "\n",
        "        return None\n",
        "    def extract_any_archive(self, file_path: Path) -> bool:\n",
        "        \"\"\"\n",
        "        Extract archive files in various formats\n",
        "\n",
        "        Args:\n",
        "            file_path (Path): Path to the archive file\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful\n",
        "        \"\"\"\n",
        "        try:\n",
        "            file_extension = file_path.suffix.lower()\n",
        "            extract_dir = self.extracted_path / file_path.stem\n",
        "            extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            logger.info(f\"Attempting to extract: {file_path.name}\")\n",
        "\n",
        "            if file_extension in ['.tgz', '.gz'] or file_path.name.endswith('.tar.gz'):\n",
        "                # Handle .tgz and .tar.gz files\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            elif file_extension == '.zip':\n",
        "                # Handle .zip files\n",
        "                import zipfile\n",
        "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(extract_dir)\n",
        "\n",
        "            elif file_extension == '.tar':\n",
        "                # Handle .tar files\n",
        "                with tarfile.open(file_path, 'r') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "            else:\n",
        "                logger.error(f\"Unsupported archive format: {file_extension}\")\n",
        "                return False\n",
        "\n",
        "            logger.info(f\"Successfully extracted {file_path.name} to {extract_dir}\")\n",
        "\n",
        "            # List extracted contents\n",
        "            extracted_items = list(extract_dir.rglob(\"*\"))\n",
        "            logger.info(f\"Extracted {len(extracted_items)} items\")\n",
        "\n",
        "            # Show directory structure\n",
        "            dirs = [item for item in extracted_items if item.is_dir()]\n",
        "            files = [item for item in extracted_items if item.is_file()]\n",
        "\n",
        "            logger.info(f\"  Directories: {len(dirs)}\")\n",
        "            logger.info(f\"  Files: {len(files)}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting {file_path}: {e}\")\n",
        "            return False\n",
        "    def extract_tgz_files(self) -> bool:\n",
        "        \"\"\"\n",
        "        Extract all archive files to the extraction directory\n",
        "\n",
        "        Returns:\n",
        "            bool: True if extraction successful, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Find potential dataset files\n",
        "            found_archives, dataset_files = self.find_dataset_files()\n",
        "\n",
        "            # Try to identify the best candidates\n",
        "            best_candidate = self.interactive_file_selection(found_archives, dataset_files)\n",
        "\n",
        "            if best_candidate:\n",
        "                logger.info(f\"🎯 Attempting to extract most likely candidate: {best_candidate.name}\")\n",
        "                if self.extract_any_archive(best_candidate):\n",
        "                    return True\n",
        "\n",
        "            # If no clear candidate, try all archive files\n",
        "            if found_archives:\n",
        "                logger.info(\"🔄 Trying to extract all found archive files...\")\n",
        "                extracted_any = False\n",
        "\n",
        "                for archive in found_archives:\n",
        "                    if self.extract_any_archive(archive):\n",
        "                        extracted_any = True\n",
        "\n",
        "                return extracted_any\n",
        "\n",
        "            # Fallback: try the original method for exact file names\n",
        "            logger.info(\"🔄 Trying original extraction method...\")\n",
        "            extracted_any = False\n",
        "\n",
        "            for dataset_name, filename in self.dataset_files.items():\n",
        "                file_path = self.base_path / filename\n",
        "\n",
        "                if not file_path.exists():\n",
        "                    logger.warning(f\"Expected file not found: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Extracting {filename}...\")\n",
        "\n",
        "                # Extract to specific subdirectory\n",
        "                extract_dir = self.extracted_path / dataset_name\n",
        "                extract_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                with tarfile.open(file_path, 'r:gz') as tar:\n",
        "                    tar.extractall(path=extract_dir)\n",
        "\n",
        "                logger.info(f\"Successfully extracted {filename}\")\n",
        "                extracted_any = True\n",
        "\n",
        "            return extracted_any\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during extraction: {e}\")\n",
        "            return False\n",
        "\n",
        "    def verify_directory_structure(self) -> Dict[str, bool]:\n",
        "        \"\"\"\n",
        "        Verify that the extracted directories match expected structure\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, bool]: Status of each dataset extraction\n",
        "        \"\"\"\n",
        "        verification_results = {}\n",
        "\n",
        "        for dataset_name, structure in self.directory_structure.items():\n",
        "            dataset_path = self.extracted_path / dataset_name / \"ADReSSo21\"\n",
        "\n",
        "            # Check if main dataset directory exists\n",
        "            if not dataset_path.exists():\n",
        "                verification_results[dataset_name] = False\n",
        "                logger.error(f\"Dataset directory not found: {dataset_path}\")\n",
        "                continue\n",
        "\n",
        "            # Verify subdirectories\n",
        "            all_dirs_exist = True\n",
        "\n",
        "            for data_type, subdirs in structure.items():\n",
        "                if dataset_name == 'progression_test':\n",
        "                    # Special case for test data\n",
        "                    seg_path = dataset_path / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "                    audio_path = dataset_path / \"progression\" / \"test-dist\" / \"audio\"\n",
        "\n",
        "                    if not (seg_path.exists() and audio_path.exists()):\n",
        "                        all_dirs_exist = False\n",
        "                        logger.error(f\"Test directories missing in {dataset_name}\")\n",
        "                else:\n",
        "                    # Regular structure for train data\n",
        "                    base_type_path = dataset_path / (\"progression\" if \"progression\" in dataset_name else \"diagnosis\")\n",
        "\n",
        "                    for subdir in subdirs:\n",
        "                        if subdir:  # Skip empty strings\n",
        "                            seg_path = base_type_path / \"train\" / \"segmentation\" / subdir\n",
        "                            audio_path = base_type_path / \"train\" / \"audio\" / subdir\n",
        "\n",
        "                            if not (seg_path.exists() and audio_path.exists()):\n",
        "                                all_dirs_exist = False\n",
        "                                logger.error(f\"Missing directories for {dataset_name}/{subdir}\")\n",
        "\n",
        "            verification_results[dataset_name] = all_dirs_exist\n",
        "\n",
        "            if all_dirs_exist:\n",
        "                logger.info(f\"✓ Directory structure verified for {dataset_name}\")\n",
        "            else:\n",
        "                logger.warning(f\"✗ Directory structure issues found for {dataset_name}\")\n",
        "\n",
        "        return verification_results\n",
        "\n",
        "    def extract_transcripts_from_csv(self, csv_file_path: Path) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract transcript data from a single CSV file following CHAT protocol\n",
        "\n",
        "        Args:\n",
        "            csv_file_path (Path): Path to the CSV file\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: List of transcript segments with metadata\n",
        "        \"\"\"\n",
        "        transcripts = []\n",
        "\n",
        "        try:\n",
        "            # Try different encodings as CSV files might have various encodings\n",
        "            encodings = ['utf-8', 'latin-1', 'cp1252']\n",
        "            df = None\n",
        "\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_file_path, encoding=encoding)\n",
        "                    break\n",
        "                except UnicodeDecodeError:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                logger.error(f\"Could not read CSV file with any encoding: {csv_file_path}\")\n",
        "                return transcripts\n",
        "\n",
        "            # Log column names for inspection\n",
        "            logger.info(f\"CSV columns in {csv_file_path.name}: {list(df.columns)}\")\n",
        "\n",
        "            # Extract relevant columns (adjust based on actual CSV structure)\n",
        "            # Common CHAT protocol columns might include: speaker, utterance, time, etc.\n",
        "            for index, row in df.iterrows():\n",
        "                transcript_entry = {\n",
        "                    'file_id': csv_file_path.stem,\n",
        "                    'row_index': index,\n",
        "                    'data': dict(row)  # Store all columns for now\n",
        "                }\n",
        "\n",
        "                # Look for text/utterance columns (common names in CHAT protocol)\n",
        "                text_columns = ['utterance', 'text', 'transcript', 'speech', 'content']\n",
        "                for col in text_columns:\n",
        "                    if col in df.columns and pd.notna(row[col]):\n",
        "                        transcript_entry['transcript'] = str(row[col])\n",
        "                        break\n",
        "\n",
        "                transcripts.append(transcript_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing CSV file {csv_file_path}: {e}\")\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def collect_all_transcripts(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Collect all transcripts from segmentation CSV files\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Dict]]: Organized transcripts by category\n",
        "        \"\"\"\n",
        "        all_transcripts = {\n",
        "            'progression_train_no_decline': [],\n",
        "            'progression_train_decline': [],\n",
        "            'progression_test': [],\n",
        "            'diagnosis_train_cn': [],\n",
        "            'diagnosis_train_ad': []\n",
        "        }\n",
        "\n",
        "        # Process progression training data\n",
        "        prog_train_path = self.extracted_path / \"progression_train\" / \"ADReSSo21\" / \"progression\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['no_decline', 'decline']:\n",
        "            csv_dir = prog_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'progression_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process progression test data\n",
        "        prog_test_path = self.extracted_path / \"progression_test\" / \"ADReSSo21\" / \"progression\" / \"test-dist\" / \"segmentation\"\n",
        "        if prog_test_path.exists():\n",
        "            for csv_file in prog_test_path.glob('*.csv'):\n",
        "                transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                all_transcripts['progression_test'].extend(transcripts)\n",
        "                logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        # Process diagnosis training data\n",
        "        diag_train_path = self.extracted_path / \"diagnosis_train\" / \"ADReSSo21\" / \"diagnosis\" / \"train\" / \"segmentation\"\n",
        "\n",
        "        for category in ['cn', 'ad']:\n",
        "            csv_dir = diag_train_path / category\n",
        "            if csv_dir.exists():\n",
        "                for csv_file in csv_dir.glob('*.csv'):\n",
        "                    transcripts = self.extract_transcripts_from_csv(csv_file)\n",
        "                    all_transcripts[f'diagnosis_train_{category}'].extend(transcripts)\n",
        "                    logger.info(f\"Processed {len(transcripts)} entries from {csv_file.name}\")\n",
        "\n",
        "        return all_transcripts\n",
        "\n",
        "    def save_transcripts_summary(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Save a summary of extracted transcripts for review\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the saved summary file\n",
        "        \"\"\"\n",
        "        summary_file = self.base_path / \"transcripts_summary.txt\"\n",
        "\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"ADReSSo21 Dataset Transcripts Summary\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            total_transcripts = 0\n",
        "            for category, transcript_list in transcripts.items():\n",
        "                count = len(transcript_list)\n",
        "                total_transcripts += count\n",
        "                f.write(f\"{category}: {count} transcript entries\\n\")\n",
        "\n",
        "                # Show sample transcript if available\n",
        "                if transcript_list and 'transcript' in transcript_list[0]:\n",
        "                    sample = transcript_list[0]['transcript'][:100] + \"...\" if len(transcript_list[0]['transcript']) > 100 else transcript_list[0]['transcript']\n",
        "                    f.write(f\"  Sample: {sample}\\n\")\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "            f.write(f\"Total transcript entries: {total_transcripts}\\n\")\n",
        "            f.write(\"\\nNote: These English transcripts need to be translated to Persian for the study.\\n\")\n",
        "            f.write(\"Translation should be done manually by native Persian speakers as per the methodology.\\n\")\n",
        "\n",
        "        logger.info(f\"Transcripts summary saved to: {summary_file}\")\n",
        "        return summary_file\n",
        "\n",
        "    def prepare_for_translation(self, transcripts: Dict[str, List[Dict]]) -> Path:\n",
        "        \"\"\"\n",
        "        Prepare transcript files for manual Persian translation\n",
        "\n",
        "        Args:\n",
        "            transcripts (Dict): Organized transcripts\n",
        "\n",
        "        Returns:\n",
        "            Path: Path to the translation directory\n",
        "        \"\"\"\n",
        "        translation_dir = self.base_path / \"for_translation\"\n",
        "        translation_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        for category, transcript_list in transcripts.items():\n",
        "            if not transcript_list:\n",
        "                continue\n",
        "\n",
        "            # Create CSV file for translation\n",
        "            csv_file = translation_dir / f\"{category}_for_translation.csv\"\n",
        "\n",
        "            with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
        "                writer = csv.writer(f)\n",
        "                writer.writerow(['ID', 'Original_English', 'Persian_Translation', 'Notes'])\n",
        "\n",
        "                for i, entry in enumerate(transcript_list):\n",
        "                    if 'transcript' in entry:\n",
        "                        transcript_id = f\"{category}_{i+1}\"\n",
        "                        english_text = entry['transcript']\n",
        "                        writer.writerow([transcript_id, english_text, '', ''])\n",
        "\n",
        "            logger.info(f\"Created translation file: {csv_file}\")\n",
        "\n",
        "        # Create translation instructions\n",
        "        instructions_file = translation_dir / \"TRANSLATION_INSTRUCTIONS.txt\"\n",
        "        with open(instructions_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"PERSIAN TRANSLATION INSTRUCTIONS\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\\n\")\n",
        "            f.write(\"IMPORTANT REQUIREMENTS:\\n\")\n",
        "            f.write(\"1. Translation must be done by native Persian speakers\\n\")\n",
        "            f.write(\"2. Translator should have at least 13 years of formal Persian education\\n\")\n",
        "            f.write(\"3. Translation should be verified by an independent linguistic expert\\n\")\n",
        "            f.write(\"4. PRESERVE ALL linguistic features:\\n\")\n",
        "            f.write(\"   - Pause words (uhm, uhh, etc.) - translate equivalent Persian pause words\\n\")\n",
        "            f.write(\"   - Repetitions - keep all repetitions\\n\")\n",
        "            f.write(\"   - Linguistic errors - preserve grammatical/syntactic errors\\n\")\n",
        "            f.write(\"   - Syntactic errors - maintain sentence structure issues\\n\")\n",
        "            f.write(\"5. EXCLUDE annotations like [clears throat], [laughs], etc.\\n\")\n",
        "            f.write(\"6. Do NOT use machine translation - manual translation only\\n\")\n",
        "            f.write(\"7. Capture cultural and linguistic nuances specific to Persian\\n\\n\")\n",
        "            f.write(\"Fill in the 'Persian_Translation' column in each CSV file.\\n\")\n",
        "            f.write(\"Use 'Notes' column for any translation decisions or concerns.\\n\")\n",
        "\n",
        "        logger.info(f\"Translation instructions saved to: {instructions_file}\")\n",
        "        return translation_dir\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute Step 1 of the pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "\n",
        "    # Initialize the data processor with the correct path based on your error\n",
        "    processor = ADReSSo21DataProcessor()\n",
        "\n",
        "    # Step 1.1: Mount Google Drive (if in Colab)\n",
        "    logger.info(\"Step 1.1: Mounting Google Drive...\")\n",
        "    drive_mounted = processor.mount_google_drive()\n",
        "\n",
        "    # Step 1.2: Search for dataset files\n",
        "    logger.info(\"Step 1.2: Searching for dataset files...\")\n",
        "    found_archives, dataset_files = processor.find_dataset_files()\n",
        "\n",
        "    if not found_archives and not dataset_files:\n",
        "        logger.error(\"❌ No potential dataset files found!\")\n",
        "        logger.info(\"\\n📋 PLEASE CHECK:\")\n",
        "        logger.info(\"1. Are the ADReSSo21 files uploaded to Google Drive?\")\n",
        "        logger.info(\"2. Are they in the correct directory?\")\n",
        "        logger.info(f\"   Expected location: {processor.base_path}\")\n",
        "        logger.info(\"3. Do they have the expected names:\")\n",
        "        logger.info(\"   - ADReSSo21-progression-train.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-progression-test.tgz\")\n",
        "        logger.info(\"   - ADReSSo21-diagnosis-train.tgz\")\n",
        "        logger.info(\"4. Or are they in a different format (.zip, .rar, etc.)?\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.3: Extract dataset files\n",
        "    logger.info(\"Step 1.3: Extracting dataset files...\")\n",
        "    extraction_success = processor.extract_tgz_files()\n",
        "\n",
        "    if not extraction_success:\n",
        "        logger.error(\"❌ Failed to extract any dataset files.\")\n",
        "        logger.info(\"\\n🔧 POSSIBLE SOLUTIONS:\")\n",
        "        logger.info(\"1. Check if files are corrupted - try re-downloading\")\n",
        "        logger.info(\"2. Try extracting files manually first\")\n",
        "        logger.info(\"3. Ensure files are not password protected\")\n",
        "        logger.info(\"4. Check if files are in an unsupported format\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.4: Verify directory structure\n",
        "    logger.info(\"Step 1.4: Verifying directory structure...\")\n",
        "    verification_results = processor.verify_directory_structure()\n",
        "\n",
        "    successful_extractions = [k for k, v in verification_results.items() if v]\n",
        "\n",
        "    if not successful_extractions:\n",
        "        logger.error(\"No datasets were successfully extracted and verified.\")\n",
        "        return False\n",
        "\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "\n",
        "    # Step 1.5: Extract transcripts from CSV files\n",
        "    logger.info(\"Step 1.5: Extracting transcripts from segmentation CSV files...\")\n",
        "    all_transcripts = processor.collect_all_transcripts()\n",
        "\n",
        "    # Check if we actually got any transcripts\n",
        "    total_transcripts = sum(len(transcripts) for transcripts in all_transcripts.values())\n",
        "\n",
        "    if total_transcripts == 0:\n",
        "        logger.error(\"No transcripts were extracted from CSV files!\")\n",
        "        logger.info(\"This might indicate:\")\n",
        "        logger.info(\"  - CSV files are in a different format than expected\")\n",
        "        logger.info(\"  - Directory structure is different\")\n",
        "        logger.info(\"  - Files are corrupted\")\n",
        "        return False\n",
        "\n",
        "    # Step 1.6: Save summary and prepare for translation\n",
        "    logger.info(\"Step 1.6: Saving transcripts summary...\")\n",
        "    processor.save_transcripts_summary(all_transcripts)\n",
        "\n",
        "    logger.info(\"Step 1.7: Preparing files for Persian translation...\")\n",
        "    translation_dir = processor.prepare_for_translation(all_transcripts)\n",
        "\n",
        "    # Final summary\n",
        "    logger.info(f\"\\n{'='*50}\")\n",
        "    logger.info(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    logger.info(f\"Successfully processed datasets: {successful_extractions}\")\n",
        "    logger.info(f\"Total transcript entries extracted: {total_transcripts}\")\n",
        "    logger.info(f\"Translation files prepared in: {translation_dir}\")\n",
        "    logger.info(\"NEXT STEPS:\")\n",
        "    logger.info(\"1. Have native Persian speakers translate the CSV files\")\n",
        "    logger.info(\"2. Verify translations with linguistic expert\")\n",
        "    logger.info(\"3. Return translated files for Step 2 (Data Preprocessing)\")\n",
        "    logger.info(f\"{'='*50}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if success:\n",
        "        print(\"\\n✅ Step 1 completed successfully!\")\n",
        "        print(\"📁 Check the translation directory for files to be translated to Persian\")\n",
        "        print(\"🔄 Once translation is complete, you can proceed to Step 2\")\n",
        "    else:\n",
        "        print(\"\\n❌ Step 1 encountered errors. Please check the logs above.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1fFwtsjbfA_",
        "outputId": "74e92c53-6e31-4b99-ccb6-d0b074144ffd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "base_path = '/content/drive/MyDrive/Voice'\n",
        "extracted_base_path = '/content/drive/MyDrive/Voice/Extracted_dataset'\n",
        "\n",
        "# Dataset file paths\n",
        "dataset_files = {\n",
        "    'diagnosis_train': '/content/drive/MyDrive/Voice/ADReSSo21-diagnosis-train.tgz',\n",
        "    'progression_test': '/content/drive/MyDrive/Voice/ADReSSo21-progression-test.tgz',\n",
        "    'progression_train': '/content/drive/MyDrive/Voice/ADReSSo21-progression-train.tgz'\n",
        "}\n",
        "\n",
        "def create_directory_structure():\n",
        "    \"\"\"Create the directory structure for extracted datasets\"\"\"\n",
        "    print(\"Creating directory structure...\")\n",
        "\n",
        "    # Create main extracted dataset folder\n",
        "    os.makedirs(extracted_base_path, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for each dataset\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_dir = os.path.join(extracted_base_path, dataset_name)\n",
        "        os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Directory structure created at: {extracted_base_path}\")\n",
        "\n",
        "def extract_dataset(tgz_path, extract_to_path, dataset_name):\n",
        "    \"\"\"Extract a .tgz file to the specified directory\"\"\"\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "\n",
        "    if not os.path.exists(tgz_path):\n",
        "        print(f\"ERROR: File not found - {tgz_path}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
        "            tar.extractall(path=extract_to_path)\n",
        "        print(f\"Successfully extracted {dataset_name} to {extract_to_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR extracting {dataset_name}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def inspect_extracted_structure():\n",
        "    \"\"\"Inspect the extracted directory structure and report findings\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INSPECTING EXTRACTED DATASET STRUCTURE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        print(f\"\\n--- {dataset_name.upper()} ---\")\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            # Walk through the directory structure\n",
        "            for root, dirs, files in os.walk(dataset_path):\n",
        "                level = root.replace(dataset_path, '').count(os.sep)\n",
        "                indent = ' ' * 2 * level\n",
        "                print(f\"{indent}{os.path.basename(root)}/\")\n",
        "\n",
        "                # Show first few files in each directory\n",
        "                sub_indent = ' ' * 2 * (level + 1)\n",
        "                for file in files[:5]:  # Show first 5 files\n",
        "                    print(f\"{sub_indent}{file}\")\n",
        "                if len(files) > 5:\n",
        "                    print(f\"{sub_indent}... and {len(files) - 5} more files\")\n",
        "        else:\n",
        "            print(f\"Directory not found: {dataset_path}\")\n",
        "\n",
        "def find_and_inspect_csv_files():\n",
        "    \"\"\"Find CSV files containing transcripts and inspect their structure\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INSPECTING CSV FILES (TRANSCRIPTS)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    csv_files_found = []\n",
        "\n",
        "    for dataset_name in dataset_files.keys():\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        # Look for CSV files in segmentation directories\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.csv'):\n",
        "                        csv_path = os.path.join(root, file)\n",
        "                        csv_files_found.append((dataset_name, root, file, csv_path))\n",
        "\n",
        "    print(f\"Found {len(csv_files_found)} CSV files total\")\n",
        "\n",
        "    # Inspect structure of first few CSV files\n",
        "    for i, (dataset_name, directory, filename, full_path) in enumerate(csv_files_found[:3]):\n",
        "        print(f\"\\n--- CSV File {i+1}: {filename} ---\")\n",
        "        print(f\"Dataset: {dataset_name}\")\n",
        "        print(f\"Directory: {directory}\")\n",
        "\n",
        "        try:\n",
        "            # Read and inspect CSV structure\n",
        "            df = pd.read_csv(full_path)\n",
        "            print(f\"Shape: {df.shape}\")\n",
        "            print(f\"Columns: {list(df.columns)}\")\n",
        "            print(\"First few rows:\")\n",
        "            print(df.head(2).to_string())\n",
        "\n",
        "            # Check for transcript-like content\n",
        "            for col in df.columns:\n",
        "                if any(keyword in col.lower() for keyword in ['transcript', 'text', 'utterance', 'speech']):\n",
        "                    print(f\"\\nSample content from '{col}':\")\n",
        "                    sample_content = df[col].dropna().head(2).tolist()\n",
        "                    for content in sample_content:\n",
        "                        print(f\"  '{str(content)[:100]}...'\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading CSV: {str(e)}\")\n",
        "\n",
        "    return csv_files_found\n",
        "\n",
        "def generate_summary_report(csv_files_found):\n",
        "    \"\"\"Generate a summary report of the extracted data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SUMMARY REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Count files by dataset and category\n",
        "    summary = {}\n",
        "    for dataset_name, directory, filename, full_path in csv_files_found:\n",
        "        if dataset_name not in summary:\n",
        "            summary[dataset_name] = {'total_csv': 0, 'categories': {}}\n",
        "\n",
        "        summary[dataset_name]['total_csv'] += 1\n",
        "\n",
        "        # Determine category (cn, ad, decline, no_decline)\n",
        "        category = 'unknown'\n",
        "        if '/cn/' in directory:\n",
        "            category = 'cn'\n",
        "        elif '/ad/' in directory:\n",
        "            category = 'ad'\n",
        "        elif '/decline/' in directory:\n",
        "            category = 'decline'\n",
        "        elif '/no_decline/' in directory:\n",
        "            category = 'no_decline'\n",
        "\n",
        "        if category not in summary[dataset_name]['categories']:\n",
        "            summary[dataset_name]['categories'][category] = 0\n",
        "        summary[dataset_name]['categories'][category] += 1\n",
        "\n",
        "    # Print summary\n",
        "    for dataset_name, data in summary.items():\n",
        "        print(f\"\\n{dataset_name.upper()}:\")\n",
        "        print(f\"  Total CSV files: {data['total_csv']}\")\n",
        "        print(\"  Categories:\")\n",
        "        for category, count in data['categories'].items():\n",
        "            print(f\"    {category}: {count} files\")\n",
        "\n",
        "    print(f\"\\nExtracted dataset location: {extracted_base_path}\")\n",
        "    print(\"Ready for Step 2: Translation to Persian\")\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    print(\"Starting Step 1: Data Acquisition and Preparation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 1.1: Create directory structure\n",
        "    create_directory_structure()\n",
        "\n",
        "    # Step 1.2: Extract all datasets\n",
        "    print(\"\\nExtracting datasets...\")\n",
        "    extraction_success = True\n",
        "\n",
        "    for dataset_name, tgz_path in dataset_files.items():\n",
        "        extract_path = os.path.join(extracted_base_path, dataset_name)\n",
        "        success = extract_dataset(tgz_path, extract_path, dataset_name)\n",
        "        if not success:\n",
        "            extraction_success = False\n",
        "\n",
        "    if not extraction_success:\n",
        "        print(\"\\nERROR: Some extractions failed. Please check the file paths and try again.\")\n",
        "        return\n",
        "\n",
        "    # Step 1.3: Inspect extracted structure\n",
        "    inspect_extracted_structure()\n",
        "\n",
        "    # Step 1.4: Find and inspect CSV files (transcripts)\n",
        "    csv_files_found = find_and_inspect_csv_files()\n",
        "\n",
        "    # Step 1.5: Generate summary report\n",
        "    generate_summary_report(csv_files_found)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 1 COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Next step: Manual translation of English transcripts to Persian\")\n",
        "    print(\"Note: The paper emphasizes using native Persian speakers for translation\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpuJVWoUjYeC",
        "outputId": "b425378f-9915-4640-dfe5-cb8f0898f54c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting Step 1: Data Acquisition and Preparation\n",
            "============================================================\n",
            "Creating directory structure...\n",
            "Directory structure created at: /content/drive/MyDrive/Voice/Extracted_dataset\n",
            "\n",
            "Extracting datasets...\n",
            "Extracting diagnosis_train...\n",
            "Successfully extracted diagnosis_train to /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train\n",
            "Extracting progression_test...\n",
            "Successfully extracted progression_test to /content/drive/MyDrive/Voice/Extracted_dataset/progression_test\n",
            "Extracting progression_train...\n",
            "Successfully extracted progression_train to /content/drive/MyDrive/Voice/Extracted_dataset/progression_train\n",
            "\n",
            "============================================================\n",
            "INSPECTING EXTRACTED DATASET STRUCTURE\n",
            "============================================================\n",
            "\n",
            "--- DIAGNOSIS_TRAIN ---\n",
            "diagnosis_train/\n",
            "  ADReSSo21/\n",
            "    diagnosis/\n",
            "      README.md\n",
            "      train/\n",
            "        adresso-train-mmse-scores.csv\n",
            "        segmentation/\n",
            "          cn/\n",
            "            adrso281.csv\n",
            "            adrso308.csv\n",
            "            adrso270.csv\n",
            "            adrso022.csv\n",
            "            adrso298.csv\n",
            "            ... and 74 more files\n",
            "          ad/\n",
            "            adrso229.csv\n",
            "            adrso106.csv\n",
            "            adrso144.csv\n",
            "            adrso049.csv\n",
            "            adrso078.csv\n",
            "            ... and 82 more files\n",
            "        audio/\n",
            "          cn/\n",
            "            adrso173.wav\n",
            "            adrso015.wav\n",
            "            adrso307.wav\n",
            "            adrso283.wav\n",
            "            adrso167.wav\n",
            "            ... and 74 more files\n",
            "          ad/\n",
            "            adrso047.wav\n",
            "            adrso128.wav\n",
            "            adrso045.wav\n",
            "            adrso110.wav\n",
            "            adrso036.wav\n",
            "            ... and 82 more files\n",
            "\n",
            "--- PROGRESSION_TEST ---\n",
            "progression_test/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      test-dist/\n",
            "        test_results_task3.csv\n",
            "        README\n",
            "        segmentation/\n",
            "          adrspt24.csv\n",
            "          adrspt15.csv\n",
            "          adrspt12.csv\n",
            "          adrspt2.csv\n",
            "          adrspt9.csv\n",
            "          ... and 10 more files\n",
            "        audio/\n",
            "          adrspt20.wav\n",
            "          adrspt15.wav\n",
            "          adrspt4.wav\n",
            "          adrspt28.wav\n",
            "          adrspt16.wav\n",
            "          ... and 27 more files\n",
            "\n",
            "--- PROGRESSION_TRAIN ---\n",
            "progression_train/\n",
            "  ADReSSo21/\n",
            "    progression/\n",
            "      README.md\n",
            "      train/\n",
            "        segmentation/\n",
            "          no_decline/\n",
            "            adrsp195.csv\n",
            "            adrsp041.csv\n",
            "            adrsp030.csv\n",
            "            adrsp052.csv\n",
            "            adrsp349.csv\n",
            "            ... and 32 more files\n",
            "          decline/\n",
            "            adrsp051.csv\n",
            "            adrsp313.csv\n",
            "            adrsp101.csv\n",
            "            adrsp055.csv\n",
            "            adrsp179.csv\n",
            "            ... and 5 more files\n",
            "        audio/\n",
            "          no_decline/\n",
            "            adrsp196.wav\n",
            "            adrsp137.wav\n",
            "            adrsp130.wav\n",
            "            adrsp349.wav\n",
            "            adrsp198.wav\n",
            "            ... and 53 more files\n",
            "          decline/\n",
            "            adrsp055.wav\n",
            "            adrsp003.wav\n",
            "            adrsp266.wav\n",
            "            adrsp300.wav\n",
            "            adrsp320.wav\n",
            "            ... and 10 more files\n",
            "\n",
            "============================================================\n",
            "INSPECTING CSV FILES (TRANSCRIPTS)\n",
            "============================================================\n",
            "Found 228 CSV files total\n",
            "\n",
            "--- CSV File 1: adrso281.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (18, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  2129\n",
            "1           2     PAR   2129  5471\n",
            "\n",
            "--- CSV File 2: adrso308.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (24, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     PAR      0  4873\n",
            "1           2     PAR   4873  7270\n",
            "\n",
            "--- CSV File 3: adrso270.csv ---\n",
            "Dataset: diagnosis_train\n",
            "Directory: /content/drive/MyDrive/Voice/Extracted_dataset/diagnosis_train/ADReSSo21/diagnosis/train/segmentation/cn\n",
            "Shape: (20, 4)\n",
            "Columns: ['Unnamed: 0', 'speaker', 'begin', 'end']\n",
            "First few rows:\n",
            "   Unnamed: 0 speaker  begin   end\n",
            "0           1     INV      0   997\n",
            "1           2     PAR    997  2294\n",
            "\n",
            "============================================================\n",
            "SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            "DIAGNOSIS_TRAIN:\n",
            "  Total CSV files: 166\n",
            "  Categories:\n",
            "    unknown: 166 files\n",
            "\n",
            "PROGRESSION_TEST:\n",
            "  Total CSV files: 15\n",
            "  Categories:\n",
            "    unknown: 15 files\n",
            "\n",
            "PROGRESSION_TRAIN:\n",
            "  Total CSV files: 47\n",
            "  Categories:\n",
            "    unknown: 47 files\n",
            "\n",
            "Extracted dataset location: /content/drive/MyDrive/Voice/Extracted_dataset\n",
            "Ready for Step 2: Translation to Persian\n",
            "\n",
            "============================================================\n",
            "STEP 1 COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Next step: Manual translation of English transcripts to Persian\n",
            "Note: The paper emphasizes using native Persian speakers for translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# Define paths\n",
        "extracted_base_path = '/content/drive/MyDrive/Voice/Extracted_dataset'\n",
        "persian_translated_path = '/content/drive/MyDrive/Voice/Persian_Translated_Dataset'\n",
        "\n",
        "def create_persian_dataset_structure():\n",
        "    \"\"\"Create directory structure for Persian translated dataset\"\"\"\n",
        "    print(\"Creating Persian dataset directory structure...\")\n",
        "\n",
        "    # Create main Persian dataset folder\n",
        "    os.makedirs(persian_translated_path, exist_ok=True)\n",
        "\n",
        "    # Mirror the original structure\n",
        "    dataset_types = ['diagnosis_train', 'progression_test', 'progression_train']\n",
        "\n",
        "    for dataset_type in dataset_types:\n",
        "        original_path = os.path.join(extracted_base_path, dataset_type)\n",
        "        persian_path = os.path.join(persian_translated_path, dataset_type)\n",
        "\n",
        "        # Walk through original structure and create mirror structure\n",
        "        for root, dirs, files in os.walk(original_path):\n",
        "            # Create corresponding Persian directory\n",
        "            relative_path = os.path.relpath(root, original_path)\n",
        "            if relative_path == '.':\n",
        "                new_path = persian_path\n",
        "            else:\n",
        "                new_path = os.path.join(persian_path, relative_path)\n",
        "            os.makedirs(new_path, exist_ok=True)\n",
        "\n",
        "    print(f\"Persian dataset structure created at: {persian_translated_path}\")\n",
        "\n",
        "def find_transcript_columns(df):\n",
        "    \"\"\"Identify columns that likely contain transcript text\"\"\"\n",
        "    transcript_cols = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        # Look for common transcript column names\n",
        "        if any(keyword in col_lower for keyword in ['transcript', 'text', 'utterance', 'speech', 'content', 'words']):\n",
        "            transcript_cols.append(col)\n",
        "        # Also check if column contains string data that looks like speech\n",
        "        elif df[col].dtype == 'object':\n",
        "            sample_data = df[col].dropna().head(5)\n",
        "            if len(sample_data) > 0:\n",
        "                # Check if it contains typical speech patterns\n",
        "                sample_text = ' '.join(sample_data.astype(str))\n",
        "                if any(word in sample_text.lower() for word in ['the', 'and', 'is', 'are', 'this', 'that', 'uhm', 'uh']):\n",
        "                    transcript_cols.append(col)\n",
        "\n",
        "    return transcript_cols\n",
        "\n",
        "def analyze_linguistic_features(text):\n",
        "    \"\"\"Analyze text for linguistic features that need to be preserved\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return {\n",
        "            'has_pause_words': False,\n",
        "            'has_repetitions': False,\n",
        "            'pause_words': [],\n",
        "            'word_count': 0,\n",
        "            'requires_translation': False\n",
        "        }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Common English pause words/fillers that need to be preserved\n",
        "    pause_words = ['uhm', 'uh', 'um', 'er', 'ah', 'hmm', 'mm']\n",
        "    found_pause_words = [pw for pw in pause_words if pw in text_lower]\n",
        "\n",
        "    # Simple repetition detection (same word appearing consecutively)\n",
        "    words = text.split()\n",
        "    has_repetitions = any(i > 0 and words[i].lower() == words[i-1].lower() for i in range(1, len(words)))\n",
        "\n",
        "    # Check if text contains English content (basic check)\n",
        "    english_indicators = ['the', 'and', 'is', 'are', 'this', 'that', 'with', 'for', 'to', 'of', 'in', 'on']\n",
        "    requires_translation = any(word in text_lower for word in english_indicators)\n",
        "\n",
        "    return {\n",
        "        'has_pause_words': len(found_pause_words) > 0,\n",
        "        'has_repetitions': has_repetitions,\n",
        "        'pause_words': found_pause_words,\n",
        "        'word_count': len(words),\n",
        "        'requires_translation': requires_translation\n",
        "    }\n",
        "\n",
        "def create_translation_template(csv_files_info):\n",
        "    \"\"\"Create a structured template for translation\"\"\"\n",
        "    translation_data = []\n",
        "\n",
        "    print(\"Creating translation template...\")\n",
        "\n",
        "    for dataset_name, directory, filename, full_path in csv_files_info:\n",
        "        try:\n",
        "            df = pd.read_csv(full_path)\n",
        "            transcript_cols = find_transcript_columns(df)\n",
        "\n",
        "            if transcript_cols:\n",
        "                print(f\"Processing {filename} - Found transcript columns: {transcript_cols}\")\n",
        "\n",
        "                for idx, row in df.iterrows():\n",
        "                    for col in transcript_cols:\n",
        "                        original_text = row[col]\n",
        "                        if pd.notna(original_text) and isinstance(original_text, str) and original_text.strip():\n",
        "\n",
        "                            features = analyze_linguistic_features(original_text)\n",
        "\n",
        "                            if features['requires_translation']:\n",
        "                                translation_entry = {\n",
        "                                    'dataset': dataset_name,\n",
        "                                    'file': filename,\n",
        "                                    'directory': directory,\n",
        "                                    'row_index': idx,\n",
        "                                    'column': col,\n",
        "                                    'original_english': original_text,\n",
        "                                    'persian_translation': '',  # To be filled\n",
        "                                    'has_pause_words': features['has_pause_words'],\n",
        "                                    'pause_words': features['pause_words'],\n",
        "                                    'has_repetitions': features['has_repetitions'],\n",
        "                                    'word_count': features['word_count'],\n",
        "                                    'translation_notes': '',\n",
        "                                    'translation_status': 'pending'\n",
        "                                }\n",
        "                                translation_data.append(translation_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    return translation_data\n",
        "\n",
        "def save_translation_template(translation_data):\n",
        "    \"\"\"Save translation template for manual translation\"\"\"\n",
        "    template_path = os.path.join(persian_translated_path, 'translation_template.json')\n",
        "\n",
        "    # Save as JSON for easy editing\n",
        "    with open(template_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(translation_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Also create a CSV version for easier viewing/editing\n",
        "    csv_template_path = os.path.join(persian_translated_path, 'translation_template.csv')\n",
        "    df_template = pd.DataFrame(translation_data)\n",
        "    df_template.to_csv(csv_template_path, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"Translation template saved:\")\n",
        "    print(f\"  JSON format: {template_path}\")\n",
        "    print(f\"  CSV format: {csv_template_path}\")\n",
        "\n",
        "    return template_path, csv_template_path\n",
        "\n",
        "def create_translation_guidelines():\n",
        "    \"\"\"Create detailed translation guidelines based on the paper's methodology\"\"\"\n",
        "    guidelines = \"\"\"\n",
        "PERSIAN TRANSLATION GUIDELINES\n",
        "==============================\n",
        "\n",
        "CRITICAL REQUIREMENTS (Based on Research Paper):\n",
        "\n",
        "1. PRESERVE ALL LINGUISTIC FEATURES:\n",
        "   - Keep ALL pause words: \"uhm\", \"uh\", \"um\", \"er\", \"ah\" etc.\n",
        "   - Translate to Persian equivalents: \"اوم\", \"اه\", \"ام\", \"ار\", \"آه\"\n",
        "   - MAINTAIN repetitions exactly as they appear\n",
        "   - PRESERVE all linguistic and syntactic errors\n",
        "   - Keep hesitations and false starts\n",
        "\n",
        "2. EXCLUDE NON-LINGUISTIC ANNOTATIONS:\n",
        "   - Remove: [clears throat], [laughs], [coughs]\n",
        "   - Remove: [inaudible], [unclear]\n",
        "   - Keep only actual speech content\n",
        "\n",
        "3. TRANSLATION PRINCIPLES:\n",
        "   - Translate meaning while preserving linguistic characteristics\n",
        "   - Maintain natural Persian flow where possible\n",
        "   - Keep cultural context appropriate to Persian speakers\n",
        "   - Preserve sentence structure patterns when possible\n",
        "\n",
        "4. SPECIFIC EXAMPLES:\n",
        "   English: \"The woman is uhm she is washing dishes\"\n",
        "   Persian: \"زن اوم او ظرف می‌شوید\" (keeping the pause word and structure)\n",
        "\n",
        "   English: \"The the boy is running\"\n",
        "   Persian: \"پسر پسر دارد می‌دود\" (preserving repetition)\n",
        "\n",
        "5. QUALITY CONTROL:\n",
        "   - Double-check each translation\n",
        "   - Ensure pause words are correctly placed\n",
        "   - Verify repetitions are maintained\n",
        "   - Check that errors are preserved appropriately\n",
        "\n",
        "PAUSE WORD EQUIVALENTS:\n",
        "- \"uhm\" → \"اوم\"\n",
        "- \"uh\" → \"اه\"\n",
        "- \"um\" → \"ام\"\n",
        "- \"er\" → \"ار\"\n",
        "- \"ah\" → \"آه\"\n",
        "- \"hmm\" → \"هوم\"\n",
        "\"\"\"\n",
        "\n",
        "    guidelines_path = os.path.join(persian_translated_path, 'translation_guidelines.txt')\n",
        "    with open(guidelines_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(guidelines)\n",
        "\n",
        "    print(f\"Translation guidelines saved: {guidelines_path}\")\n",
        "    return guidelines_path\n",
        "\n",
        "def load_and_apply_translations(template_path):\n",
        "    \"\"\"Load completed translations and apply them to create Persian dataset\"\"\"\n",
        "    print(\"Loading translations and creating Persian dataset...\")\n",
        "\n",
        "    # Load translation data\n",
        "    with open(template_path, 'r', encoding='utf-8') as f:\n",
        "        translation_data = json.load(f)\n",
        "\n",
        "    # Group by file for processing\n",
        "    files_to_process = {}\n",
        "    for entry in translation_data:\n",
        "        key = (entry['dataset'], entry['file'], entry['directory'])\n",
        "        if key not in files_to_process:\n",
        "            files_to_process[key] = []\n",
        "        files_to_process[key].append(entry)\n",
        "\n",
        "    translated_files_count = 0\n",
        "\n",
        "    for (dataset_name, filename, directory), translations in files_to_process.items():\n",
        "        # Load original CSV\n",
        "        original_path = os.path.join(directory, filename)\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(original_path)\n",
        "            df_persian = df.copy()\n",
        "\n",
        "            # Apply translations\n",
        "            for translation in translations:\n",
        "                if translation['persian_translation'].strip():  # Only if translation exists\n",
        "                    row_idx = translation['row_index']\n",
        "                    col = translation['column']\n",
        "                    df_persian.at[row_idx, col] = translation['persian_translation']\n",
        "\n",
        "            # Save Persian version\n",
        "            relative_dir = os.path.relpath(directory, extracted_base_path)\n",
        "            persian_dir = os.path.join(persian_translated_path, relative_dir)\n",
        "            os.makedirs(persian_dir, exist_ok=True)\n",
        "\n",
        "            persian_file_path = os.path.join(persian_dir, f\"persian_{filename}\")\n",
        "            df_persian.to_csv(persian_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "            translated_files_count += 1\n",
        "            print(f\"Created Persian version: {persian_file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "    print(f\"Successfully created {translated_files_count} Persian dataset files\")\n",
        "\n",
        "def get_translation_statistics(csv_files_info):\n",
        "    \"\"\"Get statistics about translation requirements\"\"\"\n",
        "    total_entries = 0\n",
        "    entries_needing_translation = 0\n",
        "    total_words = 0\n",
        "    files_with_transcripts = 0\n",
        "\n",
        "    for dataset_name, directory, filename, full_path in csv_files_info:\n",
        "        try:\n",
        "            df = pd.read_csv(full_path)\n",
        "            transcript_cols = find_transcript_columns(df)\n",
        "\n",
        "            if transcript_cols:\n",
        "                files_with_transcripts += 1\n",
        "                for col in transcript_cols:\n",
        "                    for text in df[col].dropna():\n",
        "                        if isinstance(text, str) and text.strip():\n",
        "                            total_entries += 1\n",
        "                            features = analyze_linguistic_features(text)\n",
        "                            if features['requires_translation']:\n",
        "                                entries_needing_translation += 1\n",
        "                                total_words += features['word_count']\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        'total_files_with_transcripts': files_with_transcripts,\n",
        "        'total_entries': total_entries,\n",
        "        'entries_needing_translation': entries_needing_translation,\n",
        "        'estimated_total_words': total_words\n",
        "    }\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    print(\"Starting Step 2: Translation to Persian\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Step 2.1: Create Persian dataset structure\n",
        "    create_persian_dataset_structure()\n",
        "\n",
        "    # Step 2.2: Find all CSV files from Step 1\n",
        "    print(\"\\nScanning for CSV files...\")\n",
        "    csv_files_info = []\n",
        "\n",
        "    for dataset_name in ['diagnosis_train', 'progression_test', 'progression_train']:\n",
        "        dataset_path = os.path.join(extracted_base_path, dataset_name)\n",
        "\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            if 'segmentation' in root:\n",
        "                for file in files:\n",
        "                    if file.endswith('.csv'):\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        csv_files_info.append((dataset_name, root, file, full_path))\n",
        "\n",
        "    print(f\"Found {len(csv_files_info)} CSV files to analyze\")\n",
        "\n",
        "    # Step 2.3: Get translation statistics\n",
        "    stats = get_translation_statistics(csv_files_info)\n",
        "    print(f\"\\nTranslation Statistics:\")\n",
        "    print(f\"  Files with transcripts: {stats['total_files_with_transcripts']}\")\n",
        "    print(f\"  Total text entries: {stats['total_entries']}\")\n",
        "    print(f\"  Entries needing translation: {stats['entries_needing_translation']}\")\n",
        "    print(f\"  Estimated total words: {stats['estimated_total_words']}\")\n",
        "\n",
        "    # Step 2.4: Create translation template\n",
        "    translation_data = create_translation_template(csv_files_info)\n",
        "\n",
        "    # Step 2.5: Save translation template\n",
        "    template_path, csv_template_path = save_translation_template(translation_data)\n",
        "\n",
        "    # Step 2.6: Create translation guidelines\n",
        "    guidelines_path = create_translation_guidelines()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 2 SETUP COMPLETED!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Found {len(translation_data)} text entries requiring translation\")\n",
        "    print(f\"\\nNext Actions Required:\")\n",
        "    print(f\"1. Review translation guidelines: {guidelines_path}\")\n",
        "    print(f\"2. Open translation template: {csv_template_path}\")\n",
        "    print(f\"3. Fill in 'persian_translation' column for each entry\")\n",
        "    print(f\"4. Save the completed translations\")\n",
        "    print(f\"5. Run the apply_translations() function\")\n",
        "\n",
        "    print(f\"\\nIMPORTANT REMINDERS:\")\n",
        "    print(f\"- Preserve ALL pause words (uhm, uh, etc.)\")\n",
        "    print(f\"- Keep repetitions exactly as they appear\")\n",
        "    print(f\"- Maintain linguistic errors and hesitations\")\n",
        "    print(f\"- Use native Persian speaker expertise for accuracy\")\n",
        "\n",
        "    return template_path, csv_template_path, guidelines_path\n",
        "\n",
        "def apply_completed_translations():\n",
        "    \"\"\"Call this function after completing manual translations\"\"\"\n",
        "    template_path = os.path.join(persian_translated_path, 'translation_template.json')\n",
        "\n",
        "    if os.path.exists(template_path):\n",
        "        load_and_apply_translations(template_path)\n",
        "        print(\"\\nPersian dataset creation completed!\")\n",
        "        print(\"Ready for Step 3: Data Preprocessing\")\n",
        "    else:\n",
        "        print(\"Translation template not found. Please complete Step 2 setup first.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    template_path, csv_template_path, guidelines_path = main()\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"READY FOR TRANSLATION!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Please open this file to start translating:\")\n",
        "    print(f\"{csv_template_path}\")\n",
        "    print(f\"\\nAfter completing translations, run:\")\n",
        "    print(f\"apply_completed_translations()\")\n",
        "\n",
        "# Quick function to check translation progress\n",
        "def check_translation_progress():\n",
        "    \"\"\"Check how many translations have been completed\"\"\"\n",
        "    template_path = os.path.join(persian_translated_path, 'translation_template.json')\n",
        "\n",
        "    if os.path.exists(template_path):\n",
        "        with open(template_path, 'r', encoding='utf-8') as f:\n",
        "            translation_data = json.load(f)\n",
        "\n",
        "        total = len(translation_data)\n",
        "        completed = sum(1 for entry in translation_data if entry['persian_translation'].strip())\n",
        "\n",
        "        print(f\"Translation Progress: {completed}/{total} ({completed/total*100:.1f}%)\")\n",
        "        return completed, total\n",
        "    else:\n",
        "        print(\"Translation template not found. Run main() first.\")\n",
        "        return 0, 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9MoaTzKm4X2",
        "outputId": "65c117b2-b44b-4042-bc49-f64df3253c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Step 2: Translation to Persian\n",
            "============================================================\n",
            "Creating Persian dataset directory structure...\n",
            "Persian dataset structure created at: /content/drive/MyDrive/Voice/Persian_Translated_Dataset\n",
            "\n",
            "Scanning for CSV files...\n",
            "Found 228 CSV files to analyze\n",
            "\n",
            "Translation Statistics:\n",
            "  Files with transcripts: 0\n",
            "  Total text entries: 0\n",
            "  Entries needing translation: 0\n",
            "  Estimated total words: 0\n",
            "Creating translation template...\n",
            "Translation template saved:\n",
            "  JSON format: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.json\n",
            "  CSV format: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.csv\n",
            "Translation guidelines saved: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_guidelines.txt\n",
            "\n",
            "============================================================\n",
            "STEP 2 SETUP COMPLETED!\n",
            "============================================================\n",
            "Found 0 text entries requiring translation\n",
            "\n",
            "Next Actions Required:\n",
            "1. Review translation guidelines: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_guidelines.txt\n",
            "2. Open translation template: /content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.csv\n",
            "3. Fill in 'persian_translation' column for each entry\n",
            "4. Save the completed translations\n",
            "5. Run the apply_translations() function\n",
            "\n",
            "IMPORTANT REMINDERS:\n",
            "- Preserve ALL pause words (uhm, uh, etc.)\n",
            "- Keep repetitions exactly as they appear\n",
            "- Maintain linguistic errors and hesitations\n",
            "- Use native Persian speaker expertise for accuracy\n",
            "\n",
            "============================================================\n",
            "READY FOR TRANSLATION!\n",
            "============================================================\n",
            "Please open this file to start translating:\n",
            "/content/drive/MyDrive/Voice/Persian_Translated_Dataset/translation_template.csv\n",
            "\n",
            "After completing translations, run:\n",
            "apply_completed_translations()\n"
          ]
        }
      ]
    }
  ]
}