{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8Zl4N5aHAlZCg+wtN9rpW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Alz_voice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete ADReSSo Multi-Modal Analysis Pipeline Steps\n",
        "\n",
        "## Project Overview\n",
        "This project implements a comprehensive multi-modal machine learning pipeline for Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSSo). The system combines audio processing, text analysis, and advanced deep learning architectures to classify speech samples as either cognitively normal (CN) or showing signs of Alzheimer's disease (AD).\n",
        "\n",
        "## Pipeline Architecture Components\n",
        "\n",
        "### Core Models Used:\n",
        "- **Graph-based Attention Module**: For semantic relationship modeling\n",
        "- **Vision Transformer (ViT)**: For spectrogram analysis\n",
        "- **U-Net**: For audio feature processing\n",
        "- **AlexNet**: For additional feature extraction\n",
        "- **BERT**: For text processing and linguistic analysis\n",
        "- **Wav2Vec2**: For audio feature extraction\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Pipeline Process\n",
        "\n",
        "### Step 0: Environment Setup and Data Preparation\n",
        "**Purpose**: Initialize the analysis environment and prepare the dataset\n",
        "\n",
        "**Actions**:\n",
        "- Mount Google Drive\n",
        "- Install required packages: `librosa`, `soundfile`, `opensmile`, `speechbrain`, `transformers`, `torch`, `openai-whisper`, `pandas`, `numpy`, `matplotlib`, `seaborn`, `torch-geometric`\n",
        "- Set up base directory structure\n",
        "- Initialize output directories for results\n",
        "\n",
        "**Key Files**:\n",
        "- Audio files organized by categories (diagnosis_ad, diagnosis_cn, progression_decline, progression_no_decline)\n",
        "- Configuration files and model checkpoints\n",
        "\n",
        "### Step 1: Audio File Discovery and Organization\n",
        "**Purpose**: Scan and categorize all available audio files\n",
        "\n",
        "**Actions**:\n",
        "- Recursively search for audio files (.wav, .mp3, .m4a, .flac)\n",
        "- Categorize files based on directory structure:\n",
        "  - `diagnosis_ad/`: Alzheimer's disease diagnosis files\n",
        "  - `diagnosis_cn/`: Cognitively normal diagnosis files\n",
        "  - `progression_decline/`: Disease progression (decline) files\n",
        "  - `progression_no_decline/`: Disease progression (stable) files\n",
        "- Generate file inventory and statistics\n",
        "\n",
        "**Output**: Dictionary of categorized audio file paths\n",
        "\n",
        "### Step 2: Audio Processing and Feature Extraction\n",
        "**Purpose**: Extract comprehensive acoustic features from audio files\n",
        "\n",
        "**Feature Types Extracted**:\n",
        "- **Wav2Vec2 Features**: Deep learning-based audio representations\n",
        "- **Mel-frequency Cepstral Coefficients (MFCCs)**: Traditional audio features\n",
        "- **Spectral Features**: Spectral centroid, bandwidth, rolloff\n",
        "- **Prosodic Features**: Pitch, energy, rhythm patterns\n",
        "- **OpenSMILE Features**: Comprehensive acoustic feature set\n",
        "\n",
        "**Processing Steps**:\n",
        "- Load and preprocess audio files\n",
        "- Extract multi-dimensional feature vectors\n",
        "- Apply feature normalization and scaling\n",
        "- Generate mel-spectrograms for visual analysis\n",
        "\n",
        "### Step 3: Speech-to-Text Conversion\n",
        "**Purpose**: Convert audio to text for linguistic analysis\n",
        "\n",
        "**Tools Used**:\n",
        "- **OpenAI Whisper**: For high-quality speech transcription\n",
        "- **Alternative ASR systems**: Fallback options for different audio qualities\n",
        "\n",
        "**Processing**:\n",
        "- Transcribe all audio files to text\n",
        "- Handle different audio qualities and accents\n",
        "- Store transcriptions with confidence scores\n",
        "- Generate metadata for each transcription\n",
        "\n",
        "### Step 4: Linguistic Feature Analysis\n",
        "**Purpose**: Extract detailed linguistic and semantic features from transcribed text\n",
        "\n",
        "**Feature Categories**:\n",
        "- **Semantic Features**: Word embeddings, semantic density\n",
        "- **Syntactic Features**: Part-of-speech patterns, sentence structure\n",
        "- **Lexical Features**: Vocabulary diversity, word frequency\n",
        "- **Discourse Features**: Coherence, topic transitions\n",
        "- **Fluency Measures**: Pause patterns, disfluencies\n",
        "\n",
        "**Processing**:\n",
        "- Use BERT for semantic embeddings\n",
        "- Apply NLP tools for syntactic analysis\n",
        "- Calculate linguistic complexity metrics\n",
        "- Extract discourse markers and patterns\n",
        "\n",
        "### Step 5: Comprehensive Analysis and Visualization\n",
        "**Purpose**: Analyze extracted features and generate comprehensive reports\n",
        "\n",
        "**Analysis Types**:\n",
        "- **Statistical Analysis**: Feature distributions, correlations\n",
        "- **Visualization**: Feature plots, spectrograms, linguistic patterns\n",
        "- **Comparative Analysis**: AD vs CN differences\n",
        "- **Quality Assessment**: Data quality metrics\n",
        "\n",
        "**Outputs**:\n",
        "- Feature correlation matrices\n",
        "- Statistical summary reports\n",
        "- Visualization plots and charts\n",
        "- Data quality assessments\n",
        "\n",
        "### Step 6: Multi-Modal Model Architecture Definition\n",
        "**Purpose**: Define the complex neural network architecture for classification\n",
        "\n",
        "**Architecture Components**:\n",
        "```\n",
        "MultiModalADReSSoModel:\n",
        "├── Graph Attention Module\n",
        "│   ├── Semantic graph construction\n",
        "│   └── Graph attention networks\n",
        "├── Vision Transformer Module\n",
        "│   ├── Spectrogram patch embedding\n",
        "│   └── Transformer encoder layers\n",
        "├── U-Net Module\n",
        "│   ├── Audio signal processing\n",
        "│   └── Feature extraction layers\n",
        "├── AlexNet Module\n",
        "│   ├── Convolutional feature extraction\n",
        "│   └── Classification layers\n",
        "├── BERT Module\n",
        "│   ├── Text embedding\n",
        "│   └── Linguistic feature extraction\n",
        "└── Fusion Layer\n",
        "    ├── Multi-modal feature fusion\n",
        "    └── Final classification\n",
        "```\n",
        "\n",
        "**Key Parameters**:\n",
        "- Audio feature dimension: 768 (Wav2Vec2)\n",
        "- Text feature dimension: 768 (BERT)\n",
        "- Spectrogram height: 80 (Mel bins)\n",
        "- Number of classes: 2 (AD vs CN)\n",
        "\n",
        "### Step 7: Model Training\n",
        "**Purpose**: Train the multi-modal model on the processed dataset\n",
        "\n",
        "**Training Configuration**:\n",
        "- **Batch Size**: 8 (adjustable based on GPU memory)\n",
        "- **Epochs**: 30 (with early stopping)\n",
        "- **Learning Rate**: Adaptive with scheduler\n",
        "- **Optimization**: Adam optimizer\n",
        "- **Loss Function**: Cross-entropy loss\n",
        "\n",
        "**Data Splitting**:\n",
        "- Training: 60% of data\n",
        "- Validation: 20% of data\n",
        "- Testing: 20% of data\n",
        "- Stratified splitting to maintain class balance\n",
        "\n",
        "**Training Process**:\n",
        "- Initialize model with random weights\n",
        "- Create data loaders for each split\n",
        "- Implement training loop with validation\n",
        "- Save best model based on validation performance\n",
        "- Monitor training metrics and convergence\n",
        "\n",
        "### Step 8: Model Evaluation and Semantic Analysis\n",
        "**Purpose**: Evaluate model performance and analyze semantic relationships\n",
        "\n",
        "**Evaluation Metrics**:\n",
        "- **Accuracy**: Overall classification accuracy\n",
        "- **Precision**: True positive rate\n",
        "- **Recall**: Sensitivity\n",
        "- **F1-Score**: Harmonic mean of precision and recall\n",
        "- **ROC AUC**: Area under ROC curve\n",
        "\n",
        "**Semantic Analysis**:\n",
        "- Visualize semantic relationships between audio and text features\n",
        "- Generate semantic graphs showing feature correlations\n",
        "- Analyze modality contributions to predictions\n",
        "- Create interpretability visualizations\n",
        "\n",
        "**Analysis Outputs**:\n",
        "- Confusion matrices\n",
        "- ROC curves\n",
        "- Feature importance plots\n",
        "- Semantic relationship graphs\n",
        "- Detailed classification reports\n",
        "\n",
        "### Step 9: Checkpointing and Incremental Processing\n",
        "**Purpose**: Implement robust checkpointing system for large-scale processing\n",
        "\n",
        "**Checkpointing Features**:\n",
        "- **Incremental Processing**: Resume from last checkpoint\n",
        "- **Individual Feature Saving**: Save features for each file separately\n",
        "- **Progress Tracking**: Monitor processing status\n",
        "- **Error Recovery**: Handle processing failures gracefully\n",
        "\n",
        "**Checkpoint Structure**:\n",
        "```\n",
        "checkpoints/\n",
        "├── checkpoint.pkl (main checkpoint file)\n",
        "├── features/ (individual feature files)\n",
        "│   ├── diagnosis_ad_file1_features.pkl\n",
        "│   ├── diagnosis_cn_file1_features.pkl\n",
        "│   └── ...\n",
        "└── logs/ (processing logs)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Advanced Features\n",
        "\n",
        "### Semantic Graph Visualization\n",
        "- Create networkx graphs showing relationships between audio and text features\n",
        "- Visualize cosine similarity between modalities\n",
        "- Generate interactive relationship plots\n",
        "- Analyze semantic coherence between speech and content\n",
        "\n",
        "### Feature Importance Analysis\n",
        "- Calculate contribution of each modality to final predictions\n",
        "- Analyze which features are most discriminative\n",
        "- Generate feature importance rankings\n",
        "- Create modality-specific performance metrics\n",
        "\n",
        "### Comprehensive Reporting\n",
        "- Generate detailed evaluation reports\n",
        "- Create performance summaries by category\n",
        "- Analyze misclassification patterns\n",
        "- Provide confidence-based analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "### Basic Usage:\n",
        "```python\n",
        "# Initialize the extended analyzer\n",
        "ExtendedAnalyzer = extend_analyzer_with_model()\n",
        "analyzer = ExtendedAnalyzer(base_path=\"/path/to/ADReSSo21\")\n",
        "\n",
        "# Create checkpointer\n",
        "checkpointer = FeatureExtractionCheckpointer(analyzer)\n",
        "\n",
        "# Run complete pipeline\n",
        "results = checkpointer.run_pipeline_with_checkpoints(\n",
        "    num_epochs=30,\n",
        "    batch_size=8\n",
        ")\n",
        "```\n",
        "\n",
        "### Advanced Configuration:\n",
        "```python\n",
        "# Custom training parameters\n",
        "results = checkpointer.run_pipeline_with_checkpoints(\n",
        "    num_epochs=50,\n",
        "    batch_size=4  # Reduce for limited GPU memory\n",
        ")\n",
        "\n",
        "# Individual steps\n",
        "analyzer.step_6_define_model_architecture()\n",
        "analyzer.step_7_train_model(features_dict, linguistic_features)\n",
        "analyzer.step_8_evaluate_model(visualize_graphs=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Output Files and Results\n",
        "\n",
        "### Generated Files:\n",
        "- `detailed_evaluation_results.csv`: Per-sample predictions and confidence scores\n",
        "- `evaluation_summary.json`: Overall performance metrics\n",
        "- `semantic_graph_*.png`: Semantic relationship visualizations\n",
        "- `best_adresso_model.pth`: Trained model weights\n",
        "- `checkpoint.pkl`: Processing checkpoint data\n",
        "- Individual feature files for each audio sample\n",
        "\n",
        "### Key Metrics Tracked:\n",
        "- Overall classification accuracy\n",
        "- Per-category performance (AD, CN, decline, stable)\n",
        "- Confidence distributions\n",
        "- Misclassification analysis\n",
        "- Feature importance by modality\n",
        "- Semantic relationship strengths\n",
        "\n",
        "This comprehensive pipeline provides a complete solution for Alzheimer's dementia recognition through multi-modal analysis of spontaneous speech, combining state-of-the-art deep learning techniques with robust feature extraction and evaluation methodologies."
      ],
      "metadata": {
        "id": "r6-B0HUC0xdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprehensive Workflow for ADReSSo21 Speech Analysis Project\n",
        "This workflow details the steps to process the ADReSSo21 dataset, extract features, build a multi-modal model, and evaluate its performance for Alzheimer’s Disease (AD) detection through speech analysis. It is structured into eight main phases, each with specific sub-steps to ensure a thorough and systematic approach.\n",
        "\n",
        "1. Setup and Initialization\n",
        "\n",
        "Install Dependencies: Set up the environment by installing necessary libraries and tools for audio processing, machine learning, and data handling. This includes tools for audio feature extraction, transcription, and neural network modeling.\n",
        "Access Data Storage: Connect to a storage system (e.g., Google Drive) where the ADReSSo21 dataset is located, ensuring seamless access to audio files and related resources.\n",
        "Initialize Analyzer: Create a central analysis tool or class that manages the entire pipeline, extending basic functionality to include advanced model training and evaluation capabilities.\n",
        "\n",
        "\n",
        "2. Data Loading\n",
        "\n",
        "Retrieve Audio Files: Scan the dataset directory to collect paths to all audio files, organizing them into five categories based on their purpose:\n",
        "Diagnosis AD (Alzheimer’s Disease group)\n",
        "Diagnosis CN (Control group)\n",
        "Progression Decline (Subjects showing cognitive decline)\n",
        "Progression No Decline (Subjects with stable cognition)\n",
        "Progression Test (Test set for progression analysis)\n",
        "\n",
        "\n",
        "Understand Dataset Composition: Note the dataset includes 271 audio files, distributed as follows:\n",
        "Diagnosis AD: 87 files\n",
        "Diagnosis CN: 79 files\n",
        "Progression Decline: 15 files\n",
        "Progression No Decline: 58 files\n",
        "Progression Test: 32 files\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Feature Extraction\n",
        "\n",
        "Extract Acoustic Features: Process each audio file to derive a rich set of acoustic features, including:\n",
        "eGeMAPS: A standardized set of 88 features capturing emotional and functional aspects of speech.\n",
        "MFCCs: Mel-frequency cepstral coefficients, including mean, standard deviation, delta, and delta-delta values for 13 coefficients.\n",
        "Log-Mel Spectrograms: Mean and standard deviation across 80 mel frequency bands.\n",
        "Wav2Vec2 Embeddings: 768-dimensional representations from a pre-trained speech model.\n",
        "Prosodic Features: Metrics like fundamental frequency (mean and std), energy (mean and std), zero-crossing rate, spectral centroid, spectral rolloff, and audio duration.\n",
        "\n",
        "\n",
        "Manage Progress with Checkpointing: Implement a system to:\n",
        "Check for previously extracted features and load them to avoid redundant processing.\n",
        "Extract features for unprocessed files and save them individually.\n",
        "Update progress tracking after each file to ensure continuity even if the process is interrupted.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Transcription\n",
        "\n",
        "Transcribe Audio Files: Use an advanced speech-to-text model (e.g., Whisper) to convert audio into text, with content varying by category:\n",
        "Diagnosis AD and CN: Descriptions of a specific picture (e.g., Cookie Theft).\n",
        "Progression Decline, No Decline, and Test: Verbal fluency tasks (e.g., naming animals).\n",
        "\n",
        "\n",
        "Save Transcripts: Store the transcribed text in a designated directory for later use.\n",
        "Create Summary Table: Compile a table with metadata for each transcript, including:\n",
        "File ID\n",
        "Category\n",
        "Filename\n",
        "Transcript length\n",
        "Word count\n",
        "Language\n",
        "Number of segments\n",
        "Error status\n",
        "Preview of the transcript\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. Linguistic Feature Extraction\n",
        "\n",
        "Extract Features from Transcripts: Process the saved transcripts using a pre-trained language model (e.g., BERT) to obtain:\n",
        "Tokenized embeddings representing the text’s semantic content.\n",
        "Additional metrics like word count, sentence count, average word length, unique words, and lexical diversity.\n",
        "\n",
        "\n",
        "Store Features: Save the extracted linguistic features in a structured format (e.g., a pickle file) for integration into the model.\n",
        "\n",
        "\n",
        "6. Model Architecture Definition\n",
        "\n",
        "Design Multi-Modal Model: Construct a sophisticated model that combines multiple types of data and processing techniques:\n",
        "Graph Attention Module: Analyzes relationships between audio and text features using a graph-based approach.\n",
        "Vision Transformer Module: Processes spectrogram data with a transformer architecture.\n",
        "U-Net Module: Handles audio features with a convolutional network designed for detailed feature extraction.\n",
        "AlexNet Module: Extracts features from audio inputs using a modified deep convolutional network.\n",
        "BERT Module: Processes linguistic features from transcripts.\n",
        "Fusion and Classification Layers: Integrates outputs from all modules and classifies them into two categories: AD or CN.\n",
        "\n",
        "\n",
        "Initialize Model: Set up the model with appropriate input dimensions for audio and text features, specifying two output classes.\n",
        "Prepare Training Manager: Establish a training system to oversee the model’s learning and evaluation phases.\n",
        "\n",
        "\n",
        "7. Model Training\n",
        "\n",
        "Assign Labels: Categorize the data for binary classification:\n",
        "AD/Decline: Label as 1 (Diagnosis AD and Progression Decline)\n",
        "CN/No Decline: Label as 0 (Diagnosis CN, Progression No Decline, and Progression Test)\n",
        "\n",
        "\n",
        "Split Dataset: Divide the dataset into three subsets:\n",
        "Training: 64%\n",
        "Validation: 16%\n",
        "Testing: 20%\n",
        "Ensure balanced representation of labels across splits.\n",
        "\n",
        "\n",
        "Prepare Data for Training: Organize the data into structured sets and configure batches for efficient processing.\n",
        "Train the Model: Run the training process over several iterations (e.g., 5 epochs):\n",
        "Track performance metrics like loss and accuracy on training and validation sets.\n",
        "Save the model with the best validation performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8. Model Evaluation and Analysis\n",
        "\n",
        "Load Best Model: Retrieve the top-performing model from the training phase.\n",
        "Evaluate on Test Set: Assess the model’s performance using the test data, calculating metrics such as:\n",
        "Accuracy\n",
        "Precision\n",
        "Recall\n",
        "F1-score\n",
        "ROC AUC\n",
        "\n",
        "\n",
        "Analyze Semantic Relationships: Examine a subset of samples to understand how audio and text features interact, using similarity measures (e.g., cosine similarity).\n",
        "Assess Feature Importance: Determine the contribution of each model component (e.g., Graph Attention, Vision Transformer) by analyzing their output magnitudes.\n",
        "Generate Evaluation Report: Compile a detailed report including:\n",
        "Overall performance metrics\n",
        "Results broken down by category\n",
        "Analysis of misclassified samples\n",
        "Insights into high-confidence predictions\n",
        "\n",
        "\n",
        "Save Results: Store detailed results and a summary in accessible formats (e.g., CSV and JSON).\n"
      ],
      "metadata": {
        "id": "ffhktn8O-cF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from typing import Dict, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import from adress_analyzer (assumed to exist)\n",
        "from adress_analyzer import ADReSSoAnalyzer, extend_analyzer_with_model\n",
        "\n",
        "class ADReSSoPipelineCheckpointer:\n",
        "    def __init__(self, base_path: str, output_dir: str = \"/content/drive/MyDrive/Voice/extracted/ADReSSo21/checkpoints\"):\n",
        "        \"\"\"\n",
        "        Initialize the pipeline checkpointer for ADReSSo21 analysis.\n",
        "\n",
        "        Args:\n",
        "            base_path (str): Path to ADReSSo21 dataset.\n",
        "            output_dir (str): Directory to store checkpoints and outputs.\n",
        "        \"\"\"\n",
        "        self.base_path = base_path\n",
        "        self.output_dir = output_dir\n",
        "        self.checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n",
        "        self.feature_dir = os.path.join(output_dir, \"features\")\n",
        "        self.visualization_dir = os.path.join(output_dir, \"visualizations\")\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "        os.makedirs(self.feature_dir, exist_ok=True)\n",
        "        os.makedirs(self.visualization_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize analyzer\n",
        "        ExtendedAnalyzer = extend_analyzer_with_model()\n",
        "        self.analyzer = ExtendedAnalyzer(base_path=base_path)\n",
        "\n",
        "        # Checkpoint tracking\n",
        "        self.checkpoints = {}\n",
        "        self.step_outputs = {}\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def load_checkpoint(self, step: str) -> Optional[Dict]:\n",
        "        \"\"\"Load checkpoint for a specific step.\"\"\"\n",
        "        checkpoint_file = os.path.join(self.checkpoint_dir, f\"{step}_checkpoint.pkl\")\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            try:\n",
        "                with open(checkpoint_file, 'rb') as f:\n",
        "                    checkpoint = pickle.load(f)\n",
        "                    print(f\"Loaded {step} checkpoint with {len(checkpoint.get('data', {}))} items\")\n",
        "                    return checkpoint\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {step} checkpoint: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    def save_checkpoint(self, step: str, data: Dict):\n",
        "        \"\"\"Save checkpoint for a specific step.\"\"\"\n",
        "        checkpoint_file = os.path.join(self.checkpoint_dir, f\"{step}_checkpoint.pkl\")\n",
        "        try:\n",
        "            with open(checkpoint_file, 'wb') as f:\n",
        "                pickle.dump({'data': data}, f)\n",
        "            print(f\"Saved {step} checkpoint to {checkpoint_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {step} checkpoint: {str(e)}\")\n",
        "\n",
        "    def save_visualization(self, fig, step: str, filename: str):\n",
        "        \"\"\"Save visualization figure to disk.\"\"\"\n",
        "        output_path = os.path.join(self.visualization_dir, f\"{step}_{filename}.png\")\n",
        "        fig.savefig(output_path, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "        print(f\"Saved visualization to {output_path}\")\n",
        "\n",
        "    def step_0_load_data(self):\n",
        "        \"\"\"Step 0: Load audio files with checkpointing.\"\"\"\n",
        "        step = \"step_0_load_data\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'audio_files' in checkpoint['data']:\n",
        "            print(\"Using checkpointed audio files\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 0: Getting audio files...\")\n",
        "        audio_files = self.analyzer.get_audio_files()\n",
        "        self.step_outputs[step] = {'audio_files': audio_files}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Bar plot of file counts per category\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        categories = list(audio_files.keys())\n",
        "        counts = [len(files) for files in audio_files.values()]\n",
        "        sns.barplot(x=categories, y=counts, ax=ax)\n",
        "        ax.set_title(\"Number of Audio Files per Category\")\n",
        "        ax.set_xlabel(\"Category\")\n",
        "        ax.set_ylabel(\"File Count\")\n",
        "        self.save_visualization(fig, step, \"file_counts\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_1_demonstrate_features(self):\n",
        "        \"\"\"Step 1: Demonstrate acoustic features with checkpointing.\"\"\"\n",
        "        step = \"step_1_demonstrate_features\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'features' in checkpoint['data']:\n",
        "            print(\"Using checkpointed acoustic features demo\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 1: Demonstrating acoustic features...\")\n",
        "        audio_files = self.step_outputs['step_0_load_data']['audio_files']\n",
        "        sample_file = audio_files['diagnosis_ad'][0]  # First file from diagnosis_ad\n",
        "        features = self.analyzer.extract_acoustic_features(sample_file)\n",
        "        self.analyzer.show_acoustic_features(features, sample_file)\n",
        "        self.step_outputs[step] = {'features': features, 'sample_file': sample_file}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Plot sample eGeMAPS features\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        eGeMAPS = features.get('eGeMAPS', np.zeros(88))\n",
        "        ax.plot(eGeMAPS[:20])  # Plot first 20 features for clarity\n",
        "        ax.set_title(f\"eGeMAPS Features for {os.path.basename(sample_file)}\")\n",
        "        ax.set_xlabel(\"Feature Index\")\n",
        "        ax.set_ylabel(\"Feature Value\")\n",
        "        self.save_visualization(fig, step, \"eGeMAPS_features\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_2_extract_transcripts(self):\n",
        "        \"\"\"Step 2: Extract transcripts with checkpointing.\"\"\"\n",
        "        step = \"step_2_extract_transcripts\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'transcripts' in checkpoint['data']:\n",
        "            print(\"Using checkpointed transcripts\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 2: Extracting transcripts...\")\n",
        "        audio_files = self.step_outputs['step_0_load_data']['audio_files']\n",
        "        transcripts = self.analyzer.extract_transcripts(audio_files)\n",
        "        self.step_outputs[step] = {'transcripts': transcripts}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Histogram of transcript lengths\n",
        "        lengths = [len(t['text']) for t in transcripts.values()]\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.histplot(lengths, bins=30, ax=ax)\n",
        "        ax.set_title(\"Distribution of Transcript Lengths\")\n",
        "        ax.set_xlabel(\"Transcript Length (Characters)\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        self.save_visualization(fig, step, \"transcript_lengths\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_3_save_transcripts(self):\n",
        "        \"\"\"Step 3: Save transcripts with checkpointing.\"\"\"\n",
        "        step = \"step_3_save_transcripts\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'transcript_dir' in checkpoint['data']:\n",
        "            print(\"Using checkpointed transcript directory\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 3: Saving transcripts...\")\n",
        "        transcripts = self.step_outputs['step_2_extract_transcripts']['transcripts']\n",
        "        transcript_dir = os.path.join(self.output_dir, \"transcripts\")\n",
        "        os.makedirs(transcript_dir, exist_ok=True)\n",
        "        self.analyzer.save_transcripts(transcripts, transcript_dir)\n",
        "        self.step_outputs[step] = {'transcript_dir': transcript_dir}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Pie chart of transcript error status\n",
        "        has_errors = [t.get('has_error', False) for t in transcripts.values()]\n",
        "        error_counts = [sum(has_errors), len(has_errors) - sum(has_errors)]\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.pie(error_counts, labels=['Errors', 'No Errors'], autopct='%1.1f%%')\n",
        "        ax.set_title(\"Transcript Error Status\")\n",
        "        self.save_visualization(fig, step, \"transcript_errors\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_4_create_transcript_table(self):\n",
        "        \"\"\"Step 4: Create transcript summary table with checkpointing.\"\"\"\n",
        "        step = \"step_4_create_transcript_table\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'transcript_table' in checkpoint['data']:\n",
        "            print(\"Using checkpointed transcript table\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 4: Creating transcript table...\")\n",
        "        transcripts = self.step_outputs['step_2_extract_transcripts']['transcripts']\n",
        "        transcript_table = self.analyzer.create_transcript_table(transcripts)\n",
        "        table_path = os.path.join(self.output_dir, \"transcript_summary.csv\")\n",
        "        transcript_table.to_csv(table_path, index=False)\n",
        "        self.step_outputs[step] = {'transcript_table': transcript_table, 'table_path': table_path}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Box plot of word counts by category\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.boxplot(x='Category', y='Word_Count', data=transcript_table, ax=ax)\n",
        "        ax.set_title(\"Word Count Distribution by Category\")\n",
        "        ax.set_xlabel(\"Category\")\n",
        "        ax.set_ylabel(\"Word Count\")\n",
        "        plt.xticks(rotation=45)\n",
        "        self.save_visualization(fig, step, \"word_count_by_category\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_5_extract_linguistic_features(self):\n",
        "        \"\"\"Step 5: Extract linguistic features with checkpointing.\"\"\"\n",
        "        step = \"step_5_extract_linguistic_features\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'linguistic_features' in checkpoint['data']:\n",
        "            print(\"Using checkpointed linguistic features\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 5: Extracting linguistic features...\")\n",
        "        transcripts = self.step_outputs['step_2_extract_transcripts']['transcripts']\n",
        "        linguistic_features = self.analyzer.extract_linguistic_features(transcripts)\n",
        "        feature_path = os.path.join(self.output_dir, \"linguistic_features.pkl\")\n",
        "        with open(feature_path, 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "        self.step_outputs[step] = {'linguistic_features': linguistic_features, 'feature_path': feature_path}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Heatmap of BERT embedding correlations\n",
        "        sample_features = list(linguistic_features.values())[0]['bert_embeddings'][:10]\n",
        "        corr_matrix = np.corrcoef(sample_features)\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', ax=ax)\n",
        "        ax.set_title(\"Correlation of BERT Embeddings (Sample)\")\n",
        "        self.save_visualization(fig, step, \"bert_embedding_correlations\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_6_define_model(self):\n",
        "        \"\"\"Step 6: Define model architecture with checkpointing.\"\"\"\n",
        "        step = \"step_6_define_model\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'model_state_dict' in checkpoint['data']:\n",
        "            print(\"Using checkpointed model\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 6: Defining model architecture...\")\n",
        "        self.analyzer.step_6_define_model_architecture()\n",
        "        model_state_dict = self.analyzer.model.state_dict()\n",
        "        self.step_outputs[step] = {'model_state_dict': model_state_dict}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Bar plot of model parameter counts\n",
        "        param_counts = {'Graph Attention': 5000000, 'Vision Transformer': 85000000,\n",
        "                        'U-Net': 31000000, 'AlexNet': 57000000, 'BERT': 110000000}  # Placeholder\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        sns.barplot(x=list(param_counts.keys()), y=list(param_counts.values()), ax=ax)\n",
        "        ax.set_title(\"Model Parameter Counts by Component\")\n",
        "        ax.set_xlabel(\"Component\")\n",
        "        ax.set_ylabel(\"Parameter Count\")\n",
        "        plt.xticks(rotation=45)\n",
        "        self.save_visualization(fig, step, \"model_parameters\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_7_train_model(self, num_epochs: int = 5, batch_size: int = 4):\n",
        "        \"\"\"Step 7: Train model with checkpointing.\"\"\"\n",
        "        step = \"step_7_train_model\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'model_state_dict' in checkpoint['data']:\n",
        "            print(\"Using checkpointed trained model\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 7: Training model...\")\n",
        "        audio_files = self.step_outputs['step_0_load_data']['audio_files']\n",
        "        linguistic_features = self.step_outputs['step_5_extract_linguistic_features']['linguistic_features']\n",
        "        # Use checkpointed acoustic features if available\n",
        "        acoustic_features = self.load_checkpoint('step_8_extract_features') or {}\n",
        "        if not acoustic_features:\n",
        "            acoustic_features = {'data': self.analyzer.extract_acoustic_features_for_model(audio_files)}\n",
        "            self.save_checkpoint('step_8_extract_features', acoustic_features['data'])\n",
        "\n",
        "        # Prepare dataset (simplified, assumes ADReSSoDataset exists)\n",
        "        labels = {}\n",
        "        for category, files in audio_files.items():\n",
        "            for file in files:\n",
        "                file_id = f\"{category}_{os.path.basename(file)}\"\n",
        "                labels[file_id] = 1 if category in ['diagnosis_ad', 'progression_decline'] else 0\n",
        "        dataset = ADReSSoDataset(acoustic_features['data'], linguistic_features, labels, device=self.device)\n",
        "        train_size = int(0.64 * len(dataset))\n",
        "        val_size = int(0.16 * len(dataset))\n",
        "        test_size = len(dataset) - train_size - val_size\n",
        "        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                 collate_fn=lambda x: [item.to(self.device) for item in x])\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                collate_fn=lambda x: [item.to(self.device) for item in x])\n",
        "\n",
        "        # Train model\n",
        "        self.analyzer.model.to(self.device)\n",
        "        self.analyzer.step_7_train_model(num_epochs=num_epochs, batch_size=batch_size)\n",
        "        model_state_dict = self.analyzer.model.state_dict()\n",
        "        self.step_outputs[step] = {'model_state_dict': model_state_dict, 'train_loader': train_loader,\n",
        "                                  'val_loader': val_loader, 'test_dataset': test_dataset}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Training loss curve\n",
        "        # Placeholder: Assume trainer stores loss history\n",
        "        loss_history = [0.7, 0.5, 0.4, 0.35, 0.3]  # Placeholder\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        ax.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "        ax.set_title(\"Training Loss Curve\")\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.set_ylabel(\"Loss\")\n",
        "        self.save_visualization(fig, step, \"training_loss\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def step_8_evaluate_model(self):\n",
        "        \"\"\"Step 8: Evaluate model with checkpointing.\"\"\"\n",
        "        step = \"step_8_evaluate_model\"\n",
        "        checkpoint = self.load_checkpoint(step)\n",
        "        if checkpoint and 'evaluation_results' in checkpoint['data']:\n",
        "            print(\"Using checkpointed evaluation results\")\n",
        "            self.step_outputs[step] = checkpoint['data']\n",
        "            return checkpoint['data']\n",
        "\n",
        "        print(\"Step 8: Evaluating model...\")\n",
        "        test_dataset = self.step_outputs['step_7_train_model']['test_dataset']\n",
        "        test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,\n",
        "                                 collate_fn=lambda x: [item.to(self.device) for item in x])\n",
        "        evaluation_results = self.analyzer.step_8_evaluate_model()\n",
        "        results_path = os.path.join(self.output_dir, \"evaluation_results.csv\")\n",
        "        evaluation_results['metrics'].to_csv(results_path, index=False)\n",
        "        self.step_outputs[step] = {'evaluation_results': evaluation_results, 'results_path': results_path}\n",
        "        self.save_checkpoint(step, self.step_outputs[step])\n",
        "\n",
        "        # Visualization: Confusion matrix\n",
        "        # Placeholder: Assume metrics contain confusion matrix\n",
        "        conf_matrix = np.array([[30, 5], [3, 17]])  # Placeholder\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        ax.set_title(\"Confusion Matrix\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"True\")\n",
        "        self.save_visualization(fig, step, \"confusion_matrix\")\n",
        "\n",
        "        return self.step_outputs[step]\n",
        "\n",
        "    def run_pipeline_with_checkpoints(self, num_epochs: int = 5, batch_size: int = 4):\n",
        "        \"\"\"Run the complete pipeline with checkpointing.\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"RUNNING ADReSSo PIPELINE WITH CHECKPOINTING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        self.step_0_load_data()\n",
        "        self.step_1_demonstrate_features()\n",
        "        self.step_2_extract_transcripts()\n",
        "        self.step_3_save_transcripts()\n",
        "        self.step_4_create_transcript_table()\n",
        "        self.step_5_extract_linguistic_features()\n",
        "        self.step_6_define_model()\n",
        "        self.step_7_train_model(num_epochs, batch_size)\n",
        "        self.step_8_evaluate_model()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"PIPELINE COMPLETED!\")\n",
        "        print(\"=\"*80)\n",
        "        return self.step_outputs\n",
        "\n",
        "# Placeholder ADReSSoDataset class (assumed to exist in adress_analyzer)\n",
        "class ADReSSoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, acoustic_features, linguistic_features, labels, device):\n",
        "        self.acoustic_features = acoustic_features\n",
        "        self.linguistic_features = linguistic_features\n",
        "        self.labels = labels\n",
        "        self.file_ids = list(labels.keys())\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = self.file_ids[idx]\n",
        "        acoustic = self.acoustic_features.get(file_id, {})\n",
        "        linguistic = self.linguistic_features.get(file_id, {})\n",
        "        label = self.labels[file_id]\n",
        "        data = Data(\n",
        "            x=torch.tensor(acoustic.get('node_features', np.zeros(100)), dtype=torch.float).to(self.device),\n",
        "            edge_index=torch.tensor(acoustic.get('edge_index', np.zeros((2, 100))), dtype=torch.long).to(self.device),\n",
        "            acoustic_features=torch.tensor(acoustic.get('acoustic', np.zeros(1000)), dtype=torch.float).to(self.device),\n",
        "            linguistic_features=torch.tensor(linguistic.get('bert_embeddings', np.zeros(768)), dtype=torch.float).to(self.device),\n",
        "            y=torch.tensor(label, dtype=torch.long).to(self.device)\n",
        "        )\n",
        "        return data\n",
        "\n",
        "# Usage Example\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "    except ImportError:\n",
        "        print(\"Not running in Colab, skipping drive mount\")\n",
        "\n",
        "    # Install required packages\n",
        "    try:\n",
        "        import subprocess\n",
        "        packages = [\n",
        "            \"librosa\", \"soundfile\", \"opensmile\", \"speechbrain\",\n",
        "            \"transformers\", \"torch\", \"openai-whisper\",\n",
        "            \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"torch-geometric\"\n",
        "        ]\n",
        "        for pkg in packages:\n",
        "            subprocess.check_call([\"pip\", \"install\", pkg])\n",
        "        print(\"All required packages installed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error installing packages: {str(e)}\")\n",
        "\n",
        "    # Run pipeline\n",
        "    checkpointer = ADReSSoPipelineCheckpointer(base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\")\n",
        "    results = checkpointer.run_pipeline_with_checkpoints(num_epochs=5, batch_size=4)\n",
        "\n",
        "    print(\"\\nPipeline completed successfully!\")\n",
        "    print(f\"Check output directories:\")\n",
        "    print(f\"- Features: {checkpointer.feature_dir}\")\n",
        "    print(f\"- Checkpoints: {checkpointer.checkpoint_dir}\")\n",
        "    print(f\"- Visualizations: {checkpointer.visualization_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "40I5YrXH-0Ay",
        "outputId": "301b5895-13b1-4c95-c146-04062bd20208"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'adress_analyzer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-1037230834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Import from adress_analyzer (assumed to exist)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0madress_analyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mADReSSoAnalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend_analyzer_with_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mADReSSoPipelineCheckpointer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'adress_analyzer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojq4VBsc-5kD",
        "outputId": "1be3f109-2f70-47fc-a397-90d5ca2a14f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    }
  ]
}