{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMql+rglJJfGuP97kS0zB9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Graph_Attention_and_Feature_Fusion_for_Robust_Alzheimer%E2%80%99s_Disease_Diagnosis_from_the_ADReSSo21_Speech_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Final_output Folder, Install Dependencies, and Mount Google Drive"
      ],
      "metadata": {
        "id": "cDekvCObykj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5nT2A8hr5bSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1fwYEXVvv7FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa0ea00-2b08-4abd-fbd2-5391daf00078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created output directories at /content/drive/MyDrive/Final_output\n",
            "All required packages installed successfully\n",
            "Error mounting Google Drive: Mountpoint must not already contain files\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Create Final_output directory and subdirectories\n",
        "try:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/Final_output\"\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"visualizations\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"models\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"logs\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"transcripts\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"features\"), exist_ok=True)\n",
        "    print(f\"Created output directories at {OUTPUT_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating directories: {str(e)}\")\n",
        "\n",
        "# Install required packages\n",
        "try:\n",
        "    packages = [\n",
        "        \"librosa\", \"soundfile\", \"opensmile\", \"speechbrain\",\n",
        "        \"transformers\", \"torch\", \"openai-whisper\",\n",
        "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"torch-geometric\"\n",
        "    ]\n",
        "    for pkg in packages:\n",
        "        subprocess.check_call([\"pip\", \"install\", pkg])\n",
        "    print(\"All required packages installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error installing packages: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Define Error Logging"
      ],
      "metadata": {
        "id": "vhBfVvJF1LPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "import opensmile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "import whisper\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "# Error logging function\n",
        "def log_error(message: str):\n",
        "    log_file = os.path.join(\"/content/drive/MyDrive/Final_output\", \"logs\", \"pipeline_errors.log\")\n",
        "    with open(log_file, 'a') as f:\n",
        "        f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n",
        "    print(f\"Error logged: {message}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "HgQHjFBr1HkT",
        "outputId": "93bafd47-c435-42c5-cede-9fb0dbfe2518"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-4060791591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGATConv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_mean_pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWav2Vec2Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWav2Vec2Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2154\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2155\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessingKwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessorMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTokenizedInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_wav2vec2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWav2Vec2FeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_vision_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrender_jinja_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvideo_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideoMetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/video_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaddingMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_channel_dimension_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_channel_dimension_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m from .utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/feature_column/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseColumn\u001b[0m \u001b[0;31m# line: 1777\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureTransformationCache\u001b[0m \u001b[0;31m# line: 1962\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceDenseColumn\u001b[0m \u001b[0;31m# line: 1941\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfc_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_v2_types\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfc_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_tensor_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_tf_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mInputSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAddMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_arrays_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_distributed_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_eager_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0missparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;31m# For backward compatibility with v0.19.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;31m# Deprecated namespaces, to be removed in v2.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/csgraph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    185\u001b[0m            'NegativeCycleError']\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_laplacian\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlaplacian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m from ._shortest_path import (\n\u001b[1;32m    189\u001b[0m     \u001b[0mshortest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloyd_warshall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdijkstra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbellman_ford\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjohnson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/csgraph/_laplacian.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sputils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_pydata_sparse_to_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pydata_spmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/linalg/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dsolve\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_eigen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_matfuncs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_onenormest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/linalg/_eigen/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlobpcg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_svds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/linalg/_eigen/_svds.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0m_transition_to_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"random_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m def svds(A, k=6, ncv=None, tol=0, which='LM', v0=None,\n\u001b[1;32m    101\u001b[0m          \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_singular_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(fun)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreplace_doc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0mparameter_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'rng'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_docscrape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, role, doc, config)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mNumpyDocString\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_docscrape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, docstring, config)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_docscrape.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0msections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0msection_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msection\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msections\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_docscrape.py\u001b[0m in \u001b[0;36m_read_sections\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parse_param_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_element_is_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADReSSoAnalyzer Class"
      ],
      "metadata": {
        "id": "9KC1QaYN1lX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ADReSSoAnalyzer:\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content/drive/MyDrive/Final_output\"\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "        try:\n",
        "            self.smile = opensmile.Smile(\n",
        "                feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "                feature_level=opensmile.FeatureLevel.Functionals,\n",
        "            )\n",
        "            self.whisper_model = whisper.load_model(\"base\")\n",
        "            self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "            self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            self.bert_model = BertModel.from_pretrained('bert-base-uncased').to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        except Exception as e:\n",
        "            log_error(f\"Initialization error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        try:\n",
        "            audio_files = {\n",
        "                'diagnosis_ad': [], 'diagnosis_cn': [],\n",
        "                'progression_decline': [], 'progression_no_decline': [],\n",
        "                'progression_test': []\n",
        "            }\n",
        "            paths = {\n",
        "                'diagnosis_ad': f\"{self.base_path}/diagnosis/train/audio/ad\",\n",
        "                'diagnosis_cn': f\"{self.base_path}/diagnosis/train/audio/cn\",\n",
        "                'progression_decline': f\"{self.base_path}/progression/train/audio/decline\",\n",
        "                'progression_no_decline': f\"{self.base_path}/progression/train/audio/no_decline\",\n",
        "                'progression_test': f\"{self.base_path}/progression/test-dist/audio\"\n",
        "            }\n",
        "            for category, path in paths.items():\n",
        "                if os.path.exists(path):\n",
        "                    audio_files[category] = [f\"{path}/{f}\" for f in os.listdir(path) if f.endswith('.wav')]\n",
        "                else:\n",
        "                    log_error(f\"Path not found: {path}\")\n",
        "            return audio_files\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error in get_audio_files: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_acoustic_features_single(self, audio_path: str, sr=8000, extract_wav2vec=True) -> Tuple[str, Dict[str, Any]]:\n",
        "        features = {}\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=sr)\n",
        "            features['egemaps'] = self.smile.process_file(audio_path).values.flatten()\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            features['mfccs'] = {\n",
        "                'mean': np.mean(mfccs, axis=1),\n",
        "                'std': np.std(mfccs, axis=1),\n",
        "                'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "            }\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "            log_mel = librosa.power_to_db(mel_spec)\n",
        "            features['log_mel'] = {\n",
        "                'mean': np.mean(log_mel, axis=1),\n",
        "                'std': np.std(log_mel, axis=1)\n",
        "            }\n",
        "            if extract_wav2vec:\n",
        "                input_values = self.wav2vec_processor(y, sampling_rate=sr, return_tensors=\"pt\").input_values.to(self.wav2vec_model.device)\n",
        "                with torch.no_grad():\n",
        "                    wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().cpu().numpy()\n",
        "            else:\n",
        "                features['wav2vec2'] = np.zeros(768)\n",
        "            f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "            f0_clean = f0[f0 > 0]\n",
        "            features['prosodic'] = {\n",
        "                'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                'duration': len(y) / sr\n",
        "            }\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error extracting features for {audio_path}: {str(e)}\")\n",
        "            features = {\n",
        "                'egemaps': np.zeros(88),\n",
        "                'mfccs': {'mean': np.zeros(13), 'std': np.zeros(13), 'delta': np.zeros(13), 'delta2': np.zeros(13)},\n",
        "                'log_mel': {'mean': np.zeros(80), 'std': np.zeros(80)},\n",
        "                'wav2vec2': np.zeros(768),\n",
        "                'prosodic': {'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                             'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0, 'duration': 0.0}\n",
        "            }\n",
        "        return audio_path, features\n",
        "\n",
        "    def extract_acoustic_features(self, audio_files: Dict[str, List[str]], sample_fraction=0.5, extract_wav2vec=True):\n",
        "        print(\"Extracting acoustic features...\")\n",
        "        feature_dict = {}\n",
        "        checkpoint_file = os.path.join(self.output_path, \"features\", \"acoustic_features_checkpoint.pkl\")\n",
        "\n",
        "        # Load existing checkpoint\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                feature_dict = pickle.load(f)\n",
        "            print(f\"Loaded {len(feature_dict)} features from checkpoint\")\n",
        "\n",
        "        # Prepare files for processing\n",
        "        all_files = []\n",
        "        for category, files in audio_files.items():\n",
        "            sampled_files = files[:int(len(files) * sample_fraction)]\n",
        "            all_files.extend([(f, category) for f in sampled_files if f\"{category}_{os.path.basename(f)}\" not in feature_dict])\n",
        "\n",
        "        # Process files in parallel\n",
        "        with Pool(processes=4) as pool:\n",
        "            results = pool.starmap(self.extract_acoustic_features_single, [(f[0], 8000, extract_wav2vec) for f in all_files])\n",
        "\n",
        "        for audio_path, features in results:\n",
        "            category = next(c for c, files in audio_files.items() if audio_path in files)\n",
        "            file_id = f\"{category}_{os.path.basename(audio_path)}\"\n",
        "            feature_dict[file_id] = features\n",
        "\n",
        "        # Save checkpoint\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(feature_dict, f)\n",
        "        print(f\"Saved {len(feature_dict)} features to {checkpoint_file}\")\n",
        "        self.features = feature_dict\n",
        "        return feature_dict\n",
        "\n",
        "    def visualize_features(self, features: Dict, file_id: str):\n",
        "        try:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            mfcc_data = np.vstack([features['mfccs']['mean'], features['mfccs']['std'], features['mfccs']['delta'], features['mfccs']['delta2']])\n",
        "            sns.heatmap(mfcc_data, cmap='viridis')\n",
        "            plt.title(f'MFCC Features - {file_id} (Early AD Detection)')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('MFCC Type (Mean, Std, Delta, Delta2)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_mfcc.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(features['egemaps'], label='eGeMAPS')\n",
        "            plt.title(f'eGeMAPS Features - {file_id} (Early AD Detection)')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('Value')\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_egemaps.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.scatter(range(len(features['wav2vec2'][:100])), features['wav2vec2'][:100])\n",
        "            plt.title(f'Wav2Vec2 Features (First 100 dims) - {file_id}')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('Value')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_wav2vec2.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            prosodic_values = list(features['prosodic'].values())\n",
        "            prosodic_keys = list(features['prosodic'].keys())\n",
        "            plt.bar(prosodic_keys, prosodic_values)\n",
        "            plt.title(f'Prosodic Features - {file_id} (Progression Tracking)')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_prosodic.png'))\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error visualizing features for {file_id}: {str(e)}\")\n",
        "\n",
        "    def perform_eda(self, features_dict: Dict, transcripts: Dict):\n",
        "        try:\n",
        "            print(\"\\nPerforming Exploratory Data Analysis...\")\n",
        "            eda_data = []\n",
        "            for file_id, features in features_dict.items():\n",
        "                category = file_id.split('_')[0]\n",
        "                transcript = transcripts.get(file_id, {})\n",
        "                eda_data.append({\n",
        "                    'File_ID': file_id,\n",
        "                    'Category': category,\n",
        "                    'MFCC_Mean_0': features['mfccs']['mean'][0],\n",
        "                    'F0_Mean': features['prosodic']['f0_mean'],\n",
        "                    'F0_Std': features['prosodic']['f0_std'],\n",
        "                    'Lexical_Diversity': transcript.get('lexical_diversity', 0),\n",
        "                    'Word_Count': transcript.get('word_count', 0)\n",
        "                })\n",
        "            eda_df = pd.DataFrame(eda_data)\n",
        "            eda_df.to_csv(os.path.join(self.output_path, \"eda_summary.csv\"), index=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(x='Category', y='F0_Mean', data=eda_df)\n",
        "            plt.title('F0 Mean Distribution by Category (Early AD Detection)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', 'f0_mean_boxplot.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(x='Category', y='Lexical_Diversity', data=eda_df)\n",
        "            plt.title('Lexical Diversity by Category (Early AD Detection)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', 'lexical_diversity_boxplot.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.scatterplot(x='Word_Count', y='F0_Std', hue='Category', data=eda_df)\n",
        "            plt.title('Word Count vs F0 Std (Progression Tracking)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', 'word_count_f0_std_scatter.png'))\n",
        "            plt.close()\n",
        "\n",
        "            print(\"EDA visualizations saved to\", os.path.join(self.output_path, 'visualizations'))\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error in EDA: {str(e)}\")\n",
        "\n",
        "    def extract_transcripts(self, audio_files: Dict[str, List[str]]) -> Dict[str, str]:\n",
        "        transcripts = {}\n",
        "        print(\"Extracting transcripts...\")\n",
        "        checkpoint_file = os.path.join(self.output_path, \"transcripts\", \"transcripts_checkpoint.pkl\")\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                transcripts = pickle.load(f)\n",
        "            print(f\"Loaded {len(transcripts)} transcripts from checkpoint\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            files = files[:int(len(files) * 0.5)]  # Sample 50% of files\n",
        "            for file_path in files:\n",
        "                file_id = f\"{category}_{os.path.basename(file_path)}\"\n",
        "                if file_id in transcripts:\n",
        "                    continue\n",
        "                try:\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "                    transcripts[file_id] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': os.path.basename(file_path),\n",
        "                        'transcript': result[\"text\"].strip(),\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    log_error(f\"Error transcribing {file_id}: {str(e)}\")\n",
        "                    transcripts[file_id] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': os.path.basename(file_path),\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "        return transcripts\n",
        "\n",
        "    def save_transcripts(self, transcripts: Dict[str, str]):\n",
        "        for key, data in transcripts.items():\n",
        "            filename = f\"{key}_transcript.txt\"\n",
        "            filepath = os.path.join(self.output_path, \"transcripts\", filename)\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(data['transcript'])\n",
        "        with open(os.path.join(self.output_path, \"transcripts\", \"all_transcripts.json\"), 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "        with open(os.path.join(self.output_path, \"transcripts\", \"transcripts.pkl\"), 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "        print(f\"Transcripts saved to {os.path.join(self.output_path, 'transcripts')}\")\n",
        "\n",
        "    def create_transcript_table(self, transcripts: Dict[str, str]) -> pd.DataFrame:\n",
        "        data = []\n",
        "        for key, info in transcripts.items():\n",
        "            data.append({\n",
        "                'File_ID': key,\n",
        "                'Category': info['category'],\n",
        "                'Filename': info['filename'],\n",
        "                'Transcript_Length': len(info['transcript']),\n",
        "                'Word_Count': len(info['transcript'].split()) if info['transcript'] else 0,\n",
        "                'Language': info.get('language', 'N/A'),\n",
        "                'Segments': info.get('segments', 'N/A'),\n",
        "                'Has_Error': 'error' in info,\n",
        "                'Transcript_Preview': info['transcript'][:100] + \"...\" if len(info['transcript']) > 100 else info['transcript']\n",
        "            })\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(os.path.join(self.output_path, \"transcript_summary.csv\"), index=False)\n",
        "        return df\n",
        "\n",
        "    def extract_linguistic_features(self, transcripts: Dict[str, str]) -> Dict[str, Any]:\n",
        "        linguistic_features = {}\n",
        "        print(\"Extracting linguistic features...\")\n",
        "        for key, data in transcripts.items():\n",
        "            try:\n",
        "                transcript = data['transcript']\n",
        "                if not transcript:\n",
        "                    linguistic_features[key] = {\n",
        "                        'raw_text': '', 'word_count': 0, 'sentence_count': 0, 'avg_word_length': 0,\n",
        "                        'unique_words': 0, 'lexical_diversity': 0, 'bert_tokens': [],\n",
        "                        'bert_input_ids': [], 'bert_attention_mask': []\n",
        "                    }\n",
        "                    continue\n",
        "                words = transcript.split()\n",
        "                sentences = transcript.split('.')\n",
        "                bert_encoding = self.bert_tokenizer(\n",
        "                    transcript, truncation=True, padding='max_length', max_length=512, return_tensors='pt'\n",
        "                ).to(self.bert_model.device)\n",
        "                with torch.no_grad():\n",
        "                    bert_outputs = self.bert_model(**bert_encoding)\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': transcript,\n",
        "                    'word_count': len(words),\n",
        "                    'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                    'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                    'unique_words': len(set(words)),\n",
        "                    'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                    'bert_tokens': self.bert_tokenizer.tokenize(transcript),\n",
        "                    'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                    'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                    'bert_encoding': bert_outputs.last_hidden_state.cpu()\n",
        "                }\n",
        "            except Exception as e:\n",
        "                log_error(f\"Error extracting linguistic features for {key}: {str(e)}\")\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': '', 'word_count': 0, 'sentence_count': 0, 'avg_word_length': 0,\n",
        "                    'unique_words': 0, 'lexical_diversity': 0, 'bert_tokens': [],\n",
        "                    'bert_input_ids': [], 'bert_attention_mask': [], 'bert_encoding': None\n",
        "                }\n",
        "        with open(os.path.join(self.output_path, \"linguistic_features.pkl\"), 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "        return linguistic_features"
      ],
      "metadata": {
        "id": "Dzls6vu11eFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definitions"
      ],
      "metadata": {
        "id": "frgOMT9T11bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionModule(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, num_heads=8, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            GATConv(input_dim if i == 0 else hidden_dim, hidden_dim, heads=num_heads, dropout=0.2)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        for gat_layer in self.gat_layers:\n",
        "            x = gat_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "        else:\n",
        "            x = torch.mean(x, dim=0, keepdim=True)\n",
        "        return self.projection(x)\n",
        "\n",
        "class VisionTransformerModule(nn.Module):\n",
        "    def __init__(self, input_dim=80, patch_size=8, embed_dim=768, num_heads=12, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4, dropout=0.1, activation='gelu')\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(embed_dim, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
        "        num_patches = x.shape[1]\n",
        "        x = x + self.pos_embed[:, :num_patches, :]\n",
        "        x = x.transpose(0, 1)\n",
        "        x = self.transformer(x)\n",
        "        x = x.transpose(0, 1)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class UNetModule(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=128):\n",
        "        super().__init__()\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "        self.dec4 = self.upconv_block(1024, 512)\n",
        "        self.dec3 = self.upconv_block(512, 256)\n",
        "        self.dec2 = self.upconv_block(256, 128)\n",
        "        self.dec1 = self.upconv_block(128, 64)\n",
        "        self.final = nn.Conv1d(64, out_channels, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def conv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_ch, out_ch):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool1d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool1d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool1d(e3, 2))\n",
        "        b = self.bottleneck(F.max_pool1d(e4, 2))\n",
        "        d4 = self.dec4(b)\n",
        "        d3 = self.dec3(d4)\n",
        "        d2 = self.dec2(d3)\n",
        "        d1 = self.dec1(d2)\n",
        "        out = self.final(d1)\n",
        "        out = self.pool(out).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "class AlexNetModule(nn.Module):\n",
        "    def __init__(self, input_dim=768, num_classes=256):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class MultiModalADReSSoModel(nn.Module):\n",
        "    def __init__(self, audio_feature_dim=768, text_feature_dim=768, spectrogram_height=80, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.graph_attention = GraphAttentionModule(input_dim=text_feature_dim)\n",
        "        self.vision_transformer = VisionTransformerModule(input_dim=spectrogram_height)\n",
        "        self.unet = UNetModule()\n",
        "        self.alexnet = AlexNetModule(input_dim=audio_feature_dim)\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(256 + 256 + 128 + 256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def create_semantic_graph(self, text_features, audio_features):\n",
        "        batch_size = text_features.shape[0]\n",
        "        graphs = []\n",
        "        device = text_features.device\n",
        "        for i in range(batch_size):\n",
        "            text_feat = text_features[i].unsqueeze(0)\n",
        "            audio_feat = audio_features[i].unsqueeze(0)\n",
        "            node_features = torch.cat([text_feat, audio_feat], dim=0)\n",
        "            similarity = F.cosine_similarity(text_feat, audio_feat, dim=1)\n",
        "            edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long, device=device).t() if similarity.item() > 0.1 else torch.tensor([[0, 1], [0, 1]], dtype=torch.long, device=device).t()\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            graphs.append(graph)\n",
        "        return Batch.from_data_list(graphs)\n",
        "\n",
        "    def forward(self, audio_features, text_input_ids, text_attention_mask, spectrograms):\n",
        "        bert_outputs = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_features = bert_outputs.last_hidden_state.mean(dim=1)\n",
        "        graph_batch = self.create_semantic_graph(text_features, audio_features)\n",
        "        graph_out = self.graph_attention(graph_batch.x, graph_batch.edge_index, graph_batch.batch)\n",
        "        vit_out = self.vision_transformer(spectrograms)\n",
        "        audio_1d = audio_features.unsqueeze(1)\n",
        "        unet_out = self.unet(audio_1d)\n",
        "        alexnet_out = self.alexnet(audio_features)\n",
        "        fused_features = torch.cat([graph_out, vit_out, unet_out, alexnet_out], dim=1)\n",
        "        fused_features = self.fusion_layer(fused_features)\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "vr5KhBeI12t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extended Analyzer with Model Training"
      ],
      "metadata": {
        "id": "NwW8EH9w3KFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ADReSSoAnalyzerExtended(ADReSSoAnalyzer):\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        super().__init__(base_path)\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def train_individual_model(self, model, train_loader, val_loader, model_name, num_epochs=5):\n",
        "        trainer = ADReSSoTrainer(model)\n",
        "        trainer.train(train_loader, val_loader, num_epochs=num_epochs)\n",
        "        torch.save(model.state_dict(), os.path.join(self.output_path, 'models', f'{model_name}.pth'))\n",
        "        print(f\"Saved {model_name} to {os.path.join(self.output_path, 'models', f'{model_name}.pth')}\")\n",
        "        return trainer\n",
        "\n",
        "    def step_6_define_model_architecture(self):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 6: DEFINING MODEL ARCHITECTURE\")\n",
        "        print(\"=\"*60)\n",
        "        self.model = MultiModalADReSSoModel(audio_feature_dim=768, text_feature_dim=768, spectrogram_height=80, num_classes=2)\n",
        "        self.trainer = ADReSSoTrainer(self.model)\n",
        "        print(f\"Model initialized with {sum(p.numel() for p in self.model.parameters()):,} parameters\")\n",
        "        return self.model\n",
        "\n",
        "    def step_7_train_model(self, features_dict, linguistic_features, batch_size=4, num_epochs=5):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 7: TRAINING MODEL\")\n",
        "        print(\"=\"*60)\n",
        "        if self.model is None:\n",
        "            self.step_6_define_model_architecture()\n",
        "        labels = {fid: 1 if 'diagnosis_ad' in fid or 'progression_decline' in fid else 0 for fid in features_dict.keys()}\n",
        "        file_ids = list(features_dict.keys())\n",
        "        train_ids, test_ids = train_test_split(file_ids, test_size=0.2, stratify=[labels[f] for f in file_ids], random_state=42)\n",
        "        train_ids, val_ids = train_test_split(train_ids, test_size=0.2, stratify=[labels[f] for f in train_ids], random_state=42)\n",
        "        train_features = {fid: features_dict[fid] for fid in train_ids}\n",
        "        val_features = {fid: features_dict[fid] for fid in val_ids}\n",
        "        test_features = {fid: features_dict[fid] for fid in test_ids}\n",
        "        train_linguistic = {fid: linguistic_features[fid] for fid in train_ids}\n",
        "        val_linguistic = {fid: linguistic_features[fid] for fid in val_ids}\n",
        "        test_linguistic = {fid: linguistic_features[fid] for fid in test_ids}\n",
        "        train_labels = {fid: labels[fid] for fid in train_ids}\n",
        "        val_labels = {fid: labels[fid] for fid in val_ids}\n",
        "        test_labels = {fid: labels[fid] for fid in test_ids}\n",
        "        train_dataset = ADReSSoDataset(train_features, train_linguistic, train_labels)\n",
        "        val_dataset = ADReSSoDataset(val_features, val_linguistic, val_labels)\n",
        "        test_dataset = ADReSSoDataset(test_features, test_linguistic, test_labels)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        print(\"Training individual models...\")\n",
        "        self.train_individual_model(self.model.graph_attention, train_loader, val_loader, 'graph_attention')\n",
        "        self.train_individual_model(self.model.vision_transformer, train_loader, val_loader, 'vision_transformer')\n",
        "        self.train_individual_model(self.model.unet, train_loader, val_loader, 'unet')\n",
        "        self.train_individual_model(self.model.alexnet, train_loader, val_loader, 'alexnet')\n",
        "\n",
        "        print(\"\\nTraining fused model...\")\n",
        "        self.trainer.train(train_loader, val_loader, num_epochs=num_epochs)\n",
        "        self.test_loader = test_loader\n",
        "        return self.trainer\n",
        "\n",
        "    def step_8_evaluate_model(self, visualize_graphs=True, num_graph_samples=5):\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"STEP 8: MODEL EVALUATION AND SEMANTIC ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "        try:\n",
        "            self.model.load_state_dict(torch.load(os.path.join(self.output_path, 'models', 'best_adresso_model.pth')))\n",
        "            evaluation_results = self.trainer.evaluate_detailed(self.test_loader, class_names=['CN', 'AD'])\n",
        "            if visualize_graphs:\n",
        "                print(f\"\\nVisualizing semantic relationships for {num_graph_samples} samples...\")\n",
        "                self.trainer.visualize_semantic_relationships(self.test_loader, num_samples=num_graph_samples)\n",
        "            self.trainer.analyze_feature_importance()\n",
        "            self.generate_evaluation_report(evaluation_results)\n",
        "            return evaluation_results\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error in model evaluation: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def generate_evaluation_report(self, evaluation_results):\n",
        "        report = f\"\"\"\n",
        "=== EVALUATION REPORT ===\n",
        "Accuracy: {evaluation_results['accuracy']:.4f}\n",
        "Precision: {evaluation_results['precision']:.4f}\n",
        "Recall: {evaluation_results['recall']:.4f}\n",
        "F1-Score: {evaluation_results['f1']:.4f}\n",
        "ROC AUC: {evaluation_results['auc']:.4f if evaluation_results['auc'] is not None else 'N/A'}\n",
        "Confusion Matrix:\n",
        "{evaluation_results['confusion_matrix']}\n",
        "\"\"\"\n",
        "        with open(os.path.join(self.output_path, 'evaluation_report.txt'), 'w') as f:\n",
        "            f.write(report)\n",
        "        print(report)"
      ],
      "metadata": {
        "id": "0E7fu3t_2JlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Pipeline"
      ],
      "metadata": {
        "id": "w8docDff3vs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = ADReSSoAnalyzerExtended()\n",
        "print(\"=== ADReSSo21 Speech Analysis Pipeline ===\\n\")\n",
        "audio_files = analyzer.get_audio_files()\n",
        "total_files = sum(len(files) for files in audio_files.values())\n",
        "print(f\"Found {total_files} audio files across all categories\")\n",
        "for category, files in audio_files.items():\n",
        "    print(f\"  {category}: {len(files)} files\")\n",
        "\n",
        "if total_files == 0:\n",
        "    log_error(\"No audio files found. Please check the dataset path.\")\n",
        "else:\n",
        "    print(\"\\nStep 1: Extracting acoustic features...\")\n",
        "    features_dict = analyzer.extract_acoustic_features(audio_files, sample_fraction=0.5, extract_wav2vec=False)\n",
        "\n",
        "    print(\"\\nStep 2: Visualizing acoustic features for a sample...\")\n",
        "    for category, files in audio_files.items():\n",
        "        if files:\n",
        "            file_id = f\"{category}_{os.path.basename(files[0])}\"\n",
        "            analyzer.visualize_features(features_dict[file_id], file_id)\n",
        "            break\n",
        "\n",
        "    print(\"\\nStep 3: Extracting transcripts...\")\n",
        "    transcripts = analyzer.extract_transcripts(audio_files)\n",
        "\n",
        "    print(\"\\nStep 4: Saving transcripts...\")\n",
        "    analyzer.save_transcripts(transcripts)\n",
        "\n",
        "    print(\"\\nStep 5: Creating transcript table...\")\n",
        "    transcript_df = analyzer.create_transcript_table(transcripts)\n",
        "    print(\"Transcript Summary Table:\")\n",
        "    print(transcript_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\nStep 6: Extracting linguistic features...\")\n",
        "    linguistic_features = analyzer.extract_linguistic_features(transcripts)\n",
        "\n",
        "    print(\"\\nStep 7: Performing EDA...\")\n",
        "    analyzer.perform_eda(features_dict, linguistic_features)\n",
        "\n",
        "    print(\"\\nStep 8: Defining and training model...\")\n",
        "    trainer = analyzer.step_7_train_model(features_dict, linguistic_features, batch_size=4, num_epochs=5)\n",
        "\n",
        "    print(\"\\nStep 9: Evaluating model...\")\n",
        "    evaluation_results = analyzer.step_8_evaluate_model(visualize_graphs=True, num_graph_samples=5)\n",
        "\n",
        "    print(\"\\nPipeline completed successfully!\")"
      ],
      "metadata": {
        "id": "YpB1LGJU3pkm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}