{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnjSKWo12pJN1qG+OMQPkM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Graph_Attention_and_Feature_Fusion_for_Robust_Alzheimer%E2%80%99s_Disease_Diagnosis_from_the_ADReSSo21_Speech_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Final_output Folder, Install Dependencies, and Mount Google Drive"
      ],
      "metadata": {
        "id": "cDekvCObykj8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fwYEXVvv7FS"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Create Final_output directory and subdirectories\n",
        "try:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/Final_output\"\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"visualizations\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"models\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"logs\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"transcripts\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, \"features\"), exist_ok=True)\n",
        "    print(f\"Created output directories at {OUTPUT_DIR}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating directories: {str(e)}\")\n",
        "\n",
        "# Install required packages\n",
        "try:\n",
        "    packages = [\n",
        "        \"librosa\", \"soundfile\", \"opensmile\", \"speechbrain\",\n",
        "        \"transformers\", \"torch\", \"openai-whisper\",\n",
        "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"torch-geometric\"\n",
        "    ]\n",
        "    for pkg in packages:\n",
        "        subprocess.check_call([\"pip\", \"install\", pkg])\n",
        "    print(\"All required packages installed successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error installing packages: {str(e)}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Define Error Logging"
      ],
      "metadata": {
        "id": "vhBfVvJF1LPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import warnings\n",
        "import opensmile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import networkx as nx\n",
        "import whisper\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "\n",
        "# Error logging function\n",
        "def log_error(message: str):\n",
        "    log_file = os.path.join(\"/content/drive/MyDrive/Final_output\", \"logs\", \"pipeline_errors.log\")\n",
        "    with open(log_file, 'a') as f:\n",
        "        f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n",
        "    print(f\"Error logged: {message}\")"
      ],
      "metadata": {
        "id": "HgQHjFBr1HkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADReSSoAnalyzer Class"
      ],
      "metadata": {
        "id": "9KC1QaYN1lX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ADReSSoAnalyzer:\n",
        "    def __init__(self, base_path=\"/content/drive/MyDrive/Voice/extracted/ADReSSo21\"):\n",
        "        self.base_path = base_path\n",
        "        self.output_path = \"/content/drive/MyDrive/Final_output\"\n",
        "        self.features = {}\n",
        "        self.transcripts = {}\n",
        "        try:\n",
        "            self.smile = opensmile.Smile(\n",
        "                feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "                feature_level=opensmile.FeatureLevel.Functionals,\n",
        "            )\n",
        "            self.whisper_model = whisper.load_model(\"base\")\n",
        "            self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "            self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            self.bert_model = BertModel.from_pretrained('bert-base-uncased').to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        except Exception as e:\n",
        "            log_error(f\"Initialization error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_audio_files(self) -> Dict[str, List[str]]:\n",
        "        try:\n",
        "            audio_files = {\n",
        "                'diagnosis_ad': [], 'diagnosis_cn': [],\n",
        "                'progression_decline': [], 'progression_no_decline': [],\n",
        "                'progression_test': []\n",
        "            }\n",
        "            paths = {\n",
        "                'diagnosis_ad': f\"{self.base_path}/diagnosis/train/audio/ad\",\n",
        "                'diagnosis_cn': f\"{self.base_path}/diagnosis/train/audio/cn\",\n",
        "                'progression_decline': f\"{self.base_path}/progression/train/audio/decline\",\n",
        "                'progression_no_decline': f\"{self.base_path}/progression/train/audio/no_decline\",\n",
        "                'progression_test': f\"{self.base_path}/progression/test-dist/audio\"\n",
        "            }\n",
        "            for category, path in paths.items():\n",
        "                if os.path.exists(path):\n",
        "                    audio_files[category] = [f\"{path}/{f}\" for f in os.listdir(path) if f.endswith('.wav')]\n",
        "                else:\n",
        "                    log_error(f\"Path not found: {path}\")\n",
        "            return audio_files\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error in get_audio_files: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def extract_acoustic_features_single(self, audio_path: str, sr=8000, extract_wav2vec=True) -> Tuple[str, Dict[str, Any]]:\n",
        "        features = {}\n",
        "        try:\n",
        "            y, sr = librosa.load(audio_path, sr=sr)\n",
        "            features['egemaps'] = self.smile.process_file(audio_path).values.flatten()\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            features['mfccs'] = {\n",
        "                'mean': np.mean(mfccs, axis=1),\n",
        "                'std': np.std(mfccs, axis=1),\n",
        "                'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "            }\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)\n",
        "            log_mel = librosa.power_to_db(mel_spec)\n",
        "            features['log_mel'] = {\n",
        "                'mean': np.mean(log_mel, axis=1),\n",
        "                'std': np.std(log_mel, axis=1)\n",
        "            }\n",
        "            if extract_wav2vec:\n",
        "                input_values = self.wav2vec_processor(y, sampling_rate=sr, return_tensors=\"pt\").input_values.to(self.wav2vec_model.device)\n",
        "                with torch.no_grad():\n",
        "                    wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features['wav2vec2'] = torch.mean(wav2vec_features, dim=1).squeeze().cpu().numpy()\n",
        "            else:\n",
        "                features['wav2vec2'] = np.zeros(768)\n",
        "            f0 = librosa.yin(y, fmin=50, fmax=300, sr=sr)\n",
        "            f0_clean = f0[f0 > 0]\n",
        "            features['prosodic'] = {\n",
        "                'f0_mean': np.mean(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_std': np.std(f0_clean) if len(f0_clean) > 0 else 0.0,\n",
        "                'energy_mean': np.mean(librosa.feature.rms(y=y)),\n",
        "                'energy_std': np.std(librosa.feature.rms(y=y)),\n",
        "                'zero_crossing_rate': np.mean(librosa.feature.zero_crossing_rate(y)),\n",
        "                'spectral_centroid': np.mean(librosa.feature.spectral_centroid(y=y, sr=sr)),\n",
        "                'spectral_rolloff': np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr)),\n",
        "                'duration': len(y) / sr\n",
        "            }\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error extracting features for {audio_path}: {str(e)}\")\n",
        "            features = {\n",
        "                'egemaps': np.zeros(88),\n",
        "                'mfccs': {'mean': np.zeros(13), 'std': np.zeros(13), 'delta': np.zeros(13), 'delta2': np.zeros(13)},\n",
        "                'log_mel': {'mean': np.zeros(80), 'std': np.zeros(80)},\n",
        "                'wav2vec2': np.zeros(768),\n",
        "                'prosodic': {'f0_mean': 0.0, 'f0_std': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
        "                             'zero_crossing_rate': 0.0, 'spectral_centroid': 0.0, 'spectral_rolloff': 0.0, 'duration': 0.0}\n",
        "            }\n",
        "        return audio_path, features\n",
        "\n",
        "    def extract_acoustic_features(self, audio_files: Dict[str, List[str]], sample_fraction=0.5, extract_wav2vec=True):\n",
        "        print(\"Extracting acoustic features...\")\n",
        "        feature_dict = {}\n",
        "        checkpoint_file = os.path.join(self.output_path, \"features\", \"acoustic_features_checkpoint.pkl\")\n",
        "\n",
        "        # Load existing checkpoint\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                feature_dict = pickle.load(f)\n",
        "            print(f\"Loaded {len(feature_dict)} features from checkpoint\")\n",
        "\n",
        "        # Prepare files for processing\n",
        "        all_files = []\n",
        "        for category, files in audio_files.items():\n",
        "            sampled_files = files[:int(len(files) * sample_fraction)]\n",
        "            all_files.extend([(f, category) for f in sampled_files if f\"{category}_{os.path.basename(f)}\" not in feature_dict])\n",
        "\n",
        "        # Process files in parallel\n",
        "        with Pool(processes=4) as pool:\n",
        "            results = pool.starmap(self.extract_acoustic_features_single, [(f[0], 8000, extract_wav2vec) for f in all_files])\n",
        "\n",
        "        for audio_path, features in results:\n",
        "            category = next(c for c, files in audio_files.items() if audio_path in files)\n",
        "            file_id = f\"{category}_{os.path.basename(audio_path)}\"\n",
        "            feature_dict[file_id] = features\n",
        "\n",
        "        # Save checkpoint\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(feature_dict, f)\n",
        "        print(f\"Saved {len(feature_dict)} features to {checkpoint_file}\")\n",
        "        self.features = feature_dict\n",
        "        return feature_dict\n",
        "\n",
        "    def visualize_features(self, features: Dict, file_id: str):\n",
        "        try:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            mfcc_data = np.vstack([features['mfccs']['mean'], features['mfccs']['std'], features['mfccs']['delta'], features['mfccs']['delta2']])\n",
        "            sns.heatmap(mfcc_data, cmap='viridis')\n",
        "            plt.title(f'MFCC Features - {file_id} (Early AD Detection)')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('MFCC Type (Mean, Std, Delta, Delta2)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_mfcc.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(features['egemaps'], label='eGeMAPS')\n",
        "            plt.title(f'eGeMAPS Features - {file_id} (Early AD Detection)')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('Value')\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_egemaps.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.scatter(range(len(features['wav2vec2'][:100])), features['wav2vec2'][:100])\n",
        "            plt.title(f'Wav2Vec2 Features (First 100 dims) - {file_id}')\n",
        "            plt.xlabel('Feature Index')\n",
        "            plt.ylabel('Value')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_wav2vec2.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            prosodic_values = list(features['prosodic'].values())\n",
        "            prosodic_keys = list(features['prosodic'].keys())\n",
        "            plt.bar(prosodic_keys, prosodic_values)\n",
        "            plt.title(f'Prosodic Features - {file_id} (Progression Tracking)')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', f'{file_id}_prosodic.png'))\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error visualizing features for {file_id}: {str(e)}\")\n",
        "\n",
        "    def perform_eda(self, features_dict: Dict, transcripts: Dict):\n",
        "        try:\n",
        "            print(\"\\nPerforming Exploratory Data Analysis...\")\n",
        "            eda_data = []\n",
        "            for file_id, features in features_dict.items():\n",
        "                category = file_id.split('_')[0]\n",
        "                transcript = transcripts.get(file_id, {})\n",
        "                eda_data.append({\n",
        "                    'File_ID': file_id,\n",
        "                    'Category': category,\n",
        "                    'MFCC_Mean_0': features['mfccs']['mean'][0],\n",
        "                    'F0_Mean': features['prosodic']['f0_mean'],\n",
        "                    'F0_Std': features['prosodic']['f0_std'],\n",
        "                    'Lexical_Diversity': transcript.get('lexical_diversity', 0),\n",
        "                    'Word_Count': transcript.get('word_count', 0)\n",
        "                })\n",
        "            eda_df = pd.DataFrame(eda_data)\n",
        "            eda_df.to_csv(os.path.join(self.output_path, \"eda_summary.csv\"), index=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(x='Category', y='F0_Mean', data=eda_df)\n",
        "            plt.title('F0 Mean Distribution by Category (Early AD Detection)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', 'f0_mean_boxplot.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(x='Category', y='Lexical_Diversity', data=eda_df)\n",
        "            plt.title('Lexical Diversity by Category (Early AD Detection)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', 'lexical_diversity_boxplot.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.scatterplot(x='Word_Count', y='F0_Std', hue='Category', data=eda_df)\n",
        "            plt.title('Word Count vs F0 Std (Progression Tracking)')\n",
        "            plt.savefig(os.path.join(self.output_path, 'visualizations', 'word_count_f0_std_scatter.png'))\n",
        "            plt.close()\n",
        "\n",
        "            print(\"EDA visualizations saved to\", os.path.join(self.output_path, 'visualizations'))\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error in EDA: {str(e)}\")\n",
        "\n",
        "    def extract_transcripts(self, audio_files: Dict[str, List[str]]) -> Dict[str, str]:\n",
        "        transcripts = {}\n",
        "        print(\"Extracting transcripts...\")\n",
        "        checkpoint_file = os.path.join(self.output_path, \"transcripts\", \"transcripts_checkpoint.pkl\")\n",
        "        if os.path.exists(checkpoint_file):\n",
        "            with open(checkpoint_file, 'rb') as f:\n",
        "                transcripts = pickle.load(f)\n",
        "            print(f\"Loaded {len(transcripts)} transcripts from checkpoint\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            files = files[:int(len(files) * 0.5)]  # Sample 50% of files\n",
        "            for file_path in files:\n",
        "                file_id = f\"{category}_{os.path.basename(file_path)}\"\n",
        "                if file_id in transcripts:\n",
        "                    continue\n",
        "                try:\n",
        "                    result = self.whisper_model.transcribe(file_path)\n",
        "                    transcripts[file_id] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': os.path.basename(file_path),\n",
        "                        'transcript': result[\"text\"].strip(),\n",
        "                        'language': result.get('language', 'en'),\n",
        "                        'segments': len(result.get('segments', []))\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    log_error(f\"Error transcribing {file_id}: {str(e)}\")\n",
        "                    transcripts[file_id] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': os.path.basename(file_path),\n",
        "                        'transcript': \"\",\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "        with open(checkpoint_file, 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "        return transcripts\n",
        "\n",
        "    def save_transcripts(self, transcripts: Dict[str, str]):\n",
        "        for key, data in transcripts.items():\n",
        "            filename = f\"{key}_transcript.txt\"\n",
        "            filepath = os.path.join(self.output_path, \"transcripts\", filename)\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                f.write(data['transcript'])\n",
        "        with open(os.path.join(self.output_path, \"transcripts\", \"all_transcripts.json\"), 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "        with open(os.path.join(self.output_path, \"transcripts\", \"transcripts.pkl\"), 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "        print(f\"Transcripts saved to {os.path.join(self.output_path, 'transcripts')}\")\n",
        "\n",
        "    def create_transcript_table(self, transcripts: Dict[str, str]) -> pd.DataFrame:\n",
        "        data = []\n",
        "        for key, info in transcripts.items():\n",
        "            data.append({\n",
        "                'File_ID': key,\n",
        "                'Category': info['category'],\n",
        "                'Filename': info['filename'],\n",
        "                'Transcript_Length': len(info['transcript']),\n",
        "                'Word_Count': len(info['transcript'].split()) if info['transcript'] else 0,\n",
        "                'Language': info.get('language', 'N/A'),\n",
        "                'Segments': info.get('segments', 'N/A'),\n",
        "                'Has_Error': 'error' in info,\n",
        "                'Transcript_Preview': info['transcript'][:100] + \"...\" if len(info['transcript']) > 100 else info['transcript']\n",
        "            })\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(os.path.join(self.output_path, \"transcript_summary.csv\"), index=False)\n",
        "        return df\n",
        "\n",
        "    def extract_linguistic_features(self, transcripts: Dict[str, str]) -> Dict[str, Any]:\n",
        "        linguistic_features = {}\n",
        "        print(\"Extracting linguistic features...\")\n",
        "        for key, data in transcripts.items():\n",
        "            try:\n",
        "                transcript = data['transcript']\n",
        "                if not transcript:\n",
        "                    linguistic_features[key] = {\n",
        "                        'raw_text': '', 'word_count': 0, 'sentence_count': 0, 'avg_word_length': 0,\n",
        "                        'unique_words': 0, 'lexical_diversity': 0, 'bert_tokens': [],\n",
        "                        'bert_input_ids': [], 'bert_attention_mask': []\n",
        "                    }\n",
        "                    continue\n",
        "                words = transcript.split()\n",
        "                sentences = transcript.split('.')\n",
        "                bert_encoding = self.bert_tokenizer(\n",
        "                    transcript, truncation=True, padding='max_length', max_length=512, return_tensors='pt'\n",
        "                ).to(self.bert_model.device)\n",
        "                with torch.no_grad():\n",
        "                    bert_outputs = self.bert_model(**bert_encoding)\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': transcript,\n",
        "                    'word_count': len(words),\n",
        "                    'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                    'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                    'unique_words': len(set(words)),\n",
        "                    'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                    'bert_tokens': self.bert_tokenizer.tokenize(transcript),\n",
        "                    'bert_input_ids': bert_encoding['input_ids'].squeeze().tolist(),\n",
        "                    'bert_attention_mask': bert_encoding['attention_mask'].squeeze().tolist(),\n",
        "                    'bert_encoding': bert_outputs.last_hidden_state.cpu()\n",
        "                }\n",
        "            except Exception as e:\n",
        "                log_error(f\"Error extracting linguistic features for {key}: {str(e)}\")\n",
        "                linguistic_features[key] = {\n",
        "                    'raw_text': '', 'word_count': 0, 'sentence_count': 0, 'avg_word_length': 0,\n",
        "                    'unique_words': 0, 'lexical_diversity': 0, 'bert_tokens': [],\n",
        "                    'bert_input_ids': [], 'bert_attention_mask': [], 'bert_encoding': None\n",
        "                }\n",
        "        with open(os.path.join(self.output_path, \"linguistic_features.pkl\"), 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "        return linguistic_features"
      ],
      "metadata": {
        "id": "Dzls6vu11eFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}