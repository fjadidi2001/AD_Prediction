{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# config.py\n"
      ],
      "metadata": {
        "id": "JvqOFqpMaVks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4363DPnhaNED"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Configuration module for ADReSSo21 Speech Analysis\n",
        "Handles all paths, settings, and system configuration\n",
        "\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "import multiprocessing\n",
        "\n",
        "@dataclass\n",
        "class SystemConfig:\n",
        "    \"\"\"System resource configuration\"\"\"\n",
        "    n_cores: int = min(10, multiprocessing.cpu_count())  # Use available cores, max 10\n",
        "    max_workers: int = 8  # Leave some cores for system\n",
        "    chunk_size: int = 2  # Process files in chunks\n",
        "    memory_limit_gb: int = 30  # Leave 5GB for system from your 35GB\n",
        "\n",
        "@dataclass\n",
        "class PathConfig:\n",
        "    \"\"\"Path configuration for Windows 10\"\"\"\n",
        "    base_path: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\"\n",
        "    output_path: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\output\"\n",
        "\n",
        "    # Diagnosis paths\n",
        "    diagnosis_train_audio_ad: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\"\n",
        "    diagnosis_train_audio_cn: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\"\n",
        "    diagnosis_train_seg_ad: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\ad\"\n",
        "    diagnosis_train_seg_cn: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\cn\"\n",
        "\n",
        "    # Progression train paths\n",
        "    progression_train_audio_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\"\n",
        "    progression_train_audio_no_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\"\n",
        "    progression_train_seg_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\decline\"\n",
        "    progression_train_seg_no_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\no_decline\"\n",
        "\n",
        "    # Progression test paths\n",
        "    progression_test_audio: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\"\n",
        "    progression_test_seg: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\segmentation\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create output directory if it doesn't exist\"\"\"\n",
        "        Path(self.output_path).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"features\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"transcripts\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"models\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"logs\")).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model configuration\"\"\"\n",
        "    whisper_model_size: str = \"base\"  # base, small, medium, large\n",
        "    wav2vec_model: str = \"facebook/wav2vec2-base-960h\"\n",
        "    bert_model: str = \"bert-base-uncased\"\n",
        "    sampling_rate: int = 16000\n",
        "    max_sequence_length: int = 512\n",
        "\n",
        "@dataclass\n",
        "class FeatureConfig:\n",
        "    \"\"\"Feature extraction configuration\"\"\"\n",
        "    n_mfcc: int = 13\n",
        "    n_mels: int = 80\n",
        "    f0_min: float = 50.0\n",
        "    f0_max: float = 300.0\n",
        "    egemaps_feature_count: int = 88\n",
        "    wav2vec_feature_size: int = 768\n",
        "\n",
        "# Global configuration instances\n",
        "SYSTEM_CONFIG = SystemConfig()\n",
        "PATH_CONFIG = PathConfig()\n",
        "MODEL_CONFIG = ModelConfig()\n",
        "FEATURE_CONFIG = FeatureConfig()\n",
        "\n",
        "def get_audio_file_paths() -> Dict[str, List[str]]:\n",
        "    \"\"\"Get all audio file paths organized by category\"\"\"\n",
        "    audio_files = {\n",
        "        'diagnosis_ad': [],\n",
        "        'diagnosis_cn': [],\n",
        "        'progression_decline': [],\n",
        "        'progression_no_decline': [],\n",
        "        'progression_test': []\n",
        "    }\n",
        "\n",
        "    # Helper function to safely get files\n",
        "    def get_wav_files(path: str) -> List[str]:\n",
        "        if os.path.exists(path):\n",
        "            return [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.wav')]\n",
        "        return []\n",
        "\n",
        "    # Get diagnosis files\n",
        "    audio_files['diagnosis_ad'] = get_wav_files(PATH_CONFIG.diagnosis_train_audio_ad)\n",
        "    audio_files['diagnosis_cn'] = get_wav_files(PATH_CONFIG.diagnosis_train_audio_cn)\n",
        "\n",
        "    # Get progression files\n",
        "    audio_files['progression_decline'] = get_wav_files(PATH_CONFIG.progression_train_audio_decline)\n",
        "    audio_files['progression_no_decline'] = get_wav_files(PATH_CONFIG.progression_train_audio_no_decline)\n",
        "    audio_files['progression_test'] = get_wav_files(PATH_CONFIG.progression_test_audio)\n",
        "\n",
        "    return audio_files\n",
        "\n",
        "def print_system_info():\n",
        "    \"\"\"Print system configuration info\"\"\"\n",
        "    print(\"=== System Configuration ===\")\n",
        "    print(f\"CPU Cores Available: {multiprocessing.cpu_count()}\")\n",
        "    print(f\"Using Cores: {SYSTEM_CONFIG.n_cores}\")\n",
        "    print(f\"Max Workers: {SYSTEM_CONFIG.max_workers}\")\n",
        "    print(f\"Memory Limit: {SYSTEM_CONFIG.memory_limit_gb}GB\")\n",
        "    print(f\"Output Path: {PATH_CONFIG.output_path}\")\n",
        "    print(f\"Whisper Model: {MODEL_CONFIG.whisper_model_size}\")\n",
        "    print(\"=\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix config with cluade"
      ],
      "metadata": {
        "id": "kAmqRamPAfPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fixed Configuration module with better path handling\n",
        "\"\"\"\n",
        "import os\n",
        "import multiprocessing\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class SystemConfig:\n",
        "    \"\"\"System resource configuration\"\"\"\n",
        "    n_cores: int = min(10, multiprocessing.cpu_count())\n",
        "    max_workers: int = 8\n",
        "    chunk_size: int = 2\n",
        "    memory_limit_gb: int = 30\n",
        "\n",
        "@dataclass\n",
        "class PathConfig:\n",
        "    \"\"\"Path configuration for Windows 10\"\"\"\n",
        "    base_path: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\"\n",
        "    output_path: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\output\"\n",
        "\n",
        "    # Diagnosis paths\n",
        "    diagnosis_train_audio_ad: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\"\n",
        "    diagnosis_train_audio_cn: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\"\n",
        "    diagnosis_train_seg_ad: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\ad\"\n",
        "    diagnosis_train_seg_cn: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\cn\"\n",
        "\n",
        "    # Progression train paths\n",
        "    progression_train_audio_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\"\n",
        "    progression_train_audio_no_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\"\n",
        "    progression_train_seg_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\decline\"\n",
        "    progression_train_seg_no_decline: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\no_decline\"\n",
        "\n",
        "    # Progression test paths\n",
        "    progression_test_audio: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\"\n",
        "    progression_test_seg: str = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\segmentation\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create output directory if it doesn't exist\"\"\"\n",
        "        Path(self.output_path).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"features\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"transcripts\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"models\")).mkdir(parents=True, exist_ok=True)\n",
        "        Path(os.path.join(self.output_path, \"logs\")).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Model configuration\"\"\"\n",
        "    whisper_model_size: str = \"base\"\n",
        "    wav2vec_model: str = \"facebook/wav2vec2-base-960h\"\n",
        "    bert_model: str = \"bert-base-uncased\"\n",
        "    sampling_rate: int = 16000\n",
        "    max_sequence_length: int = 512\n",
        "\n",
        "@dataclass\n",
        "class FeatureConfig:\n",
        "    \"\"\"Feature extraction configuration\"\"\"\n",
        "    n_mfcc: int = 13\n",
        "    n_mels: int = 80\n",
        "    f0_min: float = 50.0\n",
        "    f0_max: float = 300.0\n",
        "    egemaps_feature_count: int = 88\n",
        "    wav2vec_feature_size: int = 768\n",
        "\n",
        "# Global configuration instances\n",
        "SYSTEM_CONFIG = SystemConfig()\n",
        "PATH_CONFIG = PathConfig()\n",
        "MODEL_CONFIG = ModelConfig()\n",
        "FEATURE_CONFIG = FeatureConfig()\n",
        "\n",
        "def get_wav_files_safe(path: str, logger: logging.Logger = None) -> List[str]:\n",
        "    \"\"\"Safely get WAV files from a directory with detailed logging\"\"\"\n",
        "    if logger is None:\n",
        "        logger = logging.getLogger(__name__)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        logger.warning(f\"Directory does not exist: {path}\")\n",
        "        return []\n",
        "\n",
        "    if not os.path.isdir(path):\n",
        "        logger.warning(f\"Path is not a directory: {path}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        all_files = os.listdir(path)\n",
        "        wav_files = [f for f in all_files if f.lower().endswith('.wav')]\n",
        "\n",
        "        if not wav_files:\n",
        "            logger.warning(f\"No WAV files found in: {path}\")\n",
        "            logger.info(f\"Directory contains {len(all_files)} files: {all_files[:10]}...\")\n",
        "            return []\n",
        "\n",
        "        # Return full paths\n",
        "        full_paths = [os.path.join(path, f) for f in wav_files]\n",
        "\n",
        "        # Validate each file\n",
        "        valid_files = []\n",
        "        for file_path in full_paths:\n",
        "            if os.path.isfile(file_path) and os.path.getsize(file_path) > 0:\n",
        "                valid_files.append(file_path)\n",
        "            else:\n",
        "                logger.warning(f\"Invalid file skipped: {file_path}\")\n",
        "\n",
        "        logger.info(f\"Found {len(valid_files)} valid WAV files in {path}\")\n",
        "        return valid_files\n",
        "\n",
        "    except PermissionError:\n",
        "        logger.error(f\"Permission denied accessing: {path}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading directory {path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_audio_file_paths(logger: logging.Logger = None) -> Dict[str, List[str]]:\n",
        "    \"\"\"Get all audio file paths organized by category with enhanced error handling\"\"\"\n",
        "    if logger is None:\n",
        "        logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"Scanning for audio files...\")\n",
        "\n",
        "    audio_files = {\n",
        "        'diagnosis_ad': [],\n",
        "        'diagnosis_cn': [],\n",
        "        'progression_decline': [],\n",
        "        'progression_no_decline': [],\n",
        "        'progression_test': []\n",
        "    }\n",
        "\n",
        "    # Mapping of categories to paths\n",
        "    path_mapping = {\n",
        "        'diagnosis_ad': PATH_CONFIG.diagnosis_train_audio_ad,\n",
        "        'diagnosis_cn': PATH_CONFIG.diagnosis_train_audio_cn,\n",
        "        'progression_decline': PATH_CONFIG.progression_train_audio_decline,\n",
        "        'progression_no_decline': PATH_CONFIG.progression_train_audio_no_decline,\n",
        "        'progression_test': PATH_CONFIG.progression_test_audio\n",
        "    }\n",
        "\n",
        "    # Scan each category\n",
        "    for category, directory_path in path_mapping.items():\n",
        "        logger.info(f\"Scanning {category}: {directory_path}\")\n",
        "        audio_files[category] = get_wav_files_safe(directory_path, logger)\n",
        "\n",
        "    # Log summary\n",
        "    total_files = sum(len(files) for files in audio_files.values())\n",
        "    logger.info(f\"Total audio files found: {total_files}\")\n",
        "\n",
        "    for category, files in audio_files.items():\n",
        "        if files:\n",
        "            logger.info(f\"  {category}: {len(files)} files\")\n",
        "        else:\n",
        "            logger.warning(f\"  {category}: No files found!\")\n",
        "\n",
        "    return audio_files\n",
        "\n",
        "def print_system_info():\n",
        "    \"\"\"Print system configuration info\"\"\"\n",
        "    print(\"=== System Configuration ===\")\n",
        "    print(f\"CPU Cores Available: {multiprocessing.cpu_count()}\")\n",
        "    print(f\"Using Cores: {SYSTEM_CONFIG.n_cores}\")\n",
        "    print(f\"Max Workers: {SYSTEM_CONFIG.max_workers}\")\n",
        "    print(f\"Memory Limit: {SYSTEM_CONFIG.memory_limit_gb}GB\")\n",
        "    print(f\"Output Path: {PATH_CONFIG.output_path}\")\n",
        "    print(f\"Whisper Model: {MODEL_CONFIG.whisper_model_size}\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "def verify_directories():\n",
        "    \"\"\"Verify all configured directories exist\"\"\"\n",
        "    print(\"=== Directory Verification ===\")\n",
        "\n",
        "    directories_to_check = [\n",
        "        (\"Base Path\", PATH_CONFIG.base_path),\n",
        "        (\"Output Path\", PATH_CONFIG.output_path),\n",
        "        (\"Diagnosis AD Audio\", PATH_CONFIG.diagnosis_train_audio_ad),\n",
        "        (\"Diagnosis CN Audio\", PATH_CONFIG.diagnosis_train_audio_cn),\n",
        "        (\"Progression Decline Audio\", PATH_CONFIG.progression_train_audio_decline),\n",
        "        (\"Progression No-Decline Audio\", PATH_CONFIG.progression_train_audio_no_decline),\n",
        "        (\"Progression Test Audio\", PATH_CONFIG.progression_test_audio),\n",
        "    ]\n",
        "\n",
        "    for name, path in directories_to_check:\n",
        "        exists = os.path.exists(path)\n",
        "        is_dir = os.path.isdir(path) if exists else False\n",
        "        print(f\"{name}: {'✓' if exists and is_dir else '✗'} {path}\")\n",
        "\n",
        "        if exists and is_dir:\n",
        "            try:\n",
        "                file_count = len([f for f in os.listdir(path) if f.lower().endswith('.wav')])\n",
        "                print(f\"  -> Contains {file_count} WAV files\")\n",
        "            except:\n",
        "                print(f\"  -> Cannot read directory contents\")\n",
        "\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the configuration\n",
        "    print_system_info()\n",
        "    verify_directories()\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Get and display audio files\n",
        "    audio_files = get_audio_file_paths(logger)\n",
        "\n",
        "    print(f\"\\n=== Audio Files Summary ===\")\n",
        "    for category, files in audio_files.items():\n",
        "        print(f\"{category}: {len(files)} files\")\n",
        "        if files:\n",
        "            print(f\"  First file: {files[0]}\")"
      ],
      "metadata": {
        "id": "NHqiLtSDAhw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils.py - Utilities Module\n"
      ],
      "metadata": {
        "id": "Cs2VfF3SalFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utilities module for ADReSSo21 Speech Analysis\n",
        "Common utilities, logging, and helper functions\n",
        "\"\"\"\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import logging\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Any, List, Union\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "from config import PATH_CONFIG, SYSTEM_CONFIG\n",
        "\n",
        "def setup_logging(log_level: str = \"INFO\") -> logging.Logger:\n",
        "    \"\"\"Setup logging configuration\"\"\"\n",
        "    log_dir = os.path.join(PATH_CONFIG.output_path, \"logs\")\n",
        "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = os.path.join(log_dir, f\"adresso_analysis_{timestamp}.log\")\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=getattr(logging, log_level.upper()),\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.FileHandler(log_file, encoding='utf-8'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger('ADReSSoAnalyzer')\n",
        "    logger.info(f\"Logging initialized. Log file: {log_file}\")\n",
        "    return logger\n",
        "\n",
        "def monitor_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Monitor current memory usage\"\"\"\n",
        "    memory = psutil.virtual_memory()\n",
        "    return {\n",
        "        'total_gb': memory.total / (1024**3),\n",
        "        'available_gb': memory.available / (1024**3),\n",
        "        'used_gb': memory.used / (1024**3),\n",
        "        'percent_used': memory.percent\n",
        "    }\n",
        "\n",
        "def cleanup_memory():\n",
        "    \"\"\"Force garbage collection and clear GPU memory if available\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def check_memory_limit(threshold_percent: float = 85.0) -> bool:\n",
        "    \"\"\"Check if memory usage is below threshold\"\"\"\n",
        "    memory_info = monitor_memory_usage()\n",
        "    return memory_info['percent_used'] < threshold_percent\n",
        "\n",
        "def safe_save_pickle(data: Any, filepath: str, logger: logging.Logger = None):\n",
        "    \"\"\"Safely save data as pickle with error handling\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        if logger:\n",
        "            logger.info(f\"Successfully saved pickle: {filepath}\")\n",
        "    except Exception as e:\n",
        "        if logger:\n",
        "            logger.error(f\"Failed to save pickle {filepath}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def safe_load_pickle(filepath: str, logger: logging.Logger = None) -> Any:\n",
        "    \"\"\"Safely load pickle with error handling\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        if logger:\n",
        "            logger.info(f\"Successfully loaded pickle: {filepath}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        if logger:\n",
        "            logger.error(f\"Failed to load pickle {filepath}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def safe_save_json(data: Dict, filepath: str, logger: logging.Logger = None):\n",
        "    \"\"\"Safely save data as JSON with error handling\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False, default=str)\n",
        "        if logger:\n",
        "            logger.info(f\"Successfully saved JSON: {filepath}\")\n",
        "    except Exception as e:\n",
        "        if logger:\n",
        "            logger.error(f\"Failed to save JSON {filepath}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def validate_audio_file(filepath: str) -> bool:\n",
        "    \"\"\"Validate if audio file exists and is readable\"\"\"\n",
        "    return os.path.exists(filepath) and os.path.getsize(filepath) > 0\n",
        "\n",
        "def get_file_info(filepath: str) -> Dict[str, Any]:\n",
        "    \"\"\"Get basic file information\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        return {'exists': False}\n",
        "\n",
        "    stat = os.stat(filepath)\n",
        "    return {\n",
        "        'exists': True,\n",
        "        'size_mb': stat.st_size / (1024**2),\n",
        "        'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),\n",
        "        'filename': os.path.basename(filepath),\n",
        "        'extension': os.path.splitext(filepath)[1]\n",
        "    }\n",
        "\n",
        "def create_progress_bar(total: int, desc: str = \"Processing\") -> Any:\n",
        "    \"\"\"Create a progress bar for batch processing\"\"\"\n",
        "    try:\n",
        "        from tqdm import tqdm\n",
        "        return tqdm(total=total, desc=desc, unit=\"files\")\n",
        "    except ImportError:\n",
        "        # Fallback simple counter if tqdm not available\n",
        "        class SimpleProgress:\n",
        "            def __init__(self, total, desc):\n",
        "                self.total = total\n",
        "                self.current = 0\n",
        "                self.desc = desc\n",
        "\n",
        "            def update(self, n=1):\n",
        "                self.current += n\n",
        "                print(f\"\\r{self.desc}: {self.current}/{self.total}\", end=\"\")\n",
        "\n",
        "            def close(self):\n",
        "                print()  # New line\n",
        "\n",
        "        return SimpleProgress(total, desc)\n",
        "\n",
        "def batch_generator(items: List[Any], batch_size: int):\n",
        "    \"\"\"Generate batches from a list of items\"\"\"\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        yield items[i:i + batch_size]\n",
        "\n",
        "def flatten_dict(d: Dict, parent_key: str = '', sep: str = '_') -> Dict:\n",
        "    \"\"\"Flatten nested dictionary\"\"\"\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
        "        elif isinstance(v, np.ndarray):\n",
        "            # Convert numpy arrays to lists for JSON serialization\n",
        "            items.append((new_key, v.tolist() if v.ndim > 0 else float(v)))\n",
        "        else:\n",
        "            items.append((new_key, v))\n",
        "    return dict(items)\n",
        "\n",
        "def create_summary_dataframe(results: Dict[str, Any], save_path: str = None) -> pd.DataFrame:\n",
        "    \"\"\"Create a summary DataFrame from results dictionary\"\"\"\n",
        "    data = []\n",
        "\n",
        "    for key, result in results.items():\n",
        "        if isinstance(result, dict):\n",
        "            # Flatten the result dictionary\n",
        "            flat_result = flatten_dict(result)\n",
        "            flat_result['file_id'] = key\n",
        "            data.append(flat_result)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    if save_path:\n",
        "        df.to_csv(save_path, index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "def log_processing_stats(processed: int, failed: int, total: int, logger: logging.Logger):\n",
        "    \"\"\"Log processing statistics\"\"\"\n",
        "    success_rate = (processed / total * 100) if total > 0 else 0\n",
        "    logger.info(f\"Processing completed: {processed}/{total} successful ({success_rate:.1f}%)\")\n",
        "    if failed > 0:\n",
        "        logger.warning(f\"Failed files: {failed}\")\n",
        "\n",
        "class ProcessingTimer:\n",
        "    \"\"\"Context manager for timing operations\"\"\"\n",
        "\n",
        "    def __init__(self, operation_name: str, logger: logging.Logger = None):\n",
        "        self.operation_name = operation_name\n",
        "        self.logger = logger\n",
        "        self.start_time = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start_time = datetime.now()\n",
        "        if self.logger:\n",
        "            self.logger.info(f\"Starting {self.operation_name}...\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        end_time = datetime.now()\n",
        "        duration = end_time - self.start_time\n",
        "\n",
        "        if self.logger:\n",
        "            self.logger.info(f\"Completed {self.operation_name} in {duration}\")\n",
        "\n",
        "        if exc_type:\n",
        "            if self.logger:\n",
        "                self.logger.error(f\"Error in {self.operation_name}: {exc_val}\")\n",
        "\n",
        "def ensure_directory_exists(directory_path: str):\n",
        "    \"\"\"Ensure directory exists, create if not\"\"\"\n",
        "    Path(directory_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_available_models() -> Dict[str, bool]:\n",
        "    \"\"\"Check which models are available/working\"\"\"\n",
        "    models_status = {\n",
        "        'whisper': False,\n",
        "        'wav2vec2': False,\n",
        "        'bert': False,\n",
        "        'opensmile': False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        import whisper\n",
        "        models_status['whisper'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "        models_status['wav2vec2'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        from transformers import BertTokenizer, BertModel\n",
        "        models_status['bert'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import opensmile\n",
        "        models_status['opensmile'] = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    return models_status"
      ],
      "metadata": {
        "id": "2UlUQUdjaiC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# acoustic_features_service.py - Acoustic Features Extraction Service\n"
      ],
      "metadata": {
        "id": "P6QiFmxSbnzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Acoustic Features Service - Microservice for extracting acoustic features\n",
        "Handles eGeMAPS, MFCC, Log-mel, Wav2Vec2, and prosodic features\n",
        "\"\"\"\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import warnings\n",
        "from typing import Dict, Any, List, Tuple, Optional\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Model imports with error handling\n",
        "try:\n",
        "    import opensmile\n",
        "    OPENSMILE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    OPENSMILE_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "    WAV2VEC_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WAV2VEC_AVAILABLE = False\n",
        "\n",
        "from config import MODEL_CONFIG, FEATURE_CONFIG, SYSTEM_CONFIG\n",
        "from utils import setup_logging, monitor_memory_usage, cleanup_memory, safe_save_pickle\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "@dataclass\n",
        "class AcousticFeatures:\n",
        "    \"\"\"Data class to hold acoustic features\"\"\"\n",
        "    egemaps: np.ndarray\n",
        "    mfccs: Dict[str, np.ndarray]\n",
        "    log_mel: Dict[str, np.ndarray]\n",
        "    wav2vec2: np.ndarray\n",
        "    prosodic: Dict[str, float]\n",
        "    extraction_success: Dict[str, bool]\n",
        "\n",
        "class AcousticFeaturesService:\n",
        "    \"\"\"Service for extracting acoustic features from audio files\"\"\"\n",
        "\n",
        "    def __init__(self, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or setup_logging()\n",
        "        self.smile = None\n",
        "        self.wav2vec_processor = None\n",
        "        self.wav2vec_model = None\n",
        "\n",
        "        self._initialize_models()\n",
        "\n",
        "    def _initialize_models(self):\n",
        "        \"\"\"Initialize feature extraction models\"\"\"\n",
        "        self.logger.info(\"Initializing acoustic feature extraction models...\")\n",
        "\n",
        "        # Initialize OpenSMILE for eGeMAPS\n",
        "        if OPENSMILE_AVAILABLE:\n",
        "            try:\n",
        "                self.smile = opensmile.Smile(\n",
        "                    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
        "                    feature_level=opensmile.FeatureLevel.Functionals,\n",
        "                )\n",
        "                self.logger.info(\"✓ OpenSMILE (eGeMAPS) initialized\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to initialize OpenSMILE: {e}\")\n",
        "                self.smile = None\n",
        "        else:\n",
        "            self.logger.warning(\"OpenSMILE not available - eGeMAPS features will be skipped\")\n",
        "\n",
        "        # Initialize Wav2Vec2\n",
        "        if WAV2VEC_AVAILABLE:\n",
        "            try:\n",
        "                self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(MODEL_CONFIG.wav2vec_model)\n",
        "                self.wav2vec_model = Wav2Vec2Model.from_pretrained(MODEL_CONFIG.wav2vec_model)\n",
        "                self.logger.info(\"✓ Wav2Vec2 initialized\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to initialize Wav2Vec2: {e}\")\n",
        "                self.wav2vec_processor = None\n",
        "                self.wav2vec_model = None\n",
        "        else:\n",
        "            self.logger.warning(\"Transformers not available - Wav2Vec2 features will be skipped\")\n",
        "\n",
        "    def extract_egemaps_features(self, audio_path: str) -> Tuple[np.ndarray, bool]:\n",
        "        \"\"\"Extract eGeMAPS features using OpenSMILE\"\"\"\n",
        "        try:\n",
        "            if self.smile is None:\n",
        "                return np.zeros(FEATURE_CONFIG.egemaps_feature_count), False\n",
        "\n",
        "            features = self.smile.process_file(audio_path).values.flatten()\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"eGeMAPS extraction failed for {os.path.basename(audio_path)}: {e}\")\n",
        "            return np.zeros(FEATURE_CONFIG.egemaps_feature_count), False\n",
        "\n",
        "    def extract_mfcc_features(self, y: np.ndarray, sr: int) -> Tuple[Dict[str, np.ndarray], bool]:\n",
        "        \"\"\"Extract MFCC features and their derivatives\"\"\"\n",
        "        try:\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=FEATURE_CONFIG.n_mfcc)\n",
        "\n",
        "            features = {\n",
        "                'mean': np.mean(mfccs, axis=1),\n",
        "                'std': np.std(mfccs, axis=1),\n",
        "                'delta': np.mean(librosa.feature.delta(mfccs), axis=1),\n",
        "                'delta2': np.mean(librosa.feature.delta(mfccs, order=2), axis=1)\n",
        "            }\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"MFCC extraction failed: {e}\")\n",
        "            default_features = {\n",
        "                'mean': np.zeros(FEATURE_CONFIG.n_mfcc),\n",
        "                'std': np.zeros(FEATURE_CONFIG.n_mfcc),\n",
        "                'delta': np.zeros(FEATURE_CONFIG.n_mfcc),\n",
        "                'delta2': np.zeros(FEATURE_CONFIG.n_mfcc)\n",
        "            }\n",
        "            return default_features, False\n",
        "\n",
        "    def extract_logmel_features(self, y: np.ndarray, sr: int) -> Tuple[Dict[str, np.ndarray], bool]:\n",
        "        \"\"\"Extract log-mel spectrogram features\"\"\"\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=FEATURE_CONFIG.n_mels)\n",
        "            log_mel = librosa.power_to_db(mel_spec)\n",
        "\n",
        "            features = {\n",
        "                'mean': np.mean(log_mel, axis=1),\n",
        "                'std': np.std(log_mel, axis=1)\n",
        "            }\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Log-mel extraction failed: {e}\")\n",
        "            default_features = {\n",
        "                'mean': np.zeros(FEATURE_CONFIG.n_mels),\n",
        "                'std': np.zeros(FEATURE_CONFIG.n_mels)\n",
        "            }\n",
        "            return default_features, False\n",
        "\n",
        "    def extract_wav2vec_features(self, y: np.ndarray, sr: int) -> Tuple[np.ndarray, bool]:\n",
        "        \"\"\"Extract Wav2Vec2 features\"\"\"\n",
        "        try:\n",
        "            if self.wav2vec_processor is None or self.wav2vec_model is None:\n",
        "                return np.zeros(FEATURE_CONFIG.wav2vec_feature_size), False\n",
        "\n",
        "            if len(y) == 0:\n",
        "                raise ValueError(\"Empty audio signal\")\n",
        "\n",
        "            # Ensure correct sampling rate\n",
        "            if sr != MODEL_CONFIG.sampling_rate:\n",
        "                y = librosa.resample(y, orig_sr=sr, target_sr=MODEL_CONFIG.sampling_rate)\n",
        "                sr = MODEL_CONFIG.sampling_rate\n",
        "\n",
        "            input_values = self.wav2vec_processor(\n",
        "                y,\n",
        "                sampling_rate=sr,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_values\n",
        "\n",
        "            with torch.no_grad():\n",
        "                wav2vec_features = self.wav2vec_model(input_values).last_hidden_state\n",
        "                features = torch.mean(wav2vec_features, dim=1).squeeze().numpy()\n",
        "\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Wav2Vec2 extraction failed: {e}\")\n",
        "            return np.zeros(FEATURE_CONFIG.wav2vec_feature_size), False\n",
        "\n",
        "    def extract_prosodic_features(self, y: np.ndarray, sr: int) -> Tuple[Dict[str, float], bool]:\n",
        "        \"\"\"Extract prosodic features\"\"\"\n",
        "        try:\n",
        "            # F0 extraction\n",
        "            f0 = librosa.yin(y, fmin=FEATURE_CONFIG.f0_min, fmax=FEATURE_CONFIG.f0_max, sr=sr)\n",
        "            f0_clean = f0[f0 > 0]  # Remove unvoiced frames\n",
        "\n",
        "            # Energy features\n",
        "            rms = librosa.feature.rms(y=y)\n",
        "\n",
        "            # Spectral features\n",
        "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "            zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
        "\n",
        "            features = {\n",
        "                'f0_mean': float(np.mean(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_std': float(np.std(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_median': float(np.median(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'f0_range': float(np.max(f0_clean) - np.min(f0_clean)) if len(f0_clean) > 0 else 0.0,\n",
        "                'energy_mean': float(np.mean(rms)),\n",
        "                'energy_std': float(np.std(rms)),\n",
        "                'zero_crossing_rate': float(np.mean(zero_crossing_rate)),\n",
        "                'spectral_centroid': float(np.mean(spectral_centroid)),\n",
        "                'spectral_rolloff': float(np.mean(spectral_rolloff)),\n",
        "                'duration': len(y) / sr,\n",
        "                'voicing_fraction': len(f0_clean) / len(f0) if len(f0) > 0 else 0.0\n",
        "            }\n",
        "\n",
        "            return features, True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Prosodic feature extraction failed: {e}\")\n",
        "            default_features = {\n",
        "                'f0_mean': 0.0, 'f0_std': 0.0, 'f0_median': 0.0, 'f0_range': 0.0,\n",
        "                'energy_mean': 0.0, 'energy_std': 0.0, 'zero_crossing_rate': 0.0,\n",
        "                'spectral_centroid': 0.0, 'spectral_rolloff': 0.0,\n",
        "                'duration': 0.0, 'voicing_fraction': 0.0\n",
        "            }\n",
        "            return default_features, False\n",
        "\n",
        "    def extract_features_from_file(self, audio_path: str) -> Optional[AcousticFeatures]:\n",
        "        \"\"\"Extract all acoustic features from a single audio file\"\"\"\n",
        "        try:\n",
        "            # Load audio file\n",
        "            y, sr = librosa.load(audio_path, sr=MODEL_CONFIG.sampling_rate)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                self.logger.warning(f\"Empty audio file: {audio_path}\")\n",
        "                return None\n",
        "\n",
        "            # Extract all features\n",
        "            egemaps, egemaps_success = self.extract_egemaps_features(audio_path)\n",
        "            mfccs, mfccs_success = self.extract_mfcc_features(y, sr)\n",
        "            log_mel, logmel_success = self.extract_logmel_features(y, sr)\n",
        "            wav2vec2, wav2vec_success = self.extract_wav2vec_features(y, sr)\n",
        "            prosodic, prosodic_success = self.extract_prosodic_features(y, sr)\n",
        "\n",
        "            features = AcousticFeatures(\n",
        "                egemaps=egemaps,\n",
        "                mfccs=mfccs,\n",
        "                log_mel=log_mel,\n",
        "                wav2vec2=wav2vec2,\n",
        "                prosodic=prosodic,\n",
        "                extraction_success={\n",
        "                    'egemaps': egemaps_success,\n",
        "                    'mfccs': mfccs_success,\n",
        "                    'log_mel': logmel_success,\n",
        "                    'wav2vec2': wav2vec_success,\n",
        "                    'prosodic': prosodic_success\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to extract features from {audio_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_files_batch(self, file_paths: List[str]) -> Dict[str, Optional[AcousticFeatures]]:\n",
        "        \"\"\"Process a batch of audio files\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            filename = os.path.basename(file_path)\n",
        "            try:\n",
        "                features = self.extract_features_from_file(file_path)\n",
        "                results[filename] = features\n",
        "\n",
        "                # Memory cleanup for large batches\n",
        "                if len(results) % 10 == 0:\n",
        "                    cleanup_memory()\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error processing {filename}: {e}\")\n",
        "                results[filename] = None\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_features_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Dict[str, Optional[AcousticFeatures]]]:\n",
        "        \"\"\"Extract features from all audio files using parallel processing\"\"\"\n",
        "        self.logger.info(\"Starting parallel acoustic feature extraction...\")\n",
        "\n",
        "        all_results = {}\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed_files = 0\n",
        "\n",
        "        for category, file_paths in audio_files.items():\n",
        "            if not file_paths:\n",
        "                continue\n",
        "\n",
        "            self.logger.info(f\"Processing {category}: {len(file_paths)} files\")\n",
        "            category_results = {}\n",
        "\n",
        "            # Process files in batches to manage memory\n",
        "            batch_size = SYSTEM_CONFIG.chunk_size\n",
        "            batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
        "\n",
        "            with ProcessPoolExecutor(max_workers=SYSTEM_CONFIG.max_workers) as executor:\n",
        "                # Submit batch jobs\n",
        "                future_to_batch = {\n",
        "                    executor.submit(process_audio_batch_worker, batch): batch\n",
        "                    for batch in batches\n",
        "                }\n",
        "\n",
        "                # Collect results\n",
        "                for future in as_completed(future_to_batch):\n",
        "                    batch = future_to_batch[future]\n",
        "                    try:\n",
        "                        batch_results = future.result()\n",
        "                        category_results.update(batch_results)\n",
        "                        processed_files += len(batch)\n",
        "\n",
        "                        # Log progress\n",
        "                        progress = (processed_files / total_files) * 100\n",
        "                        self.logger.info(f\"Progress: {processed_files}/{total_files} ({progress:.1f}%)\")\n",
        "\n",
        "                        # Check memory usage\n",
        "                        memory_info = monitor_memory_usage()\n",
        "                        if memory_info['percent_used'] > 80:\n",
        "                            self.logger.warning(f\"High memory usage: {memory_info['percent_used']:.1f}%\")\n",
        "                            cleanup_memory()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        self.logger.error(f\"Batch processing failed: {e}\")\n",
        "\n",
        "            all_results[category] = category_results\n",
        "            self.logger.info(f\"Completed {category}: {len(category_results)} files processed\")\n",
        "\n",
        "        # Final cleanup\n",
        "        cleanup_memory()\n",
        "        self.logger.info(f\"Acoustic feature extraction completed: {processed_files}/{total_files} files\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def save_features(self, features: Dict[str, Dict[str, Optional[AcousticFeatures]]], output_dir: str):\n",
        "        \"\"\"Save extracted features to disk\"\"\"\n",
        "        self.logger.info(\"Saving acoustic features...\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save features by category\n",
        "        for category, category_features in features.items():\n",
        "            category_path = os.path.join(output_dir, f\"acoustic_features_{category}.pkl\")\n",
        "\n",
        "            # Convert AcousticFeatures objects to dictionaries for serialization\n",
        "            serializable_features = {}\n",
        "            for filename, feature_obj in category_features.items():\n",
        "                if feature_obj is not None:\n",
        "                    serializable_features[filename] = {\n",
        "                        'egemaps': feature_obj.egemaps,\n",
        "                        'mfccs': feature_obj.mfccs,\n",
        "                        'log_mel': feature_obj.log_mel,\n",
        "                        'wav2vec2': feature_obj.wav2vec2,\n",
        "                        'prosodic': feature_obj.prosodic,\n",
        "                        'extraction_success': feature_obj.extraction_success\n",
        "                    }\n",
        "                else:\n",
        "                    serializable_features[filename] = None\n",
        "\n",
        "            safe_save_pickle(serializable_features, category_path, self.logger)\n",
        "\n",
        "        # Save combined features\n",
        "        combined_path = os.path.join(output_dir, \"acoustic_features_all.pkl\")\n",
        "        safe_save_pickle(features, combined_path, self.logger)\n",
        "\n",
        "        self.logger.info(f\"Acoustic features saved to {output_dir}\")\n",
        "\n",
        "def process_audio_batch_worker(file_paths: List[str]) -> Dict[str, Optional[AcousticFeatures]]:\n",
        "    \"\"\"Worker function for parallel processing of audio batches\"\"\"\n",
        "    # Create a new service instance for each worker to avoid sharing model states\n",
        "    service = AcousticFeaturesService()\n",
        "    return service.process_files_batch(file_paths)\n",
        "\n",
        "def demonstrate_acoustic_features(audio_file_path: str, logger: Optional[logging.Logger] = None):\n",
        "    \"\"\"Demonstrate acoustic feature extraction on a single file\"\"\"\n",
        "    if logger is None:\n",
        "        logger = setup_logging()\n",
        "\n",
        "    service = AcousticFeaturesService(logger)\n",
        "\n",
        "    logger.info(f\"Demonstrating acoustic features for: {os.path.basename(audio_file_path)}\")\n",
        "\n",
        "    features = service.extract_features_from_file(audio_file_path)\n",
        "\n",
        "    if features is None:\n",
        "        logger.error(\"Failed to extract features\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n=== Acoustic Features for {os.path.basename(audio_file_path)} ===\\n\")\n",
        "\n",
        "    # eGeMAPS\n",
        "    print(f\"1. eGeMAPS Features: {len(features.egemaps)} features\")\n",
        "    print(f\"   Success: {features.extraction_success['egemaps']}\")\n",
        "    print(f\"   Shape: {features.egemaps.shape}\")\n",
        "    print(f\"   Sample values: {features.egemaps[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # MFCCs\n",
        "    print(\"2. MFCC Features:\")\n",
        "    print(f\"   Success: {features.extraction_success['mfccs']}\")\n",
        "    for key, values in features.mfccs.items():\n",
        "        print(f\"   {key}: {values.shape} - {values[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # Log-mel\n",
        "    print(\"3. Log-Mel Spectrogram Features:\")\n",
        "    print(f\"   Success: {features.extraction_success['log_mel']}\")\n",
        "    for key, values in features.log_mel.items():\n",
        "        print(f\"   {key}: {values.shape} - {values[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # Wav2Vec2\n",
        "    print(f\"4. Wav2Vec2 Features: {features.wav2vec2.shape}\")\n",
        "    print(f\"   Success: {features.extraction_success['wav2vec2']}\")\n",
        "    print(f\"   Sample values: {features.wav2vec2[:5]}\")\n",
        "    print()\n",
        "\n",
        "    # Prosodic\n",
        "    print(\"5. Prosodic Features:\")\n",
        "    print(f\"   Success: {features.extraction_success['prosodic']}\")\n",
        "    for key, value in features.prosodic.items():\n",
        "        print(f\"   {key}: {value:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # Success summary\n",
        "    successful_features = sum(features.extraction_success.values())\n",
        "    total_features = len(features.extraction_success)\n",
        "    print(f\"Feature extraction success: {successful_features}/{total_features}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the service with a single file\n",
        "    from config import get_audio_file_paths\n",
        "\n",
        "    logger = setup_logging()\n",
        "    audio_files = get_audio_file_paths()\n",
        "\n",
        "    # Find first available audio file for demonstration\n",
        "    test_file = None\n",
        "    for category, files in audio_files.items():\n",
        "        if files:\n",
        "            test_file = files[0]\n",
        "            break\n",
        "\n",
        "    if test_file:\n",
        "        demonstrate_acoustic_features(test_file, logger)\n",
        "    else:\n",
        "        logger.error(\"No audio files found for demonstration\")"
      ],
      "metadata": {
        "id": "v1NJ4TsIcDeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transcription_service.py - Speech Transcription Service\n"
      ],
      "metadata": {
        "id": "LIb-e2Zwbvln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fixed Transcription Service with better FFmpeg handling and fallback options\n",
        "\"\"\"\n",
        "import os\n",
        "import logging\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "@dataclass\n",
        "class TranscriptionResult:\n",
        "    \"\"\"Data class for transcription results\"\"\"\n",
        "    file_path: str\n",
        "    category: str\n",
        "    filename: str\n",
        "    transcript: str\n",
        "    language: str\n",
        "    segments: int\n",
        "    duration: float\n",
        "    confidence: float\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "def check_ffmpeg_comprehensive():\n",
        "    \"\"\"Comprehensive FFmpeg check with multiple methods\"\"\"\n",
        "    methods = []\n",
        "\n",
        "    # Method 1: Direct command check\n",
        "    try:\n",
        "        result = subprocess.run(['ffmpeg', '-version'],\n",
        "                              capture_output=True, text=True, timeout=10)\n",
        "        if result.returncode == 0:\n",
        "            methods.append((\"Direct command\", True, \"Working\"))\n",
        "        else:\n",
        "            methods.append((\"Direct command\", False, f\"Return code: {result.returncode}\"))\n",
        "    except FileNotFoundError:\n",
        "        methods.append((\"Direct command\", False, \"FFmpeg not found in PATH\"))\n",
        "    except subprocess.TimeoutExpired:\n",
        "        methods.append((\"Direct command\", False, \"Timeout\"))\n",
        "    except Exception as e:\n",
        "        methods.append((\"Direct command\", False, str(e)))\n",
        "\n",
        "    # Method 2: Check if ffmpeg.exe exists in common locations\n",
        "    common_paths = [\n",
        "        r\"C:\\ffmpeg\\bin\\ffmpeg.exe\",\n",
        "        r\"C:\\Program Files\\ffmpeg\\bin\\ffmpeg.exe\",\n",
        "        r\"C:\\Program Files (x86)\\ffmpeg\\bin\\ffmpeg.exe\",\n",
        "        os.path.join(os.environ.get('USERPROFILE', ''), 'ffmpeg', 'bin', 'ffmpeg.exe'),\n",
        "    ]\n",
        "\n",
        "    for path in common_paths:\n",
        "        if os.path.exists(path):\n",
        "            methods.append((\"File check\", True, f\"Found at {path}\"))\n",
        "            break\n",
        "    else:\n",
        "        methods.append((\"File check\", False, \"Not found in common locations\"))\n",
        "\n",
        "    # Method 3: Check PATH environment\n",
        "    path_env = os.environ.get('PATH', '')\n",
        "    ffmpeg_in_path = any('ffmpeg' in p.lower() for p in path_env.split(os.pathsep))\n",
        "    methods.append((\"PATH check\", ffmpeg_in_path, \"FFmpeg found in PATH\" if ffmpeg_in_path else \"No FFmpeg in PATH\"))\n",
        "\n",
        "    return methods\n",
        "\n",
        "def install_ffmpeg_windows():\n",
        "    \"\"\"Attempt to install FFmpeg automatically on Windows\"\"\"\n",
        "    print(\"Attempting to install FFmpeg automatically...\")\n",
        "\n",
        "    # Try chocolatey first\n",
        "    try:\n",
        "        result = subprocess.run(['choco', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"Chocolatey found. Installing FFmpeg...\")\n",
        "            install_result = subprocess.run(['choco', 'install', 'ffmpeg', '-y'],\n",
        "                                          capture_output=True, text=True)\n",
        "            if install_result.returncode == 0:\n",
        "                print(\"✓ FFmpeg installed successfully via Chocolatey!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Chocolatey install failed: {install_result.stderr}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Chocolatey not found.\")\n",
        "\n",
        "    # Try winget\n",
        "    try:\n",
        "        result = subprocess.run(['winget', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"Winget found. Installing FFmpeg...\")\n",
        "            install_result = subprocess.run(['winget', 'install', 'FFmpeg'],\n",
        "                                          capture_output=True, text=True)\n",
        "            if install_result.returncode == 0:\n",
        "                print(\"✓ FFmpeg installed successfully via Winget!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"Winget install failed: {install_result.stderr}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Winget not found.\")\n",
        "\n",
        "    return False\n",
        "\n",
        "def try_convert_audio_manually(input_path: str, output_path: str) -> bool:\n",
        "    \"\"\"Try to convert audio manually using available tools\"\"\"\n",
        "    try:\n",
        "        # Try with pydub + simpleaudio (fallback)\n",
        "        from pydub import AudioSegment\n",
        "        from pydub.utils import which\n",
        "\n",
        "        # Check if we have any audio conversion capability\n",
        "        if which(\"ffmpeg\") or which(\"avconv\"):\n",
        "            audio = AudioSegment.from_file(input_path)\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "            return True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"Manual conversion failed: {e}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "class TranscriptionService:\n",
        "    \"\"\"Enhanced service for transcribing audio files with better FFmpeg handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_size: str = None, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or self.setup_logging()\n",
        "        self.model_size = model_size or \"base\"\n",
        "        self.whisper_model = None\n",
        "        self.ffmpeg_available = False\n",
        "        self.temp_dir = None\n",
        "\n",
        "        # Comprehensive FFmpeg check\n",
        "        self._check_ffmpeg_comprehensive()\n",
        "\n",
        "        # Initialize model\n",
        "        self._initialize_model()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup basic logging if none provided\"\"\"\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        return logging.getLogger(__name__)\n",
        "\n",
        "    def _check_ffmpeg_comprehensive(self):\n",
        "        \"\"\"Comprehensive FFmpeg check and attempt fixes\"\"\"\n",
        "        self.logger.info(\"Performing comprehensive FFmpeg check...\")\n",
        "\n",
        "        methods = check_ffmpeg_comprehensive()\n",
        "\n",
        "        for method, success, message in methods:\n",
        "            status = \"✓\" if success else \"✗\"\n",
        "            self.logger.info(f\"{status} {method}: {message}\")\n",
        "\n",
        "        # If any method succeeds, consider FFmpeg available\n",
        "        self.ffmpeg_available = any(success for _, success, _ in methods)\n",
        "\n",
        "        if not self.ffmpeg_available:\n",
        "            self.logger.warning(\"FFmpeg not detected. Attempting automatic installation...\")\n",
        "\n",
        "            if sys.platform.startswith('win'):\n",
        "                if install_ffmpeg_windows():\n",
        "                    # Re-check after installation\n",
        "                    methods = check_ffmpeg_comprehensive()\n",
        "                    self.ffmpeg_available = any(success for _, success, _ in methods)\n",
        "\n",
        "                    if self.ffmpeg_available:\n",
        "                        self.logger.info(\"✓ FFmpeg now available after installation!\")\n",
        "                    else:\n",
        "                        self.logger.error(\"FFmpeg installation failed.\")\n",
        "\n",
        "            if not self.ffmpeg_available:\n",
        "                self.logger.error(\"FFmpeg is required for Whisper. Trying fallback methods...\")\n",
        "                self._setup_fallback_audio_processing()\n",
        "\n",
        "    def _setup_fallback_audio_processing(self):\n",
        "        \"\"\"Setup fallback audio processing without FFmpeg\"\"\"\n",
        "        self.temp_dir = tempfile.mkdtemp()\n",
        "        self.logger.info(f\"Created temporary directory for audio processing: {self.temp_dir}\")\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize Whisper model with error handling\"\"\"\n",
        "        try:\n",
        "            import whisper\n",
        "            self.logger.info(f\"Loading Whisper model: {self.model_size}\")\n",
        "            self.whisper_model = whisper.load_model(self.model_size)\n",
        "            self.logger.info(\"✓ Whisper model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load Whisper model: {e}\")\n",
        "            self.whisper_model = None\n",
        "\n",
        "    def validate_audio_file(self, file_path: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Enhanced audio file validation\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            return False, f\"File does not exist: {file_path}\"\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            return False, f\"Path is not a file: {file_path}\"\n",
        "\n",
        "        try:\n",
        "            size = os.path.getsize(file_path)\n",
        "            if size == 0:\n",
        "                return False, f\"File is empty: {file_path}\"\n",
        "            if size < 1000:\n",
        "                return False, f\"File too small ({size} bytes): {file_path}\"\n",
        "        except OSError as e:\n",
        "            return False, f\"Cannot access file: {e}\"\n",
        "\n",
        "        valid_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.mp4', '.avi', '.mov'}\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        if file_ext not in valid_extensions:\n",
        "            return False, f\"Unsupported file format: {file_ext}\"\n",
        "\n",
        "        return True, \"Valid\"\n",
        "\n",
        "    def _prepare_audio_for_whisper(self, audio_path: str) -> Tuple[bool, str, str]:\n",
        "        \"\"\"Prepare audio file for Whisper transcription\"\"\"\n",
        "        # If FFmpeg is available, use original path\n",
        "        if self.ffmpeg_available:\n",
        "            return True, audio_path, \"Using original file with FFmpeg\"\n",
        "\n",
        "        # Fallback: try to process without FFmpeg\n",
        "        self.logger.warning(\"Attempting to process audio without FFmpeg...\")\n",
        "\n",
        "        # For WAV files, try direct processing first\n",
        "        if audio_path.lower().endswith('.wav'):\n",
        "            # Try to load the file directly - sometimes Whisper can handle it\n",
        "            try:\n",
        "                # Test if we can at least read the file\n",
        "                with open(audio_path, 'rb') as f:\n",
        "                    header = f.read(44)  # WAV header is typically 44 bytes\n",
        "                    if header[:4] == b'RIFF' and header[8:12] == b'WAVE':\n",
        "                        self.logger.info(\"Valid WAV file detected, attempting direct processing\")\n",
        "                        return True, audio_path, \"Direct WAV processing\"\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Cannot read WAV header: {e}\")\n",
        "\n",
        "        # Try pydub fallback\n",
        "        temp_path = None\n",
        "        try:\n",
        "            from pydub import AudioSegment\n",
        "            self.logger.info(\"Attempting conversion using pydub...\")\n",
        "\n",
        "            # Load audio with pydub\n",
        "            audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "            # Convert to standard WAV format\n",
        "            temp_path = os.path.join(self.temp_dir, f\"temp_{os.path.basename(audio_path)}\")\n",
        "            if not temp_path.endswith('.wav'):\n",
        "                temp_path = os.path.splitext(temp_path)[0] + '.wav'\n",
        "\n",
        "            # Export as standard WAV\n",
        "            audio.export(temp_path, format=\"wav\", parameters=[\"-ar\", \"16000\", \"-ac\", \"1\"])\n",
        "\n",
        "            if os.path.exists(temp_path) and os.path.getsize(temp_path) > 0:\n",
        "                self.logger.info(f\"Successfully converted audio to: {temp_path}\")\n",
        "                return True, temp_path, \"Converted using pydub\"\n",
        "\n",
        "        except ImportError:\n",
        "            self.logger.error(\"pydub not available for audio conversion\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Audio conversion failed: {e}\")\n",
        "            if temp_path and os.path.exists(temp_path):\n",
        "                try:\n",
        "                    os.remove(temp_path)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return False, audio_path, \"No conversion method available\"\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path: str, category: str = \"\") -> TranscriptionResult:\n",
        "        \"\"\"Transcribe a single audio file with enhanced error handling\"\"\"\n",
        "        filename = os.path.basename(audio_path)\n",
        "\n",
        "        self.logger.debug(f\"Attempting to transcribe: {audio_path}\")\n",
        "\n",
        "        # Validate file\n",
        "        is_valid, validation_message = self.validate_audio_file(audio_path)\n",
        "        if not is_valid:\n",
        "            self.logger.error(f\"Validation failed for {filename}: {validation_message}\")\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=validation_message\n",
        "            )\n",
        "\n",
        "        if self.whisper_model is None:\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=\"Whisper model not available\"\n",
        "            )\n",
        "\n",
        "        # Prepare audio file\n",
        "        processed, processed_path, method = self._prepare_audio_for_whisper(audio_path)\n",
        "        if not processed:\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=f\"Could not prepare audio file: {method}\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Transcribing {filename} using {method}...\")\n",
        "\n",
        "            # Transcribe with Whisper\n",
        "            result = self.whisper_model.transcribe(\n",
        "                processed_path,\n",
        "                fp16=False,\n",
        "                language=None,\n",
        "                task=\"transcribe\",\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            transcript_text = result[\"text\"].strip()\n",
        "            segments = result.get(\"segments\", [])\n",
        "\n",
        "            # Calculate confidence\n",
        "            confidence = 0.0\n",
        "            if segments:\n",
        "                confidences = [seg.get(\"avg_logprob\", 0) for seg in segments if \"avg_logprob\" in seg]\n",
        "                if confidences:\n",
        "                    confidence = float(np.mean([np.exp(c) for c in confidences]))\n",
        "\n",
        "            self.logger.info(f\"✓ Successfully transcribed {filename}\")\n",
        "\n",
        "            # Clean up temporary file if created\n",
        "            if processed_path != audio_path and os.path.exists(processed_path):\n",
        "                try:\n",
        "                    os.remove(processed_path)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=transcript_text,\n",
        "                language=result.get(\"language\", \"unknown\"),\n",
        "                segments=len(segments),\n",
        "                duration=result.get(\"duration\", 0.0),\n",
        "                confidence=confidence,\n",
        "                success=True\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Transcription failed for {filename}: {e}\")\n",
        "            self.logger.error(f\"Error type: {type(e).__name__}\")\n",
        "\n",
        "            # Clean up temporary file if created\n",
        "            if processed_path != audio_path and os.path.exists(processed_path):\n",
        "                try:\n",
        "                    os.remove(processed_path)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=str(e)\n",
        "            )\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup temporary resources\"\"\"\n",
        "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            try:\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "                self.logger.info(\"Cleaned up temporary directory\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Failed to cleanup temporary directory: {e}\")\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Destructor to ensure cleanup\"\"\"\n",
        "        self.cleanup()\n",
        "\n",
        "def demonstrate_fixed_transcription(audio_file_path: str, logger: Optional[logging.Logger] = None):\n",
        "    \"\"\"Demonstration with the fixed transcription service\"\"\"\n",
        "    if logger is None:\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        logger = logging.getLogger(__name__)\n",
        "\n",
        "    logger.info(\"=== FIXED TRANSCRIPTION DEMONSTRATION ===\")\n",
        "\n",
        "    # Debug the file\n",
        "    logger.info(f\"Input file: {audio_file_path}\")\n",
        "    logger.info(f\"Exists: {os.path.exists(audio_file_path)}\")\n",
        "    logger.info(f\"Size: {os.path.getsize(audio_file_path) if os.path.exists(audio_file_path) else 'N/A'}\")\n",
        "\n",
        "    # Create service\n",
        "    service = TranscriptionService(logger=logger)\n",
        "\n",
        "    try:\n",
        "        # Test transcription\n",
        "        result = service.transcribe_audio_file(audio_file_path, \"demo\")\n",
        "\n",
        "        print(f\"\\n=== Transcription Result for {result.filename} ===\")\n",
        "        print(f\"Success: {result.success}\")\n",
        "        print(f\"Language: {result.language}\")\n",
        "        print(f\"Duration: {result.duration:.2f} seconds\")\n",
        "        print(f\"Segments: {result.segments}\")\n",
        "        print(f\"Confidence: {result.confidence:.3f}\")\n",
        "\n",
        "        if result.success:\n",
        "            print(f\"Transcript ({len(result.transcript)} chars, {len(result.transcript.split())} words):\")\n",
        "            print(f'\"{result.transcript}\"')\n",
        "        else:\n",
        "            print(f\"Error: {result.error_message}\")\n",
        "\n",
        "            # Provide specific guidance based on error\n",
        "            if \"ffmpeg\" in result.error_message.lower() or \"file specified\" in result.error_message.lower():\n",
        "                print(\"\\n=== TROUBLESHOOTING GUIDANCE ===\")\n",
        "                print(\"This appears to be an FFmpeg issue. Try:\")\n",
        "                print(\"1. Install FFmpeg using the instructions above\")\n",
        "                print(\"2. Install pydub for fallback processing: pip install pydub\")\n",
        "                print(\"3. Restart your Python environment after installing FFmpeg\")\n",
        "\n",
        "    finally:\n",
        "        # Always cleanup\n",
        "        service.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Enhanced test with better error handling\n",
        "    import sys\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Check if running as part of larger system\n",
        "    try:\n",
        "        # Try to import from the main config (assuming it's in the same directory)\n",
        "        import importlib.util\n",
        "\n",
        "        # Try to load config module\n",
        "        config_path = os.path.join(os.path.dirname(__file__), 'config.py')\n",
        "        if os.path.exists(config_path):\n",
        "            spec = importlib.util.spec_from_file_location(\"config\", config_path)\n",
        "            config = importlib.util.module_from_spec(spec)\n",
        "            spec.loader.exec_module(config)\n",
        "\n",
        "            logger.info(\"Getting audio file paths from config...\")\n",
        "            audio_files = config.get_audio_file_paths()\n",
        "\n",
        "            # Find first available file\n",
        "            test_file = None\n",
        "            for category, files in audio_files.items():\n",
        "                if files:\n",
        "                    for file_path in files:\n",
        "                        if os.path.exists(file_path):\n",
        "                            test_file = file_path\n",
        "                            logger.info(f\"Found test file: {test_file}\")\n",
        "                            break\n",
        "                    if test_file:\n",
        "                        break\n",
        "\n",
        "            if test_file:\n",
        "                demonstrate_fixed_transcription(test_file, logger)\n",
        "            else:\n",
        "                logger.error(\"No valid audio files found in config!\")\n",
        "\n",
        "        else:\n",
        "            logger.info(\"Config file not found, asking for manual input...\")\n",
        "            print(\"Please enter the full path to an audio file:\")\n",
        "            test_file = input().strip().strip('\"')\n",
        "\n",
        "            if test_file and os.path.exists(test_file):\n",
        "                demonstrate_fixed_transcription(test_file, logger)\n",
        "            else:\n",
        "                logger.error(f\"File not found: {test_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during execution: {e}\")\n",
        "        logger.info(\"Falling back to manual file input...\")\n",
        "\n",
        "        print(\"Please enter the full path to an audio file:\")\n",
        "        test_file = input().strip().strip('\"')\n",
        "\n",
        "        if test_file and os.path.exists(test_file):\n",
        "            demonstrate_fixed_transcription(test_file, logger)\n",
        "        else:\n",
        "            logger.error(f\"File not found: {test_file}\")"
      ],
      "metadata": {
        "id": "ODXCIYLbcF5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " python .\\transcription_service.py\n",
        "2025-07-24 09:51:08,495 - __main__ - INFO - Getting audio file paths...\n",
        "2025-07-24 09:51:08,496 - config - INFO - Scanning for audio files...\n",
        "2025-07-24 09:51:08,497 - config - INFO - Scanning diagnosis_ad: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\n",
        "2025-07-24 09:51:08,532 - config - INFO - Found 87 valid WAV files in C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\n",
        "2025-07-24 09:51:08,533 - config - INFO - Scanning diagnosis_cn: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\n",
        "2025-07-24 09:51:08,565 - config - INFO - Found 79 valid WAV files in C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\n",
        "2025-07-24 09:51:08,566 - config - INFO - Scanning progression_decline: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\n",
        "2025-07-24 09:51:08,574 - config - INFO - Found 15 valid WAV files in C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\n",
        "2025-07-24 09:51:08,575 - config - INFO - Scanning progression_no_decline: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\n",
        "2025-07-24 09:51:08,599 - config - INFO - Found 58 valid WAV files in C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\n",
        "2025-07-24 09:51:08,600 - config - INFO - Scanning progression_test: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\n",
        "2025-07-24 09:51:08,614 - config - INFO - Found 32 valid WAV files in C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\n",
        "2025-07-24 09:51:08,615 - config - INFO - Total audio files found: 271\n",
        "2025-07-24 09:51:08,615 - config - INFO -   diagnosis_ad: 87 files\n",
        "2025-07-24 09:51:08,616 - config - INFO -   diagnosis_cn: 79 files\n",
        "2025-07-24 09:51:08,616 - config - INFO -   progression_decline: 15 files\n",
        "2025-07-24 09:51:08,617 - config - INFO -   progression_no_decline: 58 files\n",
        "2025-07-24 09:51:08,617 - config - INFO -   progression_test: 32 files\n",
        "2025-07-24 09:51:08,618 - __main__ - INFO - === DEBUGGING FILE PATHS ===\n",
        "2025-07-24 09:51:08,618 - __main__ - INFO -\n",
        "Category: diagnosis_ad\n",
        "2025-07-24 09:51:08,619 - __main__ - INFO - Number of files: 87\n",
        "2025-07-24 09:51:08,620 - __main__ - INFO -   File 1: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav\n",
        "2025-07-24 09:51:08,621 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,621 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,623 - __main__ - INFO -     Size: 20802638 bytes\n",
        "2025-07-24 09:51:08,623 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav\n",
        "2025-07-24 09:51:08,624 - __main__ - INFO -   File 2: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso025.wav\n",
        "2025-07-24 09:51:08,625 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,626 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,627 - __main__ - INFO -     Size: 33719272 bytes\n",
        "2025-07-24 09:51:08,627 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso025.wav\n",
        "2025-07-24 09:51:08,628 - __main__ - INFO -   File 3: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso027.wav\n",
        "2025-07-24 09:51:08,629 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,630 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,631 - __main__ - INFO -     Size: 14549992 bytes\n",
        "2025-07-24 09:51:08,631 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso027.wav\n",
        "2025-07-24 09:51:08,632 - __main__ - INFO -   ... and 84 more files\n",
        "2025-07-24 09:51:08,632 - __main__ - INFO -\n",
        "Category: diagnosis_cn\n",
        "2025-07-24 09:51:08,633 - __main__ - INFO - Number of files: 79\n",
        "2025-07-24 09:51:08,634 - __main__ - INFO -   File 1: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\\adrso002.wav\n",
        "2025-07-24 09:51:08,635 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,636 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,637 - __main__ - INFO -     Size: 6884840 bytes\n",
        "2025-07-24 09:51:08,637 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\\adrso002.wav\n",
        "2025-07-24 09:51:08,638 - __main__ - INFO -   File 2: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\\adrso003.wav\n",
        "2025-07-24 09:51:08,639 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,640 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,641 - __main__ - INFO -     Size: 2965714 bytes\n",
        "2025-07-24 09:51:08,641 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\\adrso003.wav\n",
        "2025-07-24 09:51:08,642 - __main__ - INFO -   File 3: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\\adrso005.wav\n",
        "2025-07-24 09:51:08,643 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,644 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,645 - __main__ - INFO -     Size: 5447768 bytes\n",
        "2025-07-24 09:51:08,645 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\\adrso005.wav\n",
        "2025-07-24 09:51:08,646 - __main__ - INFO -   ... and 76 more files\n",
        "2025-07-24 09:51:08,647 - __main__ - INFO -\n",
        "Category: progression_decline\n",
        "2025-07-24 09:51:08,647 - __main__ - INFO - Number of files: 15\n",
        "2025-07-24 09:51:08,648 - __main__ - INFO -   File 1: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\\adrsp003.wav\n",
        "2025-07-24 09:51:08,649 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,649 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,650 - __main__ - INFO -     Size: 50457688 bytes\n",
        "2025-07-24 09:51:08,651 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\\adrsp003.wav\n",
        "2025-07-24 09:51:08,652 - __main__ - INFO -   File 2: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\\adrsp051.wav\n",
        "2025-07-24 09:51:08,653 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,654 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,655 - __main__ - INFO -     Size: 8027224 bytes\n",
        "2025-07-24 09:51:08,655 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\\adrsp051.wav\n",
        "2025-07-24 09:51:08,656 - __main__ - INFO -   File 3: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\\adrsp055.wav\n",
        "2025-07-24 09:51:08,657 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,658 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,659 - __main__ - INFO -     Size: 42928216 bytes\n",
        "2025-07-24 09:51:08,660 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\\adrsp055.wav\n",
        "2025-07-24 09:51:08,661 - __main__ - INFO -   ... and 12 more files\n",
        "2025-07-24 09:51:08,662 - __main__ - INFO -\n",
        "Category: progression_no_decline\n",
        "2025-07-24 09:51:08,664 - __main__ - INFO - Number of files: 58\n",
        "2025-07-24 09:51:08,665 - __main__ - INFO -   File 1: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\\adrsp001.wav\n",
        "2025-07-24 09:51:08,666 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,667 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,669 - __main__ - INFO -     Size: 54982804 bytes\n",
        "2025-07-24 09:51:08,669 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\\adrsp001.wav\n",
        "2025-07-24 09:51:08,670 - __main__ - INFO -   File 2: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\\adrsp007.wav\n",
        "2025-07-24 09:51:08,671 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,672 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,674 - __main__ - INFO -     Size: 46605400 bytes\n",
        "2025-07-24 09:51:08,674 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\\adrsp007.wav\n",
        "2025-07-24 09:51:08,675 - __main__ - INFO -   File 3: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\\adrsp019.wav\n",
        "2025-07-24 09:51:08,677 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,678 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,679 - __main__ - INFO -     Size: 43020376 bytes\n",
        "2025-07-24 09:51:08,679 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\\adrsp019.wav\n",
        "2025-07-24 09:51:08,680 - __main__ - INFO -   ... and 55 more files\n",
        "2025-07-24 09:51:08,681 - __main__ - INFO -\n",
        "Category: progression_test\n",
        "2025-07-24 09:51:08,682 - __main__ - INFO - Number of files: 32\n",
        "2025-07-24 09:51:08,683 - __main__ - INFO -   File 1: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\\adrspt1.wav\n",
        "2025-07-24 09:51:08,684 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,685 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,686 - __main__ - INFO -     Size: 45951124 bytes\n",
        "2025-07-24 09:51:08,687 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\\adrspt1.wav\n",
        "2025-07-24 09:51:08,688 - __main__ - INFO -   File 2: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\\adrspt10.wav\n",
        "2025-07-24 09:51:08,689 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,691 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,692 - __main__ - INFO -     Size: 57692248 bytes\n",
        "2025-07-24 09:51:08,692 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\\adrspt10.wav\n",
        "2025-07-24 09:51:08,693 - __main__ - INFO -   File 3: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\\adrspt11.wav\n",
        "2025-07-24 09:51:08,694 - __main__ - INFO -     Exists: True\n",
        "2025-07-24 09:51:08,696 - __main__ - INFO -     Is file: True\n",
        "2025-07-24 09:51:08,697 - __main__ - INFO -     Size: 45315160 bytes\n",
        "2025-07-24 09:51:08,698 - __main__ - INFO -     Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\\adrspt11.wav\n",
        "2025-07-24 09:51:08,699 - __main__ - INFO -   ... and 29 more files\n",
        "2025-07-24 09:51:08,700 - __main__ - INFO - === END DEBUG ===\n",
        "\n",
        "2025-07-24 09:51:08,701 - __main__ - INFO - Found valid test file: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav\n",
        "2025-07-24 09:51:08,702 - __main__ - INFO - === TRANSCRIPTION DEMONSTRATION WITH DEBUG ===\n",
        "2025-07-24 09:51:08,703 - __main__ - INFO - Input file path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav\n",
        "2025-07-24 09:51:08,704 - __main__ - INFO - Absolute path: C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav\n",
        "2025-07-24 09:51:08,705 - __main__ - INFO - File exists: True\n",
        "2025-07-24 09:51:08,706 - __main__ - INFO - Is file: True\n",
        "2025-07-24 09:51:08,707 - __main__ - INFO - File size: 20802638 bytes\n",
        "2025-07-24 09:51:08,708 - __main__ - INFO - File extension: .wav\n",
        "2025-07-24 09:51:08,709 - __main__ - INFO - Current working directory: C:\\Users\\Administrator\\Desktop\\Speech\n",
        "2025-07-24 09:51:08,710 - __main__ - INFO - Contents of directory C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad:\n",
        "2025-07-24 09:51:08,711 - __main__ - INFO -   adrso024.wav (FILE)\n",
        "2025-07-24 09:51:08,712 - __main__ - INFO -   adrso025.wav (FILE)\n",
        "2025-07-24 09:51:08,714 - __main__ - INFO -   adrso027.wav (FILE)\n",
        "2025-07-24 09:51:08,715 - __main__ - INFO -   adrso028.wav (FILE)\n",
        "2025-07-24 09:51:08,716 - __main__ - INFO -   adrso031.wav (FILE)\n",
        "2025-07-24 09:51:08,717 - __main__ - INFO -   adrso032.wav (FILE)\n",
        "2025-07-24 09:51:08,718 - __main__ - INFO -   adrso033.wav (FILE)\n",
        "2025-07-24 09:51:08,718 - __main__ - INFO -   adrso035.wav (FILE)\n",
        "2025-07-24 09:51:08,719 - __main__ - INFO -   adrso036.wav (FILE)\n",
        "2025-07-24 09:51:08,720 - __main__ - INFO -   adrso039.wav (FILE)\n",
        "2025-07-24 09:51:08,721 - __main__ - INFO - Checking prerequisites...\n",
        "2025-07-24 09:51:16,740 - __main__ - INFO - ✓ Whisper is available\n",
        "2025-07-24 09:51:16,746 - __main__ - ERROR - ✗ FFmpeg not found or not working properly\n",
        "2025-07-24 09:51:16,746 - __main__ - ERROR -\n",
        "    FFmpeg is required for Whisper to work properly. Please install it:\n",
        "\n",
        "    Option 1 - Using Chocolatey (Recommended for Windows):\n",
        "    1. Open PowerShell as Administrator\n",
        "    2. Install Chocolatey if not already installed:\n",
        "       Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n",
        "    3. Install FFmpeg: choco install ffmpeg\n",
        "    4. Restart your command prompt/IDE\n",
        "\n",
        "    Option 2 - Manual Installation:\n",
        "    1. Download FFmpeg from https://ffmpeg.org/download.html\n",
        "    2. Extract to a folder (e.g., C:\\ffmpeg)\n",
        "    3. Add C:\\ffmpeg\\bin to your system PATH environment variable\n",
        "    4. Restart your command prompt/IDE\n",
        "\n",
        "    Option 3 - Using conda:\n",
        "    conda install ffmpeg\n",
        "\n",
        "2025-07-24 09:51:16,750 - __main__ - INFO - Loading Whisper model: base\n",
        "2025-07-24 09:51:18,696 - __main__ - INFO - ✓ Whisper model loaded successfully\n",
        "2025-07-24 09:51:18,698 - __main__ - INFO - Transcribing adrso024.wav...\n",
        "2025-07-24 09:51:18,710 - __main__ - ERROR - Transcription failed for adrso024.wav: [WinError 2] The system cannot find the file specified      \n",
        "2025-07-24 09:51:18,710 - __main__ - ERROR - Error type: FileNotFoundError\n",
        "\n",
        "=== Transcription Result for adrso024.wav ===\n",
        "Success: False\n",
        "Language:\n",
        "Duration: 0.00 seconds\n",
        "Segments: 0\n",
        "Confidence: 0.000\n",
        "Error: [WinError 2] The system cannot find the file specified\n",
        "\n",
        "(env) PS C:\\Users\\Administrator\\Desktop\\Speech>"
      ],
      "metadata": {
        "id": "dEEFO_7z_ceF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revised- Grok"
      ],
      "metadata": {
        "id": "f2-LpHYQ8dAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional, Any\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "# Whisper import with error handling\n",
        "try:\n",
        "    import whisper\n",
        "    WHISPER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WHISPER_AVAILABLE = False\n",
        "\n",
        "from config import MODEL_CONFIG, SYSTEM_CONFIG, PATH_CONFIG\n",
        "from utils import (setup_logging, monitor_memory_usage, cleanup_memory,\n",
        "                   safe_save_json, safe_save_pickle, ProcessingTimer,\n",
        "                   create_progress_bar, validate_audio_file)\n",
        "\n",
        "@dataclass\n",
        "class TranscriptionResult:\n",
        "    \"\"\"Data class for transcription results\"\"\"\n",
        "    file_path: str\n",
        "    category: str\n",
        "    filename: str\n",
        "    transcript: str\n",
        "    language: str\n",
        "    segments: int\n",
        "    duration: float\n",
        "    confidence: float\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "class TranscriptionService:\n",
        "    \"\"\"Service for transcribing audio files using Whisper\"\"\"\n",
        "\n",
        "    def __init__(self, model_size: str = None, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or setup_logging()\n",
        "        self.model_size = model_size or MODEL_CONFIG.whisper_model_size\n",
        "        self.whisper_model = None\n",
        "\n",
        "        self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize Whisper model\"\"\"\n",
        "        if not WHISPER_AVAILABLE:\n",
        "            self.logger.error(\"Whisper not available. Please install: pip install openai-whisper\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Loading Whisper model: {self.model_size}\")\n",
        "            self.whisper_model = whisper.load_model(self.model_size)\n",
        "            self.logger.info(\"✓ Whisper model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load Whisper model: {e}\")\n",
        "            self.whisper_model = None\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path: str, category: str = \"\") -> TranscriptionResult:\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        filename = os.path.basename(audio_path)\n",
        "\n",
        "        # Validate file with enhanced logging\n",
        "        if not validate_audio_file(audio_path):\n",
        "            self.logger.error(f\"Invalid audio file: {audio_path} (Exists: {os.path.exists(audio_path)}, Size: {os.path.getsize(audio_path) if os.path.exists(audio_path) else 0})\")\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=\"Invalid or missing audio file\"\n",
        "            )\n",
        "\n",
        "        if self.whisper_model is None:\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=\"Whisper model not available\"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            self.logger.debug(f\"Transcribing {filename}...\")\n",
        "\n",
        "            # Transcribe with Whisper\n",
        "            result = self.whisper_model.transcribe(\n",
        "                audio_path,\n",
        "                fp16=False,  # Use fp32 for better compatibility\n",
        "                language=None,  # Auto-detect language\n",
        "                task=\"transcribe\"\n",
        "            )\n",
        "\n",
        "            transcript_text = result[\"text\"].strip()\n",
        "            segments = result.get(\"segments\", [])\n",
        "\n",
        "            # Calculate average confidence if available\n",
        "            confidence = 0.0\n",
        "            if segments:\n",
        "                confidences = [seg.get(\"avg_logprob\", 0) for seg in segments if \"avg_logprob\" in seg]\n",
        "                if confidences:\n",
        "                    # Convert log probabilities to rough confidence scores\n",
        "                    confidence = float(np.mean([np.exp(c) for c in confidences]))\n",
        "\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=transcript_text,\n",
        "                language=result.get(\"language\", \"unknown\"),\n",
        "                segments=len(segments),\n",
        "                duration=result.get(\"duration\", 0.0),\n",
        "                confidence=confidence,\n",
        "                success=True\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Transcription failed for {filename}: {e}\")\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=str(e)\n",
        "            )\n",
        "\n",
        "    def transcribe_files_batch(self, file_paths: List[str], category: str = \"\") -> Dict[str, TranscriptionResult]:\n",
        "        \"\"\"Transcribe a batch of audio files\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            filename = os.path.basename(file_path)\n",
        "\n",
        "            try:\n",
        "                result = self.transcribe_audio_file(file_path, category)\n",
        "                results[filename] = result\n",
        "\n",
        "                # Memory management\n",
        "                if len(results) % 5 == 0:  # More frequent cleanup for transcription\n",
        "                    cleanup_memory()\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error processing {filename}: {e}\")\n",
        "                results[filename] = TranscriptionResult(\n",
        "                    file_path=file_path,\n",
        "                    category=category,\n",
        "                    filename=filename,\n",
        "                    transcript=\"\",\n",
        "                    language=\"\",\n",
        "                    segments=0,\n",
        "                    duration=0.0,\n",
        "                    confidence=0.0,\n",
        "                    success=False,\n",
        "                    error_message=str(e)\n",
        "                )\n",
        "\n",
        "        return results\n",
        "\n",
        "    def transcribe_all_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Dict[str, TranscriptionResult]]:\n",
        "        \"\"\"Transcribe all audio files using parallel processing\"\"\"\n",
        "        self.logger.info(\"Starting parallel transcription...\")\n",
        "\n",
        "        all_results = {}\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed_files = 0\n",
        "\n",
        "        with ProcessingTimer(\"Complete transcription process\", self.logger):\n",
        "            for category, file_paths in audio_files.items():\n",
        "                if not file_paths:\n",
        "                    continue\n",
        "\n",
        "                self.logger.info(f\"Transcribing {category}: {len(file_paths)} files\")\n",
        "                category_results = {}\n",
        "\n",
        "                # Use smaller batches for transcription to manage memory better\n",
        "                batch_size = max(1, SYSTEM_CONFIG.chunk_size // 2)  # Smaller batches\n",
        "                batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]\n",
        "\n",
        "                # Use fewer workers for transcription as it's memory intensive\n",
        "                max_workers = min(SYSTEM_CONFIG.max_workers // 2, 4)\n",
        "\n",
        "                with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "                    # Submit batch jobs\n",
        "                    future_to_batch = {\n",
        "                        executor.submit(transcribe_batch_worker, batch, category, self.model_size): batch\n",
        "                        for batch in batches\n",
        "                    }\n",
        "\n",
        "                    # Progress tracking\n",
        "                    progress_bar = create_progress_bar(len(batches), f\"Transcribing {category}\")\n",
        "\n",
        "                    # Collect results\n",
        "                    for future in as_completed(future_to_batch):\n",
        "                        batch = future_to_batch[future]\n",
        "                        try:\n",
        "                            batch_results = future.result()\n",
        "                            category_results.update(batch_results)\n",
        "                            processed_files += len(batch)\n",
        "\n",
        "                            progress_bar.update(1)\n",
        "\n",
        "                            # Memory monitoring\n",
        "                            memory_info = monitor_memory_usage()\n",
        "                            if memory_info['percent_used'] > 75:\n",
        "                                self.logger.warning(f\"High memory usage: {memory_info['percent_used']:.1f}%\")\n",
        "                                cleanup_memory()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            self.logger.error(f\"Batch transcription failed: {e}\")\n",
        "\n",
        "                    progress_bar.close()\n",
        "\n",
        "                all_results[category] = category_results\n",
        "\n",
        "                # Log category completion stats\n",
        "                successful = sum(1 for result in category_results.values() if result.success)\n",
        "                self.logger.info(f\"Completed {category}: {successful}/{len(category_results)} successful\")\n",
        "\n",
        "        # Final cleanup\n",
        "        cleanup_memory()\n",
        "\n",
        "        # Log overall stats\n",
        "        total_successful = sum(\n",
        "            sum(1 for result in category_results.values() if result.success)\n",
        "            for category_results in all_results.values()\n",
        "        )\n",
        "        self.logger.info(f\"Transcription completed: {total_successful}/{processed_files} successful\")\n",
        "\n",
        "        return all_results\n",
        "\n",
        "    def save_transcriptions(self, transcriptions: Dict[str, Dict[str, TranscriptionResult]], output_dir: str):\n",
        "        \"\"\"Save transcriptions in multiple formats\"\"\"\n",
        "        self.logger.info(\"Saving transcriptions...\")\n",
        "\n",
        "        # Create output directories\n",
        "        transcripts_dir = os.path.join(output_dir, \"transcripts\")\n",
        "        os.makedirs(transcripts_dir, exist_ok=True)\n",
        "\n",
        "        all_transcripts = {}\n",
        "        transcript_summary = []\n",
        "\n",
        "        for category, category_results in transcriptions.items():\n",
        "            # Save individual category files\n",
        "            category_transcripts = {}\n",
        "\n",
        "            for filename, result in category_results.items():\n",
        "                # Convert to dictionary for JSON serialization\n",
        "                transcript_dict = {\n",
        "                    'file_path': result.file_path,\n",
        "                    'category': result.category,\n",
        "                    'filename': result.filename,\n",
        "                    'transcript': result.transcript,\n",
        "                    'language': result.language,\n",
        "                    'segments': result.segments,\n",
        "                    'duration': result.duration,\n",
        "                    'confidence': result.confidence,\n",
        "                    'success': result.success,\n",
        "                    'error_message': result.error_message\n",
        "                }\n",
        "\n",
        "                category_transcripts[filename] = transcript_dict\n",
        "                all_transcripts[f\"{category}_{filename}\"] = transcript_dict\n",
        "\n",
        "                # Add to summary\n",
        "                transcript_summary.append({\n",
        "                    'File_ID': f\"{category}_{filename}\",\n",
        "                    'Category': result.category,\n",
        "                    'Filename': result.filename,\n",
        "                    'Success': result.success,\n",
        "                    'Language': result.language,\n",
        "                    'Duration': result.duration,\n",
        "                    'Transcript_Length': len(result.transcript),\n",
        "                    'Word_Count': len(result.transcript.split()) if result.transcript else 0,\n",
        "                    'Segments': result.segments,\n",
        "                    'Confidence': result.confidence,\n",
        "                    'Error': result.error_message if result.error_message else \"\",\n",
        "                    'Transcript_Preview': (result.transcript[:100] + \"...\") if len(result.transcript) > 100 else result.transcript\n",
        "                })\n",
        "\n",
        "                # Save individual transcript file\n",
        "                if result.success and result.transcript:\n",
        "                    transcript_file = os.path.join(transcripts_dir, f\"{category}_{filename}_transcript.txt\")\n",
        "                    try:\n",
        "                        with open(transcript_file, 'w', encoding='utf-8') as f:\n",
        "                            f.write(result.transcript)\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Failed to save individual transcript {transcript_file}: {e}\")\n",
        "\n",
        "            # Save category JSON\n",
        "            category_json_path = os.path.join(transcripts_dir, f\"transcripts_{category}.json\")\n",
        "            safe_save_json(category_transcripts, category_json_path, self.logger)\n",
        "\n",
        "        # Save consolidated files\n",
        "        all_transcripts_path = os.path.join(transcripts_dir, \"all_transcripts.json\")\n",
        "        safe_save_json(all_transcripts, all_transcripts_path, self.logger)\n",
        "\n",
        "        # Save as pickle\n",
        "        transcripts_pickle_path = os.path.join(transcripts_dir, \"transcripts.pkl\")\n",
        "        safe_save_pickle(all_transcripts, transcripts_pickle_path, self.logger)\n",
        "\n",
        "        # Save summary CSV\n",
        "        try:\n",
        "            import pandas as pd\n",
        "            summary_df = pd.DataFrame(transcript_summary)\n",
        "            summary_csv_path = os.path.join(output_dir, \"transcript_summary.csv\")\n",
        "            summary_df.to_csv(summary_csv_path, index=False)\n",
        "            self.logger.info(f\"Transcript summary saved: {summary_csv_path}\")\n",
        "        except ImportError:\n",
        "            self.logger.warning(\"Pandas not available, skipping CSV summary\")\n",
        "\n",
        "        self.logger.info(f\"All transcriptions saved to {output_dir}\")\n",
        "        return all_transcripts\n",
        "\n",
        "def transcribe_batch_worker(file_paths: List[str], category: str, model_size: str) -> Dict[str, TranscriptionResult]:\n",
        "    \"\"\"Worker function for parallel transcription\"\"\"\n",
        "    # Create new service instance for each worker\n",
        "    service = TranscriptionService(model_size=model_size)\n",
        "    return service.transcribe_files_batch(file_paths, category)\n",
        "\n",
        "def demonstrate_transcription(audio_file_path: str, logger: Optional[logging.Logger] = None):\n",
        "    \"\"\"Demonstrate transcription on a single file\"\"\"\n",
        "    if logger is None:\n",
        "        logger = setup_logging()\n",
        "\n",
        "    service = TranscriptionService(logger=logger)\n",
        "\n",
        "    logger.info(f\"Demonstrating transcription for: {os.path.basename(audio_file_path)}\")\n",
        "\n",
        "    result = service.transcribe_audio_file(audio_file_path, \"demo\")\n",
        "\n",
        "    print(f\"\\n=== Transcription Result for {result.filename} ===\")\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Language: {result.language}\")\n",
        "    print(f\"Duration: {result.duration:.2f} seconds\")\n",
        "    print(f\"Segments: {result.segments}\")\n",
        "    print(f\"Confidence: {result.confidence:.3f}\")\n",
        "\n",
        "    if result.success:\n",
        "        print(f\"Transcript ({len(result.transcript)} chars, {len(result.transcript.split())} words):\")\n",
        "        print(f'\"{result.transcript}\"')\n",
        "    else:\n",
        "        print(f\"Error: {result.error_message}\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the service\n",
        "    from config import get_audio_file_paths\n",
        "\n",
        "    logger = setup_logging()\n",
        "    audio_files = get_audio_file_paths()\n",
        "\n",
        "    # Prioritize known valid file\n",
        "    test_file = r\"C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav\"\n",
        "    if validate_audio_file(test_file):\n",
        "        demonstrate_transcription(test_file, logger)\n",
        "    else:\n",
        "        logger.error(f\"Test file not valid: {test_file}. Falling back to available files.\")\n",
        "        # Fallback to first valid file\n",
        "        for category, files in audio_files.items():\n",
        "            for file in files:\n",
        "                if validate_audio_file(file):\n",
        "                    logger.info(f\"Using fallback file: {file}\")\n",
        "                    demonstrate_transcription(file, logger)\n",
        "                    break\n",
        "            if file and validate_audio_file(file):\n",
        "                break\n",
        "        else:\n",
        "            logger.error(\"No valid audio files found for demonstration\")\n",
        "            # Log all scanned files for debugging\n",
        "            for category, files in audio_files.items():\n",
        "                logger.debug(f\"Category {category}: {files}\")\n"
      ],
      "metadata": {
        "id": "YwSJsYOa8mGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C:\\Users\\Administrator\\Desktop\\Speech\\ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\\adrso024.wav"
      ],
      "metadata": {
        "id": "JCdutPo59NII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revised- Claude"
      ],
      "metadata": {
        "id": "s8tLjtMy_zeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced Transcription Service with automatic FFmpeg installation and robust fallbacks\n",
        "\"\"\"\n",
        "import os\n",
        "import logging\n",
        "import subprocess\n",
        "import sys\n",
        "import platform\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, List, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "import tempfile\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class TranscriptionResult:\n",
        "    \"\"\"Data class for transcription results\"\"\"\n",
        "    file_path: str\n",
        "    category: str\n",
        "    filename: str\n",
        "    transcript: str\n",
        "    language: str\n",
        "    segments: int\n",
        "    duration: float\n",
        "    confidence: float\n",
        "    success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "class FFmpegInstaller:\n",
        "    \"\"\"Handles FFmpeg installation across different platforms\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def is_ffmpeg_available() -> Tuple[bool, str]:\n",
        "        \"\"\"Check if FFmpeg is available\"\"\"\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['ffmpeg', '-version'],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=10\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                version_line = result.stdout.split('\\n')[0]\n",
        "                return True, f\"FFmpeg found: {version_line}\"\n",
        "        except FileNotFoundError:\n",
        "            return False, \"FFmpeg not found in PATH\"\n",
        "        except subprocess.TimeoutExpired:\n",
        "            return False, \"FFmpeg check timed out\"\n",
        "        except Exception as e:\n",
        "            return False, f\"FFmpeg check failed: {str(e)}\"\n",
        "\n",
        "        return False, \"FFmpeg not available\"\n",
        "\n",
        "    @staticmethod\n",
        "    def install_ffmpeg_windows() -> Tuple[bool, str]:\n",
        "        \"\"\"Install FFmpeg on Windows using various methods\"\"\"\n",
        "        methods_tried = []\n",
        "\n",
        "        # Method 1: Try Chocolatey\n",
        "        try:\n",
        "            choco_check = subprocess.run(['choco', '--version'], capture_output=True, text=True, timeout=5)\n",
        "            if choco_check.returncode == 0:\n",
        "                methods_tried.append(\"Found Chocolatey\")\n",
        "                print(\"Installing FFmpeg via Chocolatey...\")\n",
        "                install_result = subprocess.run(\n",
        "                    ['choco', 'install', 'ffmpeg', '-y'],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300  # 5 minutes timeout\n",
        "                )\n",
        "                if install_result.returncode == 0:\n",
        "                    return True, \"Successfully installed FFmpeg via Chocolatey\"\n",
        "                else:\n",
        "                    methods_tried.append(f\"Chocolatey failed: {install_result.stderr[:200]}\")\n",
        "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
        "            methods_tried.append(\"Chocolatey not available\")\n",
        "\n",
        "        # Method 2: Try winget\n",
        "        try:\n",
        "            winget_check = subprocess.run(['winget', '--version'], capture_output=True, text=True, timeout=5)\n",
        "            if winget_check.returncode == 0:\n",
        "                methods_tried.append(\"Found winget\")\n",
        "                print(\"Installing FFmpeg via winget...\")\n",
        "                install_result = subprocess.run(\n",
        "                    ['winget', 'install', 'Gyan.FFmpeg'],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300\n",
        "                )\n",
        "                if install_result.returncode == 0:\n",
        "                    return True, \"Successfully installed FFmpeg via winget\"\n",
        "                else:\n",
        "                    methods_tried.append(f\"Winget failed: {install_result.stderr[:200]}\")\n",
        "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
        "            methods_tried.append(\"Winget not available\")\n",
        "\n",
        "        # Method 3: Try scoop\n",
        "        try:\n",
        "            scoop_check = subprocess.run(['scoop', '--version'], capture_output=True, text=True, timeout=5)\n",
        "            if scoop_check.returncode == 0:\n",
        "                methods_tried.append(\"Found scoop\")\n",
        "                print(\"Installing FFmpeg via scoop...\")\n",
        "                install_result = subprocess.run(\n",
        "                    ['scoop', 'install', 'ffmpeg'],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300\n",
        "                )\n",
        "                if install_result.returncode == 0:\n",
        "                    return True, \"Successfully installed FFmpeg via scoop\"\n",
        "                else:\n",
        "                    methods_tried.append(f\"Scoop failed: {install_result.stderr[:200]}\")\n",
        "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
        "            methods_tried.append(\"Scoop not available\")\n",
        "\n",
        "        return False, f\"All installation methods failed. Tried: {'; '.join(methods_tried)}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def install_ffmpeg_conda() -> Tuple[bool, str]:\n",
        "        \"\"\"Install FFmpeg using conda\"\"\"\n",
        "        try:\n",
        "            conda_check = subprocess.run(['conda', '--version'], capture_output=True, text=True, timeout=5)\n",
        "            if conda_check.returncode == 0:\n",
        "                print(\"Installing FFmpeg via conda...\")\n",
        "                install_result = subprocess.run(\n",
        "                    ['conda', 'install', '-c', 'conda-forge', 'ffmpeg', '-y'],\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                    timeout=300\n",
        "                )\n",
        "                if install_result.returncode == 0:\n",
        "                    return True, \"Successfully installed FFmpeg via conda\"\n",
        "                else:\n",
        "                    return False, f\"Conda install failed: {install_result.stderr[:200]}\"\n",
        "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
        "            return False, \"Conda not available\"\n",
        "\n",
        "    @staticmethod\n",
        "    def auto_install_ffmpeg() -> Tuple[bool, str]:\n",
        "        \"\"\"Automatically install FFmpeg based on the platform\"\"\"\n",
        "        system = platform.system().lower()\n",
        "\n",
        "        if system == \"windows\":\n",
        "            # Try conda first (often available in data science environments)\n",
        "            success, message = FFmpegInstaller.install_ffmpeg_conda()\n",
        "            if success:\n",
        "                return success, message\n",
        "\n",
        "            # Then try Windows package managers\n",
        "            return FFmpegInstaller.install_ffmpeg_windows()\n",
        "\n",
        "        elif system == \"darwin\":  # macOS\n",
        "            try:\n",
        "                subprocess.run(['brew', 'install', 'ffmpeg'], check=True, timeout=300)\n",
        "                return True, \"Successfully installed FFmpeg via Homebrew\"\n",
        "            except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "                return False, \"Could not install FFmpeg on macOS. Please install Homebrew and run: brew install ffmpeg\"\n",
        "\n",
        "        elif system == \"linux\":\n",
        "            # Try different Linux package managers\n",
        "            managers = [\n",
        "                (['apt-get', 'update'], ['apt-get', 'install', '-y', 'ffmpeg']),\n",
        "                (['yum', 'install', '-y', 'ffmpeg']),\n",
        "                (['dnf', 'install', '-y', 'ffmpeg']),\n",
        "                (['pacman', '-S', '--noconfirm', 'ffmpeg'])\n",
        "            ]\n",
        "\n",
        "            for commands in managers:\n",
        "                try:\n",
        "                    for cmd in commands:\n",
        "                        subprocess.run(cmd, check=True, timeout=300)\n",
        "                    return True, f\"Successfully installed FFmpeg via {commands[0][0]}\"\n",
        "                except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "                    continue\n",
        "\n",
        "            return False, \"Could not install FFmpeg on Linux. Please install manually.\"\n",
        "\n",
        "        return False, f\"Unsupported platform: {system}\"\n",
        "\n",
        "class AudioProcessor:\n",
        "    \"\"\"Handles audio processing without FFmpeg dependency\"\"\"\n",
        "\n",
        "    def __init__(self, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or logging.getLogger(__name__)\n",
        "        self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup temporary directory\"\"\"\n",
        "        self.cleanup()\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up temporary files\"\"\"\n",
        "        if hasattr(self, 'temp_dir') and self.temp_dir and os.path.exists(self.temp_dir):\n",
        "            try:\n",
        "                shutil.rmtree(self.temp_dir)\n",
        "                self.logger.debug(f\"Cleaned up temp directory: {self.temp_dir}\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Failed to cleanup temp directory: {e}\")\n",
        "\n",
        "    def load_audio_with_librosa(self, file_path: str, sr: int = 16000) -> Tuple[bool, Any, str]:\n",
        "        \"\"\"Load audio using librosa\"\"\"\n",
        "        try:\n",
        "            import librosa\n",
        "            import numpy as np\n",
        "\n",
        "            # Load audio file\n",
        "            y, original_sr = librosa.load(file_path, sr=sr, mono=True)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                return False, None, \"Audio file is empty\"\n",
        "\n",
        "            self.logger.info(f\"Loaded audio with librosa: {len(y)} samples at {sr}Hz\")\n",
        "            return True, y, \"Success\"\n",
        "\n",
        "        except ImportError:\n",
        "            return False, None, \"librosa not available\"\n",
        "        except Exception as e:\n",
        "            return False, None, f\"librosa failed: {str(e)}\"\n",
        "\n",
        "    def load_audio_with_soundfile(self, file_path: str, sr: int = 16000) -> Tuple[bool, Any, str]:\n",
        "        \"\"\"Load audio using soundfile\"\"\"\n",
        "        try:\n",
        "            import soundfile as sf\n",
        "            import numpy as np\n",
        "\n",
        "            # Read audio file\n",
        "            y, original_sr = sf.read(file_path, dtype='float32')\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if len(y.shape) > 1:\n",
        "                y = np.mean(y, axis=1)\n",
        "\n",
        "            # Resample if necessary\n",
        "            if original_sr != sr:\n",
        "                # Simple resampling (not ideal but works without scipy)\n",
        "                ratio = sr / original_sr\n",
        "                new_length = int(len(y) * ratio)\n",
        "                y = np.interp(np.linspace(0, len(y), new_length), np.arange(len(y)), y)\n",
        "\n",
        "            if len(y) == 0:\n",
        "                return False, None, \"Audio file is empty\"\n",
        "\n",
        "            self.logger.info(f\"Loaded audio with soundfile: {len(y)} samples at {sr}Hz\")\n",
        "            return True, y, \"Success\"\n",
        "\n",
        "        except ImportError:\n",
        "            return False, None, \"soundfile not available\"\n",
        "        except Exception as e:\n",
        "            return False, None, f\"soundfile failed: {str(e)}\"\n",
        "\n",
        "    def save_audio_as_wav(self, audio_data: Any, output_path: str, sr: int = 16000) -> bool:\n",
        "        \"\"\"Save audio data as WAV file\"\"\"\n",
        "        try:\n",
        "            import soundfile as sf\n",
        "            sf.write(output_path, audio_data, sr)\n",
        "            return True\n",
        "        except ImportError:\n",
        "            try:\n",
        "                import wave\n",
        "                import numpy as np\n",
        "\n",
        "                # Convert to 16-bit PCM\n",
        "                audio_16bit = (audio_data * 32767).astype(np.int16)\n",
        "\n",
        "                with wave.open(output_path, 'wb') as wav_file:\n",
        "                    wav_file.setnchannels(1)  # mono\n",
        "                    wav_file.setsampwidth(2)  # 16-bit\n",
        "                    wav_file.setframerate(sr)\n",
        "                    wav_file.writeframes(audio_16bit.tobytes())\n",
        "\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to save audio: {e}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to save audio with soundfile: {e}\")\n",
        "            return False\n",
        "\n",
        "    def process_audio_for_whisper(self, file_path: str) -> Tuple[bool, str, str]:\n",
        "        \"\"\"Process audio file for Whisper without FFmpeg\"\"\"\n",
        "        self.logger.info(f\"Processing audio without FFmpeg: {file_path}\")\n",
        "\n",
        "        # Try different loading methods\n",
        "        methods = [\n",
        "            (\"librosa\", self.load_audio_with_librosa),\n",
        "            (\"soundfile\", self.load_audio_with_soundfile)\n",
        "        ]\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            self.logger.info(f\"Trying {method_name}...\")\n",
        "            success, audio_data, message = method_func(file_path)\n",
        "\n",
        "            if success:\n",
        "                # Save processed audio to temp file\n",
        "                temp_path = os.path.join(self.temp_dir, f\"processed_{os.path.basename(file_path)}\")\n",
        "                if not temp_path.endswith('.wav'):\n",
        "                    temp_path = os.path.splitext(temp_path)[0] + '.wav'\n",
        "\n",
        "                if self.save_audio_as_wav(audio_data, temp_path):\n",
        "                    self.logger.info(f\"Successfully processed audio with {method_name}\")\n",
        "                    return True, temp_path, f\"Processed with {method_name}\"\n",
        "                else:\n",
        "                    self.logger.warning(f\"Failed to save processed audio from {method_name}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"{method_name} failed: {message}\")\n",
        "\n",
        "        return False, file_path, \"All audio processing methods failed\"\n",
        "\n",
        "class EnhancedTranscriptionService:\n",
        "    \"\"\"Enhanced transcription service with automatic FFmpeg handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_size: str = \"base\", logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or self._setup_logging()\n",
        "        self.model_size = model_size\n",
        "        self.whisper_model = None\n",
        "        self.audio_processor = AudioProcessor(logger=self.logger)\n",
        "        self.ffmpeg_available = False\n",
        "\n",
        "        # Check and install FFmpeg if needed\n",
        "        self._ensure_ffmpeg()\n",
        "\n",
        "        # Initialize Whisper model\n",
        "        self._initialize_whisper()\n",
        "\n",
        "    def _setup_logging(self) -> logging.Logger:\n",
        "        \"\"\"Setup basic logging\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        return logging.getLogger(__name__)\n",
        "\n",
        "    def _ensure_ffmpeg(self):\n",
        "        \"\"\"Ensure FFmpeg is available, install if needed\"\"\"\n",
        "        self.logger.info(\"Checking FFmpeg availability...\")\n",
        "\n",
        "        available, message = FFmpegInstaller.is_ffmpeg_available()\n",
        "\n",
        "        if available:\n",
        "            self.logger.info(f\"✓ {message}\")\n",
        "            self.ffmpeg_available = True\n",
        "            return\n",
        "\n",
        "        self.logger.warning(f\"✗ {message}\")\n",
        "        self.logger.info(\"Attempting to install FFmpeg automatically...\")\n",
        "\n",
        "        # Try automatic installation\n",
        "        success, install_message = FFmpegInstaller.auto_install_ffmpeg()\n",
        "\n",
        "        if success:\n",
        "            self.logger.info(f\"✓ {install_message}\")\n",
        "\n",
        "            # Verify installation\n",
        "            available, verify_message = FFmpegInstaller.is_ffmpeg_available()\n",
        "            if available:\n",
        "                self.logger.info(\"✓ FFmpeg installation verified\")\n",
        "                self.ffmpeg_available = True\n",
        "                return\n",
        "            else:\n",
        "                self.logger.warning(f\"FFmpeg installation verification failed: {verify_message}\")\n",
        "        else:\n",
        "            self.logger.error(f\"✗ {install_message}\")\n",
        "\n",
        "        self.logger.warning(\"FFmpeg not available. Will use fallback audio processing.\")\n",
        "        self.ffmpeg_available = False\n",
        "\n",
        "    def _initialize_whisper(self):\n",
        "        \"\"\"Initialize Whisper model\"\"\"\n",
        "        try:\n",
        "            import whisper\n",
        "            self.logger.info(f\"Loading Whisper model: {self.model_size}\")\n",
        "            self.whisper_model = whisper.load_model(self.model_size)\n",
        "            self.logger.info(\"✓ Whisper model loaded successfully\")\n",
        "        except ImportError:\n",
        "            self.logger.error(\"✗ Whisper not installed. Install with: pip install openai-whisper\")\n",
        "            self.whisper_model = None\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"✗ Failed to load Whisper model: {e}\")\n",
        "            self.whisper_model = None\n",
        "\n",
        "    def validate_audio_file(self, file_path: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Validate audio file\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            return False, f\"File does not exist: {file_path}\"\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            return False, f\"Path is not a file: {file_path}\"\n",
        "\n",
        "        try:\n",
        "            size = os.path.getsize(file_path)\n",
        "            if size == 0:\n",
        "                return False, f\"File is empty: {file_path}\"\n",
        "            if size < 1000:  # Very small files are likely corrupted\n",
        "                return False, f\"File too small ({size} bytes): {file_path}\"\n",
        "        except OSError as e:\n",
        "            return False, f\"Cannot access file: {e}\"\n",
        "\n",
        "        # Check file extension\n",
        "        valid_extensions = {'.wav', '.mp3', '.flac', '.m4a', '.mp4', '.avi', '.mov', '.aac'}\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        if file_ext not in valid_extensions:\n",
        "            return False, f\"Unsupported file format: {file_ext}\"\n",
        "\n",
        "        return True, \"Valid\"\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path: str, category: str = \"\") -> TranscriptionResult:\n",
        "        \"\"\"Transcribe audio file with enhanced error handling\"\"\"\n",
        "        filename = os.path.basename(audio_path)\n",
        "\n",
        "        self.logger.info(f\"Transcribing: {filename}\")\n",
        "\n",
        "        # Validate file\n",
        "        is_valid, validation_message = self.validate_audio_file(audio_path)\n",
        "        if not is_valid:\n",
        "            self.logger.error(f\"Validation failed: {validation_message}\")\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=validation_message\n",
        "            )\n",
        "\n",
        "        # Check if Whisper is available\n",
        "        if self.whisper_model is None:\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=\"Whisper model not available\"\n",
        "            )\n",
        "\n",
        "        # Determine processing method\n",
        "        processed_path = audio_path\n",
        "        processing_method = \"direct\"\n",
        "\n",
        "        if not self.ffmpeg_available:\n",
        "            # Use fallback audio processing\n",
        "            success, processed_path, processing_method = self.audio_processor.process_audio_for_whisper(audio_path)\n",
        "            if not success:\n",
        "                return TranscriptionResult(\n",
        "                    file_path=audio_path,\n",
        "                    category=category,\n",
        "                    filename=filename,\n",
        "                    transcript=\"\",\n",
        "                    language=\"\",\n",
        "                    segments=0,\n",
        "                    duration=0.0,\n",
        "                    confidence=0.0,\n",
        "                    success=False,\n",
        "                    error_message=f\"Audio processing failed: {processing_method}\"\n",
        "                )\n",
        "\n",
        "        # Transcribe with Whisper\n",
        "        try:\n",
        "            self.logger.info(f\"Starting Whisper transcription using {processing_method}...\")\n",
        "\n",
        "            result = self.whisper_model.transcribe(\n",
        "                processed_path,\n",
        "                fp16=False,\n",
        "                language=None,\n",
        "                task=\"transcribe\",\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            transcript_text = result[\"text\"].strip()\n",
        "            segments = result.get(\"segments\", [])\n",
        "\n",
        "            # Calculate confidence\n",
        "            confidence = 0.0\n",
        "            if segments:\n",
        "                confidences = []\n",
        "                for seg in segments:\n",
        "                    if \"avg_logprob\" in seg:\n",
        "                        # Convert log probability to probability\n",
        "                        confidences.append(max(0.0, min(1.0, 2 ** seg[\"avg_logprob\"])))\n",
        "\n",
        "                if confidences:\n",
        "                    confidence = float(sum(confidences) / len(confidences))\n",
        "\n",
        "            self.logger.info(f\"✓ Successfully transcribed {filename}\")\n",
        "            self.logger.info(f\"  Language: {result.get('language', 'unknown')}\")\n",
        "            self.logger.info(f\"  Duration: {result.get('duration', 0.0):.2f}s\")\n",
        "            self.logger.info(f\"  Segments: {len(segments)}\")\n",
        "            self.logger.info(f\"  Confidence: {confidence:.3f}\")\n",
        "            self.logger.info(f\"  Text length: {len(transcript_text)} characters\")\n",
        "\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=transcript_text,\n",
        "                language=result.get(\"language\", \"unknown\"),\n",
        "                segments=len(segments),\n",
        "                duration=result.get(\"duration\", 0.0),\n",
        "                confidence=confidence,\n",
        "                success=True\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"✗ Transcription failed for {filename}: {e}\")\n",
        "            self.logger.error(f\"  Error type: {type(e).__name__}\")\n",
        "\n",
        "            return TranscriptionResult(\n",
        "                file_path=audio_path,\n",
        "                category=category,\n",
        "                filename=filename,\n",
        "                transcript=\"\",\n",
        "                language=\"\",\n",
        "                segments=0,\n",
        "                duration=0.0,\n",
        "                confidence=0.0,\n",
        "                success=False,\n",
        "                error_message=str(e)\n",
        "            )\n",
        "\n",
        "    def transcribe_batch(self, file_paths: List[str], category: str = \"\") -> List[TranscriptionResult]:\n",
        "        \"\"\"Transcribe multiple files\"\"\"\n",
        "        results = []\n",
        "        total_files = len(file_paths)\n",
        "\n",
        "        self.logger.info(f\"Starting batch transcription of {total_files} files...\")\n",
        "\n",
        "        for i, file_path in enumerate(file_paths, 1):\n",
        "            self.logger.info(f\"Processing file {i}/{total_files}: {os.path.basename(file_path)}\")\n",
        "\n",
        "            result = self.transcribe_audio_file(file_path, category)\n",
        "            results.append(result)\n",
        "\n",
        "            # Log progress\n",
        "            success_count = sum(1 for r in results if r.success)\n",
        "            self.logger.info(f\"Progress: {i}/{total_files} processed, {success_count} successful\")\n",
        "\n",
        "        # Final summary\n",
        "        success_count = sum(1 for r in results if r.success)\n",
        "        self.logger.info(f\"Batch transcription completed: {success_count}/{total_files} successful\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_results(self, results: List[TranscriptionResult], output_dir: str):\n",
        "        \"\"\"Save transcription results\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save as JSON\n",
        "        json_path = os.path.join(output_dir, \"transcription_results.json\")\n",
        "        json_data = []\n",
        "\n",
        "        for result in results:\n",
        "            json_data.append({\n",
        "                \"file_path\": result.file_path,\n",
        "                \"category\": result.category,\n",
        "                \"filename\": result.filename,\n",
        "                \"transcript\": result.transcript,\n",
        "                \"language\": result.language,\n",
        "                \"segments\": result.segments,\n",
        "                \"duration\": result.duration,\n",
        "                \"confidence\": result.confidence,\n",
        "                \"success\": result.success,\n",
        "                \"error_message\": result.error_message\n",
        "            })\n",
        "\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        self.logger.info(f\"Results saved to {json_path}\")\n",
        "\n",
        "        # Save successful transcripts as text files\n",
        "        transcript_dir = os.path.join(output_dir, \"transcripts\")\n",
        "        os.makedirs(transcript_dir, exist_ok=True)\n",
        "\n",
        "        successful_results = [r for r in results if r.success and r.transcript.strip()]\n",
        "\n",
        "        for result in successful_results:\n",
        "            # Create safe filename\n",
        "            safe_filename = \"\".join(c for c in result.filename if c.isalnum() or c in \"._-\")\n",
        "            txt_filename = f\"{safe_filename}_{result.category}.txt\"\n",
        "            txt_path = os.path.join(transcript_dir, txt_filename)\n",
        "\n",
        "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(f\"File: {result.filename}\\n\")\n",
        "                f.write(f\"Category: {result.category}\\n\")\n",
        "                f.write(f\"Language: {result.language}\\n\")\n",
        "                f.write(f\"Duration: {result.duration:.2f}s\\n\")\n",
        "                f.write(f\"Confidence: {result.confidence:.3f}\\n\")\n",
        "                f.write(f\"Segments: {result.segments}\\n\")\n",
        "                f.write(\"-\" * 50 + \"\\n\")\n",
        "                f.write(result.transcript)\n",
        "\n",
        "        self.logger.info(f\"Transcripts saved to {transcript_dir}\")\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        if hasattr(self, 'audio_processor'):\n",
        "            self.audio_processor.cleanup()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for testing\"\"\"\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Try to get config\n",
        "    try:\n",
        "        # Import config if available\n",
        "        sys.path.append(os.path.dirname(__file__))\n",
        "        import config\n",
        "\n",
        "        logger.info(\"Loading audio files from config...\")\n",
        "        audio_files = config.get_audio_file_paths(logger)\n",
        "\n",
        "        # Get a test file\n",
        "        test_file = None\n",
        "        for category, files in audio_files.items():\n",
        "            if files:\n",
        "                test_file = files[0]  # Take first file\n",
        "                test_category = category\n",
        "                break\n",
        "\n",
        "        if not test_file:\n",
        "            logger.error(\"No audio files found in config\")\n",
        "            return\n",
        "\n",
        "    except ImportError:\n",
        "        logger.info(\"Config not available, requesting manual input...\")\n",
        "        test_file = input(\"Enter path to audio file: \").strip().strip('\"')\n",
        "        test_category = \"manual_test\"\n",
        "\n",
        "        if not os.path.exists(test_file):\n",
        "            logger.error(f\"File not found: {test_file}\")\n",
        "            return\n",
        "\n",
        "    # Test transcription service\n",
        "    logger.info(\"=== Testing Enhanced Transcription Service ===\")\n",
        "\n",
        "    service = EnhancedTranscriptionService(logger=logger)\n",
        "\n",
        "    try:\n",
        "        # Test single file\n",
        "        result = service.transcribe_audio_file(test_file, test_category)\n",
        "\n",
        "        print(f\"\\n=== Transcription Result ===\")\n",
        "        print(f\"File: {result.filename}\")\n",
        "        print(f\"Success: {result.success}\")\n",
        "\n",
        "        if result.success:\n",
        "            print(f\"Language: {result.language}\")\n",
        "            print(f\"Duration: {result.duration:.2f}s\")\n",
        "            print(f\"Segments: {result.segments}\")\n",
        "            print(f\"Confidence: {result.confidence:.3f}\")\n",
        "            print(f\"Transcript length: {len(result.transcript)} characters\")\n",
        "            print(f\"Word count: {len(result.transcript.split())}\")\n",
        "            print(f\"\\nFirst 200 characters:\")\n",
        "            print(f'\"{result.transcript[:200]}...\"')\n",
        "        else:\n",
        "            print(f\"Error: {result.error_message}\")\n",
        "\n",
        "            # Provide installation guidance\n",
        "            if \"ffmpeg\" in str(result.error_message).lower():\n",
        "                print(f\"\\n=== FFmpeg Installation Guide ===\")\n",
        "                print(\"FFmpeg is required. The service attempted automatic installation.\")\n",
        "                print(\"If it failed, please install manually:\")\n",
        "                print(\"1. Windows: choco install ffmpeg\")\n",
        "                print(\"2. macOS: brew install ffmpeg\")\n",
        "                print(\"3. Linux: sudo apt install ffmpeg\")\n",
        "                print(\"4. Conda: conda install -c conda-forge ffmpeg\")\n",
        "\n",
        "        # Save results if successful\n",
        "        if result.success:\n",
        "            output_dir = os.path.join(os.path.dirname(test_file), \"transcription_output\")\n",
        "            service.save_results([result], output_dir)\n",
        "            print(f\"\\nResults saved to: {output_dir}\")\n",
        "\n",
        "    finally:\n",
        "        service.cleanup()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "WXfUwmms_3Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fix FFmpeg"
      ],
      "metadata": {
        "id": "XlauG148AW_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Chocolatey (if not already installed)\n",
        "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n",
        "\n",
        "# Install FFmpeg\n",
        "choco install ffmpeg\n",
        "\n",
        "# Restart your terminal/IDE after installation"
      ],
      "metadata": {
        "id": "jn0jaXv5AZe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# linguistic_features_service.py - Linguistic Features Service\n"
      ],
      "metadata": {
        "id": "AlNQFq6Abztw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Linguistic Features Service - Microservice for extracting linguistic features and BERT embeddings\n",
        "Handles text analysis and BERT preprocessing\n",
        "\"\"\"\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from typing import Dict, List, Optional, Any, Tuple\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# BERT imports with error handling\n",
        "try:\n",
        "    from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
        "    BERT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    BERT_AVAILABLE = False\n",
        "\n",
        "from config import MODEL_CONFIG, SYSTEM_CONFIG\n",
        "from utils import (setup_logging, monitor_memory_usage, cleanup_memory,\n",
        "                   safe_save_pickle, ProcessingTimer, batch_generator)\n",
        "from transcription_service import TranscriptionResult\n",
        "\n",
        "@dataclass\n",
        "class LinguisticFeatures:\n",
        "    \"\"\"Data class for linguistic features\"\"\"\n",
        "    # Basic text statistics\n",
        "    raw_text: str\n",
        "    word_count: int\n",
        "    sentence_count: int\n",
        "    char_count: int\n",
        "    avg_word_length: float\n",
        "    avg_sentence_length: float\n",
        "\n",
        "    # Vocabulary features\n",
        "    unique_words: int\n",
        "    lexical_diversity: float\n",
        "    function_words_ratio: float\n",
        "    content_words_ratio: float\n",
        "\n",
        "    # Syntactic features\n",
        "    noun_ratio: float\n",
        "    verb_ratio: float\n",
        "    adjective_ratio: float\n",
        "    pronoun_ratio: float\n",
        "\n",
        "    # Semantic complexity\n",
        "    syllable_count: int\n",
        "    avg_syllables_per_word: float\n",
        "    complex_words_ratio: float  # Words with 3+ syllables\n",
        "\n",
        "    # Discourse features\n",
        "    repetition_ratio: float\n",
        "    pause_indicators: int\n",
        "    filler_words: int\n",
        "\n",
        "    # BERT features\n",
        "    bert_tokens: List[str]\n",
        "    bert_input_ids: List[int]\n",
        "    bert_attention_mask: List[int]\n",
        "    bert_embeddings: Optional[np.ndarray]\n",
        "\n",
        "    # Processing metadata\n",
        "    processing_success: bool\n",
        "    error_message: Optional[str] = None\n",
        "\n",
        "class LinguisticFeaturesService:\n",
        "    \"\"\"Service for extracting linguistic features from transcripts\"\"\"\n",
        "\n",
        "    def __init__(self, logger: Optional[logging.Logger] = None):\n",
        "        self.logger = logger or setup_logging()\n",
        "        self.bert_tokenizer = None\n",
        "        self.bert_model = None\n",
        "\n",
        "        # Define word lists for analysis\n",
        "        self.function_words = self._load_function_words()\n",
        "        self.filler_words = {'um', 'uh', 'er', 'ah', 'hmm', 'well', 'like', 'you know', 'sort of', 'kind of'}\n",
        "        self.pause_indicators = {'[pause]', '[silence]', '...', '--'}\n",
        "\n",
        "        # Simple POS tag mappings for basic analysis\n",
        "        self.noun_patterns = {'NN', 'NNS', 'NNP', 'NNPS'}\n",
        "        self.verb_patterns = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}\n",
        "        self.adjective_patterns = {'JJ', 'JJR', 'JJS'}\n",
        "        self.pronoun_patterns = {'PRP', 'PRP$', 'WP', 'WP$'}\n",
        "\n",
        "        self._initialize_bert()\n",
        "\n",
        "    def _initialize_bert(self):\n",
        "        \"\"\"Initialize BERT model for embeddings\"\"\"\n",
        "        if not BERT_AVAILABLE:\n",
        "            self.logger.warning(\"Transformers not available - BERT features will be limited\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.logger.info(f\"Loading BERT model: {MODEL_CONFIG.bert_model}\")\n",
        "            self.bert_tokenizer = BertTokenizer.from_pretrained(MODEL_CONFIG.bert_model)\n",
        "            self.bert_model = BertModel.from_pretrained(MODEL_CONFIG.bert_model)\n",
        "            self.bert_model.eval()  # Set to evaluation mode\n",
        "            self.logger.info(\"✓ BERT model loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load BERT model: {e}\")\n",
        "            self.bert_tokenizer = None\n",
        "            self.bert_model = None\n",
        "\n",
        "    def _load_function_words(self) -> set:\n",
        "        \"\"\"Load common function words\"\"\"\n",
        "        function_words = {\n",
        "            # Articles\n",
        "            'a', 'an', 'the',\n",
        "            # Prepositions\n",
        "            'in', 'on', 'at', 'by', 'to', 'from', 'of', 'with', 'about', 'into', 'through', 'during',\n",
        "            'before', 'after', 'above', 'below', 'over', 'under', 'between', 'among', 'against',\n",
        "            # Conjunctions\n",
        "            'and', 'or', 'but', 'nor', 'for', 'yet', 'so', 'because', 'since', 'although', 'while',\n",
        "            'if', 'unless', 'until', 'when', 'where', 'how', 'why',\n",
        "            # Pronouns\n",
        "            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "            'my', 'your', 'his', 'her', 'its', 'our', 'their', 'mine', 'yours', 'ours', 'theirs',\n",
        "            'this', 'that', 'these', 'those', 'what', 'which', 'who', 'whom', 'whose',\n",
        "            # Auxiliary verbs\n",
        "            'am', 'is', 'are', 'was', 'were', 'be', 'being', 'been',\n",
        "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
        "            'will', 'would', 'shall', 'should', 'may', 'might', 'can', 'could', 'must',\n",
        "            # Others\n",
        "            'not', 'no', 'yes', 'there', 'here'\n",
        "        }\n",
        "        return function_words\n",
        "\n",
        "    def _count_syllables(self, word: str) -> int:\n",
        "        \"\"\"Count syllables in a word (simple heuristic)\"\"\"\n",
        "        word = word.lower().strip(\".,!?;:\")\n",
        "        if not word:\n",
        "            return 0\n",
        "\n",
        "        # Remove silent 'e' at the end\n",
        "        if word.endswith('e') and len(word) > 1:\n",
        "            word = word[:-1]\n",
        "\n",
        "        # Count vowel groups\n",
        "        vowels = \"aeiouy\"\n",
        "        syllable_count = 0\n",
        "        prev_was_vowel = False\n",
        "\n",
        "        for char in word:\n",
        "            is_vowel = char in vowels\n",
        "            if is_vowel and not prev_was_vowel:\n",
        "                syllable_count += 1\n",
        "            prev_was_vowel = is_vowel\n",
        "\n",
        "        # Every word has at least one syllable\n",
        "        return max(1, syllable_count)\n",
        "\n",
        "    def _extract_basic_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract basic text statistics\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'word_count': 0,\n",
        "                'sentence_count': 0,\n",
        "                'char_count': 0,\n",
        "                'avg_word_length': 0.0,\n",
        "                'avg_sentence_length': 0.0\n",
        "            }\n",
        "\n",
        "        # Clean text\n",
        "        clean_text = text.strip()\n",
        "\n",
        "        # Word analysis\n",
        "        words = clean_text.split()\n",
        "        word_count = len(words)\n",
        "\n",
        "        # Sentence analysis - improved sentence detection\n",
        "        sentences = re.split(r'[.!?]+', clean_text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        sentence_count = len(sentences)\n",
        "\n",
        "        # Character count (excluding spaces)\n",
        "        char_count = len(clean_text.replace(' ', ''))\n",
        "\n",
        "        # Averages\n",
        "        avg_word_length = np.mean([len(word) for word in words]) if words else 0.0\n",
        "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0.0\n",
        "\n",
        "        return {\n",
        "            'word_count': word_count,\n",
        "            'sentence_count': sentence_count,\n",
        "            'char_count': char_count,\n",
        "            'avg_word_length': avg_word_length,\n",
        "            'avg_sentence_length': avg_sentence_length\n",
        "        }\n",
        "\n",
        "    def _extract_vocabulary_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract vocabulary and lexical features\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'unique_words': 0,\n",
        "                'lexical_diversity': 0.0,\n",
        "                'function_words_ratio': 0.0,\n",
        "                'content_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        # Tokenize and clean words\n",
        "        words = text.lower().split()\n",
        "        words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]\n",
        "\n",
        "        if not words:\n",
        "            return {\n",
        "                'unique_words': 0,\n",
        "                'lexical_diversity': 0.0,\n",
        "                'function_words_ratio': 0.0,\n",
        "                'content_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        unique_words = len(set(words))\n",
        "        lexical_diversity = unique_words / len(words)\n",
        "\n",
        "        # Function vs content words\n",
        "        function_word_count = sum(1 for word in words if word in self.function_words)\n",
        "        content_word_count = len(words) - function_word_count\n",
        "\n",
        "        function_words_ratio = function_word_count / len(words)\n",
        "        content_words_ratio = content_word_count / len(words)\n",
        "\n",
        "        return {\n",
        "            'unique_words': unique_words,\n",
        "            'lexical_diversity': lexical_diversity,\n",
        "            'function_words_ratio': function_words_ratio,\n",
        "            'content_words_ratio': content_words_ratio\n",
        "        }\n",
        "\n",
        "    def _extract_syntactic_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract syntactic features (simplified POS analysis)\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'noun_ratio': 0.0,\n",
        "                'verb_ratio': 0.0,\n",
        "                'adjective_ratio': 0.0,\n",
        "                'pronoun_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        words = text.lower().split()\n",
        "        words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]\n",
        "\n",
        "        if not words:\n",
        "            return {\n",
        "                'noun_ratio': 0.0,\n",
        "                'verb_ratio': 0.0,\n",
        "                'adjective_ratio': 0.0,\n",
        "                'pronoun_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        # Simple heuristic-based POS tagging\n",
        "        noun_count = 0\n",
        "        verb_count = 0\n",
        "        adjective_count = 0\n",
        "        pronoun_count = 0\n",
        "\n",
        "        # Common verb endings and forms\n",
        "        verb_endings = {'ed', 'ing', 'es', 's'}\n",
        "        common_verbs = {'is', 'are', 'was', 'were', 'have', 'has', 'had', 'do', 'does', 'did',\n",
        "                       'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must'}\n",
        "\n",
        "        # Common adjective endings\n",
        "        adj_endings = {'ly', 'ful', 'less', 'ous', 'ive', 'able', 'ible'}\n",
        "\n",
        "        # Common pronouns (already in function words, but specific ones)\n",
        "        pronouns = {'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "                   'my', 'your', 'his', 'her', 'its', 'our', 'their', 'this', 'that', 'these', 'those'}\n",
        "\n",
        "        for word in words:\n",
        "            # Check pronouns first\n",
        "            if word in pronouns:\n",
        "                pronoun_count += 1\n",
        "            # Check common verbs\n",
        "            elif word in common_verbs or any(word.endswith(ending) for ending in verb_endings if len(word) > 3):\n",
        "                verb_count += 1\n",
        "            # Check adjectives (simple heuristic)\n",
        "            elif any(word.endswith(ending) for ending in adj_endings):\n",
        "                adjective_count += 1\n",
        "            # Default to noun if capitalized or doesn't match other patterns\n",
        "            else:\n",
        "                noun_count += 1\n",
        "\n",
        "        total_words = len(words)\n",
        "        return {\n",
        "            'noun_ratio': noun_count / total_words,\n",
        "            'verb_ratio': verb_count / total_words,\n",
        "            'adjective_ratio': adjective_count / total_words,\n",
        "            'pronoun_ratio': pronoun_count / total_words\n",
        "        }\n",
        "\n",
        "    def _extract_semantic_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract semantic complexity features\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'syllable_count': 0,\n",
        "                'avg_syllables_per_word': 0.0,\n",
        "                'complex_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        words = text.split()\n",
        "        words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]\n",
        "\n",
        "        if not words:\n",
        "            return {\n",
        "                'syllable_count': 0,\n",
        "                'avg_syllables_per_word': 0.0,\n",
        "                'complex_words_ratio': 0.0\n",
        "            }\n",
        "\n",
        "        syllable_counts = [self._count_syllables(word) for word in words]\n",
        "        total_syllables = sum(syllable_counts)\n",
        "        complex_words = sum(1 for count in syllable_counts if count >= 3)\n",
        "\n",
        "        return {\n",
        "            'syllable_count': total_syllables,\n",
        "            'avg_syllables_per_word': total_syllables / len(words),\n",
        "            'complex_words_ratio': complex_words / len(words)\n",
        "        }\n",
        "\n",
        "    def _extract_discourse_features(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract discourse and disfluency features\"\"\"\n",
        "        if not text.strip():\n",
        "            return {\n",
        "                'repetition_ratio': 0.0,\n",
        "                'pause_indicators': 0,\n",
        "                'filler_words': 0\n",
        "            }\n",
        "\n",
        "        # Count pause indicators\n",
        "        pause_count = 0\n",
        "        text_lower = text.lower()\n",
        "        for indicator in self.pause_indicators:\n",
        "            pause_count += text_lower.count(indicator)\n",
        "\n",
        "        # Count filler words\n",
        "        words = text.lower().split()\n",
        "        filler_count = 0\n",
        "        for filler in self.filler_words:\n",
        "            if ' ' in filler:  # Multi-word fillers\n",
        "                filler_count += text_lower.count(filler)\n",
        "            else:  # Single word fillers\n",
        "                filler_count += words.count(filler)\n",
        "\n",
        "        # Calculate repetition ratio (simple word repetition)\n",
        "        word_counts = Counter(words)\n",
        "        repeated_words = sum(count - 1 for count in word_counts.values() if count > 1)\n",
        "        repetition_ratio = repeated_words / len(words) if words else 0.0\n",
        "\n",
        "        return {\n",
        "            'repetition_ratio': repetition_ratio,\n",
        "            'pause_indicators': pause_count,\n",
        "            'filler_words': filler_count\n",
        "        }\n",
        "\n",
        "    def _extract_bert_features(self, text: str, max_length: int = 512) -> Dict[str, Any]:\n",
        "        \"\"\"Extract BERT tokens and embeddings\"\"\"\n",
        "        if not BERT_AVAILABLE or not self.bert_tokenizer:\n",
        "            return {\n",
        "                'bert_tokens': [],\n",
        "                'bert_input_ids': [],\n",
        "                'bert_attention_mask': [],\n",
        "                'bert_embeddings': None\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Tokenize text\n",
        "            encoded = self.bert_tokenizer.encode_plus(\n",
        "                text,\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Get tokens for analysis\n",
        "            tokens = self.bert_tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
        "\n",
        "            # Get embeddings if model is available\n",
        "            embeddings = None\n",
        "            if self.bert_model:\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.bert_model(**encoded)\n",
        "                    # Use [CLS] token embedding as sentence representation\n",
        "                    embeddings = outputs.last_hidden_state[0][0].cpu().numpy()\n",
        "\n",
        "            return {\n",
        "                'bert_tokens': tokens,\n",
        "                'bert_input_ids': encoded['input_ids'][0].tolist(),\n",
        "                'bert_attention_mask': encoded['attention_mask'][0].tolist(),\n",
        "                'bert_embeddings': embeddings\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting BERT features: {e}\")\n",
        "            return {\n",
        "                'bert_tokens': [],\n",
        "                'bert_input_ids': [],\n",
        "                'bert_attention_mask': [],\n",
        "                'bert_embeddings': None\n",
        "            }\n",
        "\n",
        "    def extract_features(self, text: str) -> LinguisticFeatures:\n",
        "        \"\"\"Extract all linguistic features from text\"\"\"\n",
        "        try:\n",
        "            if not text or not text.strip():\n",
        "                self.logger.warning(\"Empty text provided for feature extraction\")\n",
        "                return self._create_empty_features(text, \"Empty text provided\")\n",
        "\n",
        "            self.logger.info(f\"Extracting linguistic features from text ({len(text)} characters)\")\n",
        "\n",
        "            with ProcessingTimer() as timer:\n",
        "                # Extract different feature categories\n",
        "                basic_features = self._extract_basic_features(text)\n",
        "                vocabulary_features = self._extract_vocabulary_features(text)\n",
        "                syntactic_features = self._extract_syntactic_features(text)\n",
        "                semantic_features = self._extract_semantic_features(text)\n",
        "                discourse_features = self._extract_discourse_features(text)\n",
        "                bert_features = self._extract_bert_features(text)\n",
        "\n",
        "                # Create LinguisticFeatures object\n",
        "                features = LinguisticFeatures(\n",
        "                    raw_text=text,\n",
        "                    processing_success=True,\n",
        "                    **basic_features,\n",
        "                    **vocabulary_features,\n",
        "                    **syntactic_features,\n",
        "                    **semantic_features,\n",
        "                    **discourse_features,\n",
        "                    **bert_features\n",
        "                )\n",
        "\n",
        "            self.logger.info(f\"✓ Feature extraction completed in {timer.elapsed:.2f}s\")\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting linguistic features: {e}\")\n",
        "            return self._create_empty_features(text, str(e))\n",
        "\n",
        "    def _create_empty_features(self, text: str, error_message: str) -> LinguisticFeatures:\n",
        "        \"\"\"Create empty features object for error cases\"\"\"\n",
        "        return LinguisticFeatures(\n",
        "            raw_text=text or \"\",\n",
        "            word_count=0,\n",
        "            sentence_count=0,\n",
        "            char_count=0,\n",
        "            avg_word_length=0.0,\n",
        "            avg_sentence_length=0.0,\n",
        "            unique_words=0,\n",
        "            lexical_diversity=0.0,\n",
        "            function_words_ratio=0.0,\n",
        "            content_words_ratio=0.0,\n",
        "            noun_ratio=0.0,\n",
        "            verb_ratio=0.0,\n",
        "            adjective_ratio=0.0,\n",
        "            pronoun_ratio=0.0,\n",
        "            syllable_count=0,\n",
        "            avg_syllables_per_word=0.0,\n",
        "            complex_words_ratio=0.0,\n",
        "            repetition_ratio=0.0,\n",
        "            pause_indicators=0,\n",
        "            filler_words=0,\n",
        "            bert_tokens=[],\n",
        "            bert_input_ids=[],\n",
        "            bert_attention_mask=[],\n",
        "            bert_embeddings=None,\n",
        "            processing_success=False,\n",
        "            error_message=error_message\n",
        "        )\n",
        "\n",
        "    def process_transcription_result(self, result: TranscriptionResult) -> LinguisticFeatures:\n",
        "        \"\"\"Process a TranscriptionResult to extract linguistic features\"\"\"\n",
        "        try:\n",
        "            if not result.success or not result.text:\n",
        "                return self._create_empty_features(\n",
        "                    result.text or \"\",\n",
        "                    f\"Transcription failed: {result.error_message}\"\n",
        "                )\n",
        "\n",
        "            return self.extract_features(result.text)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing transcription result: {e}\")\n",
        "            return self._create_empty_features(\"\", str(e))\n",
        "\n",
        "    def batch_process_texts(self, texts: List[str], batch_size: int = 32) -> List[LinguisticFeatures]:\n",
        "        \"\"\"Process multiple texts in batches\"\"\"\n",
        "        try:\n",
        "            self.logger.info(f\"Processing {len(texts)} texts in batches of {batch_size}\")\n",
        "            results = []\n",
        "\n",
        "            with ProcessingTimer() as timer:\n",
        "                for batch in batch_generator(texts, batch_size):\n",
        "                    batch_results = []\n",
        "                    for text in batch:\n",
        "                        features = self.extract_features(text)\n",
        "                        batch_results.append(features)\n",
        "                    results.extend(batch_results)\n",
        "\n",
        "                    # Memory cleanup between batches\n",
        "                    cleanup_memory()\n",
        "\n",
        "            self.logger.info(f\"✓ Batch processing completed in {timer.elapsed:.2f}s\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in batch processing: {e}\")\n",
        "            return [self._create_empty_features(text, str(e)) for text in texts]\n",
        "\n",
        "    def get_feature_summary(self, features: LinguisticFeatures) -> Dict[str, Any]:\n",
        "        \"\"\"Get a summary of extracted features\"\"\"\n",
        "        if not features.processing_success:\n",
        "            return {\n",
        "                'status': 'failed',\n",
        "                'error': features.error_message,\n",
        "                'text_length': len(features.raw_text)\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'status': 'success',\n",
        "            'basic_stats': {\n",
        "                'words': features.word_count,\n",
        "                'sentences': features.sentence_count,\n",
        "                'characters': features.char_count,\n",
        "                'avg_word_length': round(features.avg_word_length, 2),\n",
        "                'avg_sentence_length': round(features.avg_sentence_length, 2)\n",
        "            },\n",
        "            'vocabulary': {\n",
        "                'unique_words': features.unique_words,\n",
        "                'lexical_diversity': round(features.lexical_diversity, 3),\n",
        "                'function_words_ratio': round(features.function_words_ratio, 3),\n",
        "                'content_words_ratio': round(features.content_words_ratio, 3)\n",
        "            },\n",
        "            'complexity': {\n",
        "                'avg_syllables_per_word': round(features.avg_syllables_per_word, 2),\n",
        "                'complex_words_ratio': round(features.complex_words_ratio, 3),\n",
        "                'total_syllables': features.syllable_count\n",
        "            },\n",
        "            'discourse': {\n",
        "                'repetition_ratio': round(features.repetition_ratio, 3),\n",
        "                'pause_indicators': features.pause_indicators,\n",
        "                'filler_words': features.filler_words\n",
        "            },\n",
        "            'bert_available': features.bert_embeddings is not None,\n",
        "            'bert_tokens_count': len(features.bert_tokens)\n",
        "        }\n",
        "\n",
        "    def save_features(self, features: LinguisticFeatures, filepath: str) -> bool:\n",
        "        \"\"\"Save linguistic features to file\"\"\"\n",
        "        try:\n",
        "            return safe_save_pickle(features, filepath)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving features: {e}\")\n",
        "            return False\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        try:\n",
        "            if self.bert_model:\n",
        "                del self.bert_model\n",
        "            if self.bert_tokenizer:\n",
        "                del self.bert_tokenizer\n",
        "            cleanup_memory()\n",
        "            self.logger.info(\"✓ Linguistic features service cleanup completed\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during cleanup: {e}\")\n",
        "\n",
        "# Factory function for easy service creation\n",
        "def create_linguistic_features_service(logger: Optional[logging.Logger] = None) -> LinguisticFeaturesService:\n",
        "    \"\"\"Factory function to create a LinguisticFeaturesService instance\"\"\"\n",
        "    return LinguisticFeaturesService(logger=logger)"
      ],
      "metadata": {
        "id": "-6Rgd_dsdAl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data_Manager_Service.py"
      ],
      "metadata": {
        "id": "Xf2ucroSl-i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Manager Service - Handles ADReSSo21 dataset loading and file management\n",
        "Optimized for Windows 10 with parallel processing\n",
        "\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pickle\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "from config import SYSTEM_CONFIG, MODEL_CONFIG\n",
        "from utils import setup_logging, ProcessingTimer, safe_save_pickle, cleanup_memory\n",
        "\n",
        "@dataclass\n",
        "class AudioFile:\n",
        "    \"\"\"Data class for audio file information\"\"\"\n",
        "    file_path: str\n",
        "    filename: str\n",
        "    category: str\n",
        "    label: str\n",
        "    dataset_type: str  # 'diagnosis' or 'progression'\n",
        "    split: str  # 'train' or 'test'\n",
        "    segmentation_path: Optional[str] = None\n",
        "    file_size: Optional[int] = None\n",
        "    duration: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class DatasetInfo:\n",
        "    \"\"\"Data class for dataset information\"\"\"\n",
        "    total_files: int\n",
        "    categories: Dict[str, int]\n",
        "    dataset_types: Dict[str, int]\n",
        "    splits: Dict[str, int]\n",
        "    total_size_mb: float\n",
        "    audio_files: List[AudioFile]\n",
        "\n",
        "class DataManagerService:\n",
        "    \"\"\"Service for managing ADReSSo21 dataset files and metadata\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str, logger: Optional[logging.Logger] = None):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.logger = logger or setup_logging()\n",
        "\n",
        "        # Define dataset structure based on your paths\n",
        "        self.dataset_paths = {\n",
        "            'diagnosis_train': {\n",
        "                'audio': {\n",
        "                    'ad': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\ad\",\n",
        "                    'cn': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\audio\\cn\"\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'ad': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\ad\",\n",
        "                    'cn': r\"ADReSSo21-diagnosis-train\\ADReSSo21\\diagnosis\\train\\segmentation\\cn\"\n",
        "                }\n",
        "            },\n",
        "            'progression_train': {\n",
        "                'audio': {\n",
        "                    'decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\decline\",\n",
        "                    'no_decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\audio\\no_decline\"\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\decline\",\n",
        "                    'no_decline': r\"ADReSSo21-progression-train\\ADReSSo21\\progression\\train\\segmentation\\no_decline\"\n",
        "                }\n",
        "            },\n",
        "            'progression_test': {\n",
        "                'audio': {\n",
        "                    'test': r\"ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\audio\"\n",
        "                },\n",
        "                'segmentation': {\n",
        "                    'test': r\"ADReSSo21-progression-test\\ADReSSo21\\progression\\test-dist\\segmentation\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Create output directories\n",
        "        self.output_dir = self.base_path / \"output\"\n",
        "        self.create_output_directories()\n",
        "\n",
        "    def create_output_directories(self):\n",
        "        \"\"\"Create necessary output directories\"\"\"\n",
        "        directories = [\n",
        "            self.output_dir,\n",
        "            self.output_dir / \"features\",\n",
        "            self.output_dir / \"transcripts\",\n",
        "            self.output_dir / \"models\",\n",
        "            self.output_dir / \"results\",\n",
        "            self.output_dir / \"logs\",\n",
        "            self.output_dir / \"cache\"\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.logger.info(f\"✓ Output directories created at {self.output_dir}\")\n",
        "\n",
        "    def scan_audio_files(self) -> List[AudioFile]:\n",
        "        \"\"\"Scan and catalog all audio files in the dataset\"\"\"\n",
        "        self.logger.info(\"Scanning audio files in dataset...\")\n",
        "        audio_files = []\n",
        "\n",
        "        with ProcessingTimer() as timer:\n",
        "            for dataset_name, paths in self.dataset_paths.items():\n",
        "                self.logger.info(f\"Scanning {dataset_name}...\")\n",
        "\n",
        "                # Extract dataset info from name\n",
        "                if 'diagnosis' in dataset_name:\n",
        "                    dataset_type = 'diagnosis'\n",
        "                    split = 'train'\n",
        "                else:  # progression\n",
        "                    dataset_type = 'progression'\n",
        "                    split = 'test' if 'test' in dataset_name else 'train'\n",
        "\n",
        "                # Scan audio directories\n",
        "                for label, audio_path in paths['audio'].items():\n",
        "                    full_audio_path = self.base_path / audio_path\n",
        "\n",
        "                    if not full_audio_path.exists():\n",
        "                        self.logger.warning(f\"Audio path not found: {full_audio_path}\")\n",
        "                        continue\n",
        "\n",
        "                    # Find segmentation path\n",
        "                    seg_path = None\n",
        "                    if 'segmentation' in paths:\n",
        "                        seg_key = label if label in paths['segmentation'] else 'test'\n",
        "                        if seg_key in paths['segmentation']:\n",
        "                            seg_path = self.base_path / paths['segmentation'][seg_key]\n",
        "\n",
        "                    # Get all WAV files\n",
        "                    wav_files = list(full_audio_path.glob(\"*.wav\"))\n",
        "\n",
        "                    for wav_file in wav_files:\n",
        "                        # Find corresponding segmentation file\n",
        "                        seg_file = None\n",
        "                        if seg_path and seg_path.exists():\n",
        "                            seg_file_path = seg_path / f\"{wav_file.stem}.csv\"\n",
        "                            if seg_file_path.exists():\n",
        "                                seg_file = str(seg_file_path)\n",
        "\n",
        "                        audio_file = AudioFile(\n",
        "                            file_path=str(wav_file),\n",
        "                            filename=wav_file.name,\n",
        "                            category=f\"{dataset_type}_{label}\",\n",
        "                            label=label,\n",
        "                            dataset_type=dataset_type,\n",
        "                            split=split,\n",
        "                            segmentation_path=seg_file,\n",
        "                            file_size=wav_file.stat().st_size if wav_file.exists() else None\n",
        "                        )\n",
        "\n",
        "                        audio_files.append(audio_file)\n",
        "\n",
        "        self.logger.info(f\"✓ Found {len(audio_files)} audio files in {timer.elapsed:.2f}s\")\n",
        "        return audio_files\n",
        "\n",
        "    def get_dataset_info(self, audio_files: List[AudioFile]) -> DatasetInfo:\n",
        "        \"\"\"Generate comprehensive dataset information\"\"\"\n",
        "        if not audio_files:\n",
        "            return DatasetInfo(0, {}, {}, {}, 0.0, [])\n",
        "\n",
        "        # Count by categories\n",
        "        categories = {}\n",
        "        dataset_types = {}\n",
        "        splits = {}\n",
        "        total_size = 0\n",
        "\n",
        "        for af in audio_files:\n",
        "            # Count categories\n",
        "            categories[af.category] = categories.get(af.category, 0) + 1\n",
        "            dataset_types[af.dataset_type] = dataset_types.get(af.dataset_type, 0) + 1\n",
        "            splits[af.split] = splits.get(af.split, 0) + 1\n",
        "\n",
        "            # Sum file sizes\n",
        "            if af.file_size:\n",
        "                total_size += af.file_size\n",
        "\n",
        "        return DatasetInfo(\n",
        "            total_files=len(audio_files),\n",
        "            categories=categories,\n",
        "            dataset_types=dataset_types,\n",
        "            splits=splits,\n",
        "            total_size_mb=total_size / (1024 * 1024),\n",
        "            audio_files=audio_files\n",
        "        )\n",
        "\n",
        "    def create_file_manifest(self, audio_files: List[AudioFile]) -> pd.DataFrame:\n",
        "        \"\"\"Create a detailed file manifest\"\"\"\n",
        "        data = []\n",
        "\n",
        "        for af in audio_files:\n",
        "            data.append({\n",
        "                'filename': af.filename,\n",
        "                'file_path': af.file_path,\n",
        "                'category': af.category,\n",
        "                'label': af.label,\n",
        "                'dataset_type': af.dataset_type,\n",
        "                'split': af.split,\n",
        "                'segmentation_path': af.segmentation_path,\n",
        "                'has_segmentation': af.segmentation_path is not None,\n",
        "                'file_size_mb': af.file_size / (1024 * 1024) if af.file_size else None,\n",
        "                'file_exists': os.path.exists(af.file_path)\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Save manifest\n",
        "        manifest_path = self.output_dir / \"file_manifest.csv\"\n",
        "        df.to_csv(manifest_path, index=False)\n",
        "        self.logger.info(f\"✓ File manifest saved to {manifest_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def validate_dataset(self, audio_files: List[AudioFile]) -> Dict[str, Any]:\n",
        "        \"\"\"Validate dataset integrity\"\"\"\n",
        "        self.logger.info(\"Validating dataset integrity...\")\n",
        "\n",
        "        validation_results = {\n",
        "            'total_files': len(audio_files),\n",
        "            'valid_files': 0,\n",
        "            'missing_files': 0,\n",
        "            'files_with_segmentation': 0,\n",
        "            'missing_segmentation': 0,\n",
        "            'errors': []\n",
        "        }\n",
        "\n",
        "        for af in audio_files:\n",
        "            # Check if audio file exists\n",
        "            if not os.path.exists(af.file_path):\n",
        "                validation_results['missing_files'] += 1\n",
        "                validation_results['errors'].append(f\"Missing audio: {af.file_path}\")\n",
        "                continue\n",
        "\n",
        "            validation_results['valid_files'] += 1\n",
        "\n",
        "            # Check segmentation file\n",
        "            if af.segmentation_path:\n",
        "                if os.path.exists(af.segmentation_path):\n",
        "                    validation_results['files_with_segmentation'] += 1\n",
        "                else:\n",
        "                    validation_results['missing_segmentation'] += 1\n",
        "                    validation_results['errors'].append(f\"Missing segmentation: {af.segmentation_path}\")\n",
        "\n",
        "        # Save validation report\n",
        "        report_path = self.output_dir / \"validation_report.json\"\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(validation_results, f, indent=2)\n",
        "\n",
        "        self.logger.info(f\"✓ Dataset validation complete. Report saved to {report_path}\")\n",
        "        return validation_results\n",
        "\n",
        "    def get_files_by_category(self, audio_files: List[AudioFile],\n",
        "                            category: Optional[str] = None,\n",
        "                            dataset_type: Optional[str] = None,\n",
        "                            split: Optional[str] = None) -> List[AudioFile]:\n",
        "        \"\"\"Filter audio files by category, dataset type, or split\"\"\"\n",
        "        filtered_files = audio_files\n",
        "\n",
        "        if category:\n",
        "            filtered_files = [af for af in filtered_files if af.category == category]\n",
        "\n",
        "        if dataset_type:\n",
        "            filtered_files = [af for af in filtered_files if af.dataset_type == dataset_type]\n",
        "\n",
        "        if split:\n",
        "            filtered_files = [af for af in filtered_files if af.split == split]\n",
        "\n",
        "        return filtered_files\n",
        "\n",
        "    def create_train_test_splits(self, audio_files: List[AudioFile],\n",
        "                                test_size: float = 0.2) -> Tuple[List[AudioFile], List[AudioFile]]:\n",
        "        \"\"\"Create train/test splits for datasets that don't have predefined splits\"\"\"\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        # Group by category to ensure balanced splits\n",
        "        category_files = {}\n",
        "        for af in audio_files:\n",
        "            if af.category not in category_files:\n",
        "                category_files[af.category] = []\n",
        "            category_files[af.category].append(af)\n",
        "\n",
        "        train_files = []\n",
        "        test_files = []\n",
        "\n",
        "        for category, files in category_files.items():\n",
        "            if len(files) < 2:\n",
        "                # If too few files, put all in training\n",
        "                train_files.extend(files)\n",
        "                continue\n",
        "\n",
        "            cat_train, cat_test = train_test_split(\n",
        "                files,\n",
        "                test_size=test_size,\n",
        "                random_state=42,\n",
        "                stratify=None  # Can't stratify on single category\n",
        "            )\n",
        "\n",
        "            train_files.extend(cat_train)\n",
        "            test_files.extend(cat_test)\n",
        "\n",
        "        self.logger.info(f\"✓ Created splits: {len(train_files)} train, {len(test_files)} test\")\n",
        "        return train_files, test_files\n",
        "\n",
        "    def batch_load_files(self, audio_files: List[AudioFile],\n",
        "                        batch_size: int = 32) -> List[List[AudioFile]]:\n",
        "        \"\"\"Create batches of files for parallel processing\"\"\"\n",
        "        batches = []\n",
        "        for i in range(0, len(audio_files), batch_size):\n",
        "            batch = audio_files[i:i + batch_size]\n",
        "            batches.append(batch)\n",
        "\n",
        "        self.logger.info(f\"✓ Created {len(batches)} batches of size {batch_size}\")\n",
        "        return batches\n",
        "\n",
        "    def save_dataset_cache(self, audio_files: List[AudioFile],\n",
        "                          dataset_info: DatasetInfo) -> bool:\n",
        "        \"\"\"Save dataset information to cache for faster loading\"\"\"\n",
        "        try:\n",
        "            cache_data = {\n",
        "                'audio_files': audio_files,\n",
        "                'dataset_info': dataset_info,\n",
        "                'scan_timestamp': pd.Timestamp.now().isoformat(),\n",
        "                'base_path': str(self.base_path)\n",
        "            }\n",
        "\n",
        "            cache_path = self.output_dir / \"cache\" / \"dataset_cache.pkl\"\n",
        "            success = safe_save_pickle(cache_data, cache_path)\n",
        "\n",
        "            if success:\n",
        "                self.logger.info(f\"✓ Dataset cache saved to {cache_path}\")\n",
        "\n",
        "            return success\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving dataset cache: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_dataset_cache(self) -> Optional[Tuple[List[AudioFile], DatasetInfo]]:\n",
        "        \"\"\"Load dataset information from cache\"\"\"\n",
        "        try:\n",
        "            cache_path = self.output_dir / \"cache\" / \"dataset_cache.pkl\"\n",
        "\n",
        "            if not cache_path.exists():\n",
        "                return None\n",
        "\n",
        "            with open(cache_path, 'rb') as f:\n",
        "                cache_data = pickle.load(f)\n",
        "\n",
        "            # Verify cache is for same base path\n",
        "            if cache_data.get('base_path') != str(self.base_path):\n",
        "                self.logger.warning(\"Cache base path mismatch, ignoring cache\")\n",
        "                return None\n",
        "\n",
        "            self.logger.info(\"✓ Loaded dataset from cache\")\n",
        "            return cache_data['audio_files'], cache_data['dataset_info']\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading dataset cache: {e}\")\n",
        "            return None\n",
        "\n",
        "    def print_dataset_summary(self, dataset_info: DatasetInfo):\n",
        "        \"\"\"Print a comprehensive dataset summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ADRESSO21 DATASET SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"Total Files: {dataset_info.total_files}\")\n",
        "        print(f\"Total Size: {dataset_info.total_size_mb:.2f} MB\")\n",
        "        print()\n",
        "\n",
        "        print(\"By Dataset Type:\")\n",
        "        for dtype, count in dataset_info.dataset_types.items():\n",
        "            print(f\"  {dtype}: {count} files\")\n",
        "        print()\n",
        "\n",
        "        print(\"By Split:\")\n",
        "        for split, count in dataset_info.splits.items():\n",
        "            print(f\"  {split}: {count} files\")\n",
        "        print()\n",
        "\n",
        "        print(\"By Category:\")\n",
        "        for category, count in dataset_info.categories.items():\n",
        "            print(f\"  {category}: {count} files\")\n",
        "        print()\n",
        "\n",
        "    def cleanup_output_directory(self, keep_cache: bool = True):\n",
        "        \"\"\"Clean up output directory\"\"\"\n",
        "        try:\n",
        "            for item in self.output_dir.iterdir():\n",
        "                if item.is_dir():\n",
        "                    if item.name == 'cache' and keep_cache:\n",
        "                        continue\n",
        "                    shutil.rmtree(item)\n",
        "                else:\n",
        "                    item.unlink()\n",
        "\n",
        "            # Recreate directories\n",
        "            self.create_output_directories()\n",
        "            self.logger.info(\"✓ Output directory cleaned\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error cleaning output directory: {e}\")\n",
        "\n",
        "# Factory function\n",
        "def create_data_manager(base_path: str, logger: Optional[logging.Logger] = None) -> DataManagerService:\n",
        "    \"\"\"Factory function to create DataManagerService\"\"\"\n",
        "    return DataManagerService(base_path, logger)"
      ],
      "metadata": {
        "id": "6_uUyumLfo2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model_architecture_service.py - Neural Network Models Service\n"
      ],
      "metadata": {
        "id": "1iVR4y0Ym6UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model_architecture_service.py - Neural Network Models Service\n",
        "Implements multi-modal deep learning architecture for ADReSSo21\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "from transformers import BertModel\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class GraphAttentionModule(nn.Module):\n",
        "    \"\"\"Graph-based attention module for semantic relationships\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int = 768, hidden_dim: int = 256,\n",
        "                 num_heads: int = 8, num_layers: int = 3, dropout: float = 0.2):\n",
        "        super(GraphAttentionModule, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Graph attention layers\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            GATConv(input_dim if i == 0 else hidden_dim,\n",
        "                   hidden_dim, heads=num_heads, dropout=dropout)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final projection\n",
        "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        # Apply GAT layers\n",
        "        for i, gat_layer in enumerate(self.gat_layers):\n",
        "            x = gat_layer(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global pooling if batch is provided\n",
        "        if batch is not None:\n",
        "            x = global_mean_pool(x, batch)\n",
        "        else:\n",
        "            x = torch.mean(x, dim=0, keepdim=True)\n",
        "\n",
        "        return self.projection(x)\n",
        "\n",
        "\n",
        "class VisionTransformerModule(nn.Module):\n",
        "    \"\"\"Vision Transformer for processing spectrograms\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int = 80, patch_size: int = 8,\n",
        "                 embed_dim: int = 768, num_heads: int = 12, num_layers: int = 6):\n",
        "        super(VisionTransformerModule, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Conv2d(1, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1000, embed_dim))  # Max patches\n",
        "\n",
        "        # Transformer layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,\n",
        "            dropout=0.1, activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embed_dim, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, channels, height, width)\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Create patches\n",
        "        x = self.patch_embed(x)  # (B, embed_dim, H', W')\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        num_patches = x.shape[1]\n",
        "        x = x + self.pos_embed[:, :num_patches, :]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = x.transpose(0, 1)  # (num_patches, B, embed_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.transpose(0, 1)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = torch.mean(x, dim=1)  # (B, embed_dim)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class UNetModule(nn.Module):\n",
        "    \"\"\"U-Net for audio feature processing\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int = 1, out_channels: int = 128):\n",
        "        super(UNetModule, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = self.upconv_block(1024, 512)\n",
        "        self.dec3 = self.upconv_block(512, 256)\n",
        "        self.dec2 = self.upconv_block(256, 128)\n",
        "        self.dec1 = self.upconv_block(128, 64)\n",
        "\n",
        "        # Final layer\n",
        "        self.final = nn.Conv1d(64, out_channels, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def conv_block(self, in_ch: int, out_ch: int):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def upconv_block(self, in_ch: int, out_ch: int):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_ch, out_ch, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(F.max_pool1d(e1, 2))\n",
        "        e3 = self.enc3(F.max_pool1d(e2, 2))\n",
        "        e4 = self.enc4(F.max_pool1d(e3, 2))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool1d(e4, 2))\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.dec4(b)\n",
        "        d3 = self.dec3(d4)\n",
        "        d2 = self.dec2(d3)\n",
        "        d1 = self.dec1(d2)\n",
        "\n",
        "        # Final\n",
        "        out = self.final(d1)\n",
        "        out = self.pool(out).squeeze(-1)  # Global average pooling\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class AlexNetModule(nn.Module):\n",
        "    \"\"\"Modified AlexNet for feature extraction\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int = 768, num_classes: int = 256):\n",
        "        super(AlexNetModule, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(input_dim, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiModalADReSSoModel(nn.Module):\n",
        "    \"\"\"Complete multi-modal architecture for ADReSSo21\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 audio_feature_dim: int = 768,\n",
        "                 text_feature_dim: int = 768,\n",
        "                 spectrogram_height: int = 80,\n",
        "                 num_classes: int = 2,\n",
        "                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        super(MultiModalADReSSoModel, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Initialize modules\n",
        "        self.graph_attention = GraphAttentionModule(input_dim=text_feature_dim)\n",
        "        self.vision_transformer = VisionTransformerModule(input_dim=spectrogram_height)\n",
        "        self.unet = UNetModule()\n",
        "        self.alexnet = AlexNetModule(input_dim=audio_feature_dim)\n",
        "\n",
        "        # BERT for text processing\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(256 + 256 + 128 + 256, 512),  # Graph + ViT + UNet + AlexNet\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Move to device\n",
        "        self.to(device)\n",
        "\n",
        "    def create_semantic_graph(self, text_features, audio_features):\n",
        "        \"\"\"Create semantic relationship graph between audio and text\"\"\"\n",
        "        batch_size = text_features.shape[0]\n",
        "        graphs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Compute similarity matrix\n",
        "            text_feat = text_features[i].unsqueeze(0)  # (1, dim)\n",
        "            audio_feat = audio_features[i].unsqueeze(0)  # (1, dim)\n",
        "\n",
        "            # Create nodes (text + audio features)\n",
        "            node_features = torch.cat([text_feat, audio_feat], dim=0)  # (2, dim)\n",
        "\n",
        "            # Create edges based on similarity\n",
        "            similarity = F.cosine_similarity(text_feat, audio_feat, dim=1)\n",
        "\n",
        "            # Create bidirectional edges if similarity > threshold\n",
        "            if similarity.item() > 0.1:\n",
        "                edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long).t()\n",
        "            else:\n",
        "                # Self-loops only\n",
        "                edge_index = torch.tensor([[0, 1], [0, 1]], dtype=torch.long).t()\n",
        "\n",
        "            graph = Data(x=node_features, edge_index=edge_index.to(self.device))\n",
        "            graphs.append(graph)\n",
        "\n",
        "        return Batch.from_data_list(graphs).to(self.device)\n",
        "\n",
        "    def forward(self, audio_features, text_input_ids, text_attention_mask, spectrograms):\n",
        "        batch_size = audio_features.shape[0]\n",
        "\n",
        "        # Process text with BERT\n",
        "        bert_outputs = self.bert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_features = bert_outputs.last_hidden_state.mean(dim=1)  # (batch_size, 768)\n",
        "\n",
        "        # Create semantic graph\n",
        "        graph_batch = self.create_semantic_graph(text_features, audio_features)\n",
        "\n",
        "        # Process through different modules\n",
        "        graph_out = self.graph_attention(graph_batch.x, graph_batch.edge_index, graph_batch.batch)\n",
        "        vit_out = self.vision_transformer(spectrograms)\n",
        "\n",
        "        # Prepare audio for U-Net (add channel dimension)\n",
        "        audio_1d = audio_features.unsqueeze(1)  # (batch_size, 1, features)\n",
        "        unet_out = self.unet(audio_1d)\n",
        "\n",
        "        alexnet_out = self.alexnet(audio_features)\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = torch.cat([graph_out, vit_out, unet_out, alexnet_out], dim=1)\n",
        "        fused_features = self.fusion_layer(fused_features)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(fused_features)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ModelArchitectureService:\n",
        "    \"\"\"Service for managing neural network architectures\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        logger.info(f\"ModelArchitectureService initialized on device: {self.device}\")\n",
        "\n",
        "    def create_model(self,\n",
        "                    audio_feature_dim: int = 768,\n",
        "                    text_feature_dim: int = 768,\n",
        "                    spectrogram_height: int = 80,\n",
        "                    num_classes: int = 2) -> MultiModalADReSSoModel:\n",
        "        \"\"\"Create and initialize the multi-modal model\"\"\"\n",
        "        try:\n",
        "            model = MultiModalADReSSoModel(\n",
        "                audio_feature_dim=audio_feature_dim,\n",
        "                text_feature_dim=text_feature_dim,\n",
        "                spectrogram_height=spectrogram_height,\n",
        "                num_classes=num_classes,\n",
        "                device=self.device\n",
        "            )\n",
        "\n",
        "            # Count parameters\n",
        "            total_params = sum(p.numel() for p in model.parameters())\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "            logger.info(f\"Model created successfully:\")\n",
        "            logger.info(f\"  - Total parameters: {total_params:,}\")\n",
        "            logger.info(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "            logger.info(f\"  - Model size: {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_model_info(self, model: nn.Module) -> Dict:\n",
        "        \"\"\"Get detailed model information\"\"\"\n",
        "        try:\n",
        "            total_params = sum(p.numel() for p in model.parameters())\n",
        "            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "            # Get module-wise parameter count\n",
        "            module_params = {}\n",
        "            for name, module in model.named_children():\n",
        "                module_params[name] = sum(p.numel() for p in module.parameters())\n",
        "\n",
        "            return {\n",
        "                'total_parameters': total_params,\n",
        "                'trainable_parameters': trainable_params,\n",
        "                'model_size_mb': total_params * 4 / 1024 / 1024,\n",
        "                'module_parameters': module_params,\n",
        "                'device': str(next(model.parameters()).device)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error getting model info: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def save_model(self, model: nn.Module, filepath: str,\n",
        "                   additional_info: Optional[Dict] = None) -> bool:\n",
        "        \"\"\"Save model with additional information\"\"\"\n",
        "        try:\n",
        "            save_dict = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'model_info': self.get_model_info(model),\n",
        "                'architecture': model.__class__.__name__\n",
        "            }\n",
        "\n",
        "            if additional_info:\n",
        "                save_dict.update(additional_info)\n",
        "\n",
        "            torch.save(save_dict, filepath)\n",
        "            logger.info(f\"Model saved successfully to {filepath}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving model: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def load_model(self, filepath: str,\n",
        "                   model_class: type = MultiModalADReSSoModel,\n",
        "                   **model_kwargs) -> Optional[nn.Module]:\n",
        "        \"\"\"Load model from filepath\"\"\"\n",
        "        try:\n",
        "            checkpoint = torch.load(filepath, map_location=self.device)\n",
        "\n",
        "            # Create model instance\n",
        "            model = model_class(**model_kwargs)\n",
        "\n",
        "            # Load state dict\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.to(self.device)\n",
        "\n",
        "            logger.info(f\"Model loaded successfully from {filepath}\")\n",
        "\n",
        "            # Print model info if available\n",
        "            if 'model_info' in checkpoint:\n",
        "                info = checkpoint['model_info']\n",
        "                logger.info(f\"  - Total parameters: {info.get('total_parameters', 'Unknown'):,}\")\n",
        "                logger.info(f\"  - Model size: {info.get('model_size_mb', 'Unknown'):.2f} MB\")\n",
        "\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def print_model_summary(self, model: nn.Module):\n",
        "        \"\"\"Print detailed model summary\"\"\"\n",
        "        info = self.get_model_info(model)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MODEL ARCHITECTURE SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Model Class: {model.__class__.__name__}\")\n",
        "        print(f\"Device: {info.get('device', 'Unknown')}\")\n",
        "        print(f\"Total Parameters: {info.get('total_parameters', 0):,}\")\n",
        "        print(f\"Trainable Parameters: {info.get('trainable_parameters', 0):,}\")\n",
        "        print(f\"Model Size: {info.get('model_size_mb', 0):.2f} MB\")\n",
        "\n",
        "        print(\"\\nMODULE-WISE PARAMETER COUNT:\")\n",
        "        for module_name, param_count in info.get('module_parameters', {}).items():\n",
        "            print(f\"  - {module_name}: {param_count:,} parameters\")\n",
        "        print(\"=\"*60)"
      ],
      "metadata": {
        "id": "SbcdBtT1mSTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training_service.py - Model Training Service\n"
      ],
      "metadata": {
        "id": "3Ev-U9FBnEXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training Service for ADReSSo21 Multi-Modal Model\n",
        "Handles training, validation, evaluation, and semantic relationship analysis\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,\n",
        "                           roc_auc_score, confusion_matrix)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "from transformers import BertModel\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "import multiprocessing as mp\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from config import Config\n",
        "from utils import setup_logging, create_directory, log_system_info\n",
        "from model_architecture_service import MultiModalADReSSoModel\n",
        "\n",
        "\n",
        "class ADReSSoDataset(Dataset):\n",
        "    \"\"\"Custom dataset for ADReSSo multi-modal data\"\"\"\n",
        "\n",
        "    def __init__(self, features_dict: Dict, linguistic_features: Dict,\n",
        "                 labels: Dict, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize dataset\n",
        "\n",
        "        Args:\n",
        "            features_dict: Dictionary of acoustic features\n",
        "            linguistic_features: Dictionary of linguistic features\n",
        "            labels: Dictionary of labels\n",
        "            transform: Optional transforms\n",
        "        \"\"\"\n",
        "        self.features_dict = features_dict\n",
        "        self.linguistic_features = linguistic_features\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.file_ids = list(features_dict.keys())\n",
        "\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.info(f\"Dataset initialized with {len(self.file_ids)} samples\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.file_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Get a single sample\"\"\"\n",
        "        file_id = self.file_ids[idx]\n",
        "\n",
        "        try:\n",
        "            # Get features\n",
        "            features = self.features_dict[file_id]\n",
        "            linguistic = self.linguistic_features[file_id]\n",
        "\n",
        "            # Prepare audio features (Wav2Vec2)\n",
        "            audio_features = torch.FloatTensor(features['wav2vec2'])\n",
        "\n",
        "            # Prepare text features\n",
        "            text_input_ids = torch.LongTensor(linguistic['bert_input_ids'])\n",
        "            text_attention_mask = torch.LongTensor(linguistic['bert_attention_mask'])\n",
        "\n",
        "            # Prepare spectrogram from log-mel features\n",
        "            log_mel_mean = features['log_mel']['mean']\n",
        "            log_mel_std = features['log_mel']['std']\n",
        "\n",
        "            # Create spectrogram (square format for ViT)\n",
        "            spectrogram = np.stack([log_mel_mean, log_mel_std])  # (2, 80)\n",
        "            spectrogram = np.expand_dims(spectrogram.mean(axis=0), axis=0)  # (1, 80)\n",
        "            spectrogram = np.tile(spectrogram, (1, 1, 80))  # (1, 80, 80)\n",
        "            spectrogram = torch.FloatTensor(spectrogram)\n",
        "\n",
        "            # Get label\n",
        "            label = torch.LongTensor([self.labels[file_id]])\n",
        "\n",
        "            sample = {\n",
        "                'audio_features': audio_features,\n",
        "                'text_input_ids': text_input_ids,\n",
        "                'text_attention_mask': text_attention_mask,\n",
        "                'spectrogram': spectrogram,\n",
        "                'label': label,\n",
        "                'file_id': file_id\n",
        "            }\n",
        "\n",
        "            if self.transform:\n",
        "                sample = self.transform(sample)\n",
        "\n",
        "            return sample\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading sample {file_id}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Main trainer class for multi-modal ADReSSo model\"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, config: Config):\n",
        "        \"\"\"\n",
        "        Initialize trainer\n",
        "\n",
        "        Args:\n",
        "            model: The multi-modal model to train\n",
        "            config: Configuration object\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # Setup logging\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        log_system_info(self.logger, self.device)\n",
        "\n",
        "        # Training components\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.LEARNING_RATE,\n",
        "            weight_decay=config.WEIGHT_DECAY\n",
        "        )\n",
        "        self.scheduler = ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "        # Create output directories\n",
        "        create_directory(config.MODEL_OUTPUT_PATH)\n",
        "        create_directory(config.RESULTS_PATH)\n",
        "\n",
        "        self.logger.info(f\"Trainer initialized on {self.device}\")\n",
        "        self.logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            try:\n",
        "                # Move to device\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "                text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "                spectrograms = batch['spectrogram'].to(self.device)\n",
        "                labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(\n",
        "                    audio_features, text_input_ids,\n",
        "                    text_attention_mask, spectrograms\n",
        "                )\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                # Log progress\n",
        "                if batch_idx % 10 == 0:\n",
        "                    self.logger.info(\n",
        "                        f'Batch {batch_idx}/{len(train_loader)}, '\n",
        "                        f'Loss: {loss.item():.4f}'\n",
        "                    )\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error in training batch {batch_idx}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = 100.0 * correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate(self, val_loader: DataLoader) -> Tuple[float, float, List, List, List]:\n",
        "        \"\"\"Validate the model\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                try:\n",
        "                    # Move to device\n",
        "                    audio_features = batch['audio_features'].to(self.device)\n",
        "                    text_input_ids = batch['text_input_ids'].to(self.device)\n",
        "                    text_attention_mask = batch['text_attention_mask'].to(self.device)\n",
        "                    spectrograms = batch['spectrogram'].to(self.device)\n",
        "                    labels = batch['label'].squeeze().to(self.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(\n",
        "                        audio_features, text_input_ids,\n",
        "                        text_attention_mask, spectrograms\n",
        "                    )\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "\n",
        "                    # Statistics\n",
        "                    total_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    # Store for metrics\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_probs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error in validation batch: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = 100.0 * correct / total\n",
        "\n",
        "        return avg_loss, accuracy, all_preds, all_labels, all_probs\n",
        "\n",
        "    def train_model(self, train_loader: DataLoader, val_loader: DataLoader,\n",
        "                   num_epochs: int = 30) -> Dict[str, Any]:\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        self.logger.info(f\"Starting training for {num_epochs} epochs\")\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "        patience_counter = 0\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start = datetime.now()\n",
        "            self.logger.info(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "            self.logger.info('-' * 50)\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc, val_preds, val_labels, val_probs = self.validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            epoch_time = datetime.now() - epoch_start\n",
        "\n",
        "            self.logger.info(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            self.logger.info(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "            self.logger.info(f'Epoch Time: {epoch_time.total_seconds():.2f}s')\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                self.save_model('best_adresso_model.pth')\n",
        "                patience_counter = 0\n",
        "                self.logger.info(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= self.config.EARLY_STOPPING_PATIENCE:\n",
        "                    self.logger.info('Early stopping triggered')\n",
        "                    break\n",
        "\n",
        "        total_time = datetime.now() - start_time\n",
        "        self.logger.info(f'\\nTraining completed in {total_time}')\n",
        "        self.logger.info(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
        "\n",
        "        return {\n",
        "            'best_val_accuracy': best_val_acc,\n",
        "            'total_time': total_time.total_seconds(),\n",
        "            'epochs_completed': epoch + 1\n",
        "        }\n",
        "\n",
        "    def save_model(self, filename: str):\n",
        "        \"\"\"Save model state\"\"\"\n",
        "        filepath = os.path.join(self.config.MODEL_OUTPUT_PATH, filename)\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'train_accuracies': self.train_accuracies,\n",
        "            'val_accuracies': self.val_accuracies,\n",
        "        }, filepath)\n",
        "        self.logger.info(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filename: str):\n",
        "        \"\"\"Load model state\"\"\"\n",
        "        filepath = os.path.join(self.config.MODEL_OUTPUT_PATH, filename)\n",
        "        if os.path.exists(filepath):\n",
        "            checkpoint = torch.load(filepath, map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "            self.train_losses = checkpoint.get('train_losses', [])\n",
        "            self.val_losses = checkpoint.get('val_losses', [])\n",
        "            self.train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "            self.val_accuracies = checkpoint.get('val_accuracies', [])\n",
        "            self.logger.info(f\"Model loaded from {filepath}\")\n",
        "        else:\n",
        "            self.logger.warning(f\"Model file {filepath} not found\")\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Model evaluation and analysis class\"\"\"\n",
        "\n",
        "    def __init__(self, trainer: ModelTrainer, config: Config):\n",
        "        \"\"\"\n",
        "        Initialize evaluator\n",
        "\n",
        "        Args:\n",
        "            trainer: Trained ModelTrainer instance\n",
        "            config: Configuration object\n",
        "        \"\"\"\n",
        "        self.trainer = trainer\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def evaluate_detailed(self, test_loader: DataLoader,\n",
        "                         class_names: List[str] = ['CN', 'AD']) -> Dict[str, Any]:\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        self.logger.info(\"Starting detailed evaluation...\")\n",
        "\n",
        "        self.trainer.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "        all_file_ids = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                try:\n",
        "                    # Move to device\n",
        "                    audio_features = batch['audio_features'].to(self.trainer.device)\n",
        "                    text_input_ids = batch['text_input_ids'].to(self.trainer.device)\n",
        "                    text_attention_mask = batch['text_attention_mask'].to(self.trainer.device)\n",
        "                    spectrograms = batch['spectrogram'].to(self.trainer.device)\n",
        "                    labels = batch['label'].squeeze().to(self.trainer.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.trainer.model(\n",
        "                        audio_features, text_input_ids,\n",
        "                        text_attention_mask, spectrograms\n",
        "                    )\n",
        "                    probs = F.softmax(outputs, dim=1)\n",
        "                    _, predicted = outputs.max(1)\n",
        "\n",
        "                    # Store results\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "                    all_file_ids.extend(batch['file_id'])\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error in evaluation batch: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            all_labels, all_preds, average='weighted'\n",
        "        )\n",
        "\n",
        "        # ROC AUC for binary classification\n",
        "        auc = None\n",
        "        if len(class_names) == 2:\n",
        "            probs_positive = [prob[1] for prob in all_probs]\n",
        "            auc = roc_auc_score(all_labels, probs_positive)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        # Results DataFrame\n",
        "        results_df = pd.DataFrame({\n",
        "            'file_id': all_file_ids,\n",
        "            'true_label': all_labels,\n",
        "            'predicted_label': all_preds,\n",
        "            'confidence': [max(prob) for prob in all_probs],\n",
        "            'prob_CN': [prob[0] for prob in all_probs],\n",
        "            'prob_AD': [prob[1] if len(prob) > 1 else 0 for prob in all_probs]\n",
        "        })\n",
        "\n",
        "        # Log results\n",
        "        self.logger.info(\"=\"*60)\n",
        "        self.logger.info(\"DETAILED EVALUATION RESULTS\")\n",
        "        self.logger.info(\"=\"*60)\n",
        "        self.logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
        "        self.logger.info(f\"Precision: {precision:.4f}\")\n",
        "        self.logger.info(f\"Recall: {recall:.4f}\")\n",
        "        self.logger.info(f\"F1-Score: {f1:.4f}\")\n",
        "        if auc:\n",
        "            self.logger.info(f\"ROC AUC: {auc:.4f}\")\n",
        "\n",
        "        # Visualizations\n",
        "        self.plot_confusion_matrix(cm, class_names)\n",
        "        self.plot_training_curves()\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'auc': auc,\n",
        "            'confusion_matrix': cm,\n",
        "            'results_df': results_df\n",
        "        }\n",
        "\n",
        "    def plot_confusion_matrix(self, cm: np.ndarray, class_names: List[str]):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        save_path = os.path.join(self.config.RESULTS_PATH, 'confusion_matrix.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.logger.info(f\"Confusion matrix saved to {save_path}\")\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"Plot training and validation curves\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        ax1.plot(self.trainer.train_losses, label='Training Loss', color='blue')\n",
        "        ax1.plot(self.trainer.val_losses, label='Validation Loss', color='red')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Accuracy curves\n",
        "        ax2.plot(self.trainer.train_accuracies, label='Training Accuracy', color='blue')\n",
        "        ax2.plot(self.trainer.val_accuracies, label='Validation Accuracy', color='red')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy (%)')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        save_path = os.path.join(self.config.RESULTS_PATH, 'training_curves.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        self.logger.info(f\"Training curves saved to {save_path}\")\n",
        "\n",
        "    def analyze_semantic_relationships(self, test_loader: DataLoader,\n",
        "                                     num_samples: int = 5):\n",
        "        \"\"\"Analyze semantic relationships between modalities\"\"\"\n",
        "        self.logger.info(f\"Analyzing semantic relationships for {num_samples} samples...\")\n",
        "\n",
        "        self.trainer.model.eval()\n",
        "        sample_count = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                if sample_count >= num_samples:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Process batch\n",
        "                    audio_features = batch['audio_features'].to(self.trainer.device)\n",
        "                    text_input_ids = batch['text_input_ids'].to(self.trainer.device)\n",
        "                    text_attention_mask = batch['text_attention_mask'].to(self.trainer.device)\n",
        "                    file_ids = batch['file_id']\n",
        "\n",
        "                    # Get BERT features\n",
        "                    bert_outputs = self.trainer.model.bert(\n",
        "                        input_ids=text_input_ids,\n",
        "                        attention_mask=text_attention_mask\n",
        "                    )\n",
        "                    text_features = bert_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "                    # Analyze each sample\n",
        "                    for i in range(min(len(file_ids), num_samples - sample_count)):\n",
        "                        file_id = file_ids[i]\n",
        "                        text_feat = text_features[i]\n",
        "                        audio_feat = audio_features[i]\n",
        "\n",
        "                        # Create and visualize semantic graph\n",
        "                        self.visualize_semantic_graph(\n",
        "                            text_feat.cpu(), audio_feat.cpu(), file_id\n",
        "                        )\n",
        "\n",
        "                        # Analyze relationship\n",
        "                        self.analyze_relationship_metrics(\n",
        "                            text_feat.cpu(), audio_feat.cpu(), file_id\n",
        "                        )\n",
        "\n",
        "                        sample_count += 1\n",
        "                        if sample_count >= num_samples:\n",
        "                            break\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error in semantic analysis: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    def visualize_semantic_graph(self, text_features: torch.Tensor,\n",
        "                                audio_features: torch.Tensor, file_id: str):\n",
        "        \"\"\"Visualize semantic relationship graph\"\"\"\n",
        "        # Compute similarity\n",
        "        similarity = F.cosine_similarity(\n",
        "            text_features, audio_features, dim=0\n",
        "        ).item()\n",
        "\n",
        "        # Create networkx graph\n",
        "        G = nx.Graph()\n",
        "        G.add_node(\"Text\", type=\"text\")\n",
        "        G.add_node(\"Audio\", type=\"audio\")\n",
        "\n",
        "        # Add edge if similarity is significant\n",
        "        if similarity > 0.1:\n",
        "            G.add_edge(\"Text\", \"Audio\", weight=similarity, similarity=similarity)\n",
        "\n",
        "        # Visualize\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        pos = nx.spring_layout(G, k=3, iterations=50)\n",
        "\n",
        "        # Draw nodes\n",
        "        node_colors = ['lightblue', 'lightcoral']\n",
        "        nx.draw_networkx_nodes(G, pos, node_color=node_colors,\n",
        "                              node_size=3000, alpha=0.7)\n",
        "\n",
        "        # Draw edges\n",
        "        if G.edges():\n",
        "            edge_widths = [G[u][v]['weight'] * 10 for u, v in G.edges()]\n",
        "            nx.draw_networkx_edges(G, pos, width=edge_widths,\n",
        "                                 alpha=0.6, edge_color='gray')\n",
        "\n",
        "        # Draw labels\n",
        "        nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')\n",
        "\n",
        "        # Add edge labels\n",
        "        if G.edges():\n",
        "            edge_labels = {(u, v): f\"Sim: {G[u][v]['similarity']:.3f}\"\n",
        "                          for u, v in G.edges()}\n",
        "            nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10)\n",
        "\n",
        "        plt.title(f'Semantic Relationship Graph - {file_id}\\n'\n",
        "                 f'Similarity: {similarity:.3f}')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        save_path = os.path.join(\n",
        "            self.config.RESULTS_PATH, f'semantic_graph_{file_id}.png'\n",
        "        )\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        return G, similarity\n",
        "\n",
        "    def analyze_relationship_metrics(self, text_features: torch.Tensor,\n",
        "                                   audio_features: torch.Tensor, file_id: str):\n",
        "        \"\"\"Analyze semantic relationship metrics\"\"\"\n",
        "        # Compute similarity metrics\n",
        "        cosine_sim = F.cosine_similarity(\n",
        "            text_features, audio_features, dim=0\n",
        "        ).item()\n",
        "\n",
        "        # L2 distance (normalized)\n",
        "        l2_distance = torch.norm(text_features - audio_features).item()\n",
        "        normalized_l2 = l2_distance / (\n",
        "            torch.norm(text_features) + torch.norm(audio_features)\n",
        "        ).item()\n",
        "\n",
        "        # Dot product similarity\n",
        "        dot_product = torch.dot(text_features, audio_features).item()\n",
        "\n",
        "        # Interpretation\n",
        "        if cosine_sim > 0.7:\n",
        "            relationship = \"Strong positive correlation\"\n",
        "        elif cosine_sim > 0.3:\n",
        "            relationship = \"Moderate positive correlation\"\n",
        "        elif cosine_sim > 0.1:\n",
        "            relationship = \"Weak positive correlation\"\n",
        "        elif cosine_sim > -0.1:\n",
        "            relationship = \"No significant correlation\"\n",
        "        else:\n",
        "            relationship = \"Negative correlation\"\n",
        "\n",
        "        self.logger.info(f\"Semantic Analysis for {file_id}:\")\n",
        "        self.logger.info(f\"  - Cosine Similarity: {cosine_sim:.4f}\")\n",
        "        self.logger.info(f\"  - Normalized L2 Distance: {normalized_l2:.4f}\")\n",
        "        self.logger.info(f\"  - Dot Product: {dot_product:.4f}\")\n",
        "        self.logger.info(f\"  - Interpretation: {relationship}\")\n",
        "\n",
        "        return {\n",
        "            'cosine_similarity': cosine_sim,\n",
        "            'l2_distance': normalized_l2,\n",
        "            'dot_product': dot_product,\n",
        "            'relationship': relationship\n",
        "        }\n",
        "\n",
        "    def generate_evaluation_report(self, evaluation_results: Dict[str, Any]):\n",
        "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
        "        results_df = evaluation_results['results_df']\n",
        "\n",
        "        # Create report\n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'performance_metrics': {\n",
        "                'accuracy': evaluation_results['accuracy'],\n",
        "                'precision': evaluation_results['precision'],\n",
        "                'recall': evaluation_results['recall'],\n",
        "                'f1_score': evaluation_results['f1'],\n",
        "                'roc_auc': evaluation_results['auc']\n",
        "            },\n",
        "            'dataset_summary': {\n",
        "                'total_samples': len(results_df),\n",
        "                'correct_predictions': (results_df['true_label'] ==\n",
        "                                      results_df['predicted_label']).sum(),\n",
        "                'high_confidence_predictions': (results_df['confidence'] > 0.9).sum()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save report\n",
        "        report_path = os.path.join(\n",
        "            self.config.RESULTS_PATH, 'evaluation_report.json'\n",
        "        )\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        # Save detailed results\n",
        "        results_path = os.path.join(\n",
        "            self.config.RESULTS_PATH, 'detailed_results.csv'\n",
        "        )\n",
        "        results_df.to_csv(results_path, index=False)\n",
        "\n",
        "        self.logger.info(f\"Evaluation report saved to {report_path}\")\n",
        "        self.logger.info(f\"Detailed results saved to {results_path}\")\n",
        "\n",
        "        return report\n",
        "\n",
        "\n",
        "class TrainingService:\n",
        "    \"\"\"Main training service orchestrating all components\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        \"\"\"\n",
        "        Initialize training service\n",
        "\n",
        "        Args:\n",
        "            config: Configuration object\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = setup_logging(\n",
        "            os.path.join(config.LOG_PATH, 'training_service.log')\n",
        "        )\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = MultiModalADReSSoModel(\n",
        "            audio_feature_dim=768,\n",
        "            text_feature_dim=768,\n",
        "            spectrogram_height=80,\n",
        "            num_classes=2\n",
        "        )\n",
        "\n",
        "        # Initialize trainer and evaluator\n",
        "        self.trainer = ModelTrainer(self.model, config)\n",
        "        self.evaluator = ModelEvaluator(self.trainer, config)\n",
        "\n",
        "        # Training state\n",
        "        self.is_trained = False\n",
        "        self.training_results = None\n",
        "        self.evaluation_results = None\n",
        "\n",
        "        self.logger.info(\"Training service initialized\")\n",
        "\n",
        "    def prepare_data(self, features_dict: Dict, linguistic_features: Dict,\n",
        "                    batch_size: int = 8) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "        \"\"\"Prepare data loaders for training\"\"\"\n",
        "        self.logger.info(\"Preparing data loaders...\")\n",
        "\n",
        "        # Create labels based on file naming convention\n",
        "        labels = {}\n",
        "        for file_id in features_dict.keys():\n",
        "            # Assuming file naming convention includes diagnosis/progression info\n",
        "            if any(keyword in file_id.lower() for keyword in ['ad', 'alzheimer', 'decline', 'impaired']):\n",
        "                labels[file_id] = 1  # AD/Decline\n",
        "            else:\n",
        "                labels[file_id] = 0  # CN/No decline\n",
        "\n",
        "        self.logger.info(f\"Dataset summary:\")\n",
        "        self.logger.info(f\"- Total files: {len(features_dict)}\")\n",
        "        self.logger.info(f\"- AD/Decline cases: {sum(labels.values())}\")\n",
        "        self.logger.info(f\"- CN/No decline cases: {len(labels) - sum(labels.values())}\")\n",
        "\n",
        "        # Validate that we have both classes\n",
        "        if sum(labels.values()) == 0 or sum(labels.values()) == len(labels):\n",
        "            self.logger.warning(\"Dataset appears to have only one class! Check labeling logic.\")\n",
        "\n",
        "        # Split data stratified\n",
        "        file_ids = list(features_dict.keys())\n",
        "        try:\n",
        "            train_ids, test_ids = train_test_split(\n",
        "                file_ids, test_size=0.2,\n",
        "                stratify=[labels[f] for f in file_ids],\n",
        "                random_state=42\n",
        "            )\n",
        "            train_ids, val_ids = train_test_split(\n",
        "                train_ids, test_size=0.2,\n",
        "                stratify=[labels[f] for f in train_ids],\n",
        "                random_state=42\n",
        "            )\n",
        "        except ValueError as e:\n",
        "            self.logger.warning(f\"Stratified split failed: {e}. Using random split.\")\n",
        "            train_ids, test_ids = train_test_split(file_ids, test_size=0.2, random_state=42)\n",
        "            train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "        self.logger.info(f\"Data split - Train: {len(train_ids)}, \"\n",
        "                        f\"Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
        "\n",
        "        # Create datasets\n",
        "        def create_subset(ids):\n",
        "            return (\n",
        "                {fid: features_dict[fid] for fid in ids if fid in features_dict},\n",
        "                {fid: linguistic_features[fid] for fid in ids if fid in linguistic_features},\n",
        "                {fid: labels[fid] for fid in ids if fid in labels}\n",
        "            )\n",
        "\n",
        "        train_features, train_linguistic, train_labels = create_subset(train_ids)\n",
        "        val_features, val_linguistic, val_labels = create_subset(val_ids)\n",
        "        test_features, test_linguistic, test_labels = create_subset(test_ids)\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = ADReSSoDataset(train_features, train_linguistic, train_labels)\n",
        "        val_dataset = ADReSSoDataset(val_features, val_linguistic, val_labels)\n",
        "        test_dataset = ADReSSoDataset(test_features, test_linguistic, test_labels)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=min(4, mp.cpu_count()), pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=min(4, mp.cpu_count()), pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=min(4, mp.cpu_count()), pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        self.logger.info(\"Data loaders created successfully\")\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "    def train_model(self, features_dict: Dict, linguistic_features: Dict,\n",
        "                   num_epochs: int = 30, batch_size: int = 8) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Train the multi-modal model\n",
        "\n",
        "        Args:\n",
        "            features_dict: Dictionary of acoustic features\n",
        "            linguistic_features: Dictionary of linguistic features\n",
        "            num_epochs: Number of training epochs\n",
        "            batch_size: Batch size for training\n",
        "\n",
        "        Returns:\n",
        "            Training results dictionary\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Starting model training...\")\n",
        "\n",
        "        try:\n",
        "            # Prepare data\n",
        "            train_loader, val_loader, test_loader = self.prepare_data(\n",
        "                features_dict, linguistic_features, batch_size\n",
        "            )\n",
        "\n",
        "            # Store test loader for later evaluation\n",
        "            self.test_loader = test_loader\n",
        "\n",
        "            # Train the model\n",
        "            self.training_results = self.trainer.train_model(\n",
        "                train_loader, val_loader, num_epochs\n",
        "            )\n",
        "\n",
        "            self.is_trained = True\n",
        "            self.logger.info(\"Model training completed successfully\")\n",
        "\n",
        "            return self.training_results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during model training: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, custom_test_loader: Optional[DataLoader] = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate the trained model\n",
        "\n",
        "        Args:\n",
        "            custom_test_loader: Optional custom test data loader\n",
        "\n",
        "        Returns:\n",
        "            Evaluation results dictionary\n",
        "        \"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained before evaluation\")\n",
        "\n",
        "        self.logger.info(\"Starting model evaluation...\")\n",
        "\n",
        "        try:\n",
        "            # Use provided test loader or stored one\n",
        "            test_loader = custom_test_loader or self.test_loader\n",
        "            if test_loader is None:\n",
        "                raise ValueError(\"No test data available for evaluation\")\n",
        "\n",
        "            # Perform detailed evaluation\n",
        "            self.evaluation_results = self.evaluator.evaluate_detailed(test_loader)\n",
        "\n",
        "            # Generate comprehensive report\n",
        "            self.evaluator.generate_evaluation_report(self.evaluation_results)\n",
        "\n",
        "            self.logger.info(\"Model evaluation completed successfully\")\n",
        "            return self.evaluation_results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during model evaluation: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def analyze_semantics(self, custom_test_loader: Optional[DataLoader] = None,\n",
        "                         num_samples: int = 5) -> None:\n",
        "        \"\"\"\n",
        "        Analyze semantic relationships between modalities\n",
        "\n",
        "        Args:\n",
        "            custom_test_loader: Optional custom test data loader\n",
        "            num_samples: Number of samples to analyze\n",
        "        \"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained before semantic analysis\")\n",
        "\n",
        "        self.logger.info(\"Starting semantic relationship analysis...\")\n",
        "\n",
        "        try:\n",
        "            # Use provided test loader or stored one\n",
        "            test_loader = custom_test_loader or self.test_loader\n",
        "            if test_loader is None:\n",
        "                raise ValueError(\"No test data available for semantic analysis\")\n",
        "\n",
        "            # Perform semantic analysis\n",
        "            self.evaluator.analyze_semantic_relationships(test_loader, num_samples)\n",
        "\n",
        "            self.logger.info(\"Semantic analysis completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during semantic analysis: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def save_model(self, filename: str = \"final_adresso_model.pth\") -> None:\n",
        "        \"\"\"\n",
        "        Save the trained model\n",
        "\n",
        "        Args:\n",
        "            filename: Name of the model file\n",
        "        \"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained before saving\")\n",
        "\n",
        "        try:\n",
        "            self.trainer.save_model(filename)\n",
        "            self.logger.info(f\"Model saved successfully as {filename}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error saving model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def load_model(self, filename: str = \"best_adresso_model.pth\") -> None:\n",
        "        \"\"\"\n",
        "        Load a pre-trained model\n",
        "\n",
        "        Args:\n",
        "            filename: Name of the model file to load\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.trainer.load_model(filename)\n",
        "            self.is_trained = True\n",
        "            self.logger.info(f\"Model loaded successfully from {filename}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def predict(self, features_dict: Dict, linguistic_features: Dict,\n",
        "               batch_size: int = 8) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Make predictions on new data\n",
        "\n",
        "        Args:\n",
        "            features_dict: Dictionary of acoustic features\n",
        "            linguistic_features: Dictionary of linguistic features\n",
        "            batch_size: Batch size for prediction\n",
        "\n",
        "        Returns:\n",
        "            Prediction results dictionary\n",
        "        \"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"Model must be trained or loaded before making predictions\")\n",
        "\n",
        "        self.logger.info(\"Making predictions on new data...\")\n",
        "\n",
        "        try:\n",
        "            # Create temporary labels (will be ignored)\n",
        "            temp_labels = {fid: 0 for fid in features_dict.keys()}\n",
        "\n",
        "            # Create dataset and loader\n",
        "            dataset = ADReSSoDataset(features_dict, linguistic_features, temp_labels)\n",
        "            data_loader = DataLoader(\n",
        "                dataset, batch_size=batch_size, shuffle=False,\n",
        "                num_workers=min(4, mp.cpu_count()), pin_memory=torch.cuda.is_available()\n",
        "            )\n",
        "\n",
        "            # Make predictions\n",
        "            self.trainer.model.eval()\n",
        "            all_preds = []\n",
        "            all_probs = []\n",
        "            all_file_ids = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in data_loader:\n",
        "                    # Move to device\n",
        "                    audio_features = batch['audio_features'].to(self.trainer.device)\n",
        "                    text_input_ids = batch['text_input_ids'].to(self.trainer.device)\n",
        "                    text_attention_mask = batch['text_attention_mask'].to(self.trainer.device)\n",
        "                    spectrograms = batch['spectrogram'].to(self.trainer.device)\n",
        "\n",
        "                    # Forward pass\n",
        "                    outputs = self.trainer.model(\n",
        "                        audio_features, text_input_ids,\n",
        "                        text_attention_mask, spectrograms\n",
        "                    )\n",
        "                    probs = F.softmax(outputs, dim=1)\n",
        "                    _, predicted = outputs.max(1)\n",
        "\n",
        "                    # Store results\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "                    all_file_ids.extend(batch['file_id'])\n",
        "\n",
        "            # Create results DataFrame\n",
        "            results_df = pd.DataFrame({\n",
        "                'file_id': all_file_ids,\n",
        "                'predicted_label': all_preds,\n",
        "                'predicted_class': ['CN' if pred == 0 else 'AD' for pred in all_preds],\n",
        "                'confidence': [max(prob) for prob in all_probs],\n",
        "                'prob_CN': [prob[0] for prob in all_probs],\n",
        "                'prob_AD': [prob[1] if len(prob) > 1 else 0 for prob in all_probs]\n",
        "            })\n",
        "\n",
        "            self.logger.info(f\"Predictions completed for {len(results_df)} samples\")\n",
        "\n",
        "            return {\n",
        "                'predictions': results_df,\n",
        "                'summary': {\n",
        "                    'total_samples': len(results_df),\n",
        "                    'predicted_CN': (results_df['predicted_label'] == 0).sum(),\n",
        "                    'predicted_AD': (results_df['predicted_label'] == 1).sum(),\n",
        "                    'high_confidence': (results_df['confidence'] > 0.9).sum()\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error during prediction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_training_summary(self) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get summary of training results\n",
        "\n",
        "        Returns:\n",
        "            Training summary dictionary or None if not trained\n",
        "        \"\"\"\n",
        "        if not self.is_trained or self.training_results is None:\n",
        "            return None\n",
        "\n",
        "        summary = {\n",
        "            'model_info': {\n",
        "                'total_parameters': sum(p.numel() for p in self.model.parameters()),\n",
        "                'trainable_parameters': sum(p.numel() for p in self.model.parameters() if p.requires_grad),\n",
        "                'model_size_mb': sum(p.numel() * p.element_size() for p in self.model.parameters()) / (1024 * 1024)\n",
        "            },\n",
        "            'training_results': self.training_results,\n",
        "            'evaluation_results': self.evaluation_results,\n",
        "            'training_curves': {\n",
        "                'train_losses': self.trainer.train_losses,\n",
        "                'val_losses': self.trainer.val_losses,\n",
        "                'train_accuracies': self.trainer.train_accuracies,\n",
        "                'val_accuracies': self.trainer.val_accuracies\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def run_complete_pipeline(self, features_dict: Dict, linguistic_features: Dict,\n",
        "                             num_epochs: int = 30, batch_size: int = 8,\n",
        "                             semantic_samples: int = 5) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run the complete training and evaluation pipeline\n",
        "\n",
        "        Args:\n",
        "            features_dict: Dictionary of acoustic features\n",
        "            linguistic_features: Dictionary of linguistic features\n",
        "            num_epochs: Number of training epochs\n",
        "            batch_size: Batch size for training\n",
        "            semantic_samples: Number of samples for semantic analysis\n",
        "\n",
        "        Returns:\n",
        "            Complete pipeline results\n",
        "        \"\"\"\n",
        "        self.logger.info(\"=\"*60)\n",
        "        self.logger.info(\"STARTING COMPLETE TRAINING PIPELINE\")\n",
        "        self.logger.info(\"=\"*60)\n",
        "\n",
        "        pipeline_start = datetime.now()\n",
        "\n",
        "        try:\n",
        "            # Step 1: Train model\n",
        "            self.logger.info(\"Step 1: Training model...\")\n",
        "            training_results = self.train_model(\n",
        "                features_dict, linguistic_features, num_epochs, batch_size\n",
        "            )\n",
        "\n",
        "            # Step 2: Evaluate model\n",
        "            self.logger.info(\"Step 2: Evaluating model...\")\n",
        "            evaluation_results = self.evaluate_model()\n",
        "\n",
        "            # Step 3: Analyze semantics\n",
        "            self.logger.info(\"Step 3: Analyzing semantic relationships...\")\n",
        "            self.analyze_semantics(num_samples=semantic_samples)\n",
        "\n",
        "            # Step 4: Save model\n",
        "            self.logger.info(\"Step 4: Saving final model...\")\n",
        "            self.save_model(\"final_adresso_model.pth\")\n",
        "\n",
        "            pipeline_time = datetime.now() - pipeline_start\n",
        "\n",
        "            # Compile results\n",
        "            complete_results = {\n",
        "                'pipeline_summary': {\n",
        "                    'total_time': pipeline_time.total_seconds(),\n",
        "                    'completion_time': datetime.now().isoformat(),\n",
        "                    'status': 'success'\n",
        "                },\n",
        "                'training_summary': self.get_training_summary(),\n",
        "                'best_performance': {\n",
        "                    'accuracy': evaluation_results['accuracy'],\n",
        "                    'f1_score': evaluation_results['f1'],\n",
        "                    'auc': evaluation_results['auc']\n",
        "                }\n",
        "            }\n",
        "\n",
        "            self.logger.info(\"=\"*60)\n",
        "            self.logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "            self.logger.info(f\"Total time: {pipeline_time}\")\n",
        "            self.logger.info(f\"Best accuracy: {evaluation_results['accuracy']:.4f}\")\n",
        "            self.logger.info(\"=\"*60)\n",
        "\n",
        "            return complete_results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Cleanup resources\"\"\"\n",
        "        if hasattr(self, 'logger'):\n",
        "            self.logger.info(\"Training service cleanup completed\")"
      ],
      "metadata": {
        "id": "sImQ4ePYpoyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "sm8X9-JOmQ_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline_service.py - Main Pipeline Orchestrator\n"
      ],
      "metadata": {
        "id": "qq6Ee_A3iK9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pipeline Service - Main orchestrator for ADReSSo21 speech analysis pipeline\n",
        "Coordinates all microservices for complete analysis workflow\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import multiprocessing as mp\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Import your microservices\n",
        "from config import Config\n",
        "from utils import setup_logging, ensure_directory\n",
        "from data_manager_service import DataManagerService\n",
        "from acoustic_features_service import AcousticFeaturesService\n",
        "from transcription_service import TranscriptionService\n",
        "from linguistic_features_service import LinguisticFeaturesService\n",
        "\n",
        "\n",
        "class PipelineService:\n",
        "    \"\"\"\n",
        "    Main pipeline service that orchestrates all analysis components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize pipeline with configuration and services\n",
        "\n",
        "        Args:\n",
        "            config_path: Path to configuration file\n",
        "        \"\"\"\n",
        "        # Load configuration\n",
        "        self.config = Config(config_path)\n",
        "\n",
        "        # Setup logging\n",
        "        self.logger = setup_logging(\n",
        "            log_level=self.config.get('logging.level', 'INFO'),\n",
        "            log_file=self.config.get('logging.file')\n",
        "        )\n",
        "\n",
        "        # Initialize services\n",
        "        self.data_manager = DataManagerService(self.config)\n",
        "        self.acoustic_service = AcousticFeaturesService(self.config)\n",
        "        self.transcription_service = TranscriptionService(self.config)\n",
        "        self.linguistic_service = LinguisticFeaturesService(self.config)\n",
        "\n",
        "        # Pipeline state\n",
        "        self.results = {}\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "\n",
        "        self.logger.info(\"Pipeline initialized successfully\")\n",
        "\n",
        "    def run_complete_pipeline(self, parallel: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run the complete analysis pipeline\n",
        "\n",
        "        Args:\n",
        "            parallel: Whether to use parallel processing where possible\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all pipeline results\n",
        "        \"\"\"\n",
        "        self.start_time = datetime.now()\n",
        "        self.logger.info(\"=== Starting ADReSSo21 Speech Analysis Pipeline ===\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load dataset and get audio files\n",
        "            self.logger.info(\"Step 1: Loading dataset...\")\n",
        "            audio_files = self._load_dataset()\n",
        "\n",
        "            # Step 2: Extract acoustic features\n",
        "            self.logger.info(\"Step 2: Extracting acoustic features...\")\n",
        "            acoustic_features = self._extract_acoustic_features(audio_files, parallel)\n",
        "\n",
        "            # Step 3: Extract transcripts\n",
        "            self.logger.info(\"Step 3: Extracting transcripts...\")\n",
        "            transcripts = self._extract_transcripts(audio_files, parallel)\n",
        "\n",
        "            # Step 4: Extract linguistic features\n",
        "            self.logger.info(\"Step 4: Extracting linguistic features...\")\n",
        "            linguistic_features = self._extract_linguistic_features(transcripts)\n",
        "\n",
        "            # Step 5: Combine and save results\n",
        "            self.logger.info(\"Step 5: Combining and saving results...\")\n",
        "            final_results = self._combine_and_save_results(\n",
        "                audio_files, acoustic_features, transcripts, linguistic_features\n",
        "            )\n",
        "\n",
        "            self.end_time = datetime.now()\n",
        "            duration = self.end_time - self.start_time\n",
        "\n",
        "            self.logger.info(f\"Pipeline completed successfully in {duration}\")\n",
        "            self.logger.info(f\"Results saved to: {self.config.output_path}\")\n",
        "\n",
        "            return final_results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_dataset(self) -> Dict[str, List[str]]:\n",
        "        \"\"\"Load dataset and get audio file paths\"\"\"\n",
        "        audio_files = self.data_manager.get_audio_files()\n",
        "\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        self.logger.info(f\"Found {total_files} audio files across all categories\")\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            self.logger.info(f\"  {category}: {len(files)} files\")\n",
        "\n",
        "        if total_files == 0:\n",
        "            raise ValueError(\"No audio files found. Please check the dataset path.\")\n",
        "\n",
        "        return audio_files\n",
        "\n",
        "    def _extract_acoustic_features(self, audio_files: Dict[str, List[str]],\n",
        "                                 parallel: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features from all audio files\"\"\"\n",
        "        all_features = {}\n",
        "\n",
        "        if parallel:\n",
        "            all_features = self._extract_acoustic_features_parallel(audio_files)\n",
        "        else:\n",
        "            all_features = self._extract_acoustic_features_sequential(audio_files)\n",
        "\n",
        "        # Save acoustic features\n",
        "        features_path = os.path.join(self.config.output_path, \"acoustic_features.pkl\")\n",
        "        with open(features_path, 'wb') as f:\n",
        "            pickle.dump(all_features, f)\n",
        "\n",
        "        self.logger.info(f\"Acoustic features saved to {features_path}\")\n",
        "        return all_features\n",
        "\n",
        "    def _extract_acoustic_features_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features using parallel processing\"\"\"\n",
        "        all_features = {}\n",
        "        max_workers = min(self.config.get('processing.max_workers', mp.cpu_count()), mp.cpu_count())\n",
        "\n",
        "        # Flatten all files with their categories\n",
        "        file_tasks = []\n",
        "        for category, files in audio_files.items():\n",
        "            for file_path in files:\n",
        "                file_tasks.append((file_path, category))\n",
        "\n",
        "        self.logger.info(f\"Processing {len(file_tasks)} files with {max_workers} workers\")\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_file = {\n",
        "                executor.submit(self.acoustic_service.extract_features, file_path): (file_path, category)\n",
        "                for file_path, category in file_tasks\n",
        "            }\n",
        "\n",
        "            # Collect results\n",
        "            completed = 0\n",
        "            for future in as_completed(future_to_file):\n",
        "                file_path, category = future_to_file[future]\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    features = future.result()\n",
        "                    if features is not None:\n",
        "                        all_features[f\"{category}_{filename}\"] = {\n",
        "                            'file_path': file_path,\n",
        "                            'category': category,\n",
        "                            'filename': filename,\n",
        "                            'features': features\n",
        "                        }\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Failed to extract features from {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "                completed += 1\n",
        "                if completed % 10 == 0:\n",
        "                    self.logger.info(f\"Completed acoustic feature extraction for {completed}/{len(file_tasks)} files\")\n",
        "\n",
        "        return all_features\n",
        "\n",
        "    def _extract_acoustic_features_sequential(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract acoustic features sequentially\"\"\"\n",
        "        all_features = {}\n",
        "        total_files = sum(len(files) for files in audio_files.values())\n",
        "        processed = 0\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            self.logger.info(f\"Processing acoustic features for {category}...\")\n",
        "\n",
        "            for file_path in files:\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    features = self.acoustic_service.extract_features(file_path)\n",
        "                    if features is not None:\n",
        "                        all_features[f\"{category}_{filename}\"] = {\n",
        "                            'file_path': file_path,\n",
        "                            'category': category,\n",
        "                            'filename': filename,\n",
        "                            'features': features\n",
        "                        }\n",
        "                    else:\n",
        "                        self.logger.warning(f\"Failed to extract features from {filename}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "                processed += 1\n",
        "                if processed % 10 == 0:\n",
        "                    self.logger.info(f\"Completed {processed}/{total_files} files\")\n",
        "\n",
        "        return all_features\n",
        "\n",
        "    def _extract_transcripts(self, audio_files: Dict[str, List[str]],\n",
        "                           parallel: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Extract transcripts from all audio files\"\"\"\n",
        "        if parallel:\n",
        "            transcripts = self._extract_transcripts_parallel(audio_files)\n",
        "        else:\n",
        "            transcripts = self._extract_transcripts_sequential(audio_files)\n",
        "\n",
        "        # Save transcripts\n",
        "        self._save_transcripts(transcripts)\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_transcripts_parallel(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract transcripts using parallel processing\"\"\"\n",
        "        transcripts = {}\n",
        "        max_workers = min(self.config.get('processing.transcription_workers', 2), 4)  # Limit for memory\n",
        "\n",
        "        # Flatten all files with their categories\n",
        "        file_tasks = []\n",
        "        for category, files in audio_files.items():\n",
        "            for file_path in files:\n",
        "                file_tasks.append((file_path, category))\n",
        "\n",
        "        self.logger.info(f\"Transcribing {len(file_tasks)} files with {max_workers} workers\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_file = {\n",
        "                executor.submit(self.transcription_service.transcribe_audio, file_path): (file_path, category)\n",
        "                for file_path, category in file_tasks\n",
        "            }\n",
        "\n",
        "            # Collect results\n",
        "            completed = 0\n",
        "            for future in as_completed(future_to_file):\n",
        "                file_path, category = future_to_file[future]\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    transcript_data = future.result()\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        **transcript_data\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': '',\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "                completed += 1\n",
        "                if completed % 5 == 0:\n",
        "                    self.logger.info(f\"Completed transcription for {completed}/{len(file_tasks)} files\")\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_transcripts_sequential(self, audio_files: Dict[str, List[str]]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract transcripts sequentially\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        for category, files in audio_files.items():\n",
        "            self.logger.info(f\"Transcribing {category}...\")\n",
        "\n",
        "            for file_path in files:\n",
        "                filename = os.path.basename(file_path)\n",
        "\n",
        "                try:\n",
        "                    transcript_data = self.transcription_service.transcribe_audio(file_path)\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        **transcript_data\n",
        "                    }\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error transcribing {filename}: {str(e)}\")\n",
        "                    transcripts[f\"{category}_{filename}\"] = {\n",
        "                        'file_path': file_path,\n",
        "                        'category': category,\n",
        "                        'filename': filename,\n",
        "                        'transcript': '',\n",
        "                        'error': str(e)\n",
        "                    }\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_linguistic_features(self, transcripts: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract linguistic features from transcripts\"\"\"\n",
        "        linguistic_features = self.linguistic_service.extract_features(transcripts)\n",
        "\n",
        "        # Save linguistic features\n",
        "        features_path = os.path.join(self.config.output_path, \"linguistic_features.pkl\")\n",
        "        with open(features_path, 'wb') as f:\n",
        "            pickle.dump(linguistic_features, f)\n",
        "\n",
        "        self.logger.info(f\"Linguistic features saved to {features_path}\")\n",
        "        return linguistic_features\n",
        "\n",
        "    def _save_transcripts(self, transcripts: Dict[str, Any]):\n",
        "        \"\"\"Save transcripts to various formats\"\"\"\n",
        "        transcripts_dir = os.path.join(self.config.output_path, \"transcripts\")\n",
        "        ensure_directory(transcripts_dir)\n",
        "\n",
        "        # Save individual transcript files\n",
        "        for key, data in transcripts.items():\n",
        "            if 'transcript' in data and data['transcript']:\n",
        "                filename = f\"{key}_transcript.txt\"\n",
        "                filepath = os.path.join(transcripts_dir, filename)\n",
        "\n",
        "                with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                    f.write(data['transcript'])\n",
        "\n",
        "        # Save consolidated JSON\n",
        "        json_path = os.path.join(transcripts_dir, \"all_transcripts.json\")\n",
        "        with open(json_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(transcripts, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save as pickle\n",
        "        pkl_path = os.path.join(transcripts_dir, \"transcripts.pkl\")\n",
        "        with open(pkl_path, 'wb') as f:\n",
        "            pickle.dump(transcripts, f)\n",
        "\n",
        "        self.logger.info(f\"Transcripts saved to {transcripts_dir}\")\n",
        "\n",
        "    def _combine_and_save_results(self, audio_files: Dict[str, List[str]],\n",
        "                                acoustic_features: Dict[str, Any],\n",
        "                                transcripts: Dict[str, Any],\n",
        "                                linguistic_features: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Combine all results and save comprehensive dataset\"\"\"\n",
        "\n",
        "        # Create comprehensive results dictionary\n",
        "        final_results = {\n",
        "            'pipeline_info': {\n",
        "                'start_time': self.start_time.isoformat(),\n",
        "                'end_time': self.end_time.isoformat() if self.end_time else None,\n",
        "                'total_files': sum(len(files) for files in audio_files.values()),\n",
        "                'categories': list(audio_files.keys()),\n",
        "                'config': self.config.to_dict()\n",
        "            },\n",
        "            'audio_files': audio_files,\n",
        "            'acoustic_features': acoustic_features,\n",
        "            'transcripts': transcripts,\n",
        "            'linguistic_features': linguistic_features\n",
        "        }\n",
        "\n",
        "        # Create summary DataFrame\n",
        "        summary_data = []\n",
        "        for key in set(acoustic_features.keys()) | set(transcripts.keys()):\n",
        "            row = {'file_id': key}\n",
        "\n",
        "            # Add acoustic info\n",
        "            if key in acoustic_features:\n",
        "                row.update({\n",
        "                    'category': acoustic_features[key]['category'],\n",
        "                    'filename': acoustic_features[key]['filename'],\n",
        "                    'has_acoustic_features': True\n",
        "                })\n",
        "\n",
        "            # Add transcript info\n",
        "            if key in transcripts:\n",
        "                row.update({\n",
        "                    'has_transcript': True,\n",
        "                    'transcript_length': len(transcripts[key].get('transcript', '')),\n",
        "                    'word_count': len(transcripts[key].get('transcript', '').split()),\n",
        "                    'language': transcripts[key].get('language', 'unknown'),\n",
        "                    'has_transcript_error': 'error' in transcripts[key]\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    'has_transcript': False,\n",
        "                    'transcript_length': 0,\n",
        "                    'word_count': 0\n",
        "                })\n",
        "\n",
        "            # Add linguistic info\n",
        "            if key in linguistic_features:\n",
        "                row.update({\n",
        "                    'has_linguistic_features': True,\n",
        "                    'unique_words': linguistic_features[key].get('unique_words', 0),\n",
        "                    'lexical_diversity': linguistic_features[key].get('lexical_diversity', 0)\n",
        "                })\n",
        "            else:\n",
        "                row.update({\n",
        "                    'has_linguistic_features': False,\n",
        "                    'unique_words': 0,\n",
        "                    'lexical_diversity': 0\n",
        "                })\n",
        "\n",
        "            summary_data.append(row)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "        # Save summary\n",
        "        summary_path = os.path.join(self.config.output_path, \"pipeline_summary.csv\")\n",
        "        summary_df.to_csv(summary_path, index=False)\n",
        "\n",
        "        # Save complete results\n",
        "        results_path = os.path.join(self.config.output_path, \"complete_results.pkl\")\n",
        "        with open(results_path, 'wb') as f:\n",
        "            pickle.dump(final_results, f)\n",
        "\n",
        "        self.logger.info(\"=\"*50)\n",
        "        self.logger.info(\"PIPELINE SUMMARY\")\n",
        "        self.logger.info(\"=\"*50)\n",
        "        self.logger.info(f\"Total files processed: {len(summary_data)}\")\n",
        "        self.logger.info(f\"Files with acoustic features: {summary_df['has_acoustic_features'].sum()}\")\n",
        "        self.logger.info(f\"Files with transcripts: {summary_df['has_transcript'].sum()}\")\n",
        "        self.logger.info(f\"Files with linguistic features: {summary_df['has_linguistic_features'].sum()}\")\n",
        "        self.logger.info(f\"Average words per transcript: {summary_df['word_count'].mean():.1f}\")\n",
        "        self.logger.info(\"=\"*50)\n",
        "        self.logger.info(\"Output files:\")\n",
        "        self.logger.info(f\"  - Complete results: {results_path}\")\n",
        "        self.logger.info(f\"  - Pipeline summary: {summary_path}\")\n",
        "        self.logger.info(f\"  - Acoustic features: {os.path.join(self.config.output_path, 'acoustic_features.pkl')}\")\n",
        "        self.logger.info(f\"  - Transcripts: {os.path.join(self.config.output_path, 'transcripts/')}\")\n",
        "        self.logger.info(f\"  - Linguistic features: {os.path.join(self.config.output_path, 'linguistic_features.pkl')}\")\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def run_sample_analysis(self, max_files_per_category: int = 2):\n",
        "        \"\"\"Run pipeline on a small sample for testing\"\"\"\n",
        "        self.logger.info(f\"Running sample analysis with max {max_files_per_category} files per category\")\n",
        "\n",
        "        # Get limited audio files\n",
        "        all_audio_files = self.data_manager.get_audio_files()\n",
        "        sample_audio_files = {}\n",
        "\n",
        "        for category, files in all_audio_files.items():\n",
        "            sample_audio_files[category] = files[:max_files_per_category]\n",
        "\n",
        "        # Run pipeline on sample\n",
        "        return self.run_complete_pipeline(parallel=False)\n",
        "\n",
        "    def get_pipeline_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get current pipeline status\"\"\"\n",
        "        return {\n",
        "            'start_time': self.start_time.isoformat() if self.start_time else None,\n",
        "            'end_time': self.end_time.isoformat() if self.end_time else None,\n",
        "            'is_running': self.start_time is not None and self.end_time is None,\n",
        "            'output_path': self.config.output_path,\n",
        "            'results_available': bool(self.results)\n",
        "        }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    pipeline = PipelineService()\n",
        "\n",
        "    # Run sample analysis first\n",
        "    print(\"Running sample analysis...\")\n",
        "    sample_results = pipeline.run_sample_analysis(max_files_per_category=1)\n",
        "\n",
        "    # Then run full pipeline\n",
        "    print(\"\\nRunning full pipeline...\")\n",
        "    results = pipeline.run_complete_pipeline(parallel=True)"
      ],
      "metadata": {
        "id": "L0fbaUYvivUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py - Main Application Entry Point"
      ],
      "metadata": {
        "id": "WtVwcDn6iZA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Main Application Entry Point for ADReSSo21 Speech Analysis\n",
        "Command-line interface for running the complete analysis pipeline\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "\n",
        "# Add current directory to Python path\n",
        "current_dir = Path(__file__).parent\n",
        "sys.path.append(str(current_dir))\n",
        "\n",
        "from pipeline_service import PipelineService\n",
        "from config import Config\n",
        "from utils import setup_logging\n",
        "\n",
        "\n",
        "def create_sample_config():\n",
        "    \"\"\"Create a sample configuration file for first-time setup\"\"\"\n",
        "    sample_config = {\n",
        "        \"dataset\": {\n",
        "            \"base_path\": \"C:/Users/Administrator/Desktop/Speech/ADReSSo21\",\n",
        "            \"diagnosis_train_path\": \"ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train\",\n",
        "            \"progression_train_path\": \"ADReSSo21-progression-train/ADReSSo21/progression/train\",\n",
        "            \"progression_test_path\": \"ADReSSo21-progression-test/ADReSSo21/progression/test-dist\"\n",
        "        },\n",
        "        \"output\": {\n",
        "            \"base_path\": \"C:/Users/Administrator/Desktop/Speech/output\",\n",
        "            \"create_timestamped_folders\": True\n",
        "        },\n",
        "        \"processing\": {\n",
        "            \"max_workers\": 8,\n",
        "            \"transcription_workers\": 2,\n",
        "            \"batch_size\": 10,\n",
        "            \"enable_parallel\": True\n",
        "        },\n",
        "        \"models\": {\n",
        "            \"whisper_model\": \"base\",\n",
        "            \"wav2vec_model\": \"facebook/wav2vec2-base-960h\",\n",
        "            \"bert_model\": \"bert-base-uncased\"\n",
        "        },\n",
        "        \"features\": {\n",
        "            \"acoustic\": {\n",
        "                \"sample_rate\": 16000,\n",
        "                \"n_mfcc\": 13,\n",
        "                \"n_mels\": 80,\n",
        "                \"extract_egemaps\": True,\n",
        "                \"extract_prosodic\": True\n",
        "            },\n",
        "            \"linguistic\": {\n",
        "                \"max_sequence_length\": 512,\n",
        "                \"extract_basic_stats\": True,\n",
        "                \"extract_bert_features\": True\n",
        "            }\n",
        "        },\n",
        "        \"logging\": {\n",
        "            \"level\": \"INFO\",\n",
        "            \"file\": \"adresso_pipeline.log\",\n",
        "            \"console\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    config_path = \"config.json\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(sample_config, f, indent=2)\n",
        "\n",
        "    print(f\"Sample configuration created: {config_path}\")\n",
        "    print(\"Please edit the paths in config.json to match your setup before running the pipeline.\")\n",
        "    return config_path\n",
        "\n",
        "\n",
        "def validate_paths(config: Config) -> bool:\n",
        "    \"\"\"Validate that required paths exist\"\"\"\n",
        "    base_path = config.get('dataset.base_path')\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"Error: Dataset base path does not exist: {base_path}\")\n",
        "        return False\n",
        "\n",
        "    # Check for at least one of the dataset directories\n",
        "    required_subdirs = [\n",
        "        config.get('dataset.diagnosis_train_path'),\n",
        "        config.get('dataset.progression_train_path'),\n",
        "        config.get('dataset.progression_test_path')\n",
        "    ]\n",
        "\n",
        "    found_dirs = []\n",
        "    for subdir in required_subdirs:\n",
        "        full_path = os.path.join(base_path, subdir)\n",
        "        if os.path.exists(full_path):\n",
        "            found_dirs.append(subdir)\n",
        "\n",
        "    if not found_dirs:\n",
        "        print(\"Error: No valid dataset directories found!\")\n",
        "        print(f\"Checked paths under {base_path}:\")\n",
        "        for subdir in required_subdirs:\n",
        "            print(f\"  - {subdir}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Found dataset directories: {found_dirs}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def run_pipeline_command(args):\n",
        "    \"\"\"Run the complete pipeline\"\"\"\n",
        "    try:\n",
        "        # Initialize pipeline\n",
        "        pipeline = PipelineService(args.config)\n",
        "\n",
        "        # Validate configuration\n",
        "        if not validate_paths(pipeline.config):\n",
        "            return 1\n",
        "\n",
        "        # Run pipeline\n",
        "        if args.sample:\n",
        "            print(\"Running sample analysis...\")\n",
        "            results = pipeline.run_sample_analysis(max_files_per_category=args.sample_size)\n",
        "        else:\n",
        "            print(\"Running complete pipeline...\")\n",
        "            results = pipeline.run_complete_pipeline(parallel=args.parallel)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Print summary\n",
        "        total_files = results['pipeline_info']['total_files']\n",
        "        print(f\"Total files processed: {total_files}\")\n",
        "        print(f\"Output directory: {pipeline.config.output_path}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nPipeline failed with error: {str(e)}\")\n",
        "        if args.debug:\n",
        "            print(\"\\nFull traceback:\")\n",
        "            traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "\n",
        "def run_status_command(args):\n",
        "    \"\"\"Check pipeline status\"\"\"\n",
        "    try:\n",
        "        pipeline = PipelineService(args.config)\n",
        "        status = pipeline.get_pipeline_status()\n",
        "\n",
        "        print(\"Pipeline Status:\")\n",
        "        print(f\"  Output Path: {status['output_path']}\")\n",
        "        print(f\"  Is Running: {status['is_running']}\")\n",
        "        print(f\"  Results Available: {status['results_available']}\")\n",
        "\n",
        "        if status['start_time']:\n",
        "            print(f\"  Last Start Time: {status['start_time']}\")\n",
        "        if status['end_time']:\n",
        "            print(f\"  Last End Time: {status['end_time']}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking status: {str(e)}\")\n",
        "        return 1\n",
        "\n",
        "\n",
        "def run_demo_command(args):\n",
        "    \"\"\"Run demo with single file from each category\"\"\"\n",
        "    try:\n",
        "        pipeline = PipelineService(args.config)\n",
        "\n",
        "        print(\"Running demo analysis...\")\n",
        "        print(\"This will process 1 file from each available category\")\n",
        "\n",
        "        # Run with minimal files\n",
        "        results = pipeline.run_sample_analysis(max_files_per_category=1)\n",
        "\n",
        "        print(\"\\nDemo completed successfully!\")\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Demo failed: {str(e)}\")\n",
        "        if args.debug:\n",
        "            traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main application entry point\"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"ADReSSo21 Speech Analysis Pipeline\",\n",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "        epilog=\"\"\"\n",
        "Examples:\n",
        "  # Create sample configuration\n",
        "  python main.py init\n",
        "\n",
        "  # Run demo analysis\n",
        "  python main.py demo\n",
        "\n",
        "  # Run complete pipeline\n",
        "  python main.py run\n",
        "\n",
        "  # Run with custom config\n",
        "  python main.py run --config my_config.json\n",
        "\n",
        "  # Run sample analysis only\n",
        "  python main.py run --sample --sample-size 2\n",
        "\n",
        "  # Run without parallel processing\n",
        "  python main.py run --no-parallel\n",
        "\n",
        "  # Check status\n",
        "  python main.py status\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Global arguments\n",
        "    parser.add_argument('--config', '-c', default='config.json',\n",
        "                       help='Configuration file path (default: config.json)')\n",
        "    parser.add_argument('--debug', action='store_true',\n",
        "                       help='Enable debug mode with full error traces')\n",
        "\n",
        "    # Subcommands\n",
        "    subparsers = parser.add_subparsers(dest='command', help='Available commands')\n",
        "\n",
        "    # Init command\n",
        "    init_parser = subparsers.add_parser('init', help='Create sample configuration file')\n",
        "\n",
        "    # Run command\n",
        "    run_parser = subparsers.add_parser('run', help='Run the analysis pipeline')\n",
        "    run_parser.add_argument('--sample', action='store_true',\n",
        "                           help='Run on sample data only')\n",
        "    run_parser.add_argument('--sample-size', type=int, default=2,\n",
        "                           help='Number of files per category for sample run (default: 2)')\n",
        "    run_parser.add_argument('--no-parallel', dest='parallel', action='store_false',\n",
        "                           help='Disable parallel processing')\n",
        "\n",
        "    # Demo command\n",
        "    demo_parser = subparsers.add_parser('demo', help='Run demo analysis')\n",
        "\n",
        "    # Status command\n",
        "    status_parser = subparsers.add_parser('status', help='Check pipeline status')\n",
        "\n",
        "    # Parse arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Handle commands\n",
        "    if args.command == 'init':\n",
        "        create_sample_config()\n",
        "        return 0\n",
        "\n",
        "    elif args.command == 'run':\n",
        "        return run_pipeline_command(args)\n",
        "\n",
        "    elif args.command == 'demo':\n",
        "        return run_demo_command(args)\n",
        "\n",
        "    elif args.command == 'status':\n",
        "        return run_status_command(args)\n",
        "\n",
        "    else:\n",
        "        # No command specified, show help\n",
        "        parser.print_help()\n",
        "        return 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    exit_code = main()\n",
        "    sys.exit(exit_code)"
      ],
      "metadata": {
        "id": "eZi9N2LTi0HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# requirements.txt - Project Dependencies\n"
      ],
      "metadata": {
        "id": "oCpE1WnZidLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core dependencies for ADReSSo21 Speech Analysis Pipeline\n",
        "\n",
        "# Audio processing\n",
        "librosa>=0.10.0\n",
        "soundfile>=0.12.1\n",
        "opensmile>=2.4.2\n",
        "\n",
        "# Speech recognition and transcription\n",
        "openai-whisper>=20231117\n",
        "transformers>=4.35.0\n",
        "torch>=2.0.0\n",
        "torchaudio>=2.0.0\n",
        "\n",
        "# NLP and language models\n",
        "tokenizers>=0.14.0\n",
        "numpy>=1.24.0\n",
        "scipy>=1.10.0\n",
        "\n",
        "# Data handling and processing\n",
        "pandas>=2.0.0\n",
        "scikit-learn>=1.3.0\n",
        "\n",
        "# Parallel processing\n",
        "joblib>=1.3.0\n",
        "\n",
        "# Configuration and utilities\n",
        "pyyaml>=6.0\n",
        "python-dotenv>=1.0.0\n",
        "\n",
        "# Optional GPU support (uncomment if using CUDA)\n",
        "# torch>=2.0.0+cu118\n",
        "# torchaudio>=2.0.0+cu118\n",
        "\n",
        "# Development and testing (optional)\n",
        "pytest>=7.4.0\n",
        "jupyter>=1.0.0\n",
        "matplotlib>=3.7.0\n",
        "seaborn>=0.12.0\n",
        "\n"
      ],
      "metadata": {
        "id": "2bS87LZAi2YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# setup.py - Project Setup Script\n",
        "\n"
      ],
      "metadata": {
        "id": "LIlu8w1Lin9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Setup script for ADReSSo21 Speech Analysis Pipeline\n",
        "Handles installation, environment setup, and model downloads\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import json\n",
        "\n",
        "\n",
        "class PipelineSetup:\n",
        "    \"\"\"Setup and installation handler for the pipeline\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.project_root = Path(__file__).parent\n",
        "        self.system_info = {\n",
        "            'os': platform.system(),\n",
        "            'python_version': sys.version,\n",
        "            'architecture': platform.architecture()[0]\n",
        "        }\n",
        "\n",
        "    def check_system_requirements(self):\n",
        "        \"\"\"Check if system meets minimum requirements\"\"\"\n",
        "        print(\"Checking system requirements...\")\n",
        "\n",
        "        # Check Python version\n",
        "        if sys.version_info < (3, 8):\n",
        "            print(\"❌ Python 3.8+ required. Current version:\", sys.version)\n",
        "            return False\n",
        "        print(\"✅ Python version:\", sys.version.split()[0])\n",
        "\n",
        "        # Check available memory (approximate)\n",
        "        try:\n",
        "            import psutil\n",
        "            memory_gb = psutil.virtual_memory().total / (1024**3)\n",
        "            if memory_gb < 8:\n",
        "                print(f\"⚠️  Warning: Low memory detected ({memory_gb:.1f}GB). 16GB+ recommended.\")\n",
        "            else:\n",
        "                print(f\"✅ Memory: {memory_gb:.1f}GB\")\n",
        "        except ImportError:\n",
        "            print(\"⚠️  Cannot check memory (psutil not available)\")\n",
        "\n",
        "        # Check disk space\n",
        "        try:\n",
        "            disk_space = psutil.disk_usage('.').free / (1024**3)\n",
        "            if disk_space < 10:\n",
        "                print(f\"⚠️  Warning: Low disk space ({disk_space:.1f}GB). 20GB+ recommended.\")\n",
        "            else:\n",
        "                print(f\"✅ Disk space: {disk_space:.1f}GB available\")\n",
        "        except:\n",
        "            print(\"⚠️  Cannot check disk space\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def install_dependencies(self):\n",
        "        \"\"\"Install Python dependencies\"\"\"\n",
        "        print(\"\\nInstalling Python dependencies...\")\n",
        "\n",
        "        requirements_file = self.project_root / \"requirements.txt\"\n",
        "\n",
        "        if not requirements_file.exists():\n",
        "            print(\"❌ requirements.txt not found!\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Upgrade pip first\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n",
        "                         check=True)\n",
        "\n",
        "            # Install requirements\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)],\n",
        "                         check=True)\n",
        "\n",
        "            print(\"✅ Dependencies installed successfully\")\n",
        "            return True\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"❌ Failed to install dependencies: {e}\")\n",
        "            return False\n",
        "\n",
        "    def setup_directories(self):\n",
        "        \"\"\"Create necessary directories\"\"\"\n",
        "        print(\"\\nSetting up directories...\")\n",
        "\n",
        "        directories = [\n",
        "            \"output\",\n",
        "            \"logs\",\n",
        "            \"models\",\n",
        "            \"temp\",\n",
        "            \"data\"\n",
        "        ]\n",
        "\n",
        "        for directory in directories:\n",
        "            dir_path = self.project_root / directory\n",
        "            dir_path.mkdir(exist_ok=True)\n",
        "            print(f\"✅ Created/verified: {directory}/\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def download_sample_data(self):\n",
        "        \"\"\"Download sample data for testing (if available)\"\"\"\n",
        "        print(\"\\nSetting up sample data...\")\n",
        "\n",
        "        # Create a minimal sample structure for testing\n",
        "        sample_dir = self.project_root / \"data\" / \"sample\"\n",
        "        sample_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create sample directory structure\n",
        "        sample_structure = [\n",
        "            \"diagnosis/train/audio/ad\",\n",
        "            \"diagnosis/train/audio/cn\",\n",
        "            \"diagnosis/train/segmentation/ad\",\n",
        "            \"diagnosis/train/segmentation/cn\",\n",
        "            \"progression/train/audio/decline\",\n",
        "            \"progression/train/audio/no_decline\",\n",
        "            \"progression/train/segmentation/decline\",\n",
        "            \"progression/train/segmentation/no_decline\",\n",
        "            \"progression/test-dist/audio\",\n",
        "            \"progression/test-dist/segmentation\"\n",
        "        ]\n",
        "\n",
        "        for structure in sample_structure:\n",
        "            (sample_dir / structure).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create a sample README\n",
        "        readme_content = \"\"\"\n",
        "# Sample Data Directory Structure\n",
        "\n",
        "This directory contains the expected structure for ADReSSo21 dataset.\n",
        "\n",
        "## Directory Structure:\n",
        "- diagnosis/train/audio/ad/          - Alzheimer's audio files\n",
        "- diagnosis/train/audio/cn/          - Control audio files\n",
        "- diagnosis/train/segmentation/      - Segmentation files\n",
        "- progression/train/audio/           - Progression training audio\n",
        "- progression/test-dist/audio/       - Progression test audio\n",
        "\n",
        "## Usage:\n",
        "Place your actual ADReSSo21 dataset files in this structure, or update\n",
        "the paths in config.json to point to your dataset location.\n",
        "\"\"\"\n",
        "\n",
        "        with open(sample_dir / \"README.md\", \"w\") as f:\n",
        "            f.write(readme_content)\n",
        "\n",
        "        print(\"✅ Sample directory structure created\")\n",
        "        return True\n",
        "\n",
        "    def create_default_config(self):\n",
        "        \"\"\"Create default configuration file\"\"\"\n",
        "        print(\"\\nCreating default configuration...\")\n",
        "\n",
        "        config = {\n",
        "            \"dataset\": {\n",
        "                \"base_path\": str(self.project_root / \"data\" / \"sample\"),\n",
        "                \"diagnosis_train_path\": \"diagnosis/train\",\n",
        "                \"progression_train_path\": \"progression/train\",\n",
        "                \"progression_test_path\": \"progression/test-dist\"\n",
        "            },\n",
        "            \"output\": {\n",
        "                \"base_path\": str(self.project_root / \"output\"),\n",
        "                \"create_timestamped_folders\": True\n",
        "            },\n",
        "            \"processing\": {\n",
        "                \"max_workers\": min(os.cpu_count(), 8),\n",
        "                \"transcription_workers\": 2,\n",
        "                \"batch_size\": 10,\n",
        "                \"enable_parallel\": True\n",
        "            },\n",
        "            \"models\": {\n",
        "                \"whisper_model\": \"base\",\n",
        "                \"wav2vec_model\": \"facebook/wav2vec2-base-960h\",\n",
        "                \"bert_model\": \"bert-base-uncased\"\n",
        "            },\n",
        "            \"features\": {\n",
        "                \"acoustic\": {\n",
        "                    \"sample_rate\": 16000,\n",
        "                    \"n_mfcc\": 13,\n",
        "                    \"n_mels\": 80,\n",
        "                    \"extract_egemaps\": True,\n",
        "                    \"extract_prosodic\": True\n",
        "                },\n",
        "                \"linguistic\": {\n",
        "                    \"max_sequence_length\": 512,\n",
        "                    \"extract_basic_stats\": True,\n",
        "                    \"extract_bert_features\": True\n",
        "                }\n",
        "            },\n",
        "            \"logging\": {\n",
        "                \"level\": \"INFO\",\n",
        "                \"file\": \"adresso_pipeline.log\",\n",
        "                \"console\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        config_path = self.project_root / \"config.json\"\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "\n",
        "        print(f\"✅ Default configuration created: {config_path}\")\n",
        "        return True\n",
        "\n",
        "    def verify_installation(self):\n",
        "        \"\"\"Verify that installation was successful\"\"\"\n",
        "        print(\"\\nVerifying installation...\")\n",
        "\n",
        "        # Test imports\n",
        "        test_imports = [\n",
        "            'librosa',\n",
        "            'whisper',\n",
        "            'transformers',\n",
        "            'torch',\n",
        "            'opensmile',\n",
        "            'pandas',\n",
        "            'numpy'\n",
        "        ]\n",
        "\n",
        "        failed_imports = []\n",
        "        for module in test_imports:\n",
        "            try:\n",
        "                __import__(module)\n",
        "                print(f\"✅ {module}\")\n",
        "            except ImportError as e:\n",
        "                print(f\"❌ {module}: {e}\")\n",
        "                failed_imports.append(module)\n",
        "\n",
        "        if failed_imports:\n",
        "            print(f\"\\n❌ Failed to import: {failed_imports}\")\n",
        "            print(\"Please check the installation and try running:\")\n",
        "            print(\"pip install -r requirements.txt\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n✅ All modules imported successfully!\")\n",
        "        return True\n",
        "\n",
        "    def run_setup(self):\n",
        "        \"\"\"Run complete setup process\"\"\"\n",
        "        print(\"=\"*60)\n",
        "        print(\"ADReSSo21 Speech Analysis Pipeline Setup\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        steps = [\n",
        "            (\"System Requirements\", self.check_system_requirements),\n",
        "            (\"Dependencies\", self.install_dependencies),\n",
        "            (\"Directories\", self.setup_directories),\n",
        "            (\"Sample Data\", self.download_sample_data),\n",
        "            (\"Configuration\", self.create_default_config),\n",
        "            (\"Verification\", self.verify_installation)\n",
        "        ]\n",
        "\n",
        "        for step_name, step_func in steps:\n",
        "            print(f\"\\n{'='*20} {step_name} {'='*20}\")\n",
        "            if not step_func():\n",
        "                print(f\"\\n❌ Setup failed at step: {step_name}\")\n",
        "                return False\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🎉 SETUP COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Update config.json with your dataset paths\")\n",
        "        print(\"2. Run: python main.py demo\")\n",
        "        print(\"3. Run: python main.py run\")\n",
        "        print(\"\\nFor help: python main.py --help\")\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main setup function\"\"\"\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"--help\":\n",
        "        print(\"\"\"\n",
        "ADReSSo21 Pipeline Setup\n",
        "\n",
        "Usage:\n",
        "    python setup.py                 - Run complete setup\n",
        "    python setup.py --help          - Show this help\n",
        "    python setup.py --verify-only   - Only verify installation\n",
        "    python setup.py --deps-only     - Only install dependencies\n",
        "        \"\"\")\n",
        "        return\n",
        "\n",
        "    setup = PipelineSetup()\n",
        "\n",
        "    if len(sys.argv) > 1 and sys.argv[1] == \"--verify-only\":\n",
        "        setup.verify_installation()\n",
        "    elif len(sys.argv) > 1 and sys.argv[1] == \"--deps-only\":\n",
        "        setup.install_dependencies()\n",
        "    else:\n",
        "        setup.run_setup()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bObYFUbxi8kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md - Project Documentation"
      ],
      "metadata": {
        "id": "Unh_01eXirx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADReSSo21 Speech Analysis Pipeline\n",
        "\n",
        "A modular, high-performance pipeline for analyzing speech data from the ADReSSo21 dataset (Alzheimer's Dementia Recognition through Spontaneous Speech). This pipeline extracts comprehensive acoustic, linguistic, and semantic features for dementia detection and progression analysis.\n",
        "\n",
        "## Features\n",
        "\n",
        "🎯 **Comprehensive Analysis**\n",
        "- Acoustic feature extraction (eGeMAPS, MFCCs, Mel-spectrograms, Wav2Vec2)\n",
        "- Speech-to-text transcription (Whisper)\n",
        "- Linguistic feature analysis (BERT embeddings, lexical diversity)\n",
        "- Prosodic analysis (F0, energy, spectral features)\n",
        "\n",
        "⚡ **High Performance**\n",
        "- Multi-core parallel processing\n",
        "- Optimized for Windows 10 with 35GB RAM, 10 cores\n",
        "- Memory-efficient batch processing\n",
        "- Modular microservice architecture\n",
        "\n",
        "🔧 **Easy to Use**\n",
        "- Command-line interface\n",
        "- Configurable via JSON\n",
        "- Sample data support\n",
        "- Comprehensive logging\n",
        "\n",
        "## System Requirements\n",
        "\n",
        "- **OS**: Windows 10/11, Linux, macOS\n",
        "- **Python**: 3.8+\n",
        "- **RAM**: 16GB+ recommended (35GB optimal)\n",
        "- **CPU**: Multi-core processor (10 cores optimal)\n",
        "- **Storage**: 20GB+ free space\n",
        "- **GPU**: Optional (CUDA-compatible for faster processing)\n",
        "\n",
        "## Installation\n",
        "\n",
        "### Quick Setup\n",
        "\n",
        "```bash\n",
        "# Clone or download the project\n",
        "git clone <repository-url>\n",
        "cd adresso21-pipeline\n",
        "\n",
        "# Run setup script\n",
        "python setup.py\n",
        "```\n",
        "\n",
        "### Manual Installation\n",
        "\n",
        "```bash\n",
        "# Install dependencies\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Create configuration\n",
        "python main.py init\n",
        "\n",
        "# Setup directories\n",
        "mkdir output logs models temp data\n",
        "```\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Edit `config.json` to match your setup:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"dataset\": {\n",
        "    \"base_path\": \"C:/Users/Administrator/Desktop/Speech/ADReSSo21\",\n",
        "    \"diagnosis_train_path\": \"ADReSSo21-diagnosis-train/ADReSSo21/diagnosis/train\",\n",
        "    \"progression_train_path\": \"ADReSSo21-progression-train/ADReSSo21/progression/train\",\n",
        "    \"progression_test_path\": \"ADReSSo21-progression-test/ADReSSo21/progression/test-dist\"\n",
        "  },\n",
        "  \"output\": {\n",
        "    \"base_path\": \"C:/Users/Administrator/Desktop/Speech/output\",\n",
        "    \"create_timestamped_folders\": true\n",
        "  },\n",
        "  \"processing\": {\n",
        "    \"max_workers\": 8,\n",
        "    \"transcription_workers\": 2,\n",
        "    \"enable_parallel\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "Ensure your ADReSSo21 dataset follows this structure:\n",
        "\n",
        "```\n",
        "ADReSSo21/\n",
        "├── diagnosis/train/\n",
        "│   ├── audio/\n",
        "│   │   ├── ad/*.wav          # Alzheimer's audio files\n",
        "│   │   └── cn/*.wav          # Control audio files\n",
        "│   └── segmentation/\n",
        "│       ├── ad/*.csv          # Alzheimer's segmentation\n",
        "│       └── cn/*.csv          # Control segmentation\n",
        "├── progression/train/\n",
        "│   ├── audio/\n",
        "│   │   ├── decline/*.wav     # Decline audio files\n",
        "│   │   └── no_decline/*.wav  # No decline audio files\n",
        "│   └── segmentation/\n",
        "│       ├── decline/*.csv     # Decline segmentation\n",
        "│       └── no_decline/*.csv  # No decline segmentation\n",
        "└── progression/test-dist/\n",
        "    ├── audio/*.wav           # Test audio files\n",
        "    └── segmentation/*.csv    # Test segmentation\n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Command Line Interface\n",
        "\n",
        "```bash\n",
        "# Initialize configuration\n",
        "python main.py init\n",
        "\n",
        "# Run demo analysis (1 file per category)\n",
        "python main.py demo\n",
        "\n",
        "# Run sample analysis (2 files per category)  \n",
        "python main.py run --sample --sample-size 2\n",
        "\n",
        "# Run complete pipeline\n",
        "python main.py run\n",
        "\n",
        "# Run without parallel processing\n",
        "python main.py run --no-parallel\n",
        "\n",
        "# Check pipeline status\n",
        "python main.py status\n",
        "\n",
        "# Custom configuration\n",
        "python main.py run --config my_config.json\n",
        "```\n",
        "\n",
        "### Python API\n",
        "\n",
        "```python\n",
        "from pipeline_service import PipelineService\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = PipelineService('config.json')\n",
        "\n",
        "# Run complete analysis\n",
        "results = pipeline.run_complete_pipeline(parallel=True)\n",
        "\n",
        "# Run sample analysis  \n",
        "results = pipeline.run_sample_analysis(max_files_per_category=2)\n",
        "\n",
        "# Check status\n",
        "status = pipeline.get_pipeline_status()\n",
        "```\n",
        "\n",
        "## Architecture\n",
        "\n",
        "The pipeline follows a modular microservice architecture:\n",
        "\n",
        "```\n",
        "main.py                     # Entry point and CLI\n",
        "├── pipeline_service.py     # Main orchestrator\n",
        "├── config.py              # Configuration management\n",
        "├── utils.py               # Utilities and helpers\n",
        "├── data_manager_service.py          # Dataset loading\n",
        "├── acoustic_features_service.py     # Audio feature extraction\n",
        "├── transcription_service.py         # Speech-to-text\n",
        "└── linguistic_features_service.py   # Text analysis\n",
        "```\n",
        "\n",
        "### Key Components\n",
        "\n",
        "1. **PipelineService**: Main orchestrator that coordinates all services\n",
        "2. **DataManagerService**: Handles dataset loading and file management\n",
        "3. **AcousticFeaturesService**: Extracts audio features (eGeMAPS, MFCCs, etc.)\n",
        "4. **TranscriptionService**: Converts speech to text using Whisper\n",
        "5. **LinguisticFeaturesService**: Analyzes text features and BERT embeddings\n",
        "\n",
        "## Output\n",
        "\n",
        "The pipeline generates comprehensive outputs:\n",
        "\n",
        "```\n",
        "output/\n",
        "├── acoustic_features.pkl        # All acoustic features\n",
        "├── transcripts/\n",
        "│   ├── all_transcripts.json    # All transcriptions\n",
        "│   ├── transcripts.pkl         # Pickle format\n",
        "│   └── *_transcript.txt        # Individual transcripts\n",
        "├── linguistic_features.pkl     # Text analysis results\n",
        "├── pipeline_summary.csv        # Processing summary\n",
        "├── complete_results.pkl        # Combined results\n",
        "└── adresso_pipeline.log        # Processing logs\n",
        "```\n",
        "\n",
        "### Feature Types\n",
        "\n",
        "**Acoustic Features:**\n",
        "- eGeMAPS (88 features)\n",
        "- MFCCs (13 coefficients + deltas)\n",
        "- Mel-spectrograms (80 bands)\n",
        "- Wav2Vec2 embeddings (768 dimensions)\n",
        "- Prosodic features (F0, energy, spectral)\n",
        "\n",
        "**Linguistic Features:**\n",
        "- Basic statistics (word count, sentence count)\n",
        "- Lexical diversity measures\n",
        "- BERT embeddings (768 dimensions)\n",
        "- Language detection\n",
        "- Segmentation analysis\n",
        "\n",
        "## Performance\n",
        "\n",
        "Typical processing times on recommended hardware:\n",
        "\n",
        "- **Demo** (5 files): ~2-3 minutes\n",
        "- **Sample** (20 files): ~5-10 minutes  \n",
        "- **Complete dataset** (500+ files): ~2-4 hours\n",
        "\n",
        "Memory usage:\n",
        "- Base: ~2-4 GB\n",
        "- With parallel processing: ~8-12 GB\n",
        "- Peak (large files): ~16-20 GB\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "**1. Import Errors**\n",
        "```bash\n",
        "# Reinstall dependencies\n",
        "pip install -r requirements.txt --force-reinstall\n",
        "```\n",
        "\n",
        "**2. Memory Issues**\n",
        "- Reduce `max_workers` in config\n",
        "- Disable parallel processing: `--no-parallel`\n",
        "- Process in smaller batches\n",
        "\n",
        "**3. Model Download Issues**\n",
        "```bash\n",
        "# Pre-download models\n",
        "python -c \"import whisper; whisper.load_model('base')\"\n",
        "python -c \"from transformers import AutoModel; AutoModel.from_pretrained('facebook/wav2vec2-base-960h')\"\n",
        "```\n",
        "\n",
        "**4. Path Issues**\n",
        "- Use absolute paths in config.json\n",
        "- Check file permissions\n",
        "- Verify dataset structure\n",
        "\n",
        "### Performance Optimization\n",
        "\n",
        "**For Limited RAM:**\n",
        "```json\n",
        "{\n",
        "  \"processing\": {\n",
        "    \"max_workers\": 4,\n",
        "    \"transcription_workers\": 1,\n",
        "    \"enable_parallel\": false\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**For High Performance:**\n",
        "```json\n",
        "{\n",
        "  \"processing\": {\n",
        "    \"max_workers\": 10,\n",
        "    \"transcription_workers\": 4,\n",
        "    \"batch_size\": 20,\n",
        "    \"enable_parallel\": true\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## Development\n",
        "\n",
        "### Adding New Features\n",
        "\n",
        "1. Create new service in `services/`\n",
        "2. Add configuration options\n",
        "3. Update `pipeline_service.py`\n",
        "4. Add tests and documentation\n",
        "\n",
        "### Testing\n",
        "\n",
        "```bash\n",
        "# Run demo for testing\n",
        "python main.py demo\n",
        "\n",
        "# Run with debug output\n",
        "python main.py run --debug\n",
        "\n",
        "# Test specific components\n",
        "python -c \"from acoustic_features_service import AcousticFeaturesService; service = AcousticFeaturesService()\"\n",
        "```\n",
        "\n",
        "## Contributing\n",
        "\n",
        "1. Fork the repository\n",
        "2. Create a feature branch\n",
        "3. Make changes with tests\n",
        "4. Submit a pull request\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License - see the LICENSE file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this pipeline in your research, please cite:\n",
        "\n",
        "```bibtex\n",
        "@software{adresso21_pipeline,\n",
        "  title={ADReSSo21 Speech Analysis Pipeline},\n",
        "  author={Your Name},\n",
        "  year={2024},\n",
        "  url={https://github.com/your-repo/adresso21-pipeline}\n",
        "}\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "- ADReSSo21 Dataset creators\n",
        "- OpenAI Whisper team\n",
        "- Hugging Face Transformers\n",
        "- OpenSMILE developers\n",
        "\n",
        "## Support\n",
        "\n",
        "For support and questions:\n",
        "- Check the troubleshooting section\n",
        "- Review logs in `output/adresso_pipeline.log`\n",
        "- Open an issue on GitHub\n",
        "- Contact: your.email@domain.com"
      ],
      "metadata": {
        "id": "Wzy6IAhAi-oN"
      }
    }
  ]
}