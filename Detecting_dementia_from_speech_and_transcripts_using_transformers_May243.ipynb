{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May243.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "from transformers import BertTokenizer, BertModel, ViTModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "aWtS7H2jyk6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea294fc1-3eb9-42a5-9cb4-90ac9afe8060"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_synthetic_dataset(num_ad=87, num_cn=79):\n",
        "    \"\"\"Generate synthetic dataset mimicking ADReSS.\"\"\"\n",
        "    data = []\n",
        "\n",
        "    # AD-like transcripts: hesitant, repetitive, vague\n",
        "    ad_patterns = [\n",
        "        \"Um... I see a kitchen, and uh... someone is there... washing something, maybe dishes...\",\n",
        "        \"The boy is... uh... climbing to get... um... cookies or something... I think...\",\n",
        "        \"There's water... uh... spilling and... uh... people are doing things...\"\n",
        "    ]\n",
        "\n",
        "    # CN-like transcripts: coherent, detailed\n",
        "    cn_patterns = [\n",
        "        \"In the kitchen, a woman is washing dishes while a boy reaches for a cookie jar.\",\n",
        "        \"The scene shows a sink overflowing and a child on a stool grabbing cookies.\",\n",
        "        \"A mother is cleaning dishes, and two children are nearby, one reaching for snacks.\"\n",
        "    ]\n",
        "\n",
        "    # Add variability to avoid perfect separation\n",
        "    for i in range(num_ad):\n",
        "        transcript = np.random.choice(ad_patterns) + \" \" + np.random.choice(ad_patterns, size=1)[0][:20]\n",
        "        data.append({\n",
        "            'participant_id': f'AD_{i:03d}',\n",
        "            'audio_path': f'synthetic_audio_AD_{i:03d}.wav',\n",
        "            'transcript': transcript,\n",
        "            'label': 1,\n",
        "            'class_name': 'AD'\n",
        "        })\n",
        "\n",
        "    for i in range(num_cn):\n",
        "        transcript = np.random.choice(cn_patterns) + \" \" + np.random.choice(cn_patterns, size=1)[0][:20]\n",
        "        data.append({\n",
        "            'participant_id': f'CN_{i:03d}',\n",
        "            'audio_path': f'synthetic_audio_CN_{i:03d}.wav',\n",
        "            'transcript': transcript,\n",
        "            'label': 0,\n",
        "            'class_name': 'CN'\n",
        "        })\n",
        "\n",
        "    print(f\"Created synthetic dataset: {num_ad} AD, {num_cn} CN samples\")\n",
        "    return data\n",
        "\n",
        "# Generate dataset\n",
        "dataset = create_synthetic_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-OTRs6W4zub",
        "outputId": "d45d1023-e7e4-467b-f07b-9b2b279a141e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created synthetic dataset: 87 AD, 79 CN samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioProcessor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=224, win_length=2048, hop_length=1024):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def load_audio(self, audio_path, max_length=16000*16):\n",
        "        \"\"\"Load audio or return synthetic signal if file missing.\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "        except:\n",
        "            # Simulate audio for synthetic dataset\n",
        "            audio = np.random.randn(max_length) * 0.01\n",
        "            sr = self.sample_rate\n",
        "\n",
        "        if len(audio) > max_length:\n",
        "            start = (len(audio) - max_length) // 2\n",
        "            audio = audio[start:start + max_length]\n",
        "        elif len(audio) < max_length:\n",
        "            audio = np.pad(audio, (0, max_length - len(audio)), mode='constant')\n",
        "        return audio\n",
        "\n",
        "    def extract_mel_spectrogram(self, audio):\n",
        "        \"\"\"Extract 3-channel Log-Mel spectrogram.\"\"\"\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=audio, sr=self.sample_rate, n_mels=self.n_mels,\n",
        "                n_fft=self.win_length, hop_length=self.hop_length\n",
        "            )\n",
        "            log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            delta = librosa.feature.delta(log_mel)\n",
        "            delta2 = librosa.feature.delta(log_mel, order=2)\n",
        "            return np.stack([log_mel, delta, delta2], axis=0)  # Shape: (3, n_mels, time)\n",
        "        except:\n",
        "            return np.random.randn(3, self.n_mels, 100)\n",
        "\n",
        "    def resize_spectrogram(self, spectrogram, target_size=(224, 224)):\n",
        "        \"\"\"Resize spectrogram to ViT input size.\"\"\"\n",
        "        from scipy.ndimage import zoom\n",
        "        try:\n",
        "            resized_channels = []\n",
        "            for channel in spectrogram:\n",
        "                zoom_factors = [target_size[i] / channel.shape[i] for i in range(2)]\n",
        "                resized = zoom(channel, zoom_factors, order=1)\n",
        "                resized_channels.append(resized)\n",
        "            resized = np.stack(resized_channels, axis=0)\n",
        "            resized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)\n",
        "            return resized\n",
        "        except:\n",
        "            return np.random.rand(3, target_size[0], target_size[1])"
      ],
      "metadata": {
        "id": "cIHgWQl3449E"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}