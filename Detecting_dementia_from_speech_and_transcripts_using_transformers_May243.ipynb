{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May243.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import librosa.display\n",
        "from transformers import (BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor,\n",
        "                         Wav2Vec2ForCTC, Wav2Vec2Processor, WhisperProcessor,\n",
        "                         WhisperForConditionalGeneration)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "from collections import defaultdict\n",
        "from scipy import ndimage\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SpeechTranscriber:\n",
        "    \"\"\"Automatic Speech Recognition for generating transcripts from audio\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"openai/whisper-base\", cache_dir=\"./asr_cache\"):\n",
        "        \"\"\"\n",
        "        Initialize ASR model\n",
        "        Options:\n",
        "        - openai/whisper-base: Good balance of speed/accuracy\n",
        "        - facebook/wav2vec2-base-960h: Faster but less accurate\n",
        "        - openai/whisper-small: More accurate but slower\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Loading ASR model: {model_name}\")\n",
        "\n",
        "        if \"whisper\" in model_name.lower():\n",
        "            self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "            self.asr_type = \"whisper\"\n",
        "        else:\n",
        "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            self.asr_type = \"wav2vec2\"\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"ASR model loaded on {self.device}\")\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path, use_cache=True):\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        # Check cache first\n",
        "        cache_file = self.cache_dir / f\"{audio_path.stem}_transcript.pkl\"\n",
        "        if use_cache and cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cached_result = pickle.load(f)\n",
        "                return cached_result['transcript']\n",
        "            except:\n",
        "                pass  # Cache corrupted, proceed with transcription\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(str(audio_path), sr=16000)  # Whisper expects 16kHz\n",
        "\n",
        "            # Handle empty or very short audio\n",
        "            if len(audio) < 1600:  # Less than 0.1 seconds\n",
        "                transcript = \"\"\n",
        "            else:\n",
        "                transcript = self._transcribe_audio_array(audio)\n",
        "\n",
        "            # Cache result\n",
        "            if use_cache:\n",
        "                try:\n",
        "                    with open(cache_file, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'audio_path': str(audio_path),\n",
        "                            'transcript': transcript,\n",
        "                            'model': self.model_name\n",
        "                        }, f)\n",
        "                except:\n",
        "                    pass  # Caching failed, but transcription succeeded\n",
        "\n",
        "            return transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {audio_path}: {e}\")\n",
        "            return f\"[Transcription failed for {audio_path.name}]\"\n",
        "\n",
        "    def _transcribe_audio_array(self, audio_array):\n",
        "        \"\"\"Transcribe audio array using the loaded model\"\"\"\n",
        "        try:\n",
        "            if self.asr_type == \"whisper\":\n",
        "                return self._whisper_transcribe(audio_array)\n",
        "            else:\n",
        "                return self._wav2vec2_transcribe(audio_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {e}\")\n",
        "            return \"[Transcription error]\"\n",
        "\n",
        "    def _whisper_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Whisper model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_features.to(self.device)\n",
        "\n",
        "        # Generate transcription\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(inputs, max_length=448)\n",
        "\n",
        "        # Decode\n",
        "        transcript = self.processor.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcript.strip()\n",
        "\n",
        "    def _wav2vec2_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Wav2Vec2 model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).input_values.to(self.device)\n",
        "\n",
        "        # Get logits\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs).logits\n",
        "\n",
        "        # Decode\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcript = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "        return transcript.strip().lower()\n",
        "\n",
        "    def batch_transcribe(self, audio_paths, batch_size=8):\n",
        "        \"\"\"Transcribe multiple audio files with progress bar\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(f\"Transcribing {len(audio_paths)} audio files...\")\n",
        "\n",
        "        for audio_path in tqdm(audio_paths, desc=\"Transcribing\"):\n",
        "            transcript = self.transcribe_audio_file(audio_path)\n",
        "            participant_id = self._extract_participant_id(Path(audio_path).name)\n",
        "            transcripts[participant_id] = transcript\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',         # adrs0123 or adrso123\n",
        "            r'adrsp?(\\d{3})',         # adrsp123\n",
        "            r'adrspt?(\\d{1,3})',      # adrspt1, adrspt12\n",
        "            r'(\\d{3})',               # 3-digit numbers\n",
        "            r'([A-Z]\\d{2,3})',        # Letter followed by 2-3 digits\n",
        "            r'(S\\d{3})',              # S followed by 3 digits\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem"
      ],
      "metadata": {
        "id": "PepfW9iw70QI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedADReSSDataProcessor:\n",
        "    \"\"\"Enhanced ADReSS data processor with automatic speech recognition\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./extracted_data', asr_model=\"openai/whisper-base\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize ASR\n",
        "        self.transcriber = SpeechTranscriber(model_name=asr_model)\n",
        "\n",
        "    def extract_adress_dataset(self, tar_path, dataset_name):\n",
        "        \"\"\"Extract ADReSS dataset and organize files properly\"\"\"\n",
        "        extract_path = self.output_dir / dataset_name\n",
        "        extract_path.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting {tar_path} to {extract_path}\")\n",
        "\n",
        "        try:\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "            print(f\"Successfully extracted {dataset_name}\")\n",
        "\n",
        "            # Find the actual dataset directory structure\n",
        "            self._explore_directory_structure(extract_path)\n",
        "            return extract_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {tar_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _explore_directory_structure(self, base_path):\n",
        "        \"\"\"Explore and print directory structure\"\"\"\n",
        "        print(f\"\\nDirectory structure for {base_path.name}:\")\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            level = root.replace(str(base_path), '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:5]:\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "    def process_adress_dataset_with_asr(self, extract_path):\n",
        "        \"\"\"Process ADReSS dataset with automatic speech recognition\"\"\"\n",
        "        dataset_info = {\n",
        "            'audio_files': [],\n",
        "            'transcript_files': [],\n",
        "            'metadata_files': [],\n",
        "            'labels': {},\n",
        "            'paired_data': [],\n",
        "            'generated_transcripts': {}\n",
        "        }\n",
        "\n",
        "        # Look for ADReSS structure\n",
        "        adress_dirs = list(extract_path.rglob(\"*ADReSS*\"))\n",
        "        if adress_dirs:\n",
        "            main_dir = adress_dirs[0]\n",
        "        else:\n",
        "            main_dir = extract_path\n",
        "\n",
        "        print(f\"Processing from directory: {main_dir}\")\n",
        "\n",
        "        # Find audio files\n",
        "        audio_patterns = ['**/*.wav', '**/*.mp3', '**/*.flac']\n",
        "        for pattern in audio_patterns:\n",
        "            dataset_info['audio_files'].extend(list(main_dir.glob(pattern)))\n",
        "\n",
        "        print(f\"Found {len(dataset_info['audio_files'])} audio files\")\n",
        "\n",
        "        # Generate transcripts using ASR\n",
        "        print(\"Generating transcripts using ASR...\")\n",
        "        audio_paths = [str(path) for path in dataset_info['audio_files']]\n",
        "        generated_transcripts = self.transcriber.batch_transcribe(audio_paths)\n",
        "        dataset_info['generated_transcripts'] = generated_transcripts\n",
        "\n",
        "        print(f\"Generated {len(generated_transcripts)} transcripts\")\n",
        "\n",
        "        # Process labels from directory structure\n",
        "        labels = self._extract_labels_from_structure(dataset_info['audio_files'])\n",
        "        dataset_info['labels'] = labels\n",
        "\n",
        "        # Create paired dataset with generated transcripts\n",
        "        paired_data = self._create_paired_data_with_asr(dataset_info)\n",
        "        dataset_info['paired_data'] = paired_data\n",
        "\n",
        "        return dataset_info\n",
        "\n",
        "    def _extract_labels_from_structure(self, audio_files):\n",
        "        \"\"\"Extract labels from file paths or directory structure\"\"\"\n",
        "        labels = {}\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            # Extract participant ID\n",
        "            participant_id = self._extract_participant_id(audio_file.name)\n",
        "\n",
        "            # Determine label from path\n",
        "            path_str = str(audio_file).lower()\n",
        "            if '/ad/' in path_str or 'dementia' in path_str or 'alzheimer' in path_str:\n",
        "                label = 1  # AD/Dementia\n",
        "                class_name = 'AD'\n",
        "            elif '/cn/' in path_str or 'control' in path_str or 'normal' in path_str:\n",
        "                label = 0  # Control/Normal\n",
        "                class_name = 'CN'\n",
        "            elif 'decline' in path_str:\n",
        "                label = 1  # Decline/progression\n",
        "                class_name = 'AD'\n",
        "            elif 'no_decline' in path_str or 'no-decline' in path_str:\n",
        "                label = 0  # No decline\n",
        "                class_name = 'CN'\n",
        "            else:\n",
        "                # Default classification based on filename patterns\n",
        "                if any(marker in audio_file.name.lower() for marker in ['ad', 'dem', 'alz']):\n",
        "                    label = 1\n",
        "                    class_name = 'AD'\n",
        "                else:\n",
        "                    label = 0  # Default to control\n",
        "                    class_name = 'CN'\n",
        "\n",
        "            labels[participant_id] = {\n",
        "                'label': label,\n",
        "                'class_name': class_name,\n",
        "                'audio_path': audio_file\n",
        "            }\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',\n",
        "            r'adrsp?(\\d{3})',\n",
        "            r'adrspt?(\\d{1,3})',\n",
        "            r'(\\d{3})',\n",
        "            r'([A-Z]\\d{2,3})',\n",
        "            r'(S\\d{3})',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "    def _create_paired_data_with_asr(self, dataset_info):\n",
        "        \"\"\"Create paired audio-transcript dataset using ASR-generated transcripts\"\"\"\n",
        "        paired_data = []\n",
        "\n",
        "        # Create paired dataset using generated transcripts\n",
        "        for participant_id, label_info in dataset_info['labels'].items():\n",
        "            # Get generated transcript\n",
        "            transcript = dataset_info['generated_transcripts'].get(participant_id, \"\")\n",
        "\n",
        "            # Clean and validate transcript\n",
        "            transcript = self._clean_and_validate_transcript(transcript)\n",
        "\n",
        "            # If transcript is still empty or invalid, create a meaningful placeholder\n",
        "            if not transcript or len(transcript.strip()) < 10:\n",
        "                transcript = f\"Audio sample from participant {participant_id}. Speech content unclear or silent.\"\n",
        "\n",
        "            paired_data.append({\n",
        "                'participant_id': participant_id,\n",
        "                'audio_path': str(label_info['audio_path']),\n",
        "                'transcript': transcript,\n",
        "                'label': label_info['label'],\n",
        "                'class_name': label_info['class_name'],\n",
        "                'transcript_source': 'ASR_generated'\n",
        "            })\n",
        "\n",
        "        print(f\"Created {len(paired_data)} paired samples with ASR transcripts\")\n",
        "\n",
        "        # Print class distribution\n",
        "        labels = [item['label'] for item in paired_data]\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        for cls, count in zip(unique, counts):\n",
        "            class_name = 'CN' if cls == 0 else 'AD'\n",
        "            print(f\"  {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "        # Print sample transcripts for verification\n",
        "        print(\"\\nSample generated transcripts:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, sample in enumerate(paired_data[:3]):\n",
        "            print(f\"Participant {sample['participant_id']} ({sample['class_name']}):\")\n",
        "            print(f\"Transcript: {sample['transcript'][:100]}...\")\n",
        "            print()\n",
        "\n",
        "        return paired_data\n",
        "\n",
        "    def _clean_and_validate_transcript(self, transcript):\n",
        "        \"\"\"Clean and validate ASR-generated transcript\"\"\"\n",
        "        if not transcript:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove common ASR artifacts\n",
        "        transcript = transcript.strip()\n",
        "        transcript = re.sub(r'\\[.*?\\]', '', transcript)  # Remove [NOISE], [MUSIC], etc.\n",
        "        transcript = re.sub(r'<.*?>', '', transcript)    # Remove <unk>, <pad>, etc.\n",
        "        transcript = re.sub(r'\\s+', ' ', transcript)     # Normalize whitespace\n",
        "\n",
        "        # Remove very short or repetitive transcripts\n",
        "        if len(transcript) < 5:\n",
        "            return \"\"\n",
        "\n",
        "        # Check for repetitive patterns (common ASR error)\n",
        "        words = transcript.split()\n",
        "        if len(words) > 1:\n",
        "            # If more than 70% of words are the same, likely an error\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) / len(words) < 0.3:\n",
        "                return \"\"\n",
        "\n",
        "        return transcript"
      ],
      "metadata": {
        "id": "FiZDbi9075S7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedMultiModalDataset(Dataset):\n",
        "    \"\"\"Enhanced dataset with ASR-generated transcripts and linguistic features\"\"\"\n",
        "\n",
        "    def __init__(self, data_samples, audio_processor, tokenizer,\n",
        "                 max_text_length=512, audio_max_length=16*16000,\n",
        "                 image_size=(224, 224)):\n",
        "        self.data_samples = data_samples\n",
        "        self.audio_processor = audio_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.audio_max_length = audio_max_length\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Precompute linguistic features for efficiency\n",
        "        self._precompute_linguistic_features()\n",
        "\n",
        "    def _precompute_linguistic_features(self):\n",
        "        \"\"\"Precompute linguistic features that might be important for AD detection\"\"\"\n",
        "        print(\"Precomputing linguistic features...\")\n",
        "\n",
        "        for sample in tqdm(self.data_samples, desc=\"Computing linguistic features\"):\n",
        "            transcript = sample['transcript']\n",
        "\n",
        "            # Basic linguistic metrics\n",
        "            words = transcript.split()\n",
        "            sentences = re.split(r'[.!?]+', transcript)\n",
        "\n",
        "            linguistic_features = {\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'pause_markers': transcript.count('[pause]') + transcript.count('...'),\n",
        "                'filler_words': sum(1 for word in words if word.lower() in ['um', 'uh', 'er', 'ah']),\n",
        "                'transcript_length': len(transcript)\n",
        "            }\n",
        "\n",
        "            sample['linguistic_features'] = linguistic_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        # Process text\n",
        "        text = sample['transcript']\n",
        "        if not text or text.strip() == \"\":\n",
        "            text = \"No speech content detected in audio sample\"\n",
        "\n",
        "        try:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_text_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error tokenizing text for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy encoding\n",
        "            encoding = {\n",
        "                'input_ids': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'attention_mask': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'token_type_ids': torch.zeros(self.max_text_length, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        # Process audio\n",
        "        try:\n",
        "            audio = self.audio_processor.load_audio(\n",
        "                sample['audio_path'],\n",
        "                max_length=self.audio_max_length\n",
        "            )\n",
        "\n",
        "            # Extract spectrogram features\n",
        "            spectrogram = self.audio_processor.extract_mel_spectrogram(audio)\n",
        "\n",
        "            # Resize for ViT input\n",
        "            audio_features = self.audio_processor.resize_spectrogram_to_image(\n",
        "                spectrogram, self.image_size\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy audio features\n",
        "            audio_features = np.random.rand(3, self.image_size[0], self.image_size[1])\n",
        "\n",
        "        # Linguistic features\n",
        "        ling_features = sample.get('linguistic_features', {})\n",
        "        linguistic_vector = np.array([\n",
        "            ling_features.get('word_count', 0),\n",
        "            ling_features.get('sentence_count', 0),\n",
        "            ling_features.get('avg_word_length', 0),\n",
        "            ling_features.get('unique_words', 0),\n",
        "            ling_features.get('lexical_diversity', 0),\n",
        "            ling_features.get('pause_markers', 0),\n",
        "            ling_features.get('filler_words', 0),\n",
        "            ling_features.get('transcript_length', 0)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze() if hasattr(encoding['input_ids'], 'squeeze') else encoding['input_ids'],\n",
        "            'attention_mask': encoding['attention_mask'].squeeze() if hasattr(encoding['attention_mask'], 'squeeze') else encoding['attention_mask'],\n",
        "            'audio_features': torch.FloatTensor(audio_features),\n",
        "            'linguistic_features': torch.FloatTensor(linguistic_vector),\n",
        "            'label': torch.LongTensor([sample['label']]).squeeze(),\n",
        "            'participant_id': sample['participant_id'],\n",
        "            'class_name': sample['class_name'],\n",
        "            'transcript_preview': text[:100] + \"...\" if len(text) > 100 else text\n",
        "        }"
      ],
      "metadata": {
        "id": "wesce6R-8AYY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAudioProcessor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=128):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "    def load_audio(self, audio_path, max_length=None):\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            if audio.ndim > 1:\n",
        "                audio = np.mean(audio, axis=1)\n",
        "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
        "            if np.max(np.abs(audio)) > 0:\n",
        "                audio = librosa.util.normalize(audio)\n",
        "\n",
        "            if max_length is not None:\n",
        "                if len(audio) > max_length:\n",
        "                    start = (len(audio) - max_length) // 2\n",
        "                    audio = audio[start:start + max_length]\n",
        "                elif len(audio) < max_length:\n",
        "                    pad_length = max_length - len(audio)\n",
        "                    audio = np.pad(audio, (0, pad_length), mode='constant')\n",
        "\n",
        "            return audio\n",
        "        except:\n",
        "            length = max_length if max_length else self.sample_rate * 10\n",
        "            return np.random.randn(length) * 0.01\n",
        "\n",
        "    def extract_mel_spectrogram(self, audio):\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
        "            )\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            delta = librosa.feature.delta(log_mel_spec)\n",
        "            delta2 = librosa.feature.delta(log_mel_spec, order=2)\n",
        "            return np.stack([log_mel_spec, delta, delta2], axis=0)\n",
        "        except:\n",
        "            return np.random.randn(3, self.n_mels, 100)\n",
        "\n",
        "    def resize_spectrogram_to_image(self, spectrogram, target_size=(224, 224)):\n",
        "        try:\n",
        "            if spectrogram.ndim == 3:\n",
        "                resized_channels = []\n",
        "                for i in range(spectrogram.shape[0]):\n",
        "                    channel = spectrogram[i]\n",
        "                    zoom_factors = [target_size[j] / channel.shape[j] for j in range(2)]\n",
        "                    resized_channel = ndimage.zoom(channel, zoom_factors, order=1)\n",
        "                    resized_channels.append(resized_channel)\n",
        "                resized = np.stack(resized_channels, axis=0)\n",
        "            else:\n",
        "                zoom_factors = [target_size[i] / spectrogram.shape[i] for i in range(2)]\n",
        "                resized = ndimage.zoom(spectrogram, zoom_factors, order=1)\n",
        "                resized = np.stack([resized] * 3, axis=0)\n",
        "\n",
        "            resized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)\n",
        "            return resized\n",
        "        except:\n",
        "            return np.random.rand(3, target_size[0], target_size[1])"
      ],
      "metadata": {
        "id": "92K2o9OM8GP7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedMultiModalADClassifier(nn.Module):\n",
        "    \"\"\"Enhanced multimodal classifier for Alzheimer's detection\"\"\"\n",
        "\n",
        "    def __init__(self, text_hidden_size=768, audio_hidden_size=768,\n",
        "                 linguistic_feature_size=8, fusion_hidden_size=512,\n",
        "                 num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text encoder (BERT)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Audio encoder (ViT for spectrograms)\n",
        "        self.audio_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        # Linguistic features processor\n",
        "        self.linguistic_processor = nn.Sequential(\n",
        "            nn.Linear(linguistic_feature_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Attention mechanism for modality fusion\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=fusion_hidden_size,\n",
        "            num_heads=8,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Feature projections to common dimensionality\n",
        "        self.text_projection = nn.Linear(text_hidden_size, fusion_hidden_size)\n",
        "        self.audio_projection = nn.Linear(audio_hidden_size, fusion_hidden_size)\n",
        "        self.linguistic_projection = nn.Linear(32, fusion_hidden_size)\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion_layers = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size * 3, fusion_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_hidden_size, fusion_hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size // 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights\"\"\"\n",
        "        for module in [self.linguistic_processor, self.fusion_layers, self.classifier]:\n",
        "            for layer in module:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features, linguistic_features):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Text encoding\n",
        "        text_output = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        text_features = text_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Audio encoding\n",
        "        audio_output = self.audio_encoder(pixel_values=audio_features)\n",
        "        audio_features = audio_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Linguistic features processing\n",
        "        linguistic_processed = self.linguistic_processor(linguistic_features)  # [batch_size, 32]\n",
        "\n",
        "        # Project to common dimensionality\n",
        "        text_projected = self.text_projection(text_features)        # [batch_size, 512]\n",
        "        audio_projected = self.audio_projection(audio_features)     # [batch_size, 512]\n",
        "        linguistic_projected = self.linguistic_projection(linguistic_processed)  # [batch_size, 512]\n",
        "\n",
        "        # Prepare for attention mechanism\n",
        "        # Convert to [seq_len, batch_size, embed_dim] for attention\n",
        "        modality_features = torch.stack([\n",
        "            text_projected,\n",
        "            audio_projected,\n",
        "            linguistic_projected\n",
        "        ], dim=0)  # [3, batch_size, 512]\n",
        "\n",
        "        # Apply self-attention across modalities\n",
        "        attended_features, attention_weights = self.attention(\n",
        "            modality_features, modality_features, modality_features\n",
        "        )  # [3, batch_size, 512]\n",
        "\n",
        "        # Convert back to [batch_size, features]\n",
        "        attended_features = attended_features.transpose(0, 1)  # [batch_size, 3, 512]\n",
        "\n",
        "        # Flatten for fusion\n",
        "        fused_input = attended_features.reshape(batch_size, -1)  # [batch_size, 1536]\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = self.fusion_layers(fused_input)  # [batch_size, 256]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)  # [batch_size, 2]\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'text_features': text_features,\n",
        "            'audio_features': audio_features,\n",
        "            'linguistic_features': linguistic_processed,\n",
        "            'attention_weights': attention_weights,\n",
        "            'fused_features': fused_features\n",
        "        }"
      ],
      "metadata": {
        "id": "jg5KPCBm8Lh0"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}