{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May243.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: AUTOMATIC SPEECH RECOGNITION MODULE\n",
        "# ============================================================================\n",
        "\n",
        "class SpeechTranscriber:\n",
        "    \"\"\"Automatic Speech Recognition for generating transcripts from audio\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"openai/whisper-base\", cache_dir=\"./asr_cache\"):\n",
        "        \"\"\"\n",
        "        Initialize ASR model\n",
        "        Options:\n",
        "        - openai/whisper-base: Good balance of speed/accuracy\n",
        "        - facebook/wav2vec2-base-960h: Faster but less accurate\n",
        "        - openai/whisper-small: More accurate but slower\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Loading ASR model: {model_name}\")\n",
        "\n",
        "        if \"whisper\" in model_name.lower():\n",
        "            self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "            self.asr_type = \"whisper\"\n",
        "        else:\n",
        "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            self.asr_type = \"wav2vec2\"\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"ASR model loaded on {self.device}\")\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path, use_cache=True):\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        # Check cache first\n",
        "        cache_file = self.cache_dir / f\"{audio_path.stem}_transcript.pkl\"\n",
        "        if use_cache and cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cached_result = pickle.load(f)\n",
        "                return cached_result['transcript']\n",
        "            except:\n",
        "                pass  # Cache corrupted, proceed with transcription\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(str(audio_path), sr=16000)  # Whisper expects 16kHz\n",
        "\n",
        "            # Handle empty or very short audio\n",
        "            if len(audio) < 1600:  # Less than 0.1 seconds\n",
        "                transcript = \"\"\n",
        "            else:\n",
        "                transcript = self._transcribe_audio_array(audio)\n",
        "\n",
        "            # Cache result\n",
        "            if use_cache:\n",
        "                try:\n",
        "                    with open(cache_file, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'audio_path': str(audio_path),\n",
        "                            'transcript': transcript,\n",
        "                            'model': self.model_name\n",
        "                        }, f)\n",
        "                except:\n",
        "                    pass  # Caching failed, but transcription succeeded\n",
        "\n",
        "            return transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {audio_path}: {e}\")\n",
        "            return f\"[Transcription failed for {audio_path.name}]\"\n",
        "\n",
        "    def _transcribe_audio_array(self, audio_array):\n",
        "        \"\"\"Transcribe audio array using the loaded model\"\"\"\n",
        "        try:\n",
        "            if self.asr_type == \"whisper\":\n",
        "                return self._whisper_transcribe(audio_array)\n",
        "            else:\n",
        "                return self._wav2vec2_transcribe(audio_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {e}\")\n",
        "            return \"[Transcription error]\"\n",
        "\n",
        "    def _whisper_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Whisper model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_features.to(self.device)\n",
        "\n",
        "        # Generate transcription\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(inputs, max_length=448)\n",
        "\n",
        "        # Decode\n",
        "        transcript = self.processor.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcript.strip()\n",
        "\n",
        "    def _wav2vec2_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Wav2Vec2 model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).input_values.to(self.device)\n",
        "\n",
        "        # Get logits\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs).logits\n",
        "\n",
        "        # Decode\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcript = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "        return transcript.strip().lower()\n",
        "\n",
        "    def batch_transcribe(self, audio_paths, batch_size=8):\n",
        "        \"\"\"Transcribe multiple audio files with progress bar\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(f\"Transcribing {len(audio_paths)} audio files...\")\n",
        "\n",
        "        for audio_path in tqdm(audio_paths, desc=\"Transcribing\"):\n",
        "            transcript = self.transcribe_audio_file(audio_path)\n",
        "            participant_id = self._extract_participant_id(Path(audio_path).name)\n",
        "            transcripts[participant_id] = transcript\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',         # adrs0123 or adrso123\n",
        "            r'adrsp?(\\d{3})',         # adrsp123\n",
        "            r'adrspt?(\\d{1,3})',      # adrspt1, adrspt12\n",
        "            r'(\\d{3})',               # 3-digit numbers\n",
        "            r'([A-Z]\\d{2,3})',        # Letter followed by 2-3 digits\n",
        "            r'(S\\d{3})',              # S followed by 3 digits\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: ENHANCED DATA PROCESSOR WITH ASR\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedADReSSDataProcessor:\n",
        "    \"\"\"Enhanced ADReSS data processor with automatic speech recognition\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./extracted_data', asr_model=\"openai/whisper-base\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize ASR\n",
        "        self.transcriber = SpeechTranscriber(model_name=asr_model)\n",
        "\n",
        "    def extract_adress_dataset(self, tar_path, dataset_name):\n",
        "        \"\"\"Extract ADReSS dataset and organize files properly\"\"\"\n",
        "        extract_path = self.output_dir / dataset_name\n",
        "        extract_path.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting {tar_path} to {extract_path}\")\n",
        "\n",
        "        try:\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "            print(f\"Successfully extracted {dataset_name}\")\n",
        "\n",
        "            # Find the actual dataset directory structure\n",
        "            self._explore_directory_structure(extract_path)\n",
        "            return extract_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {tar_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _explore_directory_structure(self, base_path):\n",
        "        \"\"\"Explore and print directory structure\"\"\"\n",
        "        print(f\"\\nDirectory structure for {base_path.name}:\")\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            level = root.replace(str(base_path), '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:5]:\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "    def process_adress_dataset_with_asr(self, extract_path):\n",
        "        \"\"\"Process ADReSS dataset with automatic speech recognition\"\"\"\n",
        "        dataset_info = {\n",
        "            'audio_files': [],\n",
        "            'transcript_files': [],\n",
        "            'metadata_files': [],\n",
        "            'labels': {},\n",
        "            'paired_data': [],\n",
        "            'generated_transcripts': {}\n",
        "        }\n",
        "\n",
        "        # Look for ADReSS structure\n",
        "        adress_dirs = list(extract_path.rglob(\"*ADReSS*\"))\n",
        "        if adress_dirs:\n",
        "            main_dir = adress_dirs[0]\n",
        "        else:\n",
        "            main_dir = extract_path\n",
        "\n",
        "        print(f\"Processing from directory: {main_dir}\")\n",
        "\n",
        "        # Find audio files\n",
        "        audio_patterns = ['**/*.wav', '**/*.mp3', '**/*.flac']\n",
        "        for pattern in audio_patterns:\n",
        "            dataset_info['audio_files'].extend(list(main_dir.glob(pattern)))\n",
        "\n",
        "        print(f\"Found {len(dataset_info['audio_files'])} audio files\")\n",
        "\n",
        "        # Generate transcripts using ASR\n",
        "        print(\"Generating transcripts using ASR...\")\n",
        "        audio_paths = [str(path) for path in dataset_info['audio_files']]\n",
        "        generated_transcripts = self.transcriber.batch_transcribe(audio_paths)\n",
        "        dataset_info['generated_transcripts'] = generated_transcripts\n",
        "\n",
        "        print(f\"Generated {len(generated_transcripts)} transcripts\")\n",
        "\n",
        "        # Process labels from directory structure\n",
        "        labels = self._extract_labels_from_structure(dataset_info['audio_files'])\n",
        "        dataset_info['labels'] = labels\n",
        "\n",
        "        # Create paired dataset with generated transcripts\n",
        "        paired_data = self._create_paired_data_with_asr(dataset_info)\n",
        "        dataset_info['paired_data'] = paired_data\n",
        "\n",
        "        return dataset_info\n",
        "\n",
        "    def _extract_labels_from_structure(self, audio_files):\n",
        "        \"\"\"Extract labels from file paths or directory structure\"\"\"\n",
        "        labels = {}\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            # Extract participant ID\n",
        "            participant_id = self._extract_participant_id(audio_file.name)\n",
        "\n",
        "            # Determine label from path\n",
        "            path_str = str(audio_file).lower()\n",
        "            if '/ad/' in path_str or 'dementia' in path_str or 'alzheimer' in path_str:\n",
        "                label = 1  # AD/Dementia\n",
        "                class_name = 'AD'\n",
        "            elif '/cn/' in path_str or 'control' in path_str or 'normal' in path_str:\n",
        "                label = 0  # Control/Normal\n",
        "                class_name = 'CN'\n",
        "            elif 'decline' in path_str:\n",
        "                label = 1  # Decline/progression\n",
        "                class_name = 'AD'\n",
        "            elif 'no_decline' in path_str or 'no-decline' in path_str:\n",
        "                label = 0  # No decline\n",
        "                class_name = 'CN'\n",
        "            else:\n",
        "                # Default classification based on filename patterns\n",
        "                if any(marker in audio_file.name.lower() for marker in ['ad', 'dem', 'alz']):\n",
        "                    label = 1\n",
        "                    class_name = 'AD'\n",
        "                else:\n",
        "                    label = 0  # Default to control\n",
        "                    class_name = 'CN'\n",
        "\n",
        "            labels[participant_id] = {\n",
        "                'label': label,\n",
        "                'class_name': class_name,\n",
        "                'audio_path': audio_file\n",
        "            }\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',\n",
        "            r'adrsp?(\\d{3})',\n",
        "            r'adrspt?(\\d{1,3})',\n",
        "            r'(\\d{3})',\n",
        "            r'([A-Z]\\d{2,3})',\n",
        "            r'(S\\d{3})',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "    def _create_paired_data_with_asr(self, dataset_info):\n",
        "        \"\"\"Create paired audio-transcript dataset using ASR-generated transcripts\"\"\"\n",
        "        paired_data = []\n",
        "\n",
        "        # Create paired dataset using generated transcripts\n",
        "        for participant_id, label_info in dataset_info['labels'].items():\n",
        "            # Get generated transcript\n",
        "            transcript = dataset_info['generated_transcripts'].get(participant_id, \"\")\n",
        "\n",
        "            # Clean and validate transcript\n",
        "            transcript = self._clean_and_validate_transcript(transcript)\n",
        "\n",
        "            # If transcript is still empty or invalid, create a meaningful placeholder\n",
        "            if not transcript or len(transcript.strip()) < 10:\n",
        "                transcript = f\"Audio sample from participant {participant_id}. Speech content unclear or silent.\"\n",
        "\n",
        "            paired_data.append({\n",
        "                'participant_id': participant_id,\n",
        "                'audio_path': str(label_info['audio_path']),\n",
        "                'transcript': transcript,\n",
        "                'label': label_info['label'],\n",
        "                'class_name': label_info['class_name'],\n",
        "                'transcript_source': 'ASR_generated'\n",
        "            })\n",
        "\n",
        "        print(f\"Created {len(paired_data)} paired samples with ASR transcripts\")\n",
        "\n",
        "        # Print class distribution\n",
        "        labels = [item['label'] for item in paired_data]\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        for cls, count in zip(unique, counts):\n",
        "            class_name = 'CN' if cls == 0 else 'AD'\n",
        "            print(f\"  {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "        # Print sample transcripts for verification\n",
        "        print(\"\\nSample generated transcripts:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, sample in enumerate(paired_data[:3]):\n",
        "            print(f\"Participant {sample['participant_id']} ({sample['class_name']}):\")\n",
        "            print(f\"Transcript: {sample['transcript'][:100]}...\")\n",
        "            print()\n",
        "\n",
        "        return paired_data\n",
        "\n",
        "    def _clean_and_validate_transcript(self, transcript):\n",
        "        \"\"\"Clean and validate ASR-generated transcript\"\"\"\n",
        "        if not transcript:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove common ASR artifacts\n",
        "        transcript = transcript.strip()\n",
        "        transcript = re.sub(r'\\[.*?\\]', '', transcript)  # Remove [NOISE], [MUSIC], etc.\n",
        "        transcript = re.sub(r'<.*?>', '', transcript)    # Remove <unk>, <pad>, etc.\n",
        "        transcript = re.sub(r'\\s+', ' ', transcript)     # Normalize whitespace\n",
        "\n",
        "        # Remove very short or repetitive transcripts\n",
        "        if len(transcript) < 5:\n",
        "            return \"\"\n",
        "\n",
        "        # Check for repetitive patterns (common ASR error)\n",
        "        words = transcript.split()\n",
        "        if len(words) > 1:\n",
        "            # If more than 70% of words are the same, likely an error\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) / len(words) < 0.3:\n",
        "                return \"\"\n",
        "\n",
        "        return transcript\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: ENHANCED MULTIMODAL DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedMultiModalDataset(Dataset):\n",
        "    \"\"\"Enhanced dataset with ASR-generated transcripts and linguistic features\"\"\"\n",
        "\n",
        "    def __init__(self, data_samples, audio_processor, tokenizer,\n",
        "                 max_text_length=512, audio_max_length=16*16000,\n",
        "                 image_size=(224, 224)):\n",
        "        self.data_samples = data_samples\n",
        "        self.audio_processor = audio_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.audio_max_length = audio_max_length\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Precompute linguistic features for efficiency\n",
        "        self._precompute_linguistic_features()\n",
        "\n",
        "    def _precompute_linguistic_features(self):\n",
        "        \"\"\"Precompute linguistic features that might be important for AD detection\"\"\"\n",
        "        print(\"Precomputing linguistic features...\")\n",
        "\n",
        "        for sample in tqdm(self.data_samples, desc=\"Computing linguistic features\"):\n",
        "            transcript = sample['transcript']\n",
        "\n",
        "            # Basic linguistic metrics\n",
        "            words = transcript.split()\n",
        "            sentences = re.split(r'[.!?]+', transcript)\n",
        "\n",
        "            linguistic_features = {\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'pause_markers': transcript.count('[pause]') + transcript.count('...'),\n",
        "                'filler_words': sum(1 for word in words if word.lower() in ['um', 'uh', 'er', 'ah']),\n",
        "                'transcript_length': len(transcript)\n",
        "            }\n",
        "\n",
        "            sample['linguistic_features'] = linguistic_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        # Process text\n",
        "        text = sample['transcript']\n",
        "        if not text or text.strip() == \"\":\n",
        "            text = \"No speech content detected in audio sample\"\n",
        "\n",
        "        try:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_text_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error tokenizing text for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy encoding\n",
        "            encoding = {\n",
        "                'input_ids': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'attention_mask': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'token_type_ids': torch.zeros(self.max_text_length, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        # Process audio\n",
        "        try:\n",
        "            audio = self.audio_processor.load_audio(\n",
        "                sample['audio_path'],\n",
        "                max_length=self.audio_max_length\n",
        "            )\n",
        "\n",
        "            # Extract spectrogram features\n",
        "            spectrogram = self.audio_processor.extract_mel_spectrogram(audio)\n",
        "\n",
        "            # Resize for ViT input\n",
        "            audio_features = self.audio_processor.resize_spectrogram_to_image(\n",
        "                spectrogram, self.image_size\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy audio features\n",
        "            audio_features = np.random.rand(3, self.image_size[0], self.image_size[1])\n",
        "\n",
        "        # Linguistic features\n",
        "        ling_features = sample.get('linguistic_features', {})\n",
        "        linguistic_vector = np.array([\n",
        "            ling_features.get('word_count', 0),\n",
        "            ling_features.get('sentence_count', 0),\n",
        "            ling_features.get('avg_word_length', 0),\n",
        "            ling_features.get('unique_words', 0),\n",
        "            ling_features.get('lexical_diversity', 0),\n",
        "            ling_features.get('pause_markers', 0),\n",
        "            ling_features.get('filler_words', 0),\n",
        "            ling_features.get('transcript_length', 0)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze() if hasattr(encoding['input_ids'], 'squeeze') else encoding['input_ids'],\n",
        "            'attention_mask': encoding['attention_mask'].squeeze() if hasattr(encoding['attention_mask'], 'squeeze') else encoding['attention_mask'],\n",
        "            'audio_features': torch.FloatTensor(audio_features),\n",
        "            'linguistic_features': torch.FloatTensor(linguistic_vector),\n",
        "            'label': torch.LongTensor([sample['label']]).squeeze(),\n",
        "            'participant_id': sample['participant_id'],\n",
        "            'class_name': sample['class_name'],\n",
        "            'transcript_preview': text[:100] + \"...\" if len(text) > 100 else text\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: AUDIO PROCESSOR FOR MULTIMODAL FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleAudioProcessor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=128):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "    def load_audio(self, audio_path, max_length=None):\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            if audio.ndim > 1:\n",
        "                audio = np.mean(audio, axis=1)\n",
        "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
        "            if np.max(np.abs(audio)) > 0:\n",
        "                audio = librosa.util.normalize(audio)\n",
        "\n",
        "            if max_length is not None:\n",
        "                if len(audio) > max_length:\n",
        "                    start = (len(audio) - max_length) // 2\n",
        "                    audio = audio[start:start + max_length]\n",
        "                elif len(audio) < max_length:\n",
        "                    pad_length = max_length - len(audio)\n",
        "                    audio = np.pad(audio, (0, pad_length), mode='constant')\n",
        "\n",
        "            return audio\n",
        "        except:\n",
        "            length = max_length if max_length else self.sample_rate * 10\n",
        "            return np.random.randn(length) * 0.01\n",
        "\n",
        "    def extract_mel_spectrogram(self, audio):\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
        "            )\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            delta = librosa.feature.delta(log_mel_spec)\n",
        "            delta2 = librosa.feature.delta(log_mel_spec, order=2)\n",
        "            return np.stack([log_mel_spec, delta, delta2], axis=0)\n",
        "        except:\n",
        "            return np.random.randn(3, self.n_mels, 100)\n",
        "\n",
        "    def resize_spectrogram_to_image(self, spectrogram, target_size=(224, 224)):\n",
        "        try:\n",
        "            if spectrogram.ndim == 3:\n",
        "                resized_channels = []\n",
        "                for i in range(spectrogram.shape[0]):\n",
        "                    channel = spectrogram[i]\n",
        "                    zoom_factors = [target_size[j] / channel.shape[j] for j in range(2)]\n",
        "                    resized_channel = ndimage.zoom(channel, zoom_factors, order=1)\n",
        "                    resized_channels.append(resized_channel)\n",
        "                resized = np.stack(resized_channels, axis=0)\n",
        "            else:\n",
        "                zoom_factors = [target_size[i] / spectrogram.shape[i] for i in range(2)]\n",
        "                resized = ndimage.zoom(spectrogram, zoom_factors, order=1)\n",
        "                resized = np.stack([resized] * 3, axis=0)\n",
        "\n",
        "            resized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)\n",
        "            return resized\n",
        "        except:\n",
        "            return np.random.rand(3, target_size[0], target_size[1])\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: ENHANCED MULTIMODAL MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedMultiModalADClassifier(nn.Module):\n",
        "    \"\"\"Enhanced multimodal classifier for Alzheimer's detection\"\"\"\n",
        "\n",
        "    def __init__(self, text_hidden_size=768, audio_hidden_size=768,\n",
        "                 linguistic_feature_size=8, fusion_hidden_size=512,\n",
        "                 num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text encoder (BERT)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Audio encoder (ViT for spectrograms)\n",
        "        self.audio_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        # Linguistic features processor\n",
        "        self.linguistic_processor = nn.Sequential(\n",
        "            nn.Linear(linguistic_feature_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Attention mechanism for modality fusion\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=fusion_hidden_size,\n",
        "            num_heads=8,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Feature projections to common dimensionality\n",
        "        self.text_projection = nn.Linear(text_hidden_size, fusion_hidden_size)\n",
        "        self.audio_projection = nn.Linear(audio_hidden_size, fusion_hidden_size)\n",
        "        self.linguistic_projection = nn.Linear(32, fusion_hidden_size)\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion_layers = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size * 3, fusion_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_hidden_size, fusion_hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size // 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights\"\"\"\n",
        "        for module in [self.linguistic_processor, self.fusion_layers, self.classifier]:\n",
        "            for layer in module:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features, linguistic_features):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Text encoding\n",
        "        text_output = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        text_features = text_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Audio encoding\n",
        "        audio_output = self.audio_encoder(pixel_values=audio_features)\n",
        "        audio_features = audio_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Linguistic features processing\n",
        "        linguistic_processed = self.linguistic_processor(linguistic_features)  # [batch_size, 32]\n",
        "\n",
        "        # Project to common dimensionality\n",
        "        text_projected = self.text_projection(text_features)        # [batch_size, 512]\n",
        "        audio_projected = self.audio_projection(audio_features)     # [batch_size, 512]\n",
        "        linguistic_projected = self.linguistic_projection(linguistic_processed)  # [batch_size, 512]\n",
        "\n",
        "        # Prepare for attention mechanism\n",
        "        # Convert to [seq_len, batch_size, embed_dim] for attention\n",
        "        modality_features = torch.stack([\n",
        "            text_projected,\n",
        "            audio_projected,\n",
        "            linguistic_projected\n",
        "        ], dim=0)  # [3, batch_size, 512]\n",
        "\n",
        "        # Apply self-attention across modalities\n",
        "        attended_features, attention_weights = self.attention(\n",
        "            modality_features, modality_features, modality_features\n",
        "        )  # [3, batch_size, 512]\n",
        "\n",
        "        # Convert back to [batch_size, features]\n",
        "        attended_features = attended_features.transpose(0, 1)  # [batch_size, 3, 512]\n",
        "\n",
        "        # Flatten for fusion\n",
        "        fused_input = attended_features.reshape(batch_size, -1)  # [batch_size, 1536]\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = self.fusion_layers(fused_input)  # [batch_size, 256]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)  # [batch_size, 2]\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'text_features': text_features,\n",
        "            'audio_features': audio_features,\n",
        "            'linguistic_features': linguistic_processed,\n",
        "            'attention_weights': attention_weights,\n",
        "            'fused_features': fused_features\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: TRAINING AND EVALUATION (CONTINUED)\n",
        "# ============================================================================\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Enhanced model trainer with comprehensive evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, learning_rate=2e-5, weight_decay=0.01):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='max', patience=3, factor=0.5, verbose=True\n",
        "        )\n",
        "\n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.best_val_accuracy = 0.0\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "            labels = batch['label'].to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    audio_features=audio_features,\n",
        "                    linguistic_features=linguistic_features\n",
        "                )\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct_predictions += (predictions == labels).sum().item()\n",
        "                total_predictions += labels.size(0)\n",
        "\n",
        "                # Update progress bar\n",
        "                current_accuracy = correct_predictions / total_predictions\n",
        "                progress_bar.set_postfix({\n",
        "                    'Loss': f'{loss.item():.4f}',\n",
        "                    'Acc': f'{current_accuracy:.4f}'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate_epoch(self, val_loader):\n",
        "        \"\"\"Validate for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_participant_ids = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                try:\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        audio_features=audio_features,\n",
        "                        linguistic_features=linguistic_features\n",
        "                    )\n",
        "\n",
        "                    logits = outputs['logits']\n",
        "                    loss = self.criterion(logits, labels)\n",
        "\n",
        "                    # Statistics\n",
        "                    total_loss += loss.item()\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "                    correct_predictions += (predictions == labels).sum().item()\n",
        "                    total_predictions += labels.size(0)\n",
        "\n",
        "                    # Store for detailed analysis\n",
        "                    all_predictions.extend(predictions.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_participant_ids.extend(batch['participant_id'])\n",
        "\n",
        "                    # Update progress bar\n",
        "                    current_accuracy = correct_predictions / total_predictions\n",
        "                    progress_bar.set_postfix({\n",
        "                        'Loss': f'{loss.item():.4f}',\n",
        "                        'Acc': f'{current_accuracy:.4f}'\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        return avg_loss, accuracy, all_predictions, all_labels, all_participant_ids\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=10, save_path='best_model.pt'):\n",
        "        \"\"\"Full training loop with validation\"\"\"\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "        print(f\"Training on {len(train_loader.dataset)} samples\")\n",
        "        print(f\"Validating on {len(val_loader.dataset)} samples\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_accuracy)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_accuracy, val_predictions, val_labels, val_ids = self.validate_epoch(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_accuracy)\n",
        "\n",
        "            # Save best model\n",
        "            if val_accuracy > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_model_state = self.model.state_dict().copy()\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.best_model_state,\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'best_val_accuracy': self.best_val_accuracy,\n",
        "                    'train_losses': self.train_losses,\n",
        "                    'val_losses': self.val_losses,\n",
        "                    'train_accuracies': self.train_accuracies,\n",
        "                    'val_accuracies': self.val_accuracies\n",
        "                }, save_path)\n",
        "                print(f\"✓ New best model saved with validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Detailed validation metrics for best epochs\n",
        "            if val_accuracy == self.best_val_accuracy:\n",
        "                self._print_detailed_metrics(val_labels, val_predictions)\n",
        "\n",
        "        print(f\"\\nTraining completed!\")\n",
        "        print(f\"Best validation accuracy: {self.best_val_accuracy:.4f}\")\n",
        "\n",
        "        # Load best model\n",
        "        if self.best_model_state is not None:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "            print(\"✓ Best model loaded for final evaluation\")\n",
        "\n",
        "    def _print_detailed_metrics(self, true_labels, predictions):\n",
        "        \"\"\"Print detailed classification metrics\"\"\"\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nDetailed Metrics:\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "    def evaluate_model(self, test_loader, class_names=['CN', 'AD']):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "        all_participant_ids = []\n",
        "        detailed_results = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                try:\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        audio_features=audio_features,\n",
        "                        linguistic_features=linguistic_features\n",
        "                    )\n",
        "\n",
        "                    logits = outputs['logits']\n",
        "                    probabilities = torch.softmax(logits, dim=1)\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "                    # Store results\n",
        "                    all_predictions.extend(predictions.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_probabilities.extend(probabilities.cpu().numpy())\n",
        "                    all_participant_ids.extend(batch['participant_id'])\n",
        "\n",
        "                    # Store detailed results for analysis\n",
        "                    for i in range(len(batch['participant_id'])):\n",
        "                        detailed_results.append({\n",
        "                            'participant_id': batch['participant_id'][i],\n",
        "                            'true_label': labels[i].cpu().item(),\n",
        "                            'predicted_label': predictions[i].cpu().item(),\n",
        "                            'cn_probability': probabilities[i][0].cpu().item(),\n",
        "                            'ad_probability': probabilities[i][1].cpu().item(),\n",
        "                            'correct': labels[i].cpu().item() == predictions[i].cpu().item(),\n",
        "                            'class_name': batch['class_name'][i],\n",
        "                            'transcript_preview': batch['transcript_preview'][i]\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in evaluation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        precision = precision_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "        recall = recall_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "        f1 = f1_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "\n",
        "        # Print comprehensive results\n",
        "        print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "        print(f\"Total Samples: {len(all_labels)}\")\n",
        "        print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
        "        print(f\"Overall Precision: {np.mean(precision):.4f}\")\n",
        "        print(f\"Overall Recall: {np.mean(recall):.4f}\")\n",
        "        print(f\"Overall F1-Score: {np.mean(f1):.4f}\")\n",
        "\n",
        "        print(f\"\\nPER-CLASS PERFORMANCE:\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_count = sum(1 for label in all_labels if label == i)\n",
        "            print(f\"{class_name}:\")\n",
        "            print(f\"  Count: {class_count}\")\n",
        "            print(f\"  Precision: {precision[i]:.4f}\")\n",
        "            print(f\"  Recall: {recall[i]:.4f}\")\n",
        "            print(f\"  F1-Score: {f1[i]:.4f}\")\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "        print(f\"\\nCONFUSION MATRIX:\")\n",
        "        print(f\"        Predicted\")\n",
        "        print(f\"        CN    AD\")\n",
        "        print(f\"Actual CN {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "        print(f\"       AD {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
        "\n",
        "        # Error Analysis\n",
        "        print(f\"\\nERROR ANALYSIS:\")\n",
        "        errors = [result for result in detailed_results if not result['correct']]\n",
        "        print(f\"Total Errors: {len(errors)}\")\n",
        "\n",
        "        if errors:\n",
        "            print(f\"\\nSample Errors:\")\n",
        "            for i, error in enumerate(errors[:5]):  # Show first 5 errors\n",
        "                true_class = class_names[error['true_label']]\n",
        "                pred_class = class_names[error['predicted_label']]\n",
        "                confidence = max(error['cn_probability'], error['ad_probability'])\n",
        "                print(f\"  {i+1}. Participant {error['participant_id']}: {true_class} → {pred_class} (conf: {confidence:.3f})\")\n",
        "                print(f\"     Transcript: {error['transcript_preview']}\")\n",
        "\n",
        "        # Save detailed results\n",
        "        results_df = pd.DataFrame(detailed_results)\n",
        "        results_df.to_csv('detailed_evaluation_results.csv', index=False)\n",
        "        print(f\"\\nDetailed results saved to 'detailed_evaluation_results.csv'\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'confusion_matrix': cm,\n",
        "            'detailed_results': detailed_results,\n",
        "            'predictions': all_predictions,\n",
        "            'labels': all_labels,\n",
        "            'probabilities': all_probabilities\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self, save_path='training_history.png'):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        if not self.train_losses:\n",
        "            print(\"No training history to plot\")\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot losses\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "        ax1.plot(epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot accuracies\n",
        "        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
        "        ax2.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.set_ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"Training history plot saved to {save_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: MAIN EXECUTION AND PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline for Enhanced Alzheimer's Detection\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ENHANCED ALZHEIMER'S DETECTION WITH AUTOMATIC SPEECH RECOGNITION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'data_dir': './data',\n",
        "        'output_dir': './extracted_data',\n",
        "        'model_save_path': './best_ad_model.pt',\n",
        "        'batch_size': 8,\n",
        "        'num_epochs': 15,\n",
        "        'learning_rate': 2e-5,\n",
        "        'max_text_length': 512,\n",
        "        'audio_max_length': 16*16000,  # 16 seconds\n",
        "        'test_size': 0.2,\n",
        "        'val_size': 0.15,\n",
        "        'random_state': 42,\n",
        "        'asr_model': 'openai/whisper-base'  # Can change to 'facebook/wav2vec2-base-960h'\n",
        "    }\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Initialize data processor with ASR\n",
        "        print(\"\\n1. Initializing Enhanced Data Processor with ASR...\")\n",
        "        processor = EnhancedADReSSDataProcessor(\n",
        "            output_dir=config['output_dir'],\n",
        "            asr_model=config['asr_model']\n",
        "        )\n",
        "\n",
        "        # Step 2: Look for dataset files\n",
        "        print(\"\\n2. Looking for ADReSS dataset files...\")\n",
        "        data_dir = Path(config['data_dir'])\n",
        "\n",
        "        # Look for compressed dataset files\n",
        "        dataset_files = []\n",
        "        for pattern in ['*.tar.gz', '*.tgz', '*.zip']:\n",
        "            dataset_files.extend(list(data_dir.glob(pattern)))\n",
        "\n",
        "        if not dataset_files:\n",
        "            print(\"⚠️  No dataset files found. Creating synthetic data for demonstration...\")\n",
        "            # Create synthetic dataset for demonstration\n",
        "            dataset_info = create_synthetic_dataset_with_asr(processor)\n",
        "        else:\n",
        "            print(f\"Found {len(dataset_files)} dataset files\")\n",
        "\n",
        "            # Process first dataset file\n",
        "            dataset_file = dataset_files[0]\n",
        "            print(f\"Processing: {dataset_file}\")\n",
        "\n",
        "            # Extract dataset\n",
        "            extract_path = processor.extract_adress_dataset(\n",
        "                dataset_file,\n",
        "                f\"dataset_{dataset_file.stem}\"\n",
        "            )\n",
        "\n",
        "            if extract_path is None:\n",
        "                print(\"❌ Failed to extract dataset. Creating synthetic data...\")\n",
        "                dataset_info = create_synthetic_dataset_with_asr(processor)\n",
        "            else:\n",
        "                # Process with ASR\n",
        "                dataset_info = processor.process_adress_dataset_with_asr(extract_path)\n",
        "\n",
        "        if not dataset_info['paired_data']:\n",
        "            print(\"❌ No valid data found. Exiting...\")\n",
        "            return\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(dataset_info['paired_data'])} samples\")\n",
        "\n",
        "        # Step 3: Prepare datasets\n",
        "        print(\"\\n3. Preparing datasets...\")\n",
        "\n",
        "        # Split data\n",
        "        train_data, temp_data = train_test_split(\n",
        "            dataset_info['paired_data'],\n",
        "            test_size=config['test_size'] + config['val_size'],\n",
        "            random_state=config['random_state'],\n",
        "            stratify=[item['label'] for item in dataset_info['paired_data']]\n",
        "        )\n",
        "\n",
        "        val_data, test_data = train_test_split(\n",
        "            temp_data,\n",
        "            test_size=config['test_size'] / (config['test_size'] + config['val_size']),\n",
        "            random_state=config['random_state'],\n",
        "            stratify=[item['label'] for item in temp_data]\n",
        "        )\n",
        "\n",
        "        print(f\"Train samples: {len(train_data)}\")\n",
        "        print(f\"Validation samples: {len(val_data)}\")\n",
        "        print(f\"Test samples: {len(test_data)}\")\n",
        "\n",
        "        # Initialize components\n",
        "        audio_processor = SimpleAudioProcessor()\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = EnhancedMultiModalDataset(\n",
        "            train_data, audio_processor, tokenizer,\n",
        "            max_text_length=config['max_text_length'],\n",
        "            audio_max_length=config['audio_max_length']\n",
        "        )\n",
        "\n",
        "        val_dataset = EnhancedMultiModalDataset(\n",
        "            val_data, audio_processor, tokenizer,\n",
        "            max_text_length=config['max_text_length'],\n",
        "            audio_max_length=config['audio_max_length']\n",
        "        )\n",
        "\n",
        "        test_dataset = EnhancedMultiModalDataset(\n",
        "            test_data, audio_processor, tokenizer,\n",
        "            max_text_length=config['max_text_length'],\n",
        "            audio_max_length=config['audio_max_length']\n",
        "        )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        # Step 4: Initialize model\n",
        "        print(\"\\n4. Initializing Enhanced Multimodal Model...\")\n",
        "        model = EnhancedMultiModalADClassifier()\n",
        "\n",
        "        # Step 5: Initialize trainer\n",
        "        print(\"\\n5. Initializing Model Trainer...\")\n",
        "        trainer = ModelTrainer(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            learning_rate=config['learning_rate']\n",
        "        )\n",
        "\n",
        "        # Step 6: Train model\n",
        "        print(\"\\n6. Starting Model Training...\")\n",
        "        trainer.train(\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=config['num_epochs'],\n",
        "            save_path=config['model_save_path']\n",
        "        )\n",
        "\n",
        "        # Step 7: Evaluate model\n",
        "        print(\"\\n7. Evaluating Model...\")\n",
        "        evaluation_results = trainer.evaluate_model(test_loader)\n",
        "\n",
        "        # Step 8: Plot training history\n",
        "        print(\"\\n8. Generating Training History Plot...\")\n",
        "        trainer.plot_training_history()\n",
        "\n",
        "        # Step 9: Feature Analysis\n",
        "        print(\"\\n9. Performing Feature Analysis...\")\n",
        "        analyze_model_features(trainer.model, test_loader, device)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✅ ENHANCED ALZHEIMER'S DETECTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Final Test Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
        "        print(f\"Model saved to: {config['model_save_path']}\")\n",
        "        print(\"Check the generated plots and CSV files for detailed analysis.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in main pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def create_synthetic_dataset_with_asr(processor):\n",
        "    \"\"\"Create synthetic dataset for demonstration purposes\"\"\"\n",
        "    print(\"Creating synthetic dataset with ASR for demonstration...\")\n",
        "\n",
        "    # Create synthetic data structure\n",
        "    synthetic_data = []\n",
        "\n",
        "    # Generate synthetic samples\n",
        "    for i in range(100):  # 100 synthetic samples\n",
        "        participant_id = f\"SYNTH_{i:03d}\"\n",
        "\n",
        "        # Alternate between AD and CN\n",
        "        label = i % 2\n",
        "        class_name = 'AD' if label == 1 else 'CN'\n",
        "\n",
        "        # Create synthetic transcript based on class\n",
        "        if label == 1:  # AD\n",
        "            transcript = generate_ad_like_transcript()\n",
        "        else:  # CN\n",
        "            transcript = generate_cn_like_transcript()\n",
        "\n",
        "        synthetic_data.append({\n",
        "            'participant_id': participant_id,\n",
        "            'audio_path': f'synthetic_audio_{participant_id}.wav',\n",
        "            'transcript': transcript,\n",
        "            'label': label,\n",
        "            'class_name': class_name,\n",
        "            'transcript_source': 'synthetic'\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'audio_files': [],\n",
        "        'transcript_files': [],\n",
        "        'metadata_files': [],\n",
        "        'labels': {},\n",
        "        'paired_data': synthetic_data,\n",
        "        'generated_transcripts': {}\n",
        "    }\n",
        "\n",
        "def generate_ad_like_transcript():\n",
        "    \"\"\"Generate AD-like transcript with typical characteristics\"\"\"\n",
        "    ad_patterns = [\n",
        "        \"Um, let me see... the boy is... um... he's climbing on the... the thing there...\",\n",
        "        \"There's a woman in the kitchen and she's... what is she doing... oh yes, washing dishes I think...\",\n",
        "        \"The... the thing with water is overflowing and there's... there's problems happening...\",\n",
        "        \"I see children playing and... um... something about cookies or... or food...\",\n",
        "        \"The lady is trying to... to do something with the... with the sink and water is...\",\n",
        "        \"There are people in the picture and they're... um... doing things but I can't... I can't remember...\"\n",
        "    ]\n",
        "    return np.random.choice(ad_patterns)\n",
        "\n",
        "def generate_cn_like_transcript():\n",
        "    \"\"\"Generate Control-like transcript with typical characteristics\"\"\"\n",
        "    cn_patterns = [\n",
        "        \"In this picture, I can see a kitchen scene where a woman is washing dishes at the sink. The sink appears to be overflowing with water onto the floor.\",\n",
        "        \"There's a boy who has climbed up on a stool to reach the cookie jar on the counter. His sister is asking him to give her a cookie.\",\n",
        "        \"The scene shows a typical kitchen with a woman doing dishes while children are nearby. The boy is reaching for cookies while standing on a chair.\",\n",
        "        \"I can observe a domestic scene with a mother washing dishes. There are two children in the kitchen, and one of them is trying to get cookies from a jar.\",\n",
        "        \"The picture depicts a kitchen where a woman is at the sink with running water. There are children present, and one child is reaching up to get something from the counter.\",\n",
        "        \"This shows a busy kitchen scene with a woman washing dishes while water overflows. Meanwhile, children are nearby, with one trying to access a cookie jar.\"\n",
        "    ]\n",
        "    return np.random.choice(cn_patterns)\n",
        "\n",
        "def analyze_model_features(model, test_loader, device):\n",
        "    \"\"\"Analyze model features and attention patterns\"\"\"\n",
        "    print(\"Analyzing model features and attention patterns...\")\n",
        "\n",
        "    model.eval()\n",
        "    attention_weights_list = []\n",
        "    feature_importances = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            if i >= 10:  # Analyze first 10 batches\n",
        "                break\n",
        "\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            audio_features = batch['audio_features'].to(device)\n",
        "            linguistic_features = batch['linguistic_features'].to(device)\n",
        "\n",
        "            try:\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    audio_features=audio_features,\n",
        "                    linguistic_features=linguistic_features\n",
        "                )\n",
        "\n",
        "                # Store attention weights\n",
        "                attention_weights = outputs['attention_weights'].cpu().numpy()\n",
        "                attention_weights_list.append(attention_weights)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in feature analysis: {e}\")\n",
        "                continue\n",
        "\n",
        "    if attention_weights_list:\n",
        "        # Average attention weights across batches\n",
        "        avg_attention = np.mean(attention_weights_list, axis=0)\n",
        "\n",
        "        # Plot attention patterns\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Plot attention weights for each modality\n",
        "        modalities = ['Text', 'Audio', 'Linguistic']\n",
        "        attention_by_modality = np.mean(avg_attention, axis=(0, 1))  # Average across heads and batches\n",
        "\n",
        "        plt.bar(modalities, attention_by_modality)\n",
        "        plt.title('Average Attention Weights by Modality')\n",
        "        plt.ylabel('Attention Weight')\n",
        "        plt.xlabel('Modality')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        for i, v in enumerate(attention_by_modality):\n",
        "            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('modality_attention_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"✅ Feature analysis completed. Attention analysis plot saved.\")\n",
        "    else:\n",
        "        print(\"⚠️  No attention weights collected for analysis.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "5N6y53ggBXs2",
        "outputId": "68260c8e-1248-4494-a82d-e0f497a011bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-23-d9ab2c23f4da>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-d9ab2c23f4da>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Code:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ENHANCED ADReSSo21 DATA PROCESSOR WITH SEGMENTATION FILES\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from transformers import (\n",
        "    BertTokenizer, BertModel,\n",
        "    WhisperProcessor, WhisperForConditionalGeneration,\n",
        "    Wav2Vec2Processor, Wav2Vec2ForCTC,\n",
        "    ViTModel\n",
        ")\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: ENHANCED ADRESSO21 DATA PROCESSOR\n",
        "# ============================================================================\n",
        "\n",
        "class ADReSSo21DataProcessor:\n",
        "    \"\"\"Enhanced ADReSSo21 data processor with segmentation file support\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./extracted_data', use_asr=True, asr_model=\"openai/whisper-base\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize ASR if requested\n",
        "        self.use_asr = use_asr\n",
        "        if use_asr:\n",
        "            self.transcriber = SpeechTranscriber(model_name=asr_model)\n",
        "        else:\n",
        "            self.transcriber = None\n",
        "\n",
        "    def extract_adresso_dataset(self, tar_path, dataset_name):\n",
        "        \"\"\"Extract ADReSSo21 dataset and organize files properly\"\"\"\n",
        "        extract_path = self.output_dir / dataset_name\n",
        "        extract_path.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting {tar_path} to {extract_path}\")\n",
        "\n",
        "        try:\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "            print(f\"Successfully extracted {dataset_name}\")\n",
        "\n",
        "            # Find the actual dataset directory structure\n",
        "            self._explore_directory_structure(extract_path)\n",
        "            return extract_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {tar_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _explore_directory_structure(self, base_path):\n",
        "        \"\"\"Explore and print directory structure\"\"\"\n",
        "        print(f\"\\nDirectory structure for {base_path.name}:\")\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            level = root.replace(str(base_path), '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:5]:\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "    def process_adresso_dataset(self, extract_path, dataset_type='diagnosis'):\n",
        "        \"\"\"Process ADReSSo21 dataset with segmentation files and optional ASR\"\"\"\n",
        "        dataset_info = {\n",
        "            'audio_files': [],\n",
        "            'segmentation_files': [],\n",
        "            'metadata_files': [],\n",
        "            'labels': {},\n",
        "            'paired_data': [],\n",
        "            'generated_transcripts': {},\n",
        "            'dataset_type': dataset_type\n",
        "        }\n",
        "\n",
        "        # Find ADReSSo21 directory\n",
        "        adresso_dirs = list(extract_path.rglob(\"*ADReSSo21*\"))\n",
        "        if adresso_dirs:\n",
        "            main_dir = adresso_dirs[0]\n",
        "        else:\n",
        "            main_dir = extract_path\n",
        "\n",
        "        print(f\"Processing from directory: {main_dir}\")\n",
        "\n",
        "        # Find audio files\n",
        "        audio_patterns = ['**/*.wav', '**/*.mp3', '**/*.flac']\n",
        "        for pattern in audio_patterns:\n",
        "            dataset_info['audio_files'].extend(list(main_dir.glob(pattern)))\n",
        "\n",
        "        # Find segmentation CSV files\n",
        "        csv_patterns = ['**/*.csv']\n",
        "        for pattern in csv_patterns:\n",
        "            csv_files = list(main_dir.glob(pattern))\n",
        "            for csv_file in csv_files:\n",
        "                # Filter out metadata files like MMSE scores\n",
        "                if 'mmse' not in csv_file.name.lower() and 'test_results' not in csv_file.name.lower():\n",
        "                    dataset_info['segmentation_files'].append(csv_file)\n",
        "                else:\n",
        "                    dataset_info['metadata_files'].append(csv_file)\n",
        "\n",
        "        print(f\"Found {len(dataset_info['audio_files'])} audio files\")\n",
        "        print(f\"Found {len(dataset_info['segmentation_files'])} segmentation files\")\n",
        "        print(f\"Found {len(dataset_info['metadata_files'])} metadata files\")\n",
        "\n",
        "        # Process segmentation files to get transcripts\n",
        "        transcripts_from_segmentation = self._process_segmentation_files(dataset_info['segmentation_files'])\n",
        "\n",
        "        # Generate additional transcripts using ASR if requested\n",
        "        if self.use_asr and self.transcriber:\n",
        "            print(\"Generating additional transcripts using ASR...\")\n",
        "            audio_paths = [str(path) for path in dataset_info['audio_files']]\n",
        "            generated_transcripts = self.transcriber.batch_transcribe(audio_paths)\n",
        "            dataset_info['generated_transcripts'] = generated_transcripts\n",
        "\n",
        "        # Process labels from directory structure\n",
        "        labels = self._extract_labels_from_structure(dataset_info['audio_files'], dataset_type)\n",
        "        dataset_info['labels'] = labels\n",
        "\n",
        "        # Create paired dataset\n",
        "        paired_data = self._create_paired_data(dataset_info, transcripts_from_segmentation)\n",
        "        dataset_info['paired_data'] = paired_data\n",
        "\n",
        "        return dataset_info\n",
        "\n",
        "    def _process_segmentation_files(self, segmentation_files):\n",
        "        \"\"\"Process CSV segmentation files to extract transcripts\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(\"Processing segmentation files...\")\n",
        "        for csv_file in tqdm(segmentation_files, desc=\"Processing CSV files\"):\n",
        "            try:\n",
        "                # Extract participant ID from filename\n",
        "                participant_id = self._extract_participant_id(csv_file.name)\n",
        "\n",
        "                # Read CSV file\n",
        "                df = pd.read_csv(csv_file)\n",
        "\n",
        "                # The CSV typically contains columns like: start_time, end_time, speaker, transcript\n",
        "                # Different datasets might have different column names\n",
        "                transcript_text = self._extract_transcript_from_df(df)\n",
        "\n",
        "                if transcript_text and len(transcript_text.strip()) > 0:\n",
        "                    transcripts[participant_id] = transcript_text\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {csv_file.name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Extracted transcripts from {len(transcripts)} segmentation files\")\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_transcript_from_df(self, df):\n",
        "        \"\"\"Extract transcript text from segmentation DataFrame\"\"\"\n",
        "        transcript_parts = []\n",
        "\n",
        "        # Common column name variations for transcripts\n",
        "        text_columns = []\n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower().strip()\n",
        "            if any(keyword in col_lower for keyword in ['transcript', 'text', 'word', 'utterance', 'speech']):\n",
        "                text_columns.append(col)\n",
        "\n",
        "        if not text_columns:\n",
        "            # If no obvious text column, try common patterns\n",
        "            possible_cols = ['transcript', 'text', 'word', 'utterance', 'speech_text', 'content']\n",
        "            for col in possible_cols:\n",
        "                if col in df.columns:\n",
        "                    text_columns.append(col)\n",
        "                    break\n",
        "\n",
        "        # If still no text columns found, use the last column (common pattern)\n",
        "        if not text_columns and len(df.columns) > 0:\n",
        "            text_columns = [df.columns[-1]]\n",
        "\n",
        "        # Extract text from identified columns\n",
        "        for col in text_columns:\n",
        "            try:\n",
        "                # Filter out non-speech entries (like [NOISE], [MUSIC], etc.)\n",
        "                valid_texts = df[col].dropna()\n",
        "                valid_texts = valid_texts[~valid_texts.astype(str).str.contains(r'^\\[.*\\]$', na=False)]\n",
        "                valid_texts = valid_texts[valid_texts.astype(str).str.len() > 1]\n",
        "\n",
        "                transcript_parts.extend(valid_texts.astype(str).tolist())\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        # Join all transcript parts\n",
        "        full_transcript = ' '.join(transcript_parts).strip()\n",
        "\n",
        "        # Clean the transcript\n",
        "        full_transcript = self._clean_transcript(full_transcript)\n",
        "\n",
        "        return full_transcript\n",
        "\n",
        "    def _clean_transcript(self, transcript):\n",
        "        \"\"\"Clean and normalize transcript text\"\"\"\n",
        "        if not transcript:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        transcript = re.sub(r'\\s+', ' ', transcript)\n",
        "\n",
        "        # Remove common artifacts\n",
        "        transcript = re.sub(r'\\[.*?\\]', '', transcript)  # Remove [NOISE], [MUSIC], etc.\n",
        "        transcript = re.sub(r'<.*?>', '', transcript)    # Remove <unk>, etc.\n",
        "        transcript = re.sub(r'\\*.*?\\*', '', transcript)  # Remove *action* markers\n",
        "\n",
        "        # Normalize punctuation\n",
        "        transcript = re.sub(r'[.]{2,}', '...', transcript)  # Normalize ellipses\n",
        "        transcript = re.sub(r'[?!]{2,}', '!', transcript)   # Normalize repeated punctuation\n",
        "\n",
        "        # Remove very short words that might be artifacts\n",
        "        words = transcript.split()\n",
        "        cleaned_words = [word for word in words if len(word) > 1 or word.lower() in ['i', 'a']]\n",
        "\n",
        "        return ' '.join(cleaned_words).strip()\n",
        "\n",
        "    def _extract_labels_from_structure(self, audio_files, dataset_type):\n",
        "        \"\"\"Extract labels from file paths based on dataset type\"\"\"\n",
        "        labels = {}\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            participant_id = self._extract_participant_id(audio_file.name)\n",
        "            path_str = str(audio_file).lower()\n",
        "\n",
        "            if dataset_type == 'diagnosis':\n",
        "                # For diagnosis task: AD vs CN (Control Normal)\n",
        "                if '/ad/' in path_str or 'dementia' in path_str or 'alzheimer' in path_str:\n",
        "                    label = 1  # AD\n",
        "                    class_name = 'AD'\n",
        "                elif '/cn/' in path_str or 'control' in path_str or 'normal' in path_str:\n",
        "                    label = 0  # CN\n",
        "                    class_name = 'CN'\n",
        "                else:\n",
        "                    # Default based on filename patterns\n",
        "                    if any(marker in audio_file.name.lower() for marker in ['ad', 'dem', 'alz']):\n",
        "                        label = 1\n",
        "                        class_name = 'AD'\n",
        "                    else:\n",
        "                        label = 0\n",
        "                        class_name = 'CN'\n",
        "\n",
        "            elif dataset_type == 'progression':\n",
        "                # For progression task: decline vs no_decline\n",
        "                if '/decline/' in path_str and '/no_decline/' not in path_str:\n",
        "                    label = 1  # Decline\n",
        "                    class_name = 'Decline'\n",
        "                elif '/no_decline/' in path_str or 'no-decline' in path_str:\n",
        "                    label = 0  # No decline\n",
        "                    class_name = 'No_Decline'\n",
        "                else:\n",
        "                    # Default classification for progression\n",
        "                    if 'decline' in path_str and 'no' not in path_str:\n",
        "                        label = 1\n",
        "                        class_name = 'Decline'\n",
        "                    else:\n",
        "                        label = 0\n",
        "                        class_name = 'No_Decline'\n",
        "            else:\n",
        "                # Unknown dataset type, default to binary classification\n",
        "                label = 0\n",
        "                class_name = 'Unknown'\n",
        "\n",
        "            labels[participant_id] = {\n",
        "                'label': label,\n",
        "                'class_name': class_name,\n",
        "                'audio_path': audio_file\n",
        "            }\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso(\\d{3})',          # adrso123\n",
        "            r'adrsp(\\d{3})',          # adrsp123\n",
        "            r'adrspt(\\d{1,3})',       # adrspt1, adrspt12\n",
        "            r'(\\d{3})',               # 3-digit numbers\n",
        "            r'([A-Z]\\d{2,3})',        # Letter followed by 2-3 digits\n",
        "            r'(S\\d{3})',              # S followed by 3 digits\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "    def _create_paired_data(self, dataset_info, transcripts_from_segmentation):\n",
        "        \"\"\"Create paired audio-transcript dataset\"\"\"\n",
        "        paired_data = []\n",
        "\n",
        "        for participant_id, label_info in dataset_info['labels'].items():\n",
        "            # Get transcript from segmentation file first\n",
        "            transcript = transcripts_from_segmentation.get(participant_id, \"\")\n",
        "\n",
        "            # If no segmentation transcript and ASR is available, use ASR\n",
        "            if not transcript and dataset_info.get('generated_transcripts'):\n",
        "                transcript = dataset_info['generated_transcripts'].get(participant_id, \"\")\n",
        "\n",
        "            # Clean and validate transcript\n",
        "            transcript = self._clean_transcript(transcript)\n",
        "\n",
        "            # Create meaningful placeholder if transcript is still empty\n",
        "            if not transcript or len(transcript.strip()) < 10:\n",
        "                transcript = f\"Audio sample from participant {participant_id}. Limited speech content available.\"\n",
        "\n",
        "            # Determine transcript source\n",
        "            if participant_id in transcripts_from_segmentation and transcripts_from_segmentation[participant_id]:\n",
        "                transcript_source = 'segmentation_file'\n",
        "            elif dataset_info.get('generated_transcripts', {}).get(participant_id):\n",
        "                transcript_source = 'ASR_generated'\n",
        "            else:\n",
        "                transcript_source = 'placeholder'\n",
        "\n",
        "            paired_data.append({\n",
        "                'participant_id': participant_id,\n",
        "                'audio_path': str(label_info['audio_path']),\n",
        "                'transcript': transcript,\n",
        "                'label': label_info['label'],\n",
        "                'class_name': label_info['class_name'],\n",
        "                'transcript_source': transcript_source,\n",
        "                'dataset_type': dataset_info['dataset_type']\n",
        "            })\n",
        "\n",
        "        print(f\"Created {len(paired_data)} paired samples\")\n",
        "\n",
        "        # Print class distribution\n",
        "        labels = [item['label'] for item in paired_data]\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        for cls, count in zip(unique, counts):\n",
        "            class_names = list(set([item['class_name'] for item in paired_data if item['label'] == cls]))\n",
        "            class_name = class_names[0] if class_names else f'Class_{cls}'\n",
        "            print(f\"  {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "        # Print transcript source distribution\n",
        "        sources = [item['transcript_source'] for item in paired_data]\n",
        "        unique_sources, source_counts = np.unique(sources, return_counts=True)\n",
        "        print(f\"\\nTranscript sources:\")\n",
        "        for source, count in zip(unique_sources, source_counts):\n",
        "            print(f\"  {source}: {count} samples ({count/len(sources)*100:.1f}%)\")\n",
        "\n",
        "        # Print sample transcripts for verification\n",
        "        print(f\"\\nSample transcripts:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, sample in enumerate(paired_data[:3]):\n",
        "            print(f\"Participant {sample['participant_id']} ({sample['class_name']}) - Source: {sample['transcript_source']}\")\n",
        "            print(f\"Transcript: {sample['transcript'][:150]}...\")\n",
        "            print()\n",
        "\n",
        "        return paired_data\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: SPEECH TRANSCRIBER (OPTIONAL ASR)\n",
        "# ============================================================================\n",
        "\n",
        "class SpeechTranscriber:\n",
        "    \"\"\"Automatic Speech Recognition for generating transcripts from audio\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"openai/whisper-base\", cache_dir=\"./asr_cache\"):\n",
        "        \"\"\"Initialize ASR model\"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Loading ASR model: {model_name}\")\n",
        "\n",
        "        if \"whisper\" in model_name.lower():\n",
        "            self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "            self.asr_type = \"whisper\"\n",
        "        else:\n",
        "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            self.asr_type = \"wav2vec2\"\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"ASR model loaded on {self.device}\")\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path, use_cache=True):\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        # Check cache first\n",
        "        cache_file = self.cache_dir / f\"{audio_path.stem}_transcript.pkl\"\n",
        "        if use_cache and cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cached_result = pickle.load(f)\n",
        "                return cached_result['transcript']\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(str(audio_path), sr=16000)\n",
        "\n",
        "            if len(audio) < 1600:  # Less than 0.1 seconds\n",
        "                transcript = \"\"\n",
        "            else:\n",
        "                transcript = self._transcribe_audio_array(audio)\n",
        "\n",
        "            # Cache result\n",
        "            if use_cache:\n",
        "                try:\n",
        "                    with open(cache_file, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'audio_path': str(audio_path),\n",
        "                            'transcript': transcript,\n",
        "                            'model': self.model_name\n",
        "                        }, f)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            return transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {audio_path}: {e}\")\n",
        "            return f\"[Transcription failed for {audio_path.name}]\"\n",
        "\n",
        "    def _transcribe_audio_array(self, audio_array):\n",
        "        \"\"\"Transcribe audio array using the loaded model\"\"\"\n",
        "        try:\n",
        "            if self.asr_type == \"whisper\":\n",
        "                return self._whisper_transcribe(audio_array)\n",
        "            else:\n",
        "                return self._wav2vec2_transcribe(audio_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {e}\")\n",
        "            return \"[Transcription error]\"\n",
        "\n",
        "    def _whisper_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Whisper model\"\"\"\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_features.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(inputs, max_length=448)\n",
        "\n",
        "        transcript = self.processor.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcript.strip()\n",
        "\n",
        "    def _wav2vec2_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Wav2Vec2 model\"\"\"\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).input_values.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs).logits\n",
        "\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcript = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "        return transcript.strip().lower()\n",
        "\n",
        "    def batch_transcribe(self, audio_paths, batch_size=8):\n",
        "        \"\"\"Transcribe multiple audio files with progress bar\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(f\"Transcribing {len(audio_paths)} audio files...\")\n",
        "\n",
        "        for audio_path in tqdm(audio_paths, desc=\"Transcribing\"):\n",
        "            transcript = self.transcribe_audio_file(audio_path)\n",
        "            participant_id = self._extract_participant_id(Path(audio_path).name)\n",
        "            transcripts[participant_id] = transcript\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso(\\d{3})',\n",
        "            r'adrsp(\\d{3})',\n",
        "            r'adrspt(\\d{1,3})',\n",
        "            r'(\\d{3})',\n",
        "            r'([A-Z]\\d{2,3})',\n",
        "            r'(S\\d{3})',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: ENHANCED MULTIMODAL DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedMultiModalDataset(Dataset):\n",
        "    \"\"\"Enhanced dataset with segmentation-based transcripts and linguistic features\"\"\"\n",
        "\n",
        "    def __init__(self, data_samples, audio_processor, tokenizer,\n",
        "                 max_text_length=512, audio_max_length=16*16000,\n",
        "                 image_size=(224, 224)):\n",
        "        self.data_samples = data_samples\n",
        "        self.audio_processor = audio_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.audio_max_length = audio_max_length\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Precompute linguistic features\n",
        "        self._precompute_linguistic_features()\n",
        "\n",
        "    def _precompute_linguistic_features(self):\n",
        "        \"\"\"Precompute comprehensive linguistic features\"\"\"\n",
        "        print(\"Precomputing linguistic features...\")\n",
        "\n",
        "        for sample in tqdm(self.data_samples, desc=\"Computing linguistic features\"):\n",
        "            transcript = sample['transcript']\n",
        "\n",
        "            # Basic linguistic metrics\n",
        "            words = transcript.split()\n",
        "            sentences = re.split(r'[.!?]+', transcript)\n",
        "            sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "            # Advanced linguistic features\n",
        "            linguistic_features = {\n",
        "                # Basic counts\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len(sentences),\n",
        "                'character_count': len(transcript),\n",
        "\n",
        "                # Word-level features\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'long_words_ratio': sum(1 for word in words if len(word) > 6) / len(words) if words else 0,\n",
        "\n",
        "                # Sentence-level features\n",
        "                'avg_sentence_length': np.mean([len(s.split()) for s in sentences]) if sentences else 0,\n",
        "                'max_sentence_length': max([len(s.split()) for s in sentences]) if sentences else 0,\n",
        "\n",
        "                # Fluency indicators\n",
        "                'pause_markers': transcript.count('[pause]') + transcript.count('...') + transcript.count('..'),\n",
        "                'filler_words': sum(1 for word in words if word.lower() in ['um', 'uh', 'er', 'ah', 'hmm']),\n",
        "                'repetitions': self._count_repetitions(words),\n",
        "\n",
        "                # Semantic features\n",
        "                'content_words': sum(1 for word in words if len(word) > 3),\n",
        "                'function_words': sum(1 for word in words if word.lower() in\n",
        "                                    ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with']),\n",
        "\n",
        "                # Complexity measures\n",
        "                'subordinate_clauses': transcript.count('that') + transcript.count('which') + transcript.count('because'),\n",
        "                'coordination': transcript.count(' and ') + transcript.count(' or ') + transcript.count(' but '),\n",
        "            }\n",
        "\n",
        "            sample['linguistic_features'] = linguistic_features\n",
        "\n",
        "    def _count_repetitions(self, words):\n",
        "        \"\"\"Count word repetitions (potential indicator of cognitive issues)\"\"\"\n",
        "        if len(words) < 2:\n",
        "            return 0\n",
        "\n",
        "        repetitions = 0\n",
        "        for i in range(len(words) - 1):\n",
        "            if words[i].lower() == words[i + 1].lower():\n",
        "                repetitions += 1\n",
        "\n",
        "        return repetitions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        # Process text\n",
        "        text = sample['transcript']\n",
        "        if not text or text.strip() == \"\":\n",
        "            text = \"No speech content detected in audio sample\"\n",
        "\n",
        "        try:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_text_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error tokenizing text for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy encoding\n",
        "            encoding = {\n",
        "                'input_ids': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'attention_mask': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'token_type_ids': torch.zeros(self.max_text_length, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        # Process audio\n",
        "        try:\n",
        "            audio = self.audio_processor.load_audio(\n",
        "                sample['audio_path'],\n",
        "                max_length=self.audio_max_length\n",
        "            )\n",
        "\n",
        "            spectrogram = self.audio_processor.extract_mel_spectrogram(audio)\n",
        "            audio_features = self.audio_processor.resize_spectrogram_to_image(\n",
        "                spectrogram, self.image_size\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio for {sample['participant_id']}: {e}\")\n",
        "            audio_features = np.random.rand(3, self.image_size[0], self.image_size[1])\n",
        "\n",
        "        # Enhanced linguistic features vector\n",
        "        ling_features = sample.get('linguistic_features', {})\n",
        "        linguistic_vector = np.array([\n",
        "            ling_features.get('word_count', 0),\n",
        "            ling_features.get('sentence_count', 0),\n",
        "            ling_features.get('avg_word_length', 0),\n",
        "            ling_features.get('unique_words', 0),\n",
        "            ling_features.get('lexical_diversity', 0),\n",
        "            ling_features.get('pause_markers', 0),\n",
        "            ling_features.get('filler_words', 0),\n",
        "            ling_features.get('repetitions', 0),\n",
        "            ling_features.get('avg_sentence_length', 0),\n",
        "            ling_features.get('long_words_ratio', 0),\n",
        "            ling_features.get('content_words', 0),\n",
        "            ling_features.get('function_words', 0),\n",
        "            ling_features.get('subordinate_clauses', 0),\n",
        "            ling_features.get('coordination', 0),\n",
        "            ling_features.get('character_count', 0)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze() if hasattr(encoding['input_ids'], 'squeeze') else encoding['input_ids'],\n",
        "            'attention_mask': encoding['attention_mask'].squeeze() if hasattr(encoding['attention_mask'], 'squeeze') else encoding['attention_mask'],\n",
        "            'audio_features': torch.FloatTensor(audio_features),\n",
        "            'linguistic_features': torch.FloatTensor(linguistic_vector),\n",
        "            'label': torch.LongTensor([sample['label']]).squeeze(),\n",
        "            'participant_id': sample['participant_id'],\n",
        "            'class_name': sample['class_name'],\n",
        "            'transcript_preview': text[:100] + \"...\" if len(text) > 100 else text,\n",
        "            'transcript_source': sample.get('transcript_source', 'unknown'),\n",
        "            'dataset_type': sample.get('dataset_type', 'unknown')\n",
        "        }\n"
      ],
      "metadata": {
        "id": "OeQPOx0uBbnf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: COMPLETE AUDIO PROCESSOR\n",
        "# ============================================================================\n",
        "\n",
        "class SimpleAudioProcessor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=128):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "    def load_audio(self, audio_path, max_length=None):\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            if audio.ndim > 1:\n",
        "                audio = np.mean(audio, axis=1)\n",
        "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
        "            if np.max(np.abs(audio)) > 0:\n",
        "                audio = librosa.util.normalize(audio)\n",
        "\n",
        "            if max_length is not None:\n",
        "                if len(audio) > max_length:\n",
        "                    start = (len(audio) - max_length) // 2\n",
        "                    audio = audio[start:start + max_length]\n",
        "                elif len(audio) < max_length:\n",
        "                    pad_length = max_length - len(audio)\n",
        "                    audio = np.pad(audio, (0, pad_length), mode='constant')\n",
        "\n",
        "            return audio\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading audio: {e}\")\n",
        "            length = max_length if max_length else self.sample_rate * 10\n",
        "            return np.random.randn(length) * 0.01\n",
        "\n",
        "    def extract_mel_spectrogram(self, audio):\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
        "            )\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            return log_mel_spec\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting mel spectrogram: {e}\")\n",
        "            return np.random.randn(self.n_mels, 100)\n",
        "\n",
        "    def resize_spectrogram_to_image(self, spectrogram, target_size=(224, 224)):\n",
        "        \"\"\"Convert spectrogram to image format for vision models\"\"\"\n",
        "        try:\n",
        "            # Normalize to [0, 1]\n",
        "            spec_normalized = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram) + 1e-8)\n",
        "\n",
        "            # Resize to target size\n",
        "            spec_resized = ndimage.zoom(spec_normalized,\n",
        "                                      (target_size[0] / spec_normalized.shape[0],\n",
        "                                       target_size[1] / spec_normalized.shape[1]))\n",
        "\n",
        "            # Convert to 3-channel image (RGB)\n",
        "            spec_image = np.stack([spec_resized, spec_resized, spec_resized], axis=0)\n",
        "\n",
        "            return spec_image\n",
        "        except Exception as e:\n",
        "            print(f\"Error resizing spectrogram: {e}\")\n",
        "            return np.random.rand(3, target_size[0], target_size[1])\n",
        "\n",
        "    def extract_acoustic_features(self, audio):\n",
        "        \"\"\"Extract comprehensive acoustic features\"\"\"\n",
        "        try:\n",
        "            features = {}\n",
        "\n",
        "            # Basic spectral features\n",
        "            features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=audio, sr=self.sample_rate))\n",
        "            features['spectral_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=self.sample_rate))\n",
        "            features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=self.sample_rate))\n",
        "            features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "\n",
        "            # MFCC features\n",
        "            mfccs = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=13)\n",
        "            for i in range(13):\n",
        "                features[f'mfcc_{i}'] = np.mean(mfccs[i])\n",
        "                features[f'mfcc_{i}_std'] = np.std(mfccs[i])\n",
        "\n",
        "            # Prosodic features (fundamental frequency)\n",
        "            f0 = librosa.yin(audio, fmin=50, fmax=300)\n",
        "            f0_valid = f0[f0 > 0]  # Remove unvoiced frames\n",
        "            if len(f0_valid) > 0:\n",
        "                features['f0_mean'] = np.mean(f0_valid)\n",
        "                features['f0_std'] = np.std(f0_valid)\n",
        "                features['f0_range'] = np.max(f0_valid) - np.min(f0_valid)\n",
        "            else:\n",
        "                features['f0_mean'] = features['f0_std'] = features['f0_range'] = 0\n",
        "\n",
        "            # Energy features\n",
        "            rms = librosa.feature.rms(y=audio)[0]\n",
        "            features['energy_mean'] = np.mean(rms)\n",
        "            features['energy_std'] = np.std(rms)\n",
        "\n",
        "            # Temporal features\n",
        "            features['duration'] = len(audio) / self.sample_rate\n",
        "\n",
        "            return features\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting acoustic features: {e}\")\n",
        "            # Return default features\n",
        "            return {f'feature_{i}': 0.0 for i in range(30)}\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: ENHANCED MULTIMODAL MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class EnhancedMultiModalModel(nn.Module):\n",
        "    \"\"\"Advanced multimodal model combining text, audio, and linguistic features\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=2, bert_model='bert-base-uncased',\n",
        "                 linguistic_features_dim=15, dropout_rate=0.3):\n",
        "        super(EnhancedMultiModalModel, self).__init__()\n",
        "\n",
        "        # Text encoder (BERT)\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.text_dim = self.bert.config.hidden_size\n",
        "\n",
        "        # Audio encoder (CNN for spectrogram images)\n",
        "        self.audio_encoder = nn.Sequential(\n",
        "            # First conv block\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "\n",
        "            # Second conv block\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "\n",
        "            # Third conv block\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.25),\n",
        "\n",
        "            # Fourth conv block\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((4, 4)),\n",
        "            nn.Dropout2d(0.5),\n",
        "        )\n",
        "\n",
        "        # Calculate audio feature dimension\n",
        "        self.audio_dim = 256 * 4 * 4  # 4096\n",
        "\n",
        "        # Linguistic features encoder\n",
        "        self.linguistic_encoder = nn.Sequential(\n",
        "            nn.Linear(linguistic_features_dim, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "        self.linguistic_dim = 128\n",
        "\n",
        "        # Feature dimension reduction layers\n",
        "        self.text_projector = nn.Sequential(\n",
        "            nn.Linear(self.text_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "\n",
        "        self.audio_projector = nn.Sequential(\n",
        "            nn.Linear(self.audio_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "\n",
        "        # Cross-modal attention mechanism\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            embed_dim=512, num_heads=8, dropout=dropout_rate, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fusion layer\n",
        "        total_dim = 512 + 512 + self.linguistic_dim  # text + audio + linguistic\n",
        "        self.fusion_layer = nn.Sequential(\n",
        "            nn.Linear(total_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize model weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d) or isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features, linguistic_features):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Text encoding\n",
        "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = text_outputs.pooler_output  # [batch_size, 768]\n",
        "        text_projected = self.text_projector(text_features)  # [batch_size, 512]\n",
        "\n",
        "        # Audio encoding\n",
        "        audio_encoded = self.audio_encoder(audio_features)  # [batch_size, 256, 4, 4]\n",
        "        audio_flattened = audio_encoded.view(batch_size, -1)  # [batch_size, 4096]\n",
        "        audio_projected = self.audio_projector(audio_flattened)  # [batch_size, 512]\n",
        "\n",
        "        # Linguistic features encoding\n",
        "        linguistic_encoded = self.linguistic_encoder(linguistic_features)  # [batch_size, 128]\n",
        "\n",
        "        # Cross-modal attention between text and audio\n",
        "        text_expanded = text_projected.unsqueeze(1)  # [batch_size, 1, 512]\n",
        "        audio_expanded = audio_projected.unsqueeze(1)  # [batch_size, 1, 512]\n",
        "\n",
        "        # Apply cross-attention\n",
        "        attended_text, _ = self.cross_attention(text_expanded, audio_expanded, audio_expanded)\n",
        "        attended_audio, _ = self.cross_attention(audio_expanded, text_expanded, text_expanded)\n",
        "\n",
        "        attended_text = attended_text.squeeze(1)  # [batch_size, 512]\n",
        "        attended_audio = attended_audio.squeeze(1)  # [batch_size, 512]\n",
        "\n",
        "        # Feature fusion\n",
        "        fused_features = torch.cat([attended_text, attended_audio, linguistic_encoded], dim=1)\n",
        "        fused_output = self.fusion_layer(fused_features)  # [batch_size, 512]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_output)  # [batch_size, num_classes]\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'text_features': text_projected,\n",
        "            'audio_features': audio_projected,\n",
        "            'linguistic_features': linguistic_encoded,\n",
        "            'fused_features': fused_output\n",
        "        }\n"
      ],
      "metadata": {
        "id": "EF3NwcrfFuv-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Enhanced trainer with comprehensive evaluation metrics\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, num_classes=2):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "        self.model.to(device)\n",
        "\n",
        "        # Initialize metrics tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.learning_rates = []\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, num_epochs=50,\n",
        "                   learning_rate=2e-5, weight_decay=1e-4, patience=10,\n",
        "                   warmup_epochs=3):\n",
        "        \"\"\"Train the model with early stopping and learning rate scheduling\"\"\"\n",
        "\n",
        "        # Optimizer with different learning rates for different components\n",
        "        bert_params = list(self.model.bert.parameters())\n",
        "        other_params = [p for p in self.model.parameters() if p not in bert_params]\n",
        "\n",
        "        optimizer = optim.AdamW([\n",
        "            {'params': bert_params, 'lr': learning_rate * 0.1},  # Lower LR for BERT\n",
        "            {'params': other_params, 'lr': learning_rate}\n",
        "        ], weight_decay=weight_decay)\n",
        "\n",
        "        # Learning rate scheduler with warmup\n",
        "        total_steps = len(train_loader) * num_epochs\n",
        "        warmup_steps = len(train_loader) * warmup_epochs\n",
        "\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(max(1, warmup_steps))\n",
        "            return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
        "\n",
        "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "        # Loss function with label smoothing\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Early stopping\n",
        "        best_val_loss = float('inf')\n",
        "        best_val_f1 = 0.0\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "        print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "        print(f\"Batch size: {train_loader.batch_size}\")\n",
        "        print(f\"Steps per epoch: {len(train_loader)}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training phase\n",
        "            train_loss, train_acc = self._train_epoch(train_loader, optimizer, criterion, scheduler)\n",
        "\n",
        "            # Validation phase\n",
        "            val_loss, val_acc, val_metrics = self._validate_epoch(val_loader, criterion)\n",
        "\n",
        "            # Save metrics\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "            self.learning_rates.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch {epoch+1:3d}/{num_epochs}: \"\n",
        "                  f\"Train[Loss: {train_loss:.4f}, Acc: {train_acc:.4f}] \"\n",
        "                  f\"Val[Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_metrics['f1_score']:.4f}] \"\n",
        "                  f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "            # Early stopping check (using F1 score as primary metric)\n",
        "            if val_metrics['f1_score'] > best_val_f1:\n",
        "                best_val_f1 = val_metrics['f1_score']\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                best_model_state = self.model.state_dict().copy()\n",
        "                print(f\"  → New best model! F1: {best_val_f1:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1} (patience: {patience})\")\n",
        "                break\n",
        "\n",
        "            # Print detailed metrics every 5 epochs\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                print(f\"  Detailed metrics: Precision: {val_metrics['precision']:.4f}, \"\n",
        "                      f\"Recall: {val_metrics['recall']:.4f}\")\n",
        "                self._print_confusion_matrix(val_metrics['confusion_matrix'])\n",
        "\n",
        "        # Load best model\n",
        "        if best_model_state is not None:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "            print(f\"\\nLoaded best model with F1: {best_val_f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'train_losses': self.train_losses,\n",
        "            'val_losses': self.val_losses,\n",
        "            'train_accuracies': self.train_accuracies,\n",
        "            'val_accuracies': self.val_accuracies,\n",
        "            'learning_rates': self.learning_rates,\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'best_val_f1': best_val_f1,\n",
        "            'total_epochs': len(self.train_losses)\n",
        "        }\n",
        "\n",
        "    def _train_epoch(self, train_loader, optimizer, criterion, scheduler):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            # Move to device\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "            labels = batch['label'].to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(input_ids, attention_mask, audio_features, linguistic_features)\n",
        "            loss = criterion(outputs['logits'], labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs['logits'], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            current_acc = correct / total\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{current_acc:.4f}',\n",
        "                'LR': f'{scheduler.get_last_lr()[0]:.2e}'\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def _validate_epoch(self, val_loader, criterion):\n",
        "        \"\"\"Validate for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in progress_bar:\n",
        "                # Move to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(input_ids, attention_mask, audio_features, linguistic_features)\n",
        "                loss = criterion(outputs['logits'], labels)\n",
        "\n",
        "                # Get predictions and probabilities\n",
        "                probabilities = torch.softmax(outputs['logits'], dim=1)\n",
        "                _, predicted = torch.max(outputs['logits'], 1)\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "\n",
        "        # Calculate comprehensive metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': all_predictions,\n",
        "            'true_labels': all_labels,\n",
        "            'probabilities': all_probabilities\n",
        "        }\n",
        "\n",
        "        return avg_loss, accuracy, metrics\n",
        "\n",
        "    def evaluate_model(self, test_loader, detailed=True):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "        participant_results = {}\n",
        "        feature_analysis = defaultdict(list)\n",
        "\n",
        "        print(\"Evaluating model on test set...\")\n",
        "        progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in progress_bar:\n",
        "                # Move to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(input_ids, attention_mask, audio_features, linguistic_features)\n",
        "                probabilities = torch.softmax(outputs['logits'], dim=1)\n",
        "                _, predicted = torch.max(outputs['logits'], 1)\n",
        "\n",
        "                # Store results\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "                # Store per-participant results and feature analysis\n",
        "                if detailed:\n",
        "                    for i in range(len(labels)):\n",
        "                        participant_id = batch['participant_id'][i] if 'participant_id' in batch else f\"sample_{len(participant_results)}\"\n",
        "\n",
        "                        participant_results[participant_id] = {\n",
        "                            'true_label': labels[i].item(),\n",
        "                            'predicted_label': predicted[i].item(),\n",
        "                            'probability': probabilities[i].cpu().numpy(),\n",
        "                            'confidence': torch.max(probabilities[i]).item(),\n",
        "                            'text_features': outputs['text_features'][i].cpu().numpy(),\n",
        "                            'audio_features': outputs['audio_features'][i].cpu().numpy(),\n",
        "                            'linguistic_features': outputs['linguistic_features'][i].cpu().numpy(),\n",
        "                        }\n",
        "\n",
        "                        # Feature analysis\n",
        "                        label = labels[i].item()\n",
        "                        feature_analysis[f'text_features_class_{label}'].append(\n",
        "                            outputs['text_features'][i].cpu().numpy()\n",
        "                        )\n",
        "                        feature_analysis[f'audio_features_class_{label}'].append(\n",
        "                            outputs['audio_features'][i].cpu().numpy()\n",
        "                        )\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        # Detailed class-wise metrics\n",
        "        class_precision = precision_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "        class_recall = recall_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "        class_f1 = f1_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "\n",
        "        # Calculate confidence statistics\n",
        "        confidence_stats = self._analyze_confidence(all_probabilities, all_predictions, all_labels)\n",
        "\n",
        "        evaluation_results = {\n",
        "            'overall_metrics': {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_score': f1,\n",
        "                'confusion_matrix': cm\n",
        "            },\n",
        "            'class_wise_metrics': {\n",
        "                'precision': class_precision,\n",
        "                'recall': class_recall,\n",
        "                'f1_score': class_f1\n",
        "            },\n",
        "            'confidence_analysis': confidence_stats,\n",
        "            'participant_results': participant_results if detailed else None,\n",
        "            'feature_analysis': dict(feature_analysis) if detailed else None,\n",
        "            'predictions': all_predictions,\n",
        "            'true_labels': all_labels,\n",
        "            'probabilities': all_probabilities\n",
        "        }\n",
        "\n",
        "        return evaluation_results\n",
        "\n",
        "    def _analyze_confidence(self, probabilities, predictions, true_labels):\n",
        "        \"\"\"Analyze model confidence and calibration\"\"\"\n",
        "        probs_array = np.array(probabilities)\n",
        "        max_probs = np.max(probs_array, axis=1)\n",
        "        predictions = np.array(predictions)\n",
        "        true_labels = np.array(true_labels)\n",
        "\n",
        "        # Overall confidence stats\n",
        "        confidence_stats = {\n",
        "            'mean_confidence': np.mean(max_probs),\n",
        "            'std_confidence': np.std(max_probs),\n",
        "            'min_confidence': np.min(max_probs),\n",
        "            'max_confidence': np.max(max_probs)\n",
        "        }\n",
        "\n",
        "        # Confidence for correct vs incorrect predictions\n",
        "        correct_mask = predictions == true_labels\n",
        "        confidence_stats['correct_confidence'] = np.mean(max_probs[correct_mask])\n",
        "        confidence_stats['incorrect_confidence'] = np.mean(max_probs[~correct_mask])\n",
        "\n",
        "        # Calibration analysis\n",
        "        n_bins = 10\n",
        "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_lowers = bin_boundaries[:-1]\n",
        "        bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "        calibration_data = []\n",
        "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "            in_bin = (max_probs > bin_lower) & (max_probs <= bin_upper)\n",
        "            prop_in_bin = in_bin.mean()\n",
        "\n",
        "            if prop_in_bin > 0:\n",
        "                accuracy_in_bin = correct_mask[in_bin].mean()\n",
        "                avg_confidence_in_bin = max_probs[in_bin].mean()\n",
        "                calibration_data.append({\n",
        "                    'bin_lower': bin_lower,\n",
        "                    'bin_upper': bin_upper,\n",
        "                    'accuracy': accuracy_in_bin,\n",
        "                    'confidence': avg_confidence_in_bin,\n",
        "                    'count': in_bin.sum()\n",
        "                })\n",
        "\n",
        "        confidence_stats['calibration_data'] = calibration_data\n",
        "\n",
        "        return confidence_stats\n",
        "\n",
        "    def _print_confusion_matrix(self, cm, class_names=None):\n",
        "        \"\"\"Print formatted confusion matrix\"\"\"\n",
        "        if class_names is None:\n",
        "            class_names = ['Control', 'Dementia']\n",
        "\n",
        "        print(\"  Confusion Matrix:\")\n",
        "        print(\"  \" + \"-\" * 25)\n",
        "        print(f\"          Predicted\")\n",
        "        print(f\"        {class_names[0]:<8} {class_names[1]:<8}\")\n",
        "        print(f\"  True\")\n",
        "        for i, true_class in enumerate(class_names):\n",
        "            print(f\"  {true_class:<8} {cm[i,0]:<8} {cm[i,1]:<8}\")\n",
        "        print()\n",
        "\n",
        "    def plot_training_history(self, save_path=None):\n",
        "        \"\"\"Plot comprehensive training history\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "        # Training and validation loss\n",
        "        axes[0,0].plot(self.train_losses, label='Training Loss', color='blue', alpha=0.7)\n",
        "        axes[0,0].plot(self.val_losses, label='Validation Loss', color='red', alpha=0.7)\n",
        "        axes[0,0].set_title('Training and Validation Loss')\n",
        "        axes[0,0].set_xlabel('Epoch')\n",
        "        axes[0,0].set_ylabel('Loss')\n",
        "        axes[0,0].legend()\n",
        "        axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Training and validation accuracy\n",
        "        axes[0,1].plot(self.train_accuracies, label='Training Accuracy', color='blue', alpha=0.7)\n",
        "        axes[0,1].plot(self.val_accuracies, label='Validation Accuracy', color='red', alpha=0.7)\n",
        "        axes[0,1].set_title('Training and Validation Accuracy')\n",
        "        axes[0,1].set_xlabel('Epoch')\n",
        "        axes[0,1].set_ylabel('Accuracy')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning rate schedule\n",
        "        axes[0,2].plot(self.learning_rates, label='Learning Rate', color='green')\n",
        "        axes[0,2].set_title('Learning Rate Schedule')\n",
        "        axes[0,2].set_xlabel('Epoch')\n",
        "        axes[0,2].set_ylabel('Learning Rate')\n",
        "        axes[0,2].set_yscale('log')\n",
        "        axes[0,2].legend()\n",
        "        axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss difference (overfitting indicator)\n",
        "        loss_diff = [abs(t - v) for t, v in zip(self.train_losses, self.val_losses)]\n",
        "        axes[1,0].plot(loss_diff, label='|Train - Val| Loss', color='orange')\n",
        "        axes[1,0].set_title('Training-Validation Loss Gap')\n",
        "        axes[1,0].set_xlabel('Epoch')\n",
        "        axes[1,0].set_ylabel('Absolute Difference')\n",
        "        axes[1,0].legend()\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy difference\n",
        "        acc_diff = [abs(t - v) for t, v in zip(self.train_accuracies, self.val_accuracies)]\n",
        "        axes[1,1].plot(acc_diff, label='|Train - Val| Accuracy', color='purple')\n",
        "        axes[1,1].set_title('Training-Validation Accuracy Gap')\n",
        "        axes[1,1].set_xlabel('Epoch')\n",
        "        axes[1,1].set_ylabel('Absolute Difference')\n",
        "        axes[1,1].legend()\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Training summary\n",
        "        if len(self.train_losses) > 0:\n",
        "            summary_text = (\n",
        "                f'Training Complete!\\n\\n'\n",
        "                f'Total Epochs: {len(self.train_losses)}\\n'\n",
        "                f'Final Train Acc: {self.train_accuracies[-1]:.4f}\\n'\n",
        "                f'Final Val Acc: {self.val_accuracies[-1]:.4f}\\n'\n",
        "                f'Final Train Loss: {self.train_losses[-1]:.4f}\\n'\n",
        "                f'Final Val Loss: {self.val_losses[-1]:.4f}\\n'\n",
        "                f'Best Val Acc: {max(self.val_accuracies):.4f}\\n'\n",
        "                f'Min Val Loss: {min(self.val_losses):.4f}'\n",
        "            )\n",
        "        else:\n",
        "            summary_text = 'No training data available'\n",
        "\n",
        "        axes[1,2].text(0.5, 0.5, summary_text,\n",
        "                      horizontalalignment='center', verticalalignment='center',\n",
        "                      transform=axes[1,2].transAxes, fontsize=11,\n",
        "                      bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
        "        axes[1,2].set_title('Training Summary')\n",
        "        axes[1,2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Training history plot saved to: {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_evaluation_results(self, evaluation_results, save_path=None):\n",
        "        \"\"\"Plot comprehensive evaluation results\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = evaluation_results['overall_metrics']['confusion_matrix']\n",
        "        class_names = ['Control', 'Dementia']\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names,\n",
        "                   ax=axes[0,0])\n",
        "        axes[0,0].set_title('Confusion Matrix')\n",
        "        axes[0,0].set_xlabel('Predicted')\n",
        "        axes[0,0].set_ylabel('True')\n",
        "\n",
        "        # Class-wise metrics\n",
        "        class_metrics = evaluation_results['class_wise_metrics']\n",
        "        metrics_names = ['Precision', 'Recall', 'F1-Score']\n",
        "        x = np.arange(len(class_names))\n",
        "        width = 0.25\n",
        "\n",
        "        for i, metric in enumerate(['precision', 'recall', 'f1_score']):\n",
        "            axes[0,1].bar(x + i*width, class_metrics[metric], width,\n",
        "                         label=metrics_names[i], alpha=0.8)\n",
        "\n",
        "        axes[0,1].set_xlabel('Classes')\n",
        "        axes[0,1].set_ylabel('Score')\n",
        "        axes[0,1].set_title('Class-wise Performance Metrics')\n",
        "        axes[0,1].set_xticks(x + width)\n",
        "        axes[0,1].set_xticklabels(class_names)\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Confidence Analysis\n",
        "        if 'confidence_analysis' in evaluation_results:\n",
        "            conf_stats = evaluation_results['confidence_analysis']\n",
        "\n",
        "            # Confidence distribution\n",
        "            probabilities = np.array(evaluation_results['probabilities'])\n",
        "            max_probs = np.max(probabilities, axis=1)\n",
        "\n",
        "            axes[1,0].hist(max_probs, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[1,0].axvline(conf_stats['mean_confidence'], color='red', linestyle='--',\n",
        "                             label=f'Mean: {conf_stats[\"mean_confidence\"]:.3f}')\n",
        "            axes[1,0].set_xlabel('Model Confidence')\n",
        "            axes[1,0].set_ylabel('Count')\n",
        "            axes[1,0].set_title('Confidence Distribution')\n",
        "            axes[1,0].legend()\n",
        "            axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Overall metrics summary\n",
        "        overall_metrics = evaluation_results['overall_metrics']\n",
        "        metrics_text = (\n",
        "            f'Overall Performance\\n\\n'\n",
        "            f'Accuracy: {overall_metrics[\"accuracy\"]:.4f}\\n'\n",
        "            f'Precision: {overall_metrics[\"precision\"]:.4f}\\n'\n",
        "            f'Recall: {overall_metrics[\"recall\"]:.4f}\\n'\n",
        "            f'F1-Score: {overall_metrics[\"f1_score\"]:.4f}\\n\\n'\n",
        "        )\n",
        "\n",
        "        if 'confidence_analysis' in evaluation_results:\n",
        "            conf_stats = evaluation_results['confidence_analysis']\n",
        "            metrics_text += (\n",
        "                f'Confidence Stats\\n'\n",
        "                f'Mean Confidence: {conf_stats[\"mean_confidence\"]:.4f}\\n'\n",
        "                f'Correct Predictions: {conf_stats[\"correct_confidence\"]:.4f}\\n'\n",
        "                f'Incorrect Predictions: {conf_stats[\"incorrect_confidence\"]:.4f}'\n",
        "            )\n",
        "\n",
        "        axes[1,1].text(0.1, 0.5, metrics_text,\n",
        "                      horizontalalignment='left', verticalalignment='center',\n",
        "                      transform=axes[1,1].transAxes, fontsize=12,\n",
        "                      bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgreen\", alpha=0.8))\n",
        "        axes[1,1].set_title('Performance Summary')\n",
        "        axes[1,1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"Evaluation results plot saved to: {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def save_model(self, save_path, evaluation_results=None):\n",
        "        \"\"\"Save model and training information\"\"\"\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'model_config': {\n",
        "                'num_classes': self.num_classes,\n",
        "            },\n",
        "            'training_history': {\n",
        "                'train_losses': self.train_losses,\n",
        "                'val_losses': self.val_losses,\n",
        "                'train_accuracies': self.train_accuracies,\n",
        "                'val_accuracies': self.val_accuracies,\n",
        "                'learning_rates': self.learning_rates,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if evaluation_results:\n",
        "            checkpoint['evaluation_results'] = evaluation_results\n",
        "\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Model checkpoint saved to: {save_path}\")\n",
        "\n",
        "    def load_model(self, load_path):\n",
        "        \"\"\"Load model from checkpoint\"\"\"\n",
        "        checkpoint = torch.load(load_path, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        if 'training_history' in checkpoint:\n",
        "            history = checkpoint['training_history']\n",
        "            self.train_losses = history.get('train_losses', [])\n",
        "            self.val_losses = history.get('val_losses', [])\n",
        "            self.train_accuracies = history.get('train_accuracies', [])\n",
        "            self.val_accuracies = history.get('val_accuracies', [])\n",
        "            self.learning_rates = history.get('learning_rates', [])\n",
        "\n",
        "        print(f\"Model loaded from: {load_path}\")\n",
        "        return checkpoint.get('evaluation_results', None)"
      ],
      "metadata": {
        "id": "4CrhCk-1Gm3G"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}