{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May243.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "from transformers import BertTokenizer, BertModel, ViTModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "aWtS7H2jyk6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea294fc1-3eb9-42a5-9cb4-90ac9afe8060"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_synthetic_dataset(num_ad=87, num_cn=79):\n",
        "    \"\"\"Generate synthetic dataset mimicking ADReSS.\"\"\"\n",
        "    data = []\n",
        "\n",
        "    # AD-like transcripts: hesitant, repetitive, vague\n",
        "    ad_patterns = [\n",
        "        \"Um... I see a kitchen, and uh... someone is there... washing something, maybe dishes...\",\n",
        "        \"The boy is... uh... climbing to get... um... cookies or something... I think...\",\n",
        "        \"There's water... uh... spilling and... uh... people are doing things...\"\n",
        "    ]\n",
        "\n",
        "    # CN-like transcripts: coherent, detailed\n",
        "    cn_patterns = [\n",
        "        \"In the kitchen, a woman is washing dishes while a boy reaches for a cookie jar.\",\n",
        "        \"The scene shows a sink overflowing and a child on a stool grabbing cookies.\",\n",
        "        \"A mother is cleaning dishes, and two children are nearby, one reaching for snacks.\"\n",
        "    ]\n",
        "\n",
        "    # Add variability to avoid perfect separation\n",
        "    for i in range(num_ad):\n",
        "        transcript = np.random.choice(ad_patterns) + \" \" + np.random.choice(ad_patterns, size=1)[0][:20]\n",
        "        data.append({\n",
        "            'participant_id': f'AD_{i:03d}',\n",
        "            'audio_path': f'synthetic_audio_AD_{i:03d}.wav',\n",
        "            'transcript': transcript,\n",
        "            'label': 1,\n",
        "            'class_name': 'AD'\n",
        "        })\n",
        "\n",
        "    for i in range(num_cn):\n",
        "        transcript = np.random.choice(cn_patterns) + \" \" + np.random.choice(cn_patterns, size=1)[0][:20]\n",
        "        data.append({\n",
        "            'participant_id': f'CN_{i:03d}',\n",
        "            'audio_path': f'synthetic_audio_CN_{i:03d}.wav',\n",
        "            'transcript': transcript,\n",
        "            'label': 0,\n",
        "            'class_name': 'CN'\n",
        "        })\n",
        "\n",
        "    print(f\"Created synthetic dataset: {num_ad} AD, {num_cn} CN samples\")\n",
        "    return data\n",
        "\n",
        "# Generate dataset\n",
        "dataset = create_synthetic_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-OTRs6W4zub",
        "outputId": "d45d1023-e7e4-467b-f07b-9b2b279a141e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created synthetic dataset: 87 AD, 79 CN samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioProcessor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=224, win_length=2048, hop_length=1024):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "        self.win_length = win_length\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "    def load_audio(self, audio_path, max_length=16000*16):\n",
        "        \"\"\"Load audio or return synthetic signal if file missing.\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "        except:\n",
        "            # Simulate audio for synthetic dataset\n",
        "            audio = np.random.randn(max_length) * 0.01\n",
        "            sr = self.sample_rate\n",
        "\n",
        "        if len(audio) > max_length:\n",
        "            start = (len(audio) - max_length) // 2\n",
        "            audio = audio[start:start + max_length]\n",
        "        elif len(audio) < max_length:\n",
        "            audio = np.pad(audio, (0, max_length - len(audio)), mode='constant')\n",
        "        return audio\n",
        "\n",
        "    def extract_mel_spectrogram(self, audio):\n",
        "        \"\"\"Extract 3-channel Log-Mel spectrogram.\"\"\"\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=audio, sr=self.sample_rate, n_mels=self.n_mels,\n",
        "                n_fft=self.win_length, hop_length=self.hop_length\n",
        "            )\n",
        "            log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            delta = librosa.feature.delta(log_mel)\n",
        "            delta2 = librosa.feature.delta(log_mel, order=2)\n",
        "            return np.stack([log_mel, delta, delta2], axis=0)  # Shape: (3, n_mels, time)\n",
        "        except:\n",
        "            return np.random.randn(3, self.n_mels, 100)\n",
        "\n",
        "    def resize_spectrogram(self, spectrogram, target_size=(224, 224)):\n",
        "        \"\"\"Resize spectrogram to ViT input size.\"\"\"\n",
        "        from scipy.ndimage import zoom\n",
        "        try:\n",
        "            resized_channels = []\n",
        "            for channel in spectrogram:\n",
        "                zoom_factors = [target_size[i] / channel.shape[i] for i in range(2)]\n",
        "                resized = zoom(channel, zoom_factors, order=1)\n",
        "                resized_channels.append(resized)\n",
        "            resized = np.stack(resized_channels, axis=0)\n",
        "            resized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)\n",
        "            return resized\n",
        "        except:\n",
        "            return np.random.rand(3, target_size[0], target_size[1])"
      ],
      "metadata": {
        "id": "cIHgWQl3449E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, data, audio_processor, tokenizer, max_text_length=512, audio_max_length=16000*16):\n",
        "        self.data = data\n",
        "        self.audio_processor = audio_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.audio_max_length = audio_max_length\n",
        "        self._precompute_linguistic_features()\n",
        "\n",
        "    def _precompute_linguistic_features(self):\n",
        "        \"\"\"Compute linguistic features for each transcript.\"\"\"\n",
        "        for sample in tqdm(self.data, desc=\"Computing linguistic features\"):\n",
        "            transcript = sample['transcript']\n",
        "            words = transcript.split()\n",
        "            sentences = re.split(r'[.!?]+', transcript)\n",
        "            sample['linguistic_features'] = {\n",
        "                'word_count': len(words),\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'filler_words': sum(1 for w in words if w.lower() in ['um', 'uh', 'er'])\n",
        "            }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Process text\n",
        "        text = sample['transcript'] or \"No speech content detected\"\n",
        "        encoding = self.tokenizer(\n",
        "            text, truncation=True, padding='max_length', max_length=self.max_text_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Process audio\n",
        "        audio = self.audio_processor.load_audio(sample['audio_path'], self.audio_max_length)\n",
        "        spectrogram = self.audio_processor.extract_mel_spectrogram(audio)\n",
        "        audio_features = self.audio_processor.resize_spectrogram(spectrogram)\n",
        "\n",
        "        # Linguistic features\n",
        "        ling_features = sample['linguistic_features']\n",
        "        ling_vector = np.array([\n",
        "            ling_features['word_count'],\n",
        "            ling_features['unique_words'],\n",
        "            ling_features['lexical_diversity'],\n",
        "            ling_features['filler_words']\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'audio_features': torch.FloatTensor(audio_features),\n",
        "            'linguistic_features': torch.FloatTensor(ling_vector),\n",
        "            'label': torch.LongTensor([sample['label']]).squeeze(),\n",
        "            'participant_id': sample['participant_id'],\n",
        "            'class_name': sample['class_name']\n",
        "        }"
      ],
      "metadata": {
        "id": "7YGYfmLE4-O1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalADClassifier(nn.Module):\n",
        "    def __init__(self, text_hidden_size=768, audio_hidden_size=768, ling_hidden_size=4,\n",
        "                 fusion_hidden_size=512, num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.audio_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "        self.ling_processor = nn.Sequential(\n",
        "            nn.Linear(ling_hidden_size, 32), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(32, 32), nn.ReLU()\n",
        "        )\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=fusion_hidden_size, num_heads=8, dropout=dropout)\n",
        "        self.text_projection = nn.Linear(text_hidden_size, fusion_hidden_size)\n",
        "        self.audio_projection = nn.Linear(audio_hidden_size, fusion_hidden_size)\n",
        "        self.ling_projection = nn.Linear(32, fusion_hidden_size)\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size * 3, fusion_hidden_size), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_hidden_size, fusion_hidden_size // 2), nn.ReLU(), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.classifier = nn.Linear(fusion_hidden_size // 2, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        for module in [self.ling_processor, self.fusion, self.classifier]:\n",
        "            for layer in module:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features, linguistic_features):\n",
        "        # Encode text\n",
        "        text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_features = text_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Encode audio\n",
        "        audio_output = self.audio_encoder(pixel_values=audio_features)\n",
        "        audio_features = audio_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Process linguistic features\n",
        "        ling_features = self.ling_processor(linguistic_features)  # [batch_size, 32]\n",
        "\n",
        "        # Project to common dimension\n",
        "        text_proj = self.text_projection(text_features)  # [batch_size, 512]\n",
        "        audio_proj = self.audio_projection(audio_features)  # [batch_size, 512]\n",
        "        ling_proj = self.ling_projection(ling_features)  # [batch_size, 512]\n",
        "\n",
        "        # Crossmodal attention\n",
        "        modality_features = torch.stack([text_proj, audio_proj, ling_proj], dim=0)  # [3, batch_size, 512]\n",
        "        attended_features, attention_weights = self.attention(modality_features, modality_features, modality_features)\n",
        "        attended_features = attended_features.transpose(0, 1).reshape(-1, 3 * 512)  # [batch_size, 1536]\n",
        "\n",
        "        # Fusion and classification\n",
        "        fused = self.fusion(attended_features)\n",
        "        logits = self.classifier(fused)\n",
        "\n",
        "        return {'logits': logits, 'attention_weights': attention_weights}"
      ],
      "metadata": {
        "id": "ohJEuZJ55DbG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, model, device, learning_rate=2e-5):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3)\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            ling_features = batch['linguistic_features'].to(self.device)\n",
        "            labels = batch['label'].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(input_ids, attention_mask, audio_features, ling_features)\n",
        "            loss = self.criterion(outputs['logits'], labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs['logits'], dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        return total_loss / len(train_loader), correct / total\n",
        "\n",
        "    def validate_epoch(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        preds, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                ling_features = batch['linguistic_features'].to(self.device)\n",
        "                batch_labels = batch['label'].to(self.device)\n",
        "\n",
        "                outputs = self.model(input_ids, attention_mask, audio_features, ling_features)\n",
        "                loss = self.criterion(outputs['logits'], batch_labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                batch_preds = torch.argmax(outputs['logits'], dim=1)\n",
        "                correct += (batch_preds == batch_labels).sum().item()\n",
        "                total += batch_labels.size(0)\n",
        "                preds.extend(batch_preds.cpu().numpy())\n",
        "                labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        return total_loss / len(val_loader), correct / total, preds, labels\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=10, patience=6):\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            val_loss, val_acc, _, _ = self.validate_epoch(val_loader)\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_model_state = self.model.state_dict().copy()\n",
        "                print(\"✓ Saved best model\")\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        if self.best_model_state:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "            print(\"✓ Loaded best model\")\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        self.model.eval()\n",
        "        preds, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                ling_features = batch['linguistic_features'].to(self.device)\n",
        "                batch_labels = batch['label'].to(self.device)\n",
        "\n",
        "                outputs = self.model(input_ids, attention_mask, audio_features, ling_features)\n",
        "                batch_preds = torch.argmax(outputs['logits'], dim=1)\n",
        "                preds.extend(batch_preds.cpu().numpy())\n",
        "                labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        precision = precision_score(labels, preds, average='weighted', zero_division=0)\n",
        "        recall = recall_score(labels, preds, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(labels, preds, average='weighted', zero_division=0)\n",
        "        cm = confusion_matrix(labels, preds)\n",
        "\n",
        "        print(\"\\nEvaluation Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(f\"        CN    AD\")\n",
        "        print(f\"CN     {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "        print(f\"AD     {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
        "\n",
        "        return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'cm': cm}"
      ],
      "metadata": {
        "id": "hus8uket5IpG"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}