{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May243.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import librosa.display\n",
        "from transformers import (BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor,\n",
        "                         Wav2Vec2ForCTC, Wav2Vec2Processor, WhisperProcessor,\n",
        "                         WhisperForConditionalGeneration)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "from collections import defaultdict\n",
        "from scipy import ndimage\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SpeechTranscriber:\n",
        "    \"\"\"Automatic Speech Recognition for generating transcripts from audio\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"openai/whisper-base\", cache_dir=\"./asr_cache\"):\n",
        "        \"\"\"\n",
        "        Initialize ASR model\n",
        "        Options:\n",
        "        - openai/whisper-base: Good balance of speed/accuracy\n",
        "        - facebook/wav2vec2-base-960h: Faster but less accurate\n",
        "        - openai/whisper-small: More accurate but slower\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Loading ASR model: {model_name}\")\n",
        "\n",
        "        if \"whisper\" in model_name.lower():\n",
        "            self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "            self.asr_type = \"whisper\"\n",
        "        else:\n",
        "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            self.asr_type = \"wav2vec2\"\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"ASR model loaded on {self.device}\")\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path, use_cache=True):\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        # Check cache first\n",
        "        cache_file = self.cache_dir / f\"{audio_path.stem}_transcript.pkl\"\n",
        "        if use_cache and cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cached_result = pickle.load(f)\n",
        "                return cached_result['transcript']\n",
        "            except:\n",
        "                pass  # Cache corrupted, proceed with transcription\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(str(audio_path), sr=16000)  # Whisper expects 16kHz\n",
        "\n",
        "            # Handle empty or very short audio\n",
        "            if len(audio) < 1600:  # Less than 0.1 seconds\n",
        "                transcript = \"\"\n",
        "            else:\n",
        "                transcript = self._transcribe_audio_array(audio)\n",
        "\n",
        "            # Cache result\n",
        "            if use_cache:\n",
        "                try:\n",
        "                    with open(cache_file, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'audio_path': str(audio_path),\n",
        "                            'transcript': transcript,\n",
        "                            'model': self.model_name\n",
        "                        }, f)\n",
        "                except:\n",
        "                    pass  # Caching failed, but transcription succeeded\n",
        "\n",
        "            return transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {audio_path}: {e}\")\n",
        "            return f\"[Transcription failed for {audio_path.name}]\"\n",
        "\n",
        "    def _transcribe_audio_array(self, audio_array):\n",
        "        \"\"\"Transcribe audio array using the loaded model\"\"\"\n",
        "        try:\n",
        "            if self.asr_type == \"whisper\":\n",
        "                return self._whisper_transcribe(audio_array)\n",
        "            else:\n",
        "                return self._wav2vec2_transcribe(audio_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {e}\")\n",
        "            return \"[Transcription error]\"\n",
        "\n",
        "    def _whisper_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Whisper model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_features.to(self.device)\n",
        "\n",
        "        # Generate transcription\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(inputs, max_length=448)\n",
        "\n",
        "        # Decode\n",
        "        transcript = self.processor.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcript.strip()\n",
        "\n",
        "    def _wav2vec2_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Wav2Vec2 model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).input_values.to(self.device)\n",
        "\n",
        "        # Get logits\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs).logits\n",
        "\n",
        "        # Decode\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcript = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "        return transcript.strip().lower()\n",
        "\n",
        "    def batch_transcribe(self, audio_paths, batch_size=8):\n",
        "        \"\"\"Transcribe multiple audio files with progress bar\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(f\"Transcribing {len(audio_paths)} audio files...\")\n",
        "\n",
        "        for audio_path in tqdm(audio_paths, desc=\"Transcribing\"):\n",
        "            transcript = self.transcribe_audio_file(audio_path)\n",
        "            participant_id = self._extract_participant_id(Path(audio_path).name)\n",
        "            transcripts[participant_id] = transcript\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',         # adrs0123 or adrso123\n",
        "            r'adrsp?(\\d{3})',         # adrsp123\n",
        "            r'adrspt?(\\d{1,3})',      # adrspt1, adrspt12\n",
        "            r'(\\d{3})',               # 3-digit numbers\n",
        "            r'([A-Z]\\d{2,3})',        # Letter followed by 2-3 digits\n",
        "            r'(S\\d{3})',              # S followed by 3 digits\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem"
      ],
      "metadata": {
        "id": "PepfW9iw70QI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedADReSSDataProcessor:\n",
        "    \"\"\"Enhanced ADReSS data processor with automatic speech recognition\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./extracted_data', asr_model=\"openai/whisper-base\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize ASR\n",
        "        self.transcriber = SpeechTranscriber(model_name=asr_model)\n",
        "\n",
        "    def extract_adress_dataset(self, tar_path, dataset_name):\n",
        "        \"\"\"Extract ADReSS dataset and organize files properly\"\"\"\n",
        "        extract_path = self.output_dir / dataset_name\n",
        "        extract_path.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting {tar_path} to {extract_path}\")\n",
        "\n",
        "        try:\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "            print(f\"Successfully extracted {dataset_name}\")\n",
        "\n",
        "            # Find the actual dataset directory structure\n",
        "            self._explore_directory_structure(extract_path)\n",
        "            return extract_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {tar_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _explore_directory_structure(self, base_path):\n",
        "        \"\"\"Explore and print directory structure\"\"\"\n",
        "        print(f\"\\nDirectory structure for {base_path.name}:\")\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            level = root.replace(str(base_path), '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:5]:\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "    def process_adress_dataset_with_asr(self, extract_path):\n",
        "        \"\"\"Process ADReSS dataset with automatic speech recognition\"\"\"\n",
        "        dataset_info = {\n",
        "            'audio_files': [],\n",
        "            'transcript_files': [],\n",
        "            'metadata_files': [],\n",
        "            'labels': {},\n",
        "            'paired_data': [],\n",
        "            'generated_transcripts': {}\n",
        "        }\n",
        "\n",
        "        # Look for ADReSS structure\n",
        "        adress_dirs = list(extract_path.rglob(\"*ADReSS*\"))\n",
        "        if adress_dirs:\n",
        "            main_dir = adress_dirs[0]\n",
        "        else:\n",
        "            main_dir = extract_path\n",
        "\n",
        "        print(f\"Processing from directory: {main_dir}\")\n",
        "\n",
        "        # Find audio files\n",
        "        audio_patterns = ['**/*.wav', '**/*.mp3', '**/*.flac']\n",
        "        for pattern in audio_patterns:\n",
        "            dataset_info['audio_files'].extend(list(main_dir.glob(pattern)))\n",
        "\n",
        "        print(f\"Found {len(dataset_info['audio_files'])} audio files\")\n",
        "\n",
        "        # Generate transcripts using ASR\n",
        "        print(\"Generating transcripts using ASR...\")\n",
        "        audio_paths = [str(path) for path in dataset_info['audio_files']]\n",
        "        generated_transcripts = self.transcriber.batch_transcribe(audio_paths)\n",
        "        dataset_info['generated_transcripts'] = generated_transcripts\n",
        "\n",
        "        print(f\"Generated {len(generated_transcripts)} transcripts\")\n",
        "\n",
        "        # Process labels from directory structure\n",
        "        labels = self._extract_labels_from_structure(dataset_info['audio_files'])\n",
        "        dataset_info['labels'] = labels\n",
        "\n",
        "        # Create paired dataset with generated transcripts\n",
        "        paired_data = self._create_paired_data_with_asr(dataset_info)\n",
        "        dataset_info['paired_data'] = paired_data\n",
        "\n",
        "        return dataset_info\n",
        "\n",
        "    def _extract_labels_from_structure(self, audio_files):\n",
        "        \"\"\"Extract labels from file paths or directory structure\"\"\"\n",
        "        labels = {}\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            # Extract participant ID\n",
        "            participant_id = self._extract_participant_id(audio_file.name)\n",
        "\n",
        "            # Determine label from path\n",
        "            path_str = str(audio_file).lower()\n",
        "            if '/ad/' in path_str or 'dementia' in path_str or 'alzheimer' in path_str:\n",
        "                label = 1  # AD/Dementia\n",
        "                class_name = 'AD'\n",
        "            elif '/cn/' in path_str or 'control' in path_str or 'normal' in path_str:\n",
        "                label = 0  # Control/Normal\n",
        "                class_name = 'CN'\n",
        "            elif 'decline' in path_str:\n",
        "                label = 1  # Decline/progression\n",
        "                class_name = 'AD'\n",
        "            elif 'no_decline' in path_str or 'no-decline' in path_str:\n",
        "                label = 0  # No decline\n",
        "                class_name = 'CN'\n",
        "            else:\n",
        "                # Default classification based on filename patterns\n",
        "                if any(marker in audio_file.name.lower() for marker in ['ad', 'dem', 'alz']):\n",
        "                    label = 1\n",
        "                    class_name = 'AD'\n",
        "                else:\n",
        "                    label = 0  # Default to control\n",
        "                    class_name = 'CN'\n",
        "\n",
        "            labels[participant_id] = {\n",
        "                'label': label,\n",
        "                'class_name': class_name,\n",
        "                'audio_path': audio_file\n",
        "            }\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',\n",
        "            r'adrsp?(\\d{3})',\n",
        "            r'adrspt?(\\d{1,3})',\n",
        "            r'(\\d{3})',\n",
        "            r'([A-Z]\\d{2,3})',\n",
        "            r'(S\\d{3})',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "    def _create_paired_data_with_asr(self, dataset_info):\n",
        "        \"\"\"Create paired audio-transcript dataset using ASR-generated transcripts\"\"\"\n",
        "        paired_data = []\n",
        "\n",
        "        # Create paired dataset using generated transcripts\n",
        "        for participant_id, label_info in dataset_info['labels'].items():\n",
        "            # Get generated transcript\n",
        "            transcript = dataset_info['generated_transcripts'].get(participant_id, \"\")\n",
        "\n",
        "            # Clean and validate transcript\n",
        "            transcript = self._clean_and_validate_transcript(transcript)\n",
        "\n",
        "            # If transcript is still empty or invalid, create a meaningful placeholder\n",
        "            if not transcript or len(transcript.strip()) < 10:\n",
        "                transcript = f\"Audio sample from participant {participant_id}. Speech content unclear or silent.\"\n",
        "\n",
        "            paired_data.append({\n",
        "                'participant_id': participant_id,\n",
        "                'audio_path': str(label_info['audio_path']),\n",
        "                'transcript': transcript,\n",
        "                'label': label_info['label'],\n",
        "                'class_name': label_info['class_name'],\n",
        "                'transcript_source': 'ASR_generated'\n",
        "            })\n",
        "\n",
        "        print(f\"Created {len(paired_data)} paired samples with ASR transcripts\")\n",
        "\n",
        "        # Print class distribution\n",
        "        labels = [item['label'] for item in paired_data]\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        for cls, count in zip(unique, counts):\n",
        "            class_name = 'CN' if cls == 0 else 'AD'\n",
        "            print(f\"  {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "        # Print sample transcripts for verification\n",
        "        print(\"\\nSample generated transcripts:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, sample in enumerate(paired_data[:3]):\n",
        "            print(f\"Participant {sample['participant_id']} ({sample['class_name']}):\")\n",
        "            print(f\"Transcript: {sample['transcript'][:100]}...\")\n",
        "            print()\n",
        "\n",
        "        return paired_data\n",
        "\n",
        "    def _clean_and_validate_transcript(self, transcript):\n",
        "        \"\"\"Clean and validate ASR-generated transcript\"\"\"\n",
        "        if not transcript:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove common ASR artifacts\n",
        "        transcript = transcript.strip()\n",
        "        transcript = re.sub(r'\\[.*?\\]', '', transcript)  # Remove [NOISE], [MUSIC], etc.\n",
        "        transcript = re.sub(r'<.*?>', '', transcript)    # Remove <unk>, <pad>, etc.\n",
        "        transcript = re.sub(r'\\s+', ' ', transcript)     # Normalize whitespace\n",
        "\n",
        "        # Remove very short or repetitive transcripts\n",
        "        if len(transcript) < 5:\n",
        "            return \"\"\n",
        "\n",
        "        # Check for repetitive patterns (common ASR error)\n",
        "        words = transcript.split()\n",
        "        if len(words) > 1:\n",
        "            # If more than 70% of words are the same, likely an error\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) / len(words) < 0.3:\n",
        "                return \"\"\n",
        "\n",
        "        return transcript"
      ],
      "metadata": {
        "id": "FiZDbi9075S7"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}