{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/AD_Prediction/blob/main/Detecting_dementia_from_speech_and_transcripts_using_transformers_May243.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import librosa.display\n",
        "from transformers import (BertTokenizer, BertModel, ViTModel, ViTFeatureExtractor,\n",
        "                         Wav2Vec2ForCTC, Wav2Vec2Processor, WhisperProcessor,\n",
        "                         WhisperForConditionalGeneration)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "from collections import defaultdict\n",
        "from scipy import ndimage\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SpeechTranscriber:\n",
        "    \"\"\"Automatic Speech Recognition for generating transcripts from audio\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"openai/whisper-base\", cache_dir=\"./asr_cache\"):\n",
        "        \"\"\"\n",
        "        Initialize ASR model\n",
        "        Options:\n",
        "        - openai/whisper-base: Good balance of speed/accuracy\n",
        "        - facebook/wav2vec2-base-960h: Faster but less accurate\n",
        "        - openai/whisper-small: More accurate but slower\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Loading ASR model: {model_name}\")\n",
        "\n",
        "        if \"whisper\" in model_name.lower():\n",
        "            self.processor = WhisperProcessor.from_pretrained(model_name)\n",
        "            self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
        "            self.asr_type = \"whisper\"\n",
        "        else:\n",
        "            self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "            self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "            self.asr_type = \"wav2vec2\"\n",
        "\n",
        "        # Move to GPU if available\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"ASR model loaded on {self.device}\")\n",
        "\n",
        "    def transcribe_audio_file(self, audio_path, use_cache=True):\n",
        "        \"\"\"Transcribe a single audio file\"\"\"\n",
        "        audio_path = Path(audio_path)\n",
        "\n",
        "        # Check cache first\n",
        "        cache_file = self.cache_dir / f\"{audio_path.stem}_transcript.pkl\"\n",
        "        if use_cache and cache_file.exists():\n",
        "            try:\n",
        "                with open(cache_file, 'rb') as f:\n",
        "                    cached_result = pickle.load(f)\n",
        "                return cached_result['transcript']\n",
        "            except:\n",
        "                pass  # Cache corrupted, proceed with transcription\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(str(audio_path), sr=16000)  # Whisper expects 16kHz\n",
        "\n",
        "            # Handle empty or very short audio\n",
        "            if len(audio) < 1600:  # Less than 0.1 seconds\n",
        "                transcript = \"\"\n",
        "            else:\n",
        "                transcript = self._transcribe_audio_array(audio)\n",
        "\n",
        "            # Cache result\n",
        "            if use_cache:\n",
        "                try:\n",
        "                    with open(cache_file, 'wb') as f:\n",
        "                        pickle.dump({\n",
        "                            'audio_path': str(audio_path),\n",
        "                            'transcript': transcript,\n",
        "                            'model': self.model_name\n",
        "                        }, f)\n",
        "                except:\n",
        "                    pass  # Caching failed, but transcription succeeded\n",
        "\n",
        "            return transcript\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing {audio_path}: {e}\")\n",
        "            return f\"[Transcription failed for {audio_path.name}]\"\n",
        "\n",
        "    def _transcribe_audio_array(self, audio_array):\n",
        "        \"\"\"Transcribe audio array using the loaded model\"\"\"\n",
        "        try:\n",
        "            if self.asr_type == \"whisper\":\n",
        "                return self._whisper_transcribe(audio_array)\n",
        "            else:\n",
        "                return self._wav2vec2_transcribe(audio_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Transcription error: {e}\")\n",
        "            return \"[Transcription error]\"\n",
        "\n",
        "    def _whisper_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Whisper model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_features.to(self.device)\n",
        "\n",
        "        # Generate transcription\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(inputs, max_length=448)\n",
        "\n",
        "        # Decode\n",
        "        transcript = self.processor.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcript.strip()\n",
        "\n",
        "    def _wav2vec2_transcribe(self, audio_array):\n",
        "        \"\"\"Transcribe using Wav2Vec2 model\"\"\"\n",
        "        # Process audio\n",
        "        inputs = self.processor(\n",
        "            audio_array,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).input_values.to(self.device)\n",
        "\n",
        "        # Get logits\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(inputs).logits\n",
        "\n",
        "        # Decode\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcript = self.processor.batch_decode(predicted_ids)[0]\n",
        "\n",
        "        return transcript.strip().lower()\n",
        "\n",
        "    def batch_transcribe(self, audio_paths, batch_size=8):\n",
        "        \"\"\"Transcribe multiple audio files with progress bar\"\"\"\n",
        "        transcripts = {}\n",
        "\n",
        "        print(f\"Transcribing {len(audio_paths)} audio files...\")\n",
        "\n",
        "        for audio_path in tqdm(audio_paths, desc=\"Transcribing\"):\n",
        "            transcript = self.transcribe_audio_file(audio_path)\n",
        "            participant_id = self._extract_participant_id(Path(audio_path).name)\n",
        "            transcripts[participant_id] = transcript\n",
        "\n",
        "        return transcripts\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',         # adrs0123 or adrso123\n",
        "            r'adrsp?(\\d{3})',         # adrsp123\n",
        "            r'adrspt?(\\d{1,3})',      # adrspt1, adrspt12\n",
        "            r'(\\d{3})',               # 3-digit numbers\n",
        "            r'([A-Z]\\d{2,3})',        # Letter followed by 2-3 digits\n",
        "            r'(S\\d{3})',              # S followed by 3 digits\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem"
      ],
      "metadata": {
        "id": "PepfW9iw70QI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedADReSSDataProcessor:\n",
        "    \"\"\"Enhanced ADReSS data processor with automatic speech recognition\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir='./extracted_data', asr_model=\"openai/whisper-base\"):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize ASR\n",
        "        self.transcriber = SpeechTranscriber(model_name=asr_model)\n",
        "\n",
        "    def extract_adress_dataset(self, tar_path, dataset_name):\n",
        "        \"\"\"Extract ADReSS dataset and organize files properly\"\"\"\n",
        "        extract_path = self.output_dir / dataset_name\n",
        "        extract_path.mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"Extracting {tar_path} to {extract_path}\")\n",
        "\n",
        "        try:\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=extract_path)\n",
        "            print(f\"Successfully extracted {dataset_name}\")\n",
        "\n",
        "            # Find the actual dataset directory structure\n",
        "            self._explore_directory_structure(extract_path)\n",
        "            return extract_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {tar_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _explore_directory_structure(self, base_path):\n",
        "        \"\"\"Explore and print directory structure\"\"\"\n",
        "        print(f\"\\nDirectory structure for {base_path.name}:\")\n",
        "        for root, dirs, files in os.walk(base_path):\n",
        "            level = root.replace(str(base_path), '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:5]:\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 5:\n",
        "                print(f\"{subindent}... and {len(files) - 5} more files\")\n",
        "\n",
        "    def process_adress_dataset_with_asr(self, extract_path):\n",
        "        \"\"\"Process ADReSS dataset with automatic speech recognition\"\"\"\n",
        "        dataset_info = {\n",
        "            'audio_files': [],\n",
        "            'transcript_files': [],\n",
        "            'metadata_files': [],\n",
        "            'labels': {},\n",
        "            'paired_data': [],\n",
        "            'generated_transcripts': {}\n",
        "        }\n",
        "\n",
        "        # Look for ADReSS structure\n",
        "        adress_dirs = list(extract_path.rglob(\"*ADReSS*\"))\n",
        "        if adress_dirs:\n",
        "            main_dir = adress_dirs[0]\n",
        "        else:\n",
        "            main_dir = extract_path\n",
        "\n",
        "        print(f\"Processing from directory: {main_dir}\")\n",
        "\n",
        "        # Find audio files\n",
        "        audio_patterns = ['**/*.wav', '**/*.mp3', '**/*.flac']\n",
        "        for pattern in audio_patterns:\n",
        "            dataset_info['audio_files'].extend(list(main_dir.glob(pattern)))\n",
        "\n",
        "        print(f\"Found {len(dataset_info['audio_files'])} audio files\")\n",
        "\n",
        "        # Generate transcripts using ASR\n",
        "        print(\"Generating transcripts using ASR...\")\n",
        "        audio_paths = [str(path) for path in dataset_info['audio_files']]\n",
        "        generated_transcripts = self.transcriber.batch_transcribe(audio_paths)\n",
        "        dataset_info['generated_transcripts'] = generated_transcripts\n",
        "\n",
        "        print(f\"Generated {len(generated_transcripts)} transcripts\")\n",
        "\n",
        "        # Process labels from directory structure\n",
        "        labels = self._extract_labels_from_structure(dataset_info['audio_files'])\n",
        "        dataset_info['labels'] = labels\n",
        "\n",
        "        # Create paired dataset with generated transcripts\n",
        "        paired_data = self._create_paired_data_with_asr(dataset_info)\n",
        "        dataset_info['paired_data'] = paired_data\n",
        "\n",
        "        return dataset_info\n",
        "\n",
        "    def _extract_labels_from_structure(self, audio_files):\n",
        "        \"\"\"Extract labels from file paths or directory structure\"\"\"\n",
        "        labels = {}\n",
        "\n",
        "        for audio_file in audio_files:\n",
        "            # Extract participant ID\n",
        "            participant_id = self._extract_participant_id(audio_file.name)\n",
        "\n",
        "            # Determine label from path\n",
        "            path_str = str(audio_file).lower()\n",
        "            if '/ad/' in path_str or 'dementia' in path_str or 'alzheimer' in path_str:\n",
        "                label = 1  # AD/Dementia\n",
        "                class_name = 'AD'\n",
        "            elif '/cn/' in path_str or 'control' in path_str or 'normal' in path_str:\n",
        "                label = 0  # Control/Normal\n",
        "                class_name = 'CN'\n",
        "            elif 'decline' in path_str:\n",
        "                label = 1  # Decline/progression\n",
        "                class_name = 'AD'\n",
        "            elif 'no_decline' in path_str or 'no-decline' in path_str:\n",
        "                label = 0  # No decline\n",
        "                class_name = 'CN'\n",
        "            else:\n",
        "                # Default classification based on filename patterns\n",
        "                if any(marker in audio_file.name.lower() for marker in ['ad', 'dem', 'alz']):\n",
        "                    label = 1\n",
        "                    class_name = 'AD'\n",
        "                else:\n",
        "                    label = 0  # Default to control\n",
        "                    class_name = 'CN'\n",
        "\n",
        "            labels[participant_id] = {\n",
        "                'label': label,\n",
        "                'class_name': class_name,\n",
        "                'audio_path': audio_file\n",
        "            }\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _extract_participant_id(self, filename):\n",
        "        \"\"\"Extract participant ID from filename\"\"\"\n",
        "        patterns = [\n",
        "            r'adrso?(\\d{3})',\n",
        "            r'adrsp?(\\d{3})',\n",
        "            r'adrspt?(\\d{1,3})',\n",
        "            r'(\\d{3})',\n",
        "            r'([A-Z]\\d{2,3})',\n",
        "            r'(S\\d{3})',\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, filename)\n",
        "            if match:\n",
        "                return match.group(1) if pattern.startswith(r'(\\d') else match.group(0)\n",
        "\n",
        "        return Path(filename).stem\n",
        "\n",
        "    def _create_paired_data_with_asr(self, dataset_info):\n",
        "        \"\"\"Create paired audio-transcript dataset using ASR-generated transcripts\"\"\"\n",
        "        paired_data = []\n",
        "\n",
        "        # Create paired dataset using generated transcripts\n",
        "        for participant_id, label_info in dataset_info['labels'].items():\n",
        "            # Get generated transcript\n",
        "            transcript = dataset_info['generated_transcripts'].get(participant_id, \"\")\n",
        "\n",
        "            # Clean and validate transcript\n",
        "            transcript = self._clean_and_validate_transcript(transcript)\n",
        "\n",
        "            # If transcript is still empty or invalid, create a meaningful placeholder\n",
        "            if not transcript or len(transcript.strip()) < 10:\n",
        "                transcript = f\"Audio sample from participant {participant_id}. Speech content unclear or silent.\"\n",
        "\n",
        "            paired_data.append({\n",
        "                'participant_id': participant_id,\n",
        "                'audio_path': str(label_info['audio_path']),\n",
        "                'transcript': transcript,\n",
        "                'label': label_info['label'],\n",
        "                'class_name': label_info['class_name'],\n",
        "                'transcript_source': 'ASR_generated'\n",
        "            })\n",
        "\n",
        "        print(f\"Created {len(paired_data)} paired samples with ASR transcripts\")\n",
        "\n",
        "        # Print class distribution\n",
        "        labels = [item['label'] for item in paired_data]\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        for cls, count in zip(unique, counts):\n",
        "            class_name = 'CN' if cls == 0 else 'AD'\n",
        "            print(f\"  {class_name}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "        # Print sample transcripts for verification\n",
        "        print(\"\\nSample generated transcripts:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, sample in enumerate(paired_data[:3]):\n",
        "            print(f\"Participant {sample['participant_id']} ({sample['class_name']}):\")\n",
        "            print(f\"Transcript: {sample['transcript'][:100]}...\")\n",
        "            print()\n",
        "\n",
        "        return paired_data\n",
        "\n",
        "    def _clean_and_validate_transcript(self, transcript):\n",
        "        \"\"\"Clean and validate ASR-generated transcript\"\"\"\n",
        "        if not transcript:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove common ASR artifacts\n",
        "        transcript = transcript.strip()\n",
        "        transcript = re.sub(r'\\[.*?\\]', '', transcript)  # Remove [NOISE], [MUSIC], etc.\n",
        "        transcript = re.sub(r'<.*?>', '', transcript)    # Remove <unk>, <pad>, etc.\n",
        "        transcript = re.sub(r'\\s+', ' ', transcript)     # Normalize whitespace\n",
        "\n",
        "        # Remove very short or repetitive transcripts\n",
        "        if len(transcript) < 5:\n",
        "            return \"\"\n",
        "\n",
        "        # Check for repetitive patterns (common ASR error)\n",
        "        words = transcript.split()\n",
        "        if len(words) > 1:\n",
        "            # If more than 70% of words are the same, likely an error\n",
        "            unique_words = set(words)\n",
        "            if len(unique_words) / len(words) < 0.3:\n",
        "                return \"\"\n",
        "\n",
        "        return transcript"
      ],
      "metadata": {
        "id": "FiZDbi9075S7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedMultiModalDataset(Dataset):\n",
        "    \"\"\"Enhanced dataset with ASR-generated transcripts and linguistic features\"\"\"\n",
        "\n",
        "    def __init__(self, data_samples, audio_processor, tokenizer,\n",
        "                 max_text_length=512, audio_max_length=16*16000,\n",
        "                 image_size=(224, 224)):\n",
        "        self.data_samples = data_samples\n",
        "        self.audio_processor = audio_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_text_length = max_text_length\n",
        "        self.audio_max_length = audio_max_length\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Precompute linguistic features for efficiency\n",
        "        self._precompute_linguistic_features()\n",
        "\n",
        "    def _precompute_linguistic_features(self):\n",
        "        \"\"\"Precompute linguistic features that might be important for AD detection\"\"\"\n",
        "        print(\"Precomputing linguistic features...\")\n",
        "\n",
        "        for sample in tqdm(self.data_samples, desc=\"Computing linguistic features\"):\n",
        "            transcript = sample['transcript']\n",
        "\n",
        "            # Basic linguistic metrics\n",
        "            words = transcript.split()\n",
        "            sentences = re.split(r'[.!?]+', transcript)\n",
        "\n",
        "            linguistic_features = {\n",
        "                'word_count': len(words),\n",
        "                'sentence_count': len([s for s in sentences if s.strip()]),\n",
        "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
        "                'unique_words': len(set(words)),\n",
        "                'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
        "                'pause_markers': transcript.count('[pause]') + transcript.count('...'),\n",
        "                'filler_words': sum(1 for word in words if word.lower() in ['um', 'uh', 'er', 'ah']),\n",
        "                'transcript_length': len(transcript)\n",
        "            }\n",
        "\n",
        "            sample['linguistic_features'] = linguistic_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data_samples[idx]\n",
        "\n",
        "        # Process text\n",
        "        text = sample['transcript']\n",
        "        if not text or text.strip() == \"\":\n",
        "            text = \"No speech content detected in audio sample\"\n",
        "\n",
        "        try:\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_text_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error tokenizing text for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy encoding\n",
        "            encoding = {\n",
        "                'input_ids': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'attention_mask': torch.zeros(self.max_text_length, dtype=torch.long),\n",
        "                'token_type_ids': torch.zeros(self.max_text_length, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        # Process audio\n",
        "        try:\n",
        "            audio = self.audio_processor.load_audio(\n",
        "                sample['audio_path'],\n",
        "                max_length=self.audio_max_length\n",
        "            )\n",
        "\n",
        "            # Extract spectrogram features\n",
        "            spectrogram = self.audio_processor.extract_mel_spectrogram(audio)\n",
        "\n",
        "            # Resize for ViT input\n",
        "            audio_features = self.audio_processor.resize_spectrogram_to_image(\n",
        "                spectrogram, self.image_size\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing audio for {sample['participant_id']}: {e}\")\n",
        "            # Create dummy audio features\n",
        "            audio_features = np.random.rand(3, self.image_size[0], self.image_size[1])\n",
        "\n",
        "        # Linguistic features\n",
        "        ling_features = sample.get('linguistic_features', {})\n",
        "        linguistic_vector = np.array([\n",
        "            ling_features.get('word_count', 0),\n",
        "            ling_features.get('sentence_count', 0),\n",
        "            ling_features.get('avg_word_length', 0),\n",
        "            ling_features.get('unique_words', 0),\n",
        "            ling_features.get('lexical_diversity', 0),\n",
        "            ling_features.get('pause_markers', 0),\n",
        "            ling_features.get('filler_words', 0),\n",
        "            ling_features.get('transcript_length', 0)\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze() if hasattr(encoding['input_ids'], 'squeeze') else encoding['input_ids'],\n",
        "            'attention_mask': encoding['attention_mask'].squeeze() if hasattr(encoding['attention_mask'], 'squeeze') else encoding['attention_mask'],\n",
        "            'audio_features': torch.FloatTensor(audio_features),\n",
        "            'linguistic_features': torch.FloatTensor(linguistic_vector),\n",
        "            'label': torch.LongTensor([sample['label']]).squeeze(),\n",
        "            'participant_id': sample['participant_id'],\n",
        "            'class_name': sample['class_name'],\n",
        "            'transcript_preview': text[:100] + \"...\" if len(text) > 100 else text\n",
        "        }"
      ],
      "metadata": {
        "id": "wesce6R-8AYY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleAudioProcessor:\n",
        "    def __init__(self, sample_rate=16000, n_mels=128):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "    def load_audio(self, audio_path, max_length=None):\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            if audio.ndim > 1:\n",
        "                audio = np.mean(audio, axis=1)\n",
        "            audio, _ = librosa.effects.trim(audio, top_db=20)\n",
        "            if np.max(np.abs(audio)) > 0:\n",
        "                audio = librosa.util.normalize(audio)\n",
        "\n",
        "            if max_length is not None:\n",
        "                if len(audio) > max_length:\n",
        "                    start = (len(audio) - max_length) // 2\n",
        "                    audio = audio[start:start + max_length]\n",
        "                elif len(audio) < max_length:\n",
        "                    pad_length = max_length - len(audio)\n",
        "                    audio = np.pad(audio, (0, pad_length), mode='constant')\n",
        "\n",
        "            return audio\n",
        "        except:\n",
        "            length = max_length if max_length else self.sample_rate * 10\n",
        "            return np.random.randn(length) * 0.01\n",
        "\n",
        "    def extract_mel_spectrogram(self, audio):\n",
        "        try:\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=audio, sr=self.sample_rate, n_mels=self.n_mels\n",
        "            )\n",
        "            log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "            delta = librosa.feature.delta(log_mel_spec)\n",
        "            delta2 = librosa.feature.delta(log_mel_spec, order=2)\n",
        "            return np.stack([log_mel_spec, delta, delta2], axis=0)\n",
        "        except:\n",
        "            return np.random.randn(3, self.n_mels, 100)\n",
        "\n",
        "    def resize_spectrogram_to_image(self, spectrogram, target_size=(224, 224)):\n",
        "        try:\n",
        "            if spectrogram.ndim == 3:\n",
        "                resized_channels = []\n",
        "                for i in range(spectrogram.shape[0]):\n",
        "                    channel = spectrogram[i]\n",
        "                    zoom_factors = [target_size[j] / channel.shape[j] for j in range(2)]\n",
        "                    resized_channel = ndimage.zoom(channel, zoom_factors, order=1)\n",
        "                    resized_channels.append(resized_channel)\n",
        "                resized = np.stack(resized_channels, axis=0)\n",
        "            else:\n",
        "                zoom_factors = [target_size[i] / spectrogram.shape[i] for i in range(2)]\n",
        "                resized = ndimage.zoom(spectrogram, zoom_factors, order=1)\n",
        "                resized = np.stack([resized] * 3, axis=0)\n",
        "\n",
        "            resized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)\n",
        "            return resized\n",
        "        except:\n",
        "            return np.random.rand(3, target_size[0], target_size[1])"
      ],
      "metadata": {
        "id": "92K2o9OM8GP7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedMultiModalADClassifier(nn.Module):\n",
        "    \"\"\"Enhanced multimodal classifier for Alzheimer's detection\"\"\"\n",
        "\n",
        "    def __init__(self, text_hidden_size=768, audio_hidden_size=768,\n",
        "                 linguistic_feature_size=8, fusion_hidden_size=512,\n",
        "                 num_classes=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text encoder (BERT)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Audio encoder (ViT for spectrograms)\n",
        "        self.audio_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        # Linguistic features processor\n",
        "        self.linguistic_processor = nn.Sequential(\n",
        "            nn.Linear(linguistic_feature_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Attention mechanism for modality fusion\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=fusion_hidden_size,\n",
        "            num_heads=8,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Feature projections to common dimensionality\n",
        "        self.text_projection = nn.Linear(text_hidden_size, fusion_hidden_size)\n",
        "        self.audio_projection = nn.Linear(audio_hidden_size, fusion_hidden_size)\n",
        "        self.linguistic_projection = nn.Linear(32, fusion_hidden_size)\n",
        "\n",
        "        # Fusion layers\n",
        "        self.fusion_layers = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size * 3, fusion_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_hidden_size, fusion_hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_hidden_size // 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize model weights\"\"\"\n",
        "        for module in [self.linguistic_processor, self.fusion_layers, self.classifier]:\n",
        "            for layer in module:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, audio_features, linguistic_features):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Text encoding\n",
        "        text_output = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        text_features = text_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Audio encoding\n",
        "        audio_output = self.audio_encoder(pixel_values=audio_features)\n",
        "        audio_features = audio_output.pooler_output  # [batch_size, 768]\n",
        "\n",
        "        # Linguistic features processing\n",
        "        linguistic_processed = self.linguistic_processor(linguistic_features)  # [batch_size, 32]\n",
        "\n",
        "        # Project to common dimensionality\n",
        "        text_projected = self.text_projection(text_features)        # [batch_size, 512]\n",
        "        audio_projected = self.audio_projection(audio_features)     # [batch_size, 512]\n",
        "        linguistic_projected = self.linguistic_projection(linguistic_processed)  # [batch_size, 512]\n",
        "\n",
        "        # Prepare for attention mechanism\n",
        "        # Convert to [seq_len, batch_size, embed_dim] for attention\n",
        "        modality_features = torch.stack([\n",
        "            text_projected,\n",
        "            audio_projected,\n",
        "            linguistic_projected\n",
        "        ], dim=0)  # [3, batch_size, 512]\n",
        "\n",
        "        # Apply self-attention across modalities\n",
        "        attended_features, attention_weights = self.attention(\n",
        "            modality_features, modality_features, modality_features\n",
        "        )  # [3, batch_size, 512]\n",
        "\n",
        "        # Convert back to [batch_size, features]\n",
        "        attended_features = attended_features.transpose(0, 1)  # [batch_size, 3, 512]\n",
        "\n",
        "        # Flatten for fusion\n",
        "        fused_input = attended_features.reshape(batch_size, -1)  # [batch_size, 1536]\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = self.fusion_layers(fused_input)  # [batch_size, 256]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)  # [batch_size, 2]\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'text_features': text_features,\n",
        "            'audio_features': audio_features,\n",
        "            'linguistic_features': linguistic_processed,\n",
        "            'attention_weights': attention_weights,\n",
        "            'fused_features': fused_features\n",
        "        }"
      ],
      "metadata": {
        "id": "jg5KPCBm8Lh0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    \"\"\"Enhanced model trainer with comprehensive evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, learning_rate=2e-5, weight_decay=0.01):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='max', patience=3, factor=0.5, verbose=True\n",
        "        )\n",
        "\n",
        "        # Training history\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.best_val_accuracy = 0.0\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            audio_features = batch['audio_features'].to(self.device)\n",
        "            linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "            labels = batch['label'].to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    audio_features=audio_features,\n",
        "                    linguistic_features=linguistic_features\n",
        "                )\n",
        "\n",
        "                logits = outputs['logits']\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping to prevent exploding gradients\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                total_loss += loss.item()\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct_predictions += (predictions == labels).sum().item()\n",
        "                total_predictions += labels.size(0)\n",
        "\n",
        "                # Update progress bar\n",
        "                current_accuracy = correct_predictions / total_predictions\n",
        "                progress_bar.set_postfix({\n",
        "                    'Loss': f'{loss.item():.4f}',\n",
        "                    'Acc': f'{current_accuracy:.4f}'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in training batch: {e}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        return avg_loss, accuracy\n",
        "\n",
        "    def validate_epoch(self, val_loader):\n",
        "        \"\"\"Validate for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_participant_ids = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                try:\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        audio_features=audio_features,\n",
        "                        linguistic_features=linguistic_features\n",
        "                    )\n",
        "\n",
        "                    logits = outputs['logits']\n",
        "                    loss = self.criterion(logits, labels)\n",
        "\n",
        "                    # Statistics\n",
        "                    total_loss += loss.item()\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "                    correct_predictions += (predictions == labels).sum().item()\n",
        "                    total_predictions += labels.size(0)\n",
        "\n",
        "                    # Store for detailed analysis\n",
        "                    all_predictions.extend(predictions.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_participant_ids.extend(batch['participant_id'])\n",
        "\n",
        "                    # Update progress bar\n",
        "                    current_accuracy = correct_predictions / total_predictions\n",
        "                    progress_bar.set_postfix({\n",
        "                        'Loss': f'{loss.item():.4f}',\n",
        "                        'Acc': f'{current_accuracy:.4f}'\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in validation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "        return avg_loss, accuracy, all_predictions, all_labels, all_participant_ids\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=10, save_path='best_model.pt'):\n",
        "        \"\"\"Full training loop with validation\"\"\"\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "        print(f\"Training on {len(train_loader.dataset)} samples\")\n",
        "        print(f\"Validating on {len(val_loader.dataset)} samples\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Training\n",
        "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_accuracy)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_accuracy, val_predictions, val_labels, val_ids = self.validate_epoch(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_accuracy)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_accuracy)\n",
        "\n",
        "            # Save best model\n",
        "            if val_accuracy > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_model_state = self.model.state_dict().copy()\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.best_model_state,\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'best_val_accuracy': self.best_val_accuracy,\n",
        "                    'train_losses': self.train_losses,\n",
        "                    'val_losses': self.val_losses,\n",
        "                    'train_accuracies': self.train_accuracies,\n",
        "                    'val_accuracies': self.val_accuracies\n",
        "                }, save_path)\n",
        "                print(f\"✓ New best model saved with validation accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Detailed validation metrics for best epochs\n",
        "            if val_accuracy == self.best_val_accuracy:\n",
        "                self._print_detailed_metrics(val_labels, val_predictions)\n",
        "\n",
        "        print(f\"\\nTraining completed!\")\n",
        "        print(f\"Best validation accuracy: {self.best_val_accuracy:.4f}\")\n",
        "\n",
        "        # Load best model\n",
        "        if self.best_model_state is not None:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "            print(\"✓ Best model loaded for final evaluation\")\n",
        "\n",
        "    def _print_detailed_metrics(self, true_labels, predictions):\n",
        "        \"\"\"Print detailed classification metrics\"\"\"\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        print(f\"\\nDetailed Metrics:\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "    def evaluate_model(self, test_loader, class_names=['CN', 'AD']):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "        all_participant_ids = []\n",
        "        detailed_results = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                audio_features = batch['audio_features'].to(self.device)\n",
        "                linguistic_features = batch['linguistic_features'].to(self.device)\n",
        "                labels = batch['label'].to(self.device)\n",
        "\n",
        "                try:\n",
        "                    # Forward pass\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        audio_features=audio_features,\n",
        "                        linguistic_features=linguistic_features\n",
        "                    )\n",
        "\n",
        "                    logits = outputs['logits']\n",
        "                    probabilities = torch.softmax(logits, dim=1)\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "                    # Store results\n",
        "                    all_predictions.extend(predictions.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                    all_probabilities.extend(probabilities.cpu().numpy())\n",
        "                    all_participant_ids.extend(batch['participant_id'])\n",
        "\n",
        "                    # Store detailed results for analysis\n",
        "                    for i in range(len(batch['participant_id'])):\n",
        "                        detailed_results.append({\n",
        "                            'participant_id': batch['participant_id'][i],\n",
        "                            'true_label': labels[i].cpu().item(),\n",
        "                            'predicted_label': predictions[i].cpu().item(),\n",
        "                            'cn_probability': probabilities[i][0].cpu().item(),\n",
        "                            'ad_probability': probabilities[i][1].cpu().item(),\n",
        "                            'correct': labels[i].cpu().item() == predictions[i].cpu().item(),\n",
        "                            'class_name': batch['class_name'][i],\n",
        "                            'transcript_preview': batch['transcript_preview'][i]\n",
        "                        })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in evaluation batch: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        precision = precision_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "        recall = recall_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "        f1 = f1_score(all_labels, all_predictions, average=None, zero_division=0)\n",
        "\n",
        "        # Print comprehensive results\n",
        "        print(f\"\\nOVERALL PERFORMANCE:\")\n",
        "        print(f\"Total Samples: {len(all_labels)}\")\n",
        "        print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
        "        print(f\"Overall Precision: {np.mean(precision):.4f}\")\n",
        "        print(f\"Overall Recall: {np.mean(recall):.4f}\")\n",
        "        print(f\"Overall F1-Score: {np.mean(f1):.4f}\")\n",
        "\n",
        "        print(f\"\\nPER-CLASS PERFORMANCE:\")\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_count = sum(1 for label in all_labels if label == i)\n",
        "            print(f\"{class_name}:\")\n",
        "            print(f\"  Count: {class_count}\")\n",
        "            print(f\"  Precision: {precision[i]:.4f}\")\n",
        "            print(f\"  Recall: {recall[i]:.4f}\")\n",
        "            print(f\"  F1-Score: {f1[i]:.4f}\")\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(all_labels, all_predictions)\n",
        "        print(f\"\\nCONFUSION MATRIX:\")\n",
        "        print(f\"        Predicted\")\n",
        "        print(f\"        CN    AD\")\n",
        "        print(f\"Actual CN {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
        "        print(f\"       AD {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
        "\n",
        "        # Error Analysis\n",
        "        print(f\"\\nERROR ANALYSIS:\")\n",
        "        errors = [result for result in detailed_results if not result['correct']]\n",
        "        print(f\"Total Errors: {len(errors)}\")\n",
        "\n",
        "        if errors:\n",
        "            print(f\"\\nSample Errors:\")\n",
        "            for i, error in enumerate(errors[:5]):  # Show first 5 errors\n",
        "                true_class = class_names[error['true_label']]\n",
        "                pred_class = class_names[error['predicted_label']]\n",
        "                confidence = max(error['cn_probability'], error['ad_probability'])\n",
        "                print(f\"  {i+1}. Participant {error['participant_id']}: {true_class} → {pred_class} (conf: {confidence:.3f})\")\n",
        "                print(f\"     Transcript: {error['transcript_preview']}\")\n",
        "\n",
        "        # Save detailed results\n",
        "        results_df = pd.DataFrame(detailed_results)\n",
        "        results_df.to_csv('detailed_evaluation_results.csv', index=False)\n",
        "        print(f\"\\nDetailed results saved to 'detailed_evaluation_results.csv'\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'confusion_matrix': cm,\n",
        "            'detailed_results': detailed_results,\n",
        "            'predictions': all_predictions,\n",
        "            'labels': all_labels,\n",
        "            'probabilities': all_probabilities\n",
        "        }\n",
        "\n",
        "    def plot_training_history(self, save_path='training_history.png'):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        if not self.train_losses:\n",
        "            print(\"No training history to plot\")\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot losses\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "        ax1.plot(epochs, self.val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot accuracies\n",
        "        ax2.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
        "        ax2.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.set_ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"Training history plot saved to {save_path}\")"
      ],
      "metadata": {
        "id": "kcmGubOr8SuM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_model_features(model, test_loader, device, max_batch_size=8):\n",
        "    \"\"\"Analyze model features and attention patterns\"\"\"\n",
        "    print(\"Analyzing model features and attention patterns...\")\n",
        "\n",
        "    model.eval()\n",
        "    attention_weights_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            if i >= 10:  # Analyze first 10 batches\n",
        "                break\n",
        "\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            audio_features = batch['audio_features'].to(device)\n",
        "            linguistic_features = batch['linguistic_features'].to(device)\n",
        "\n",
        "            try:\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    audio_features=audio_features,\n",
        "                    linguistic_features=linguistic_features\n",
        "                )\n",
        "\n",
        "                # Get attention weights and pad if necessary\n",
        "                attention_weights = outputs['attention_weights'].cpu().numpy()  # Shape: [num_heads, seq_len, seq_len]\n",
        "                batch_size = input_ids.size(0)\n",
        "\n",
        "                # Pad attention weights to max_batch_size\n",
        "                if batch_size < max_batch_size:\n",
        "                    pad_shape = (max_batch_size - batch_size, attention_weights.shape[1], attention_weights.shape[2])\n",
        "                    padded_weights = np.zeros((max_batch_size, attention_weights.shape[1], attention_weights.shape[2]))\n",
        "                    padded_weights[:batch_size] = attention_weights\n",
        "                    attention_weights = padded_weights\n",
        "\n",
        "                attention_weights_list.append(attention_weights)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in feature analysis: {e}\")\n",
        "                continue\n",
        "\n",
        "    if attention_weights_list:\n",
        "        # Average attention weights across batches\n",
        "        # All arrays now have shape [max_batch_size, num_heads, seq_len, seq_len]\n",
        "        avg_attention = np.mean(attention_weights_list, axis=0)  # Shape: [max_batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "        # Average over batch dimension to get [num_heads, seq_len, seq_len]\n",
        "        avg_attention = np.mean(avg_attention, axis=0)  # Shape: [num_heads, seq_len, seq_len]\n",
        "\n",
        "        # Plot attention patterns\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Average attention weights for each modality\n",
        "        modalities = ['Text', 'Audio', 'Linguistic']\n",
        "        attention_by_modality = np.mean(avg_attention, axis=(0, 1))  # Average across heads and source tokens\n",
        "\n",
        "        plt.bar(modalities, attention_by_modality)\n",
        "        plt.title('Average Attention Weights by Modality')\n",
        "        plt.ylabel('Attention Weight')\n",
        "        plt.xlabel('Modality')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        for i, v in enumerate(attention_by_modality):\n",
        "            plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('modality_attention_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"✅ Feature analysis completed. Attention analysis plot saved.\")\n",
        "    else:\n",
        "        print(\"⚠️  No attention weights collected for analysis.\")"
      ],
      "metadata": {
        "id": "_doRyYgz8aqK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ17szzE813I",
        "outputId": "121bd6b3-0fc9-41c0-92da-1fdd821d82d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution pipeline for Enhanced Alzheimer's Detection\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"ENHANCED ALZHEIMER'S DETECTION WITH AUTOMATIC SPEECH RECOGNITION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'data_dir': './data',\n",
        "        'output_dir': './extracted_data',\n",
        "        'model_save_path': './best_ad_model.pt',\n",
        "        'batch_size': 8,\n",
        "        'num_epochs': 15,\n",
        "        'learning_rate': 2e-5,\n",
        "        'max_text_length': 512,\n",
        "        'audio_max_length': 16*16000,  # 16 seconds\n",
        "        'test_size': 0.2,\n",
        "        'val_size': 0.15,\n",
        "        'random_state': 42,\n",
        "        'asr_model': 'openai/whisper-base'  # Can change to 'facebook/wav2vec2-base-960h'\n",
        "    }\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Initialize data processor with ASR\n",
        "        print(\"\\n1. Initializing Enhanced Data Processor with ASR...\")\n",
        "        processor = EnhancedADReSSDataProcessor(\n",
        "            output_dir=config['output_dir'],\n",
        "            asr_model=config['asr_model']\n",
        "        )\n",
        "\n",
        "        # Step 2: Look for dataset files\n",
        "        print(\"\\n2. Looking for ADReSS dataset files...\")\n",
        "        data_dir = Path(config['data_dir'])\n",
        "\n",
        "        # Look for compressed dataset files\n",
        "        dataset_files = []\n",
        "        for pattern in ['*.tar.gz', '*.tgz', '*.zip']:\n",
        "            dataset_files.extend(list(data_dir.glob(pattern)))\n",
        "\n",
        "        if not dataset_files:\n",
        "            print(\"⚠️  No dataset files found. Creating synthetic data for demonstration...\")\n",
        "            # Create synthetic dataset for demonstration\n",
        "            dataset_info = create_synthetic_dataset_with_asr(processor)\n",
        "        else:\n",
        "            print(f\"Found {len(dataset_files)} dataset files\")\n",
        "\n",
        "            # Process first dataset file\n",
        "            dataset_file = dataset_files[0]\n",
        "            print(f\"Processing: {dataset_file}\")\n",
        "\n",
        "            # Extract dataset\n",
        "            extract_path = processor.extract_adress_dataset(\n",
        "                dataset_file,\n",
        "                f\"dataset_{dataset_file.stem}\"\n",
        "            )\n",
        "\n",
        "            if extract_path is None:\n",
        "                print(\"❌ Failed to extract dataset. Creating synthetic data...\")\n",
        "                dataset_info = create_synthetic_dataset_with_asr(processor)\n",
        "            else:\n",
        "                # Process with ASR\n",
        "                dataset_info = processor.process_adress_dataset_with_asr(extract_path)\n",
        "\n",
        "        if not dataset_info['paired_data']:\n",
        "            print(\"❌ No valid data found. Exiting...\")\n",
        "            return\n",
        "\n",
        "        print(f\"✅ Successfully processed {len(dataset_info['paired_data'])} samples\")\n",
        "\n",
        "        # Step 3: Prepare datasets\n",
        "        print(\"\\n3. Preparing datasets...\")\n",
        "\n",
        "        # Split data\n",
        "        train_data, temp_data = train_test_split(\n",
        "            dataset_info['paired_data'],\n",
        "            test_size=config['test_size'] + config['val_size'],\n",
        "            random_state=config['random_state'],\n",
        "            stratify=[item['label'] for item in dataset_info['paired_data']]\n",
        "        )\n",
        "\n",
        "        val_data, test_data = train_test_split(\n",
        "            temp_data,\n",
        "            test_size=config['test_size'] / (config['test_size'] + config['val_size']),\n",
        "            random_state=config['random_state'],\n",
        "            stratify=[item['label'] for item in temp_data]\n",
        "        )\n",
        "\n",
        "        print(f\"Train samples: {len(train_data)}\")\n",
        "        print(f\"Validation samples: {len(val_data)}\")\n",
        "        print(f\"Test samples: {len(test_data)}\")\n",
        "\n",
        "        # Initialize components\n",
        "        audio_processor = SimpleAudioProcessor()\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = EnhancedMultiModalDataset(\n",
        "            train_data, audio_processor, tokenizer,\n",
        "            max_text_length=config['max_text_length'],\n",
        "            audio_max_length=config['audio_max_length']\n",
        "        )\n",
        "\n",
        "        val_dataset = EnhancedMultiModalDataset(\n",
        "            val_data, audio_processor, tokenizer,\n",
        "            max_text_length=config['max_text_length'],\n",
        "            audio_max_length=config['audio_max_length']\n",
        "        )\n",
        "\n",
        "        test_dataset = EnhancedMultiModalDataset(\n",
        "            test_data, audio_processor, tokenizer,\n",
        "            max_text_length=config['max_text_length'],\n",
        "            audio_max_length=config['audio_max_length']\n",
        "        )\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=0  # Set to 0 to avoid multiprocessing issues\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=config['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        # Step 4: Initialize model\n",
        "        print(\"\\n4. Initializing Enhanced Multimodal Model...\")\n",
        "        model = EnhancedMultiModalADClassifier()\n",
        "\n",
        "        # Step 5: Initialize trainer\n",
        "        print(\"\\n5. Initializing Model Trainer...\")\n",
        "        trainer = ModelTrainer(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            learning_rate=config['learning_rate']\n",
        "        )\n",
        "\n",
        "        # Step 6: Train model\n",
        "        print(\"\\n6. Starting Model Training...\")\n",
        "        trainer.train(\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=config['num_epochs'],\n",
        "            save_path=config['model_save_path']\n",
        "        )\n",
        "\n",
        "        # Step 7: Evaluate model\n",
        "        print(\"\\n7. Evaluating Model...\")\n",
        "        evaluation_results = trainer.evaluate_model(test_loader)\n",
        "\n",
        "        # Step 8: Plot training history\n",
        "        print(\"\\n8. Generating Training History Plot...\")\n",
        "        trainer.plot_training_history()\n",
        "\n",
        "        # Step 9: Feature Analysis\n",
        "        print(\"\\n9. Performing Feature Analysis...\")\n",
        "        analyze_model_features(trainer.model, test_loader, device)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"✅ ENHANCED ALZHEIMER'S DETECTION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Final Test Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
        "        print(f\"Model saved to: {config['model_save_path']}\")\n",
        "        print(\"Check the generated plots and CSV files for detailed analysis.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in main pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def create_synthetic_dataset_with_asr(processor):\n",
        "    \"\"\"Create synthetic dataset for demonstration purposes\"\"\"\n",
        "    print(\"Creating synthetic dataset with ASR for demonstration...\")\n",
        "\n",
        "    # Create synthetic data structure\n",
        "    synthetic_data = []\n",
        "\n",
        "    # Generate synthetic samples\n",
        "    for i in range(100):  # 100 synthetic samples\n",
        "        participant_id = f\"SYNTH_{i:03d}\"\n",
        "\n",
        "        # Alternate between AD and CN\n",
        "        label = i % 2\n",
        "        class_name = 'AD' if label == 1 else 'CN'\n",
        "\n",
        "        # Create synthetic transcript based on class\n",
        "        if label == 1:  # AD\n",
        "            transcript = generate_ad_like_transcript()\n",
        "        else:  # CN\n",
        "            transcript = generate_cn_like_transcript()\n",
        "\n",
        "        synthetic_data.append({\n",
        "            'participant_id': participant_id,\n",
        "            'audio_path': f'synthetic_audio_{participant_id}.wav',\n",
        "            'transcript': transcript,\n",
        "            'label': label,\n",
        "            'class_name': class_name,\n",
        "            'transcript_source': 'synthetic'\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'audio_files': [],\n",
        "        'transcript_files': [],\n",
        "        'metadata_files': [],\n",
        "        'labels': {},\n",
        "        'paired_data': synthetic_data,\n",
        "        'generated_transcripts': {}\n",
        "    }\n",
        "\n",
        "def generate_ad_like_transcript():\n",
        "    \"\"\"Generate AD-like transcript with typical characteristics\"\"\"\n",
        "    ad_patterns = [\n",
        "        \"Um, let me see... the boy is... um... he's climbing on the... the thing there...\",\n",
        "        \"There's a woman in the kitchen and she's... what is she doing... oh yes, washing dishes I think...\",\n",
        "        \"The... the thing with water is overflowing and there's... there's problems happening...\",\n",
        "        \"I see children playing and... um... something about cookies or... or food...\",\n",
        "        \"The lady is trying to... to do something with the... with the sink and water is...\",\n",
        "        \"There are people in the picture and they're... um... doing things but I can't... I can't remember...\"\n",
        "    ]\n",
        "    return np.random.choice(ad_patterns)\n",
        "\n",
        "def generate_cn_like_transcript():\n",
        "    \"\"\"Generate Control-like transcript with typical characteristics\"\"\"\n",
        "    cn_patterns = [\n",
        "        \"In this picture, I can see a kitchen scene where a woman is washing dishes at the sink. The sink appears to be overflowing with water onto the floor.\",\n",
        "        \"There's a boy who has climbed up on a stool to reach the cookie jar on the counter. His sister is asking him to give her a cookie.\",\n",
        "        \"The scene shows a typical kitchen with a woman doing dishes while children are nearby. The boy is reaching for cookies while standing on a chair.\",\n",
        "        \"I can observe a domestic scene with a mother washing dishes. There are two children in the kitchen, and one of them is trying to get cookies from a jar.\",\n",
        "        \"The picture depicts a kitchen where a woman is at the sink with running water. There are children present, and one child is reaching up to get something from the counter.\",\n",
        "        \"This shows a busy kitchen scene with a woman washing dishes while water overflows. Meanwhile, children are nearby, with one trying to access a cookie jar.\"\n",
        "    ]\n",
        "    return np.random.choice(cn_patterns)"
      ],
      "metadata": {
        "id": "jtl3y6d_8hgO"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}